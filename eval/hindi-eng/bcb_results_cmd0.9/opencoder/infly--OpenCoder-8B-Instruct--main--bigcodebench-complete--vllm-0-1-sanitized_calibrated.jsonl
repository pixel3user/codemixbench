{"task_id": "BigCodeBench/0", "solution": "import itertools\nfrom random import shuffle\ndef task_func(numbers=list(range(1, 11))):\n    \"\"\"\n    Dee gaee list ke sabhee permutations ke lie, consecutive numbers ke pratyek pair ke beech ke absolute differences ke yog ka average nikala jaata hai. \n    Pratyek permutation, differences kee ganana karane se pahale, pherabadal kiya jaata hai. \n    Args:\n    - numbers (list): numbers kee ek list. Default 1 se 10 tak kee sankhyaen hain.\n\n    Returns:\n        float: list ke pratyek pherabadal permutation ke lie absolute differences ke yog ka average.\n\n    Requirements:\n    - itertools\n    - random.shuffle\n\n    Example:\n    >>> result = task_func([1, 2, 3])\n    >>> isinstance(result, float)\n    True\n\n    \"\"\"\n    # Generate all permutations of the list\n    perms = list(itertools.permutations(numbers))\n    \n    # Shuffle the permutations to ensure randomness\n    shuffle(perms)\n    \n    # Calculate the average of absolute differences for each permutation\n    averages = []\n    for perm in perms:\n        diffs = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n        avg_diff = sum(diffs) / len(diffs)\n        averages.append(avg_diff)\n    \n    # Return the average of all permutation averages\n    return sum(averages) / len(averages)", "raw_solution": "import itertools\nfrom random import shuffle\n\ndef task_func(numbers=list(range(1, 11))):\n    \"\"\"\n    Dee gaee list ke sabhee permutations ke lie, consecutive numbers ke pratyek pair ke beech ke absolute differences ke yog ka average nikala jaata hai. \n    Pratyek permutation, differences kee ganana karane se pahale, pherabadal kiya jaata hai. \n    Args:\n    - numbers (list): numbers kee ek list. Default 1 se 10 tak kee sankhyaen hain.\n\n    Returns:\n        float: list ke pratyek pherabadal permutation ke lie absolute differences ke yog ka average.\n\n    Requirements:\n    - itertools\n    - random.shuffle\n\n    Example:\n    >>> result = task_func([1, 2, 3])\n    >>> isinstance(result, float)\n    True\n\n    \"\"\"\n    # Generate all permutations of the list\n    perms = list(itertools.permutations(numbers))\n    \n    # Shuffle the permutations to ensure randomness\n    shuffle(perms)\n    \n    # Calculate the average of absolute differences for each permutation\n    averages = []\n    for perm in perms:\n        diffs = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n        avg_diff = sum(diffs) / len(diffs)\n        averages.append(avg_diff)\n    \n    # Return the average of all permutation averages\n    return sum(averages) / len(averages)\n```\n"}
{"task_id": "BigCodeBench/1", "solution": "import collections\nimport random\nimport string\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative number\")\n\n    # Generate a random string of the specified length\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n\n    # Count occurrences of each character in the string\n    char_count = collections.Counter(random_string)\n\n    return dict(char_count)", "raw_solution": "import collections\nimport random\nimport string\n\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative number\")\n\n    # Generate a random string of the specified length\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n\n    # Count occurrences of each character in the string\n    char_count = collections.Counter(random_string)\n\n    return dict(char_count)\n\n# Example usage:"}
{"task_id": "BigCodeBench/2", "solution": "import random\nimport statistics\ndef task_func(LETTERS):\n    random_dict = {letter: [random.randint(0, 100) for _ in range(5)] for letter in LETTERS}\n    mean_dict = {letter: statistics.mean(values) for letter, values in random_dict.items()}\n    sorted_dict = {k: v for k, v in sorted(mean_dict.items(), key=lambda item: item[1])}\n    \n    return sorted_dict", "raw_solution": "import random\nimport statistics\n\ndef task_func(LETTERS):\n    random_dict = {letter: [random.randint(0, 100) for _ in range(5)] for letter in LETTERS}\n    mean_dict = {letter: statistics.mean(values) for letter, values in random_dict.items()}\n    sorted_dict = {k: v for k, v in sorted(mean_dict.items(), key=lambda item: item[1])}\n    \n    return sorted_dict\n\n# Example usage:\nrandom.seed(42)\nsorted_dict = task_func(['a', 'b', 'c'])"}
{"task_id": "BigCodeBench/3", "solution": "import random\nimport numpy as np\ndef task_func(LETTERS):\n    mean_dict = {}\n    for letter in LETTERS:\n        random_integers = [random.randint(1, 100) for _ in range(10)]\n        mean_value = np.mean(random_integers)\n        mean_dict[letter] = mean_value\n    return mean_dict\nLETTERS = ['a', 'b', 'c']", "raw_solution": "import random\nimport numpy as np\n\ndef task_func(LETTERS):\n    mean_dict = {}\n    for letter in LETTERS:\n        random_integers = [random.randint(1, 100) for _ in range(10)]\n        mean_value = np.mean(random_integers)\n        mean_dict[letter] = mean_value\n    return mean_dict\n\n# Example usage:\nLETTERS = ['a', 'b', 'c']\nmean_dict = task_func(LETTERS)"}
{"task_id": "BigCodeBench/4", "solution": "from collections import Counter\nimport itertools\ndef task_func(d):\n    # Initialize a Counter object to count occurrences of each integer\n    counter = Counter()\n\n    # Iterate over each list in the dictionary\n    for values in d.values():\n        # Update the counter with the counts from the current list\n        counter.update(values)\n\n    # Convert the counter to a dictionary and return it\n    return dict(counter)\nd = {'a': [1, 2, 3, 1], 'b': [3, 4, 5], 'c': [1, 2]}", "raw_solution": "from collections import Counter\nimport itertools\n\ndef task_func(d):\n    # Initialize a Counter object to count occurrences of each integer\n    counter = Counter()\n\n    # Iterate over each list in the dictionary\n    for values in d.values():\n        # Update the counter with the counts from the current list\n        counter.update(values)\n\n    # Convert the counter to a dictionary and return it\n    return dict(counter)\n\n# Example usage:\nd = {'a': [1, 2, 3, 1], 'b': [3, 4, 5], 'c': [1, 2]}\ncount_dict = task_func(d)"}
{"task_id": "BigCodeBench/5", "solution": "import random\nimport math\ndef task_func(LETTERS=[chr(i) for i in range(97, 123)]):\n    result = {}\n    for letter in LETTERS:\n        random_integers = [random.randint(0, 100) for _ in range(1, 11)]\n        mean = sum(random_integers) / len(random_integers)\n        variance = sum((x - mean) ** 2 for x in random_integers) / len(random_integers)\n        std_dev = math.sqrt(variance)\n        result[letter] = std_dev\n\n    return result", "raw_solution": "import random\nimport math\n\ndef task_func(LETTERS=[chr(i) for i in range(97, 123)]):\n    result = {}\n    for letter in LETTERS:\n        random_integers = [random.randint(0, 100) for _ in range(1, 11)]\n        mean = sum(random_integers) / len(random_integers)\n        variance = sum((x - mean) ** 2 for x in random_integers) / len(random_integers)\n        std_dev = math.sqrt(variance)\n        result[letter] = std_dev\n\n    return result\n\n# Example usage:\nrandom.seed(42)\nsd_dict = task_func()"}
{"task_id": "BigCodeBench/6", "solution": "import os\nimport re\ndef task_func(pattern, log_dir='/var/log/'):\n    # Check if the directory exists\n    if not os.path.isdir(log_dir):\n        return None\n\n    # List all files in the directory\n    files = os.listdir(log_dir)\n    \n    # Filter files using the regex pattern\n    matching_files = [f for f in files if re.match(pattern, f)]\n    \n    # If no matching files found, return None\n    if not matching_files:\n        return None\n\n    # Sort files by modification time in descending order\n    # This will give us the most recent file\n    recent_file = max(matching_files, key=os.path.getmtime)\n    \n    # Return the path of the most recent file\n    return os.path.join(log_dir, recent_file)", "raw_solution": "import os\nimport re\n\ndef task_func(pattern, log_dir='/var/log/'):\n    # Check if the directory exists\n    if not os.path.isdir(log_dir):\n        return None\n\n    # List all files in the directory\n    files = os.listdir(log_dir)\n    \n    # Filter files using the regex pattern\n    matching_files = [f for f in files if re.match(pattern, f)]\n    \n    # If no matching files found, return None\n    if not matching_files:\n        return None\n\n    # Sort files by modification time in descending order\n    # This will give us the most recent file\n    recent_file = max(matching_files, key=os.path.getmtime)\n    \n    # Return the path of the most recent file\n    return os.path.join(log_dir, recent_file)\n```\n"}
{"task_id": "BigCodeBench/7", "solution": "import csv\nimport collections\nimport operator\ndef task_func(csv_file_path):\n    product_sales = collections.defaultdict(int)\n\n    with open(csv_file_path, mode='r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            product = row['product']\n            quantity = int(row['quantity'])\n            product_sales[product] += quantity\n\n    best_selling_product = max(product_sales.items(), key=operator.itemgetter(1))[0]\n\n    return best_selling_product", "raw_solution": "import csv\nimport collections\nimport operator\n\ndef task_func(csv_file_path):\n    product_sales = collections.defaultdict(int)\n\n    with open(csv_file_path, mode='r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            product = row['product']\n            quantity = int(row['quantity'])\n            product_sales[product] += quantity\n\n    best_selling_product = max(product_sales.items(), key=operator.itemgetter(1))[0]\n\n    return best_selling_product\n```\n"}
{"task_id": "BigCodeBench/8", "solution": "from collections import Counter\nimport itertools\nfrom random import randint\ndef task_func(T1, RANGE=100):\n    # Convert string representations of numbers to integers\n    numbers = [int(num) for sublist in T1 for num in sublist]\n\n    # Generate random numbers and add them to the list\n    random_numbers = [randint(0, RANGE) for _ in numbers]\n    numbers.extend(random_numbers)\n\n    # Count occurrences of each number in the list\n    counts = Counter(numbers)\n\n    return counts", "raw_solution": "from collections import Counter\nimport itertools\nfrom random import randint\n\ndef task_func(T1, RANGE=100):\n    # Convert string representations of numbers to integers\n    numbers = [int(num) for sublist in T1 for num in sublist]\n\n    # Generate random numbers and add them to the list\n    random_numbers = [randint(0, RANGE) for _ in numbers]\n    numbers.extend(random_numbers)\n\n    # Count occurrences of each number in the list\n    counts = Counter(numbers)\n\n    return counts\n\n# Example usage:"}
{"task_id": "BigCodeBench/9", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(list_of_pairs):\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['category', 'value'])\n    \n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='category', y='value', data=df)\n    ax.set_title('category vs value')\n    \n    return df, ax\nlist_of_pairs = [('fruits', 5), ('vegetables', 9)]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(list_of_pairs):\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['category', 'value'])\n    \n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='category', y='value', data=df)\n    ax.set_title('category vs value')\n    \n    return df, ax\n\n# Example usage:\nlist_of_pairs = [('fruits', 5), ('vegetables', 9)]\ndf, ax = task_func(list_of_pairs)"}
{"task_id": "BigCodeBench/10", "solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\ndef task_func(T1, RANGE=100):\n    # Convert strings in tuples to integers\n    T1 = tuple(tuple(map(int, t)) for t in T1)\n\n    # Flatten the tuple of tuples into a single list\n    flat_list = list(itertools.chain.from_iterable(T1))\n\n    # Generate a random list of integers with the same size as flat_list\n    random_list = [random.randint(0, RANGE) for _ in range(len(flat_list))]\n\n    # Calculate mean, median, and mode\n    try:\n        mean_value = statistics.mean(random_list)\n        median_value = statistics.median(random_list)\n        mode_value = statistics.mode(random_list)\n    except statistics.StatisticsError as e:\n        raise statistics.StatisticsError(\"T1 is empty\") from e\n\n    return mean_value, median_value, mode_value, random_list", "raw_solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\n\ndef task_func(T1, RANGE=100):\n    # Convert strings in tuples to integers\n    T1 = tuple(tuple(map(int, t)) for t in T1)\n\n    # Flatten the tuple of tuples into a single list\n    flat_list = list(itertools.chain.from_iterable(T1))\n\n    # Generate a random list of integers with the same size as flat_list\n    random_list = [random.randint(0, RANGE) for _ in range(len(flat_list))]\n\n    # Calculate mean, median, and mode\n    try:\n        mean_value = statistics.mean(random_list)\n        median_value = statistics.median(random_list)\n        mode_value = statistics.mode(random_list)\n    except statistics.StatisticsError as e:\n        raise statistics.StatisticsError(\"T1 is empty\") from e\n\n    return mean_value, median_value, mode_value, random_list\n```\n"}
{"task_id": "BigCodeBench/11", "solution": "import numpy as np\nimport itertools\nimport random\ndef task_func(T1, max_value=100):\n    # Convert string representations of integers to integers\n    T1 = tuple(tuple(map(int, sublist)) for sublist in T1)\n    \n    # Calculate the sum of all integers in T1\n    total_sum = sum(itertools.chain.from_iterable(T1))\n    \n    # Generate a list of random integers\n    random_list = [random.randint(0, max_value) for _ in range(total_sum)]\n    \n    # Calculate the 25th, 50th, and 75th percentiles\n    p25 = np.percentile(random_list, 25)\n    p50 = np.percentile(random_list, 50)\n    p75 = np.percentile(random_list, 75)\n    \n    return p25, p50, p75", "raw_solution": "import numpy as np\nimport itertools\nimport random\n\ndef task_func(T1, max_value=100):\n    # Convert string representations of integers to integers\n    T1 = tuple(tuple(map(int, sublist)) for sublist in T1)\n    \n    # Calculate the sum of all integers in T1\n    total_sum = sum(itertools.chain.from_iterable(T1))\n    \n    # Generate a list of random integers\n    random_list = [random.randint(0, max_value) for _ in range(total_sum)]\n    \n    # Calculate the 25th, 50th, and 75th percentiles\n    p25 = np.percentile(random_list, 25)\n    p50 = np.percentile(random_list, 50)\n    p75 = np.percentile(random_list, 75)\n    \n    return p25, p50, p75\n\n# Example usage"}
{"task_id": "BigCodeBench/12", "solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    if not os.path.isfile(script_name):\n        raise FileNotFoundError(f\"The script file {script_name} does not exist.\")\n\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    try:\n        result = subprocess.run(['bash', script_name], check=True)\n        exit_status = result.returncode\n    except subprocess.CalledProcessError as e:\n        exit_status = e.returncode\n        raise RuntimeError(f\"Script execution failed with error: {e}\")\n\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    log_data = {\n        'start_time': start_time,\n        'end_time': end_time,\n        'exit_status': exit_status\n    }\n\n    with open(log_file, 'w') as f:\n        json.dump(log_data, f, indent=4)\n\n    return log_data", "raw_solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\n\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    if not os.path.isfile(script_name):\n        raise FileNotFoundError(f\"The script file {script_name} does not exist.\")\n\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    try:\n        result = subprocess.run(['bash', script_name], check=True)\n        exit_status = result.returncode\n    except subprocess.CalledProcessError as e:\n        exit_status = e.returncode\n        raise RuntimeError(f\"Script execution failed with error: {e}\")\n\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    log_data = {\n        'start_time': start_time,\n        'end_time': end_time,\n        'exit_status': exit_status\n    }\n\n    with open(log_file, 'w') as f:\n        json.dump(log_data, f, indent=4)\n\n    return log_data\n```\n"}
{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n        ftp.cwd(ftp_dir)\n\n        # List all files in the directory\n        files = ftp.nlst()\n        for file in files:\n            # Download each file using subprocess\n            subprocess.run(['wget', f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{file}'], check=True)\n            filenames.append(file)\n\n    except ftplib.all_errors as e:\n        print(f\"ftp_server {ftp_server} se connect karne mein failure: {str(e)}\")\n    except Exception as e:\n        print(f\"user {ftp_user} ke saath ftp_server {ftp_server} mein log in karne mein failure: {str(e)}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"server {ftp_server} par directory {ftp_dir} mein badalne mein failure: {str(e)}\")\n\n    return filenames", "raw_solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n        ftp.cwd(ftp_dir)\n\n        # List all files in the directory\n        files = ftp.nlst()\n        for file in files:\n            # Download each file using subprocess\n            subprocess.run(['wget', f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{file}'], check=True)\n            filenames.append(file)\n\n    except ftplib.all_errors as e:\n        print(f\"ftp_server {ftp_server} se connect karne mein failure: {str(e)}\")\n    except Exception as e:\n        print(f\"user {ftp_user} ke saath ftp_server {ftp_server} mein log in karne mein failure: {str(e)}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"server {ftp_server} par directory {ftp_dir} mein badalne mein failure: {str(e)}\")\n\n    return filenames\n```\nThis script connects to an FTP server, changes the directory, and downloads all files using the `wget` command. It returns a list of filenames downloaded. The script handles exceptions for FTP connection, login, and subprocess errors.\n"}
{"task_id": "BigCodeBench/14", "solution": "import configparser\nimport os\nimport shutil\ndef task_func(config_file_path, archieve_dir ='/home/user/archive'):\n    # Check if the config file exists\n    if not os.path.isfile(config_file_path):\n        raise FileNotFoundError(f\"The config file {config_file_path} does not exist.\")\n    \n    # Read the config file\n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n    \n    # Check if the project section and directory key exists in the config file\n    if 'project' not in config or 'directory' not in config['project']:\n        raise ValueError(\"The config file must contain a 'project' section with a 'directory' key.\")\n    \n    # Get the project directory from the config file\n    project_dir = config['project']['directory']\n    \n    # Check if the project directory exists\n    if not os.path.isdir(project_dir):\n        raise FileNotFoundError(f\"The project directory {project_dir} does not exist.\")\n    \n    # Create the archive directory if it does not exist\n    if not os.path.isdir(archieve_dir):\n        os.makedirs(archieve_dir)\n    \n    # Create the archive file path\n    archive_file_path = os.path.join(archieve_dir, os.path.basename(project_dir) + '.zip')\n    \n    try:\n        # Create the zip file\n        shutil.make_archive(os.path.join(archieve_dir, os.path.basename(project_dir)), 'zip', project_dir)\n        return True\n    except Exception as e:\n        print(f\"An error occurred while creating the archive: {e}\")\n        return False", "raw_solution": "import configparser\nimport os\nimport shutil\n\ndef task_func(config_file_path, archieve_dir ='/home/user/archive'):\n    # Check if the config file exists\n    if not os.path.isfile(config_file_path):\n        raise FileNotFoundError(f\"The config file {config_file_path} does not exist.\")\n    \n    # Read the config file\n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n    \n    # Check if the project section and directory key exists in the config file\n    if 'project' not in config or 'directory' not in config['project']:\n        raise ValueError(\"The config file must contain a 'project' section with a 'directory' key.\")\n    \n    # Get the project directory from the config file\n    project_dir = config['project']['directory']\n    \n    # Check if the project directory exists\n    if not os.path.isdir(project_dir):\n        raise FileNotFoundError(f\"The project directory {project_dir} does not exist.\")\n    \n    # Create the archive directory if it does not exist\n    if not os.path.isdir(archieve_dir):\n        os.makedirs(archieve_dir)\n    \n    # Create the archive file path\n    archive_file_path = os.path.join(archieve_dir, os.path.basename(project_dir) + '.zip')\n    \n    try:\n        # Create the zip file\n        shutil.make_archive(os.path.join(archieve_dir, os.path.basename(project_dir)), 'zip', project_dir)\n        return True\n    except Exception as e:\n        print(f\"An error occurred while creating the archive: {e}\")\n        return False\n```\n"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    with open(commands_file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        next(reader)  # Skip header if any\n        output_files = []\n        for index, row in enumerate(reader):\n            command = row[0]\n            try:\n                result = subprocess.run(command, shell=True, capture_output=True, text=True)\n                output_file_path = os.path.join(output_dir_path, f\"command_{index+1}_output.txt\")\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n                output_files.append(output_file_path)\n            except subprocess.CalledProcessError as e:\n                error_file_path = os.path.join(output_dir_path, f\"command_{index+1}_error.txt\")\n                with open(error_file_path, 'w') as error_file:\n                    error_file.write(f\"Command failed with error: {e.stderr}\\nExit code: {e.returncode}\")\n                output_files.append(error_file_path)\n    \n    return output_files", "raw_solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    with open(commands_file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        next(reader)  # Skip header if any\n        output_files = []\n        for index, row in enumerate(reader):\n            command = row[0]\n            try:\n                result = subprocess.run(command, shell=True, capture_output=True, text=True)\n                output_file_path = os.path.join(output_dir_path, f\"command_{index+1}_output.txt\")\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n                output_files.append(output_file_path)\n            except subprocess.CalledProcessError as e:\n                error_file_path = os.path.join(output_dir_path, f\"command_{index+1}_error.txt\")\n                with open(error_file_path, 'w') as error_file:\n                    error_file.write(f\"Command failed with error: {e.stderr}\\nExit code: {e.returncode}\")\n                output_files.append(error_file_path)\n    \n    return output_files\n```\n"}
{"task_id": "BigCodeBench/16", "solution": "import os\nimport glob\nimport subprocess\ndef task_func(directory, backup_dir='/path/to/backup'):\n    if not os.path.exists(backup_dir):\n        raise FileNotFoundError(f\"The backup directory '{backup_dir}' does not exist.\")\n\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n    if not log_files:\n        return \"No log files found.\"\n\n    tar_command = ['tar', '-czf', os.path.join(backup_dir, 'logs_backup.tar.gz')] + log_files\n    subprocess.run(tar_command, check=True)\n\n    for log_file in log_files:\n        os.remove(log_file)\n\n    return os.path.join(backup_dir, 'logs_backup.tar.gz')", "raw_solution": "import os\nimport glob\nimport subprocess\n\ndef task_func(directory, backup_dir='/path/to/backup'):\n    if not os.path.exists(backup_dir):\n        raise FileNotFoundError(f\"The backup directory '{backup_dir}' does not exist.\")\n\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n    if not log_files:\n        return \"No log files found.\"\n\n    tar_command = ['tar', '-czf', os.path.join(backup_dir, 'logs_backup.tar.gz')] + log_files\n    subprocess.run(tar_command, check=True)\n\n    for log_file in log_files:\n        os.remove(log_file)\n\n    return os.path.join(backup_dir, 'logs_backup.tar.gz')\n```\n"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['name']):\n        if proc.info['name'] == process_name:\n            # If the process is running, terminate it\n            for p in psutil.process_iter(['pid', 'name']):\n                if p.info['name'] == process_name:\n                    p.terminate()\n                    time.sleep(1)  # Give some time for the process to terminate\n            return f\"Process found. Restarting {process_name}.\"\n\n    # If the process is not running, start it\n    try:\n        subprocess.Popen(process_name)\n        return f\"Process not found. Starting {process_name}.\"\n    except Exception as e:\n        return f\"Failed to start {process_name}: {e}\"", "raw_solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['name']):\n        if proc.info['name'] == process_name:\n            # If the process is running, terminate it\n            for p in psutil.process_iter(['pid', 'name']):\n                if p.info['name'] == process_name:\n                    p.terminate()\n                    time.sleep(1)  # Give some time for the process to terminate\n            return f\"Process found. Restarting {process_name}.\"\n\n    # If the process is not running, start it\n    try:\n        subprocess.Popen(process_name)\n        return f\"Process not found. Starting {process_name}.\"\n    except Exception as e:\n        return f\"Failed to start {process_name}: {e}\"\n```\n"}
{"task_id": "BigCodeBench/18", "solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\ndef task_func(file):\n    if not os.path.exists(file):\n        print(\"File does not exist.\")\n        return []\n\n    try:\n        # Split the file into smaller files\n        subprocess.run(['split', '-l', '1000', file, 'split_'], check=True)\n\n        # Get list of all split files\n        split_files = glob.glob('split_*')\n\n        # Shuffle rows in each split file\n        for split_file in split_files:\n            with open(split_file, 'r') as f:\n                reader = csv.reader(f)\n                rows = list(reader)\n\n            random.shuffle(rows)\n\n            with open(split_file, 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerows(rows)\n\n        return split_files\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []", "raw_solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\n\ndef task_func(file):\n    if not os.path.exists(file):\n        print(\"File does not exist.\")\n        return []\n\n    try:\n        # Split the file into smaller files\n        subprocess.run(['split', '-l', '1000', file, 'split_'], check=True)\n\n        # Get list of all split files\n        split_files = glob.glob('split_*')\n\n        # Shuffle rows in each split file\n        for split_file in split_files:\n            with open(split_file, 'r') as f:\n                reader = csv.reader(f)\n                rows = list(reader)\n\n            random.shuffle(rows)\n\n            with open(split_file, 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerows(rows)\n\n        return split_files\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []\n\n# Example usage:\n# print(task_func('/path/to/file.csv'))\n```\nThis script uses the subprocess module to run the shell command 'split' to divide the input CSV file into smaller files. It then reads each of these files, shuffles the rows, and writes them back out. The function returns a list of paths to the split files. If any error occurs during this process, the function returns an empty list.\n"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Find all files in the directory and its subdirectories\n    file_paths = glob.glob(os.path.join(directory, '**'), recursive=True)\n    file_paths = [file for file in file_paths if os.path.isfile(file)]\n\n    if not file_paths:\n        return None\n\n    # Create a zip file\n    zip_file_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        for file in file_paths:\n            zip_file.write(file, os.path.relpath(file, directory))\n\n    return zip_file_path", "raw_solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Find all files in the directory and its subdirectories\n    file_paths = glob.glob(os.path.join(directory, '**'), recursive=True)\n    file_paths = [file for file in file_paths if os.path.isfile(file)]\n\n    if not file_paths:\n        return None\n\n    # Create a zip file\n    zip_file_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        for file in file_paths:\n            zip_file.write(file, os.path.relpath(file, directory))\n\n    return zip_file_path\n```\n"}
{"task_id": "BigCodeBench/20", "solution": "import ast\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert string representations of dictionaries to actual dictionaries\n    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n\n    # Create a pairplot using seaborn\n    ax = sns.pairplot(df)\n\n    # Show the plot\n    plt.show()\n\n    return df, ax", "raw_solution": "import ast\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert string representations of dictionaries to actual dictionaries\n    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n\n    # Create a pairplot using seaborn\n    ax = sns.pairplot(df)\n\n    # Show the plot\n    plt.show()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/21", "solution": "import psutil\nimport platform\ndef task_func():\n    # Fetching system information\n    os_name = platform.system()\n    architecture = platform.architecture()[0]\n    memory_info = psutil.virtual_memory()\n\n    # Calculating memory usage percentage\n    memory_usage_percent = memory_info.percent\n\n    # Creating a dictionary with system information\n    system_info = {\n        'OS': os_name,\n        'Architecture': architecture,\n        'Memory Usage': f\"{memory_usage_percent}%\"\n    }\n\n    return system_info", "raw_solution": "import psutil\nimport platform\n\ndef task_func():\n    # Fetching system information\n    os_name = platform.system()\n    architecture = platform.architecture()[0]\n    memory_info = psutil.virtual_memory()\n\n    # Calculating memory usage percentage\n    memory_usage_percent = memory_info.percent\n\n    # Creating a dictionary with system information\n    system_info = {\n        'OS': os_name,\n        'Architecture': architecture,\n        'Memory Usage': f\"{memory_usage_percent}%\"\n    }\n\n    return system_info\n\n# Example usage"}
{"task_id": "BigCodeBench/22", "solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\ndef task_func(l1, l2, K=10):\n    # Zip the two lists together, filling in None for shorter lists\n    zipped_lists = zip_longest(l1, l2, fillvalue=None)\n    \n    # Filter out None values and create a list of pairs\n    pairs = [(x, y) for x, y in zipped_lists if x is not None or y is not None]\n    \n    # Select a random sample of size K from the pairs\n    sample = choices(pairs, k=K)\n    \n    # Count the frequency of each element in the sample\n    counter = collections.Counter(sample)\n    \n    return counter", "raw_solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\n\ndef task_func(l1, l2, K=10):\n    # Zip the two lists together, filling in None for shorter lists\n    zipped_lists = zip_longest(l1, l2, fillvalue=None)\n    \n    # Filter out None values and create a list of pairs\n    pairs = [(x, y) for x, y in zipped_lists if x is not None or y is not None]\n    \n    # Select a random sample of size K from the pairs\n    sample = choices(pairs, k=K)\n    \n    # Count the frequency of each element in the sample\n    counter = collections.Counter(sample)\n    \n    return counter\n\n# Example usage:"}
{"task_id": "BigCodeBench/23", "solution": "import numpy as np\nfrom itertools import zip_longest\ndef task_func(l1, l2, THRESHOLD=0.5):\n    \"\"\"\n    Do numeric lists (numeric lists) se elements (elements) ko baari-baari se leta hai, pratyek element ke poorvnirdharit seema (threshold) se absolute difference (absolute difference) ki ganana karta hai, aur is seema ke sabse closest wale element ko lautaata hai.\n\n    Notes:\n        Dhyaan den:\n        Yadi l1 aur l2 different lengths (lengths) ke hain, to shorter list mein bina kisi sangat pair (corresponding pair) wale longer list ke elements (elements) ko 'None' ke saath joda nahi jaega. Keval maujuda numeric elements (numeric elements) par vichaar kiya jaata hai.\n        Seema 0.5 par tay hai. Seema mein adjustments (adjustments) ke liye THRESHOLD constant (constant) mein changes (changes) ki aavashyakta hoti hai.\n\n    Parameters:\n    - l1 (list): first input (input) list (list) jismein numeric value (numeric values) hain.\n    - l2 (list): second input (input) list (list) jismein numeric value (numeric values) hain.\n    - THRESHOLD (float): Poorvnirdharit constant (constant) jo comparison (comparison) ke liye ek reference point (reference point) ke roop mein upayog kiye jaane wale numeric value (numeric value) ka pratinidhitv karta hai. default roop se 0.5.\n\n    Returns:\n        float (float): Sanyukt list (combined list) ka element (element) jo 0.5 ki seema (threshold) ke sabse closest hai.\n\n    Requirements:\n    - numpy\n    - itertools.zip_longest\n\n    Example:\n    >>> l1 = [0.3, 1, 2, 3]\n    >>> l2 = [0.7, 11, 12, 13]\n    >>> closest = task_func(l1, l2)\n    >>> print(closest)\n    0.7\n\n    \"\"\"\n    # Convert lists to numpy arrays\n    l1_array = np.array(l1)\n    l2_array = np.array(l2)\n\n    # Zip the arrays with padding using itertools.zip_longest\n    zipped = zip_longest(l1_array, l2_array, fillvalue=None)\n\n    # Calculate absolute differences and find the minimum difference\n    min_diff = float('inf')\n    closest_value = None\n    for l1_val, l2_val in zipped:\n        if l1_val is not None and l2_val is not None:\n            diff = abs(l1_val - l2_val)\n            if diff < min_diff and diff <= THRESHOLD:\n                min_diff = diff\n                closest_value = l2_val\n\n    return closest_value\nl1 = [0.3, 1, 2, 3]\nl2 = [0.7, 11, 12, 13]", "raw_solution": "import numpy as np\nfrom itertools import zip_longest\n\ndef task_func(l1, l2, THRESHOLD=0.5):\n    \"\"\"\n    Do numeric lists (numeric lists) se elements (elements) ko baari-baari se leta hai, pratyek element ke poorvnirdharit seema (threshold) se absolute difference (absolute difference) ki ganana karta hai, aur is seema ke sabse closest wale element ko lautaata hai.\n\n    Notes:\n        Dhyaan den:\n        Yadi l1 aur l2 different lengths (lengths) ke hain, to shorter list mein bina kisi sangat pair (corresponding pair) wale longer list ke elements (elements) ko 'None' ke saath joda nahi jaega. Keval maujuda numeric elements (numeric elements) par vichaar kiya jaata hai.\n        Seema 0.5 par tay hai. Seema mein adjustments (adjustments) ke liye THRESHOLD constant (constant) mein changes (changes) ki aavashyakta hoti hai.\n\n    Parameters:\n    - l1 (list): first input (input) list (list) jismein numeric value (numeric values) hain.\n    - l2 (list): second input (input) list (list) jismein numeric value (numeric values) hain.\n    - THRESHOLD (float): Poorvnirdharit constant (constant) jo comparison (comparison) ke liye ek reference point (reference point) ke roop mein upayog kiye jaane wale numeric value (numeric value) ka pratinidhitv karta hai. default roop se 0.5.\n\n    Returns:\n        float (float): Sanyukt list (combined list) ka element (element) jo 0.5 ki seema (threshold) ke sabse closest hai.\n\n    Requirements:\n    - numpy\n    - itertools.zip_longest\n\n    Example:\n    >>> l1 = [0.3, 1, 2, 3]\n    >>> l2 = [0.7, 11, 12, 13]\n    >>> closest = task_func(l1, l2)\n    >>> print(closest)\n    0.7\n\n    \"\"\"\n    # Convert lists to numpy arrays\n    l1_array = np.array(l1)\n    l2_array = np.array(l2)\n\n    # Zip the arrays with padding using itertools.zip_longest\n    zipped = zip_longest(l1_array, l2_array, fillvalue=None)\n\n    # Calculate absolute differences and find the minimum difference\n    min_diff = float('inf')\n    closest_value = None\n    for l1_val, l2_val in zipped:\n        if l1_val is not None and l2_val is not None:\n            diff = abs(l1_val - l2_val)\n            if diff < min_diff and diff <= THRESHOLD:\n                min_diff = diff\n                closest_value = l2_val\n\n    return closest_value\n\n# Test cases\nl1 = [0.3, 1, 2, 3]\nl2 = [0.7, 11, 12, 13]"}
{"task_id": "BigCodeBench/24", "solution": "import base64\nimport hashlib\nimport os\ndef task_func(password, SALT_LENGTH = 32):\n    if not password:\n        raise ValueError(\"Password cannot be None or empty.\")\n\n    # Generate a random salt\n    salt = os.urandom(SALT_LENGTH)\n    salt_b64 = base64.b64encode(salt)\n\n    # Hash the password using PBKDF2 with HMAC-SHA256\n    dk = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    hashed_password_b64 = base64.b64encode(dk)\n\n    return salt_b64, hashed_password_b64", "raw_solution": "import base64\nimport hashlib\nimport os\n\ndef task_func(password, SALT_LENGTH = 32):\n    if not password:\n        raise ValueError(\"Password cannot be None or empty.\")\n\n    # Generate a random salt\n    salt = os.urandom(SALT_LENGTH)\n    salt_b64 = base64.b64encode(salt)\n\n    # Hash the password using PBKDF2 with HMAC-SHA256\n    dk = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    hashed_password_b64 = base64.b64encode(dk)\n\n    return salt_b64, hashed_password_b64\n\n# Example usage:\n# salt, hashed_password = task_func('my_password')\n# print(isinstance(salt, bytes))  # Output: True\n# print(isinstance(hashed_password, bytes))  # Output: True\n```\n"}
{"task_id": "BigCodeBench/25", "solution": "import base64\nimport json\nimport zlib\ndef task_func(data_dict):\n    # Serialize the dictionary to a JSON string\n    json_str = json.dumps(data_dict)\n    \n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode('utf-8'))\n    \n    # Encode the compressed data using base64\n    encoded_data = base64.b64encode(compressed_data).decode('utf-8')\n    \n    return encoded_data\ndata = {'key1': 'value1', 'key2': 'value2'}", "raw_solution": "import base64\nimport json\nimport zlib\n\ndef task_func(data_dict):\n    # Serialize the dictionary to a JSON string\n    json_str = json.dumps(data_dict)\n    \n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode('utf-8'))\n    \n    # Encode the compressed data using base64\n    encoded_data = base64.b64encode(compressed_data).decode('utf-8')\n    \n    return encoded_data\n\n# Example usage:\ndata = {'key1': 'value1', 'key2': 'value2'}\nencoded_data = task_func(data)"}
{"task_id": "BigCodeBench/26", "solution": "import base64\nfrom cryptography.fernet import Fernet\ndef task_func(message, encryption_key):\n    # Convert the encryption key to bytes and encode it in base64\n    key_bytes = encryption_key.encode()\n    encoded_key = base64.urlsafe_b64encode(key_bytes)\n\n    # Create a Fernet object with the encoded key\n    cipher = Fernet(encoded_key)\n\n    # Encrypt the message\n    encrypted_message = cipher.encrypt(message.encode())\n\n    # Encode the encrypted message in base64\n    encoded_encrypted_message = base64.urlsafe_b64encode(encrypted_message)\n\n    # Convert the encoded encrypted message to a string\n    encoded_encrypted_message_str = encoded_encrypted_message.decode()\n\n    return encoded_encrypted_message_str", "raw_solution": "import base64\nfrom cryptography.fernet import Fernet\n\ndef task_func(message, encryption_key):\n    # Convert the encryption key to bytes and encode it in base64\n    key_bytes = encryption_key.encode()\n    encoded_key = base64.urlsafe_b64encode(key_bytes)\n\n    # Create a Fernet object with the encoded key\n    cipher = Fernet(encoded_key)\n\n    # Encrypt the message\n    encrypted_message = cipher.encrypt(message.encode())\n\n    # Encode the encrypted message in base64\n    encoded_encrypted_message = base64.urlsafe_b64encode(encrypted_message)\n\n    # Convert the encoded encrypted message to a string\n    encoded_encrypted_message_str = encoded_encrypted_message.decode()\n\n    return encoded_encrypted_message_str\n\n# Example usage:\n# encrypted_message = task_func('Hello, World!', '01234567890123456789012345678901')\n# print(encrypted_message)\n```\n"}
{"task_id": "BigCodeBench/27", "solution": "import json\nimport base64\nfrom datetime import datetime\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    # Add current timestamp to the data dictionary\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n\n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(data)\n\n    # Encode the JSON string using base64\n    base64_encoded = base64.b64encode(json_str.encode('ascii')).decode('ascii')\n\n    return base64_encoded\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}", "raw_solution": "import json\nimport base64\nfrom datetime import datetime\n\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    # Add current timestamp to the data dictionary\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n\n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(data)\n\n    # Encode the JSON string using base64\n    base64_encoded = base64.b64encode(json_str.encode('ascii')).decode('ascii')\n\n    return base64_encoded\n\n# Example usage:\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}\nencoded_data = task_func(data)"}
{"task_id": "BigCodeBench/28", "solution": "import requests\nimport json\nimport base64\ndef task_func(data, url=\"http://your-api-url.com\"):\n    # Convert dictionary to JSON string\n    json_data = json.dumps(data)\n\n    # Encode JSON string to base64\n    base64_data = base64.b64encode(json_data.encode('utf-8')).decode('utf-8')\n\n    # Prepare headers\n    headers = {'Content-Type': 'application/json'}\n\n    # Send POST request\n    response = requests.post(url, headers=headers, data=json.dumps({'payload': base64_data}))\n\n    return response\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}", "raw_solution": "import requests\nimport json\nimport base64\n\ndef task_func(data, url=\"http://your-api-url.com\"):\n    # Convert dictionary to JSON string\n    json_data = json.dumps(data)\n\n    # Encode JSON string to base64\n    base64_data = base64.b64encode(json_data.encode('utf-8')).decode('utf-8')\n\n    # Prepare headers\n    headers = {'Content-Type': 'application/json'}\n\n    # Send POST request\n    response = requests.post(url, headers=headers, data=json.dumps({'payload': base64_data}))\n\n    return response\n\n# Example usage:\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}\nresponse = task_func(data, url=\"http://example-api-url.com\")"}
{"task_id": "BigCodeBench/29", "solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\ndef task_func(data):\n    \"\"\"\n    sklearn ke StandardScaler ka upayog kar ke ek numeric array ko standardized karen and standardized data ko base64 format mein ek ascii string ke roop mein encode karen.\n\n    Parameters:\n    - data (numpy.ndarray): standardized and encode karne ke liye numpy array.\n\n    Returns:\n        str: standardized data ka base64-encoded ascii string representation.\n\n    Requirements:\n    - sklearn.preprocessing.StandardScaler\n    - numpy\n    - base64\n\n    Example:\n    >>> data = np.array([[0, 0], [0, 0], [1, 1], [1, 1]])\n    >>> encoded_data = task_func(data)\n    >>> print(encoded_data)\n    W1stMS4gLTEuXQogWy0xLiAtMS5dCiBbIDEuICAxLl0KIFsgMS4gIDEuXV0=\n\n    \"\"\"\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Encoding the standardized data to base64\n    encoded_data = base64.b64encode(standardized_data.tobytes()).decode('ascii')\n\n    return encoded_data", "raw_solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\n\ndef task_func(data):\n    \"\"\"\n    sklearn ke StandardScaler ka upayog kar ke ek numeric array ko standardized karen and standardized data ko base64 format mein ek ascii string ke roop mein encode karen.\n\n    Parameters:\n    - data (numpy.ndarray): standardized and encode karne ke liye numpy array.\n\n    Returns:\n        str: standardized data ka base64-encoded ascii string representation.\n\n    Requirements:\n    - sklearn.preprocessing.StandardScaler\n    - numpy\n    - base64\n\n    Example:\n    >>> data = np.array([[0, 0], [0, 0], [1, 1], [1, 1]])\n    >>> encoded_data = task_func(data)\n    >>> print(encoded_data)\n    W1stMS4gLTEuXQogWy0xLiAtMS5dCiBbIDEuICAxLl0KIFsgMS4gIDEuXV0=\n\n    \"\"\"\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Encoding the standardized data to base64\n    encoded_data = base64.b64encode(standardized_data.tobytes()).decode('ascii')\n\n    return encoded_data\n```\n"}
{"task_id": "BigCodeBench/30", "solution": "import json\nimport os\nimport re\ndef task_func(\n    file_path,\n    attribute,\n    INPUT_JSON={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": str},  \n            \"age\": {\"type\": int},   \n            \"email\": {\"type\": str}  \n        },\n        \"required\": [\"name\", \"age\", \"email\"]\n    },\n    EMAIL_REGEX=r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"):\n    \"\"\"\n    json file ki sanrachna and contents ko purvanirdharit schema rules ke viruddh satyapit karein and json object se ek nirdisht attribute prapt karein. Sunischit karta hai ki sabhi avashyak field maujood hain, unke paribhashit types se mel khaate hain, and ek regular expression ka upyog karke email format ki validity ki jaanch karta hai.\n    errors:\n    - Yadi file maujood nahi hai, avashyak attributes gayab hain, type mel nahi khaate hain, or email format invalid hai, to ValueError badhata hai.\n\n    Parameters:\n    - file_path (str): json file ka path.\n    - attribute (str): json object se prapt ki jaane wali attribute.\n    - INPUT_json (dict): manye karne ke liye input json. Default maan hai:\n    - '{'\n    - \"type\": \"object\",\n    - \"properties\": {\n    - \"name\": {\"type\": str},\n    - \"age\": {\"type\": int},\n    - \"email\": {\"type\": str}\n    - },\n    - \"required\": [\"name\", \"age\", \"email\"]\n    - }'\n    - EMAIL_REGEX (str): email validity ki jaanch ke liye upyog kiye jaane wala regex. Default 'r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"' hai.\n\n    Returns:\n        Any: json schema mein paribhashit type ke anuroop, nirdisht attribute ka maan.\n\n    Requirements:\n    - json\n    - os\n    - re\n\n    Example:\n    >>> task_func('/path/to/file.json', 'email')\n    'john.doe@example.com'\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise ValueError(\"File does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    if not all(key in data for key in INPUT_JSON['required']):\n        raise ValueError(\"Missing required fields.\")\n    \n    for key, value in INPUT_JSON['properties'].items():\n        if not isinstance(data[key], value['type']):\n            raise ValueError(f\"Invalid type for {key}. Expected {value['type']}.\")\n    \n    if not re.match(EMAIL_REGEX, data[attribute]):\n        raise ValueError(f\"Invalid email format for {attribute}.\")\n    \n    return data[attribute]", "raw_solution": "import json\nimport os\nimport re\n\ndef task_func(\n    file_path,\n    attribute,\n    INPUT_JSON={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": str},  \n            \"age\": {\"type\": int},   \n            \"email\": {\"type\": str}  \n        },\n        \"required\": [\"name\", \"age\", \"email\"]\n    },\n    EMAIL_REGEX=r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"):\n    \"\"\"\n    json file ki sanrachna and contents ko purvanirdharit schema rules ke viruddh satyapit karein and json object se ek nirdisht attribute prapt karein. Sunischit karta hai ki sabhi avashyak field maujood hain, unke paribhashit types se mel khaate hain, and ek regular expression ka upyog karke email format ki validity ki jaanch karta hai.\n    errors:\n    - Yadi file maujood nahi hai, avashyak attributes gayab hain, type mel nahi khaate hain, or email format invalid hai, to ValueError badhata hai.\n\n    Parameters:\n    - file_path (str): json file ka path.\n    - attribute (str): json object se prapt ki jaane wali attribute.\n    - INPUT_json (dict): manye karne ke liye input json. Default maan hai:\n    - '{'\n    - \"type\": \"object\",\n    - \"properties\": {\n    - \"name\": {\"type\": str},\n    - \"age\": {\"type\": int},\n    - \"email\": {\"type\": str}\n    - },\n    - \"required\": [\"name\", \"age\", \"email\"]\n    - }'\n    - EMAIL_REGEX (str): email validity ki jaanch ke liye upyog kiye jaane wala regex. Default 'r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"' hai.\n\n    Returns:\n        Any: json schema mein paribhashit type ke anuroop, nirdisht attribute ka maan.\n\n    Requirements:\n    - json\n    - os\n    - re\n\n    Example:\n    >>> task_func('/path/to/file.json', 'email')\n    'john.doe@example.com'\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise ValueError(\"File does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    if not all(key in data for key in INPUT_JSON['required']):\n        raise ValueError(\"Missing required fields.\")\n    \n    for key, value in INPUT_JSON['properties'].items():\n        if not isinstance(data[key], value['type']):\n            raise ValueError(f\"Invalid type for {key}. Expected {value['type']}.\")\n    \n    if not re.match(EMAIL_REGEX, data[attribute]):\n        raise ValueError(f\"Invalid email format for {attribute}.\")\n    \n    return data[attribute]\n```\n"}
{"task_id": "BigCodeBench/31", "solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nPUNCTUATION = set(punctuation)\ndef task_func(text):\n    \"\"\"\n    Ek text mein \"$\" character se shuru hone wale words ki frequency ka ek baar chart banaen. Ve shabd jo '$' character se shuru hote hain, lekin jinmein only punctuation hote hain (e.g., '$!$' aur '$.$'), frequency ganana mein shamil nahin hain.\n\n    Parameters:\n    - text (str): Input text.\n\n    Returns:\n        matplotlib.axes._axes.Axes: '$' character se shuru hone wale words ki frequency darshane wala plot.\n\n    Requirements:\n    - nltk\n    - string\n    - seaborn\n    - matplotlib\n\n    Example:\n    >>> text = \"$child than resource indicate star $community station onto best green $exactly onto then age charge $friend than ready child really $let product coach decision professional $camera life off management factor $alone beat idea bit call $campaign fill stand Congress stuff $performance follow your resource road $data performance himself school here\"\n    >>> ax = task_func(text)\n    >>> print(ax)\n    Axes(0.125,0.11;0.775x0.77)\n\n    \"\"\"\n    # Tokenize the text\n    tokens = nltk.word_tokenize(text)\n\n    # Filter words starting with '$' and containing only punctuation\n    dollar_words = [word for word in tokens if word.startswith('$') and all(char in PUNCTUATION for char in word[1:])]\n\n    # Calculate frequency of each word\n    frequency = nltk.FreqDist(dollar_words)\n\n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=list(frequency.keys()), y=list(frequency.values()))\n    ax.set_title('Frequency of $-prefixed Words with Only Punctuation')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPUNCTUATION = set(punctuation)\n\n\ndef task_func(text):\n    \"\"\"\n    Ek text mein \"$\" character se shuru hone wale words ki frequency ka ek baar chart banaen. Ve shabd jo '$' character se shuru hote hain, lekin jinmein only punctuation hote hain (e.g., '$!$' aur '$.$'), frequency ganana mein shamil nahin hain.\n\n    Parameters:\n    - text (str): Input text.\n\n    Returns:\n        matplotlib.axes._axes.Axes: '$' character se shuru hone wale words ki frequency darshane wala plot.\n\n    Requirements:\n    - nltk\n    - string\n    - seaborn\n    - matplotlib\n\n    Example:\n    >>> text = \"$child than resource indicate star $community station onto best green $exactly onto then age charge $friend than ready child really $let product coach decision professional $camera life off management factor $alone beat idea bit call $campaign fill stand Congress stuff $performance follow your resource road $data performance himself school here\"\n    >>> ax = task_func(text)\n    >>> print(ax)\n    Axes(0.125,0.11;0.775x0.77)\n\n    \"\"\"\n    # Tokenize the text\n    tokens = nltk.word_tokenize(text)\n\n    # Filter words starting with '$' and containing only punctuation\n    dollar_words = [word for word in tokens if word.startswith('$') and all(char in PUNCTUATION for char in word[1:])]\n\n    # Calculate frequency of each word\n    frequency = nltk.FreqDist(dollar_words)\n\n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=list(frequency.keys()), y=list(frequency.values()))\n    ax.set_title('Frequency of $-prefixed Words with Only Punctuation')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/32", "solution": "import requests\nfrom bs4 import BeautifulSoup\ndef task_func(url, tag):\n    \"\"\"\n    Ek web page se specified html tag ki pehli occurrence ko scrape karein aur uski text content lautayein.\n\n    Parameters:\n    - url (str): Scrape karne ke liye website ka url.\n    - tag (str): html tag jise text khojne aur punarprapt karne ke liye.\n\n    Returns:\n        str: Yadi specified html tag paya jata hai, to uski text content, otherwise None lautata hai.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n\n    Example:\n    >>> task_func(\"https://www.google.com/\", \"title\")\n    'Google'\n\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    element = soup.find(tag)\n\n    return element.text if element else None", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, tag):\n    \"\"\"\n    Ek web page se specified html tag ki pehli occurrence ko scrape karein aur uski text content lautayein.\n\n    Parameters:\n    - url (str): Scrape karne ke liye website ka url.\n    - tag (str): html tag jise text khojne aur punarprapt karne ke liye.\n\n    Returns:\n        str: Yadi specified html tag paya jata hai, to uski text content, otherwise None lautata hai.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n\n    Example:\n    >>> task_func(\"https://www.google.com/\", \"title\")\n    'Google'\n\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    element = soup.find(tag)\n\n    return element.text if element else None\n```\n"}
{"task_id": "BigCodeBench/33", "solution": "import numpy as np\nfrom functools import reduce\ndef task_func(list_of_pairs):\n    \"\"\"\n    Ek list of tuples mein har tuple ke second values ka product calculate karein and product ko ek single-element numeric array ke roop mein return karein.\n\n    Parameters:\n    - list_of_pairs (list): tuples ki ek list, jahan first element category hai, and doosra element numeric value hai.\n\n    Returns:\n        numpy.ndarray: Ek 1D numpy array jismein ek single element shaamil hai jo list of tuples mein second values ka product hai.\n\n    Requirements:\n    - numpy\n    - functools.reduce\n\n    Example:\n    >>> list_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\n    >>> product_array = task_func(list_of_pairs)\n    >>> print(product_array)\n    [360]\n\n    \"\"\"\n    # Extract the second element of each tuple and calculate the product\n    product = reduce(lambda x, y: x * y, [pair[1] for pair in list_of_pairs])\n    \n    # Convert the product to a numpy array\n    product_array = np.array([product])\n    \n    return product_array\nlist_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]", "raw_solution": "import numpy as np\nfrom functools import reduce\n\ndef task_func(list_of_pairs):\n    \"\"\"\n    Ek list of tuples mein har tuple ke second values ka product calculate karein and product ko ek single-element numeric array ke roop mein return karein.\n\n    Parameters:\n    - list_of_pairs (list): tuples ki ek list, jahan first element category hai, and doosra element numeric value hai.\n\n    Returns:\n        numpy.ndarray: Ek 1D numpy array jismein ek single element shaamil hai jo list of tuples mein second values ka product hai.\n\n    Requirements:\n    - numpy\n    - functools.reduce\n\n    Example:\n    >>> list_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\n    >>> product_array = task_func(list_of_pairs)\n    >>> print(product_array)\n    [360]\n\n    \"\"\"\n    # Extract the second element of each tuple and calculate the product\n    product = reduce(lambda x, y: x * y, [pair[1] for pair in list_of_pairs])\n    \n    # Convert the product to a numpy array\n    product_array = np.array([product])\n    \n    return product_array\n\n# Test cases\nlist_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n\n    # Generate word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n    # Check if any words are generated\n    if not wordcloud.words_:\n        raise ValueError(\"Urls hatane ke baad word cloud banaane ke lie koi words available nahin hai.\")\n\n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n\n    return wordcloud", "raw_solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n\n    # Generate word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n    # Check if any words are generated\n    if not wordcloud.words_:\n        raise ValueError(\"Urls hatane ke baad word cloud banaane ke lie koi words available nahin hai.\")\n\n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n\n    return wordcloud\n```\n"}
{"task_id": "BigCodeBench/35", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df, target_values=[1, 3, 4]):\n    # Replace non-target values with zeros\n    for col in df.columns:\n        df[col] = np.where(df[col].isin(target_values), df[col], 0)\n\n    # Plot distribution of each column\n    fig, axes = plt.subplots(nrows=len(df.columns), ncols=1, figsize=(8, len(df.columns) * 4))\n    for ax, col in zip(axes, df.columns):\n        sns.histplot(df[col], ax=ax, kde=True)\n        ax.set_title(f'Distribution of {col}')\n        ax.set_xlabel(col)\n        ax.set_ylabel('Frequency')\n\n    plt.tight_layout()\n    return df, axes", "raw_solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df, target_values=[1, 3, 4]):\n    # Replace non-target values with zeros\n    for col in df.columns:\n        df[col] = np.where(df[col].isin(target_values), df[col], 0)\n\n    # Plot distribution of each column\n    fig, axes = plt.subplots(nrows=len(df.columns), ncols=1, figsize=(8, len(df.columns) * 4))\n    for ax, col in zip(axes, df.columns):\n        sns.histplot(df[col], ax=ax, kde=True)\n        ax.set_title(f'Distribution of {col}')\n        ax.set_xlabel(col)\n        ax.set_ylabel('Frequency')\n\n    plt.tight_layout()\n    return df, axes\n```\n"}
{"task_id": "BigCodeBench/36", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTARGET_VALUES = np.array([1, 3, 4])\ndef task_func(df):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"Input DataFrame is empty\")\n\n    # Initialize an empty DataFrame to store transformed data\n    transformed_df = pd.DataFrame()\n\n    # Apply Box-Cox transformation to each column\n    for column in df.columns:\n        if column not in TARGET_VALUES:\n            # Check if data is constant (all values are the same)\n            if df[column].nunique() == 1:\n                transformed_df[column] = np.zeros(len(df))\n            else:\n                transformed_df[column], _ = stats.boxcox(df[column] + 1)  # Adding 1 to avoid log(0)\n\n    # Plot KDE for each transformed column\n    fig, axes = plt.subplots(nrows=len(df.columns), ncols=1, figsize=(8, 6))\n    for i, column in enumerate(transformed_df.columns):\n        axes[i].plot(transformed_df[column], np.zeros_like(transformed_df[column]), '|k', alpha=0.5)\n        axes[i].hist(transformed_df[column], bins=20, alpha=0.5)\n        axes[i].set_title(f'KDE Plot for {column}')\n        axes[i].set_xlabel('Value')\n        axes[i].set_ylabel('Frequency')\n\n    plt.tight_layout()\n    plt.show()\n\n    return transformed_df, fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nTARGET_VALUES = np.array([1, 3, 4])\n\ndef task_func(df):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"Input DataFrame is empty\")\n\n    # Initialize an empty DataFrame to store transformed data\n    transformed_df = pd.DataFrame()\n\n    # Apply Box-Cox transformation to each column\n    for column in df.columns:\n        if column not in TARGET_VALUES:\n            # Check if data is constant (all values are the same)\n            if df[column].nunique() == 1:\n                transformed_df[column] = np.zeros(len(df))\n            else:\n                transformed_df[column], _ = stats.boxcox(df[column] + 1)  # Adding 1 to avoid log(0)\n\n    # Plot KDE for each transformed column\n    fig, axes = plt.subplots(nrows=len(df.columns), ncols=1, figsize=(8, 6))\n    for i, column in enumerate(transformed_df.columns):\n        axes[i].plot(transformed_df[column], np.zeros_like(transformed_df[column]), '|k', alpha=0.5)\n        axes[i].hist(transformed_df[column], bins=20, alpha=0.5)\n        axes[i].set_title(f'KDE Plot for {column}')\n        axes[i].set_xlabel('Value')\n        axes[i].set_ylabel('Frequency')\n\n    plt.tight_layout()\n    plt.show()\n\n    return transformed_df, fig\n```\n"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n    # Initialize the RandomForestClassifier\n    model = RandomForestClassifier(random_state=42)\n    model.fit(df.drop(columns=[target_column]), df[target_column])\n\n    # Get feature importances\n    feature_importances = model.feature_importances_\n\n    # Create a DataFrame for feature importances\n    feature_importance_df = pd.DataFrame({\n        'Feature': df.drop(columns=[target_column]).columns,\n        'Importance': feature_importances\n    })\n\n    # Sort the DataFrame by importance\n    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Feature')\n    ax.set_title('Feature Importance Visualization')\n\n    return model, ax", "raw_solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_column):\n    # Initialize the RandomForestClassifier\n    model = RandomForestClassifier(random_state=42)\n    model.fit(df.drop(columns=[target_column]), df[target_column])\n\n    # Get feature importances\n    feature_importances = model.feature_importances_\n\n    # Create a DataFrame for feature importances\n    feature_importance_df = pd.DataFrame({\n        'Feature': df.drop(columns=[target_column]).columns,\n        'Importance': feature_importances\n    })\n\n    # Sort the DataFrame by importance\n    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Feature')\n    ax.set_title('Feature Importance Visualization')\n\n    return model, ax\n```\n"}
{"task_id": "BigCodeBench/38", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\ndef task_func(data_matrix):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    \n    # Calculate mean for each row\n    df['Mean'] = df.mean(axis=1)\n    \n    # Plot histogram of means\n    ax = df['Mean'].plot(kind='hist', bins=10, title='Distribution of Means')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\n\ndef task_func(data_matrix):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    \n    # Calculate mean for each row\n    df['Mean'] = df.mean(axis=1)\n    \n    # Plot histogram of means\n    ax = df['Mean'].plot(kind='hist', bins=10, title='Distribution of Means')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/39", "solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\nALPHA = 0.05\ndef task_func(data_matrix):\n    means = np.mean(data_matrix, axis=1)\n    population_mean = np.mean(data_matrix)\n    significant_indices = []\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(means, label='Means', color='red')\n\n    for i, mean in enumerate(means):\n        t_stat, p_value = ttest_1samp(data_matrix[i], population_mean)\n        if p_value < ALPHA:\n            significant_indices.append(i)\n            plt.scatter(i, mean, color='blue')\n\n    plt.axhline(population_mean, color='green', linestyle='--', label='Population Mean')\n    plt.xlabel('Row Index')\n    plt.ylabel('Mean Value')\n    plt.title('Mean Value Analysis')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return significant_indices, plt.gca()", "raw_solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHA = 0.05\n\ndef task_func(data_matrix):\n    means = np.mean(data_matrix, axis=1)\n    population_mean = np.mean(data_matrix)\n    significant_indices = []\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(means, label='Means', color='red')\n\n    for i, mean in enumerate(means):\n        t_stat, p_value = ttest_1samp(data_matrix[i], population_mean)\n        if p_value < ALPHA:\n            significant_indices.append(i)\n            plt.scatter(i, mean, color='blue')\n\n    plt.axhline(population_mean, color='green', linestyle='--', label='Population Mean')\n    plt.xlabel('Row Index')\n    plt.ylabel('Mean Value')\n    plt.title('Mean Value Analysis')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return significant_indices, plt.gca()\n```\n"}
{"task_id": "BigCodeBench/40", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data_matrix):\n    # Convert the data_matrix to a pandas DataFrame\n    df = pd.DataFrame(data_matrix, columns=[f'feature {i+1}' for i in range(data_matrix.shape[1])])\n    \n    # Calculate Z-scores for each row\n    z_scores = df.apply(zscore, axis=1)\n    \n    # Calculate mean of each row\n    z_scores['Mean'] = z_scores.mean(axis=1)\n    \n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(z_scores.drop('Mean', axis=1), annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Z-Scores Heatmap')\n    plt.show()\n    \n    return z_scores, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data_matrix):\n    # Convert the data_matrix to a pandas DataFrame\n    df = pd.DataFrame(data_matrix, columns=[f'feature {i+1}' for i in range(data_matrix.shape[1])])\n    \n    # Calculate Z-scores for each row\n    z_scores = df.apply(zscore, axis=1)\n    \n    # Calculate mean of each row\n    z_scores['Mean'] = z_scores.mean(axis=1)\n    \n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(z_scores.drop('Mean', axis=1), annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Z-Scores Heatmap')\n    plt.show()\n    \n    return z_scores, ax\n\n# Example usage:\ndata = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\ndf, ax = task_func(data)"}
{"task_id": "BigCodeBench/41", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\ndef task_func(data_matrix):\n    # Calculate skewness for each row\n    skewness_values = [skew(row) for row in data_matrix]\n\n    # Create a DataFrame with the skewness values\n    df = pd.DataFrame(skewness_values, columns=['Skewness'])\n\n    # Plot the distribution of skewness values\n    ax = df['Skewness'].plot(kind='hist', bins=10, title='Distribution of Skewness')\n    ax.set_xlabel('Skewness')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n\ndef task_func(data_matrix):\n    # Calculate skewness for each row\n    skewness_values = [skew(row) for row in data_matrix]\n\n    # Create a DataFrame with the skewness values\n    df = pd.DataFrame(skewness_values, columns=['Skewness'])\n\n    # Plot the distribution of skewness values\n    ax = df['Skewness'].plot(kind='hist', bins=10, title='Distribution of Skewness')\n    ax.set_xlabel('Skewness')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/42", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\ndef task_func(data_matrix, n_components=2):\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data_matrix)\n    \n    # Create a DataFrame with the transformed data\n    df = pd.DataFrame(transformed_data, columns=[f'component {i+1}' for i in range(n_components)])\n    \n    # Calculate the mean of each component\n    df['mean'] = df.mean(axis=1)\n    \n    # Plot the cumulative explained variance\n    fig, ax = plt.subplots()\n    ax.plot(np.cumsum(pca.explained_variance_ratio_))\n    ax.set_xlabel('Number of Components')\n    ax.set_ylabel('Cumulative Explained Variance')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\ndef task_func(data_matrix, n_components=2):\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data_matrix)\n    \n    # Create a DataFrame with the transformed data\n    df = pd.DataFrame(transformed_data, columns=[f'component {i+1}' for i in range(n_components)])\n    \n    # Calculate the mean of each component\n    df['mean'] = df.mean(axis=1)\n    \n    # Plot the cumulative explained variance\n    fig, ax = plt.subplots()\n    ax.plot(np.cumsum(pca.explained_variance_ratio_))\n    ax.set_xlabel('Number of Components')\n    ax.set_ylabel('Cumulative Explained Variance')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/43", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    # Initialize an empty list to store plot objects\n    plots = []\n\n    # Calculate statistics for each numeric column\n    description = df.describe()\n\n    # Create distribution plots for each numeric column\n    for column in df.select_dtypes(include=[np.number]).columns:\n        plt.figure(figsize=(8, 4))\n        sns.histplot(df[column], bins=10, kde=True)\n        plt.title(f'Distribution of {column}')\n        plt.xlabel(column)\n        plt.ylabel('Frequency')\n        plots.append(plt.gca())  # Store the current plot object\n        plt.close()  # Close the plot to prevent it from displaying automatically\n\n    return description, plots", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df):\n    # Initialize an empty list to store plot objects\n    plots = []\n\n    # Calculate statistics for each numeric column\n    description = df.describe()\n\n    # Create distribution plots for each numeric column\n    for column in df.select_dtypes(include=[np.number]).columns:\n        plt.figure(figsize=(8, 4))\n        sns.histplot(df[column], bins=10, kde=True)\n        plt.title(f'Distribution of {column}')\n        plt.xlabel(column)\n        plt.ylabel('Frequency')\n        plots.append(plt.gca())  # Store the current plot object\n        plt.close()  # Close the plot to prevent it from displaying automatically\n\n    return description, plots\n```\n"}
{"task_id": "BigCodeBench/44", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    fig, ax = plt.subplots()\n    df.boxplot(column=numeric_cols, ax=ax)\n    ax.set_title('Box Plot of Numeric Columns')\n    ax.set_ylabel('Normalized Value')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    fig, ax = plt.subplots()\n    df.boxplot(column=numeric_cols, ax=ax)\n    ax.set_title('Box Plot of Numeric Columns')\n    ax.set_ylabel('Normalized Value')\n\n    return df, ax\n\n# Example usage:\n# df = pd.DataFrame([[1,2,3],[4,5,6],[7,np.nan,9]], columns=[\"c1\",\"c2\",\"c3\"])\n# df, ax = task_func(df)\n# print(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/45", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame):\n    # Drop non-numeric columns\n    df_numeric = df.select_dtypes(include=[np.number])\n\n    # Fill missing values with column means\n    df_filled = df_numeric.apply(lambda x: x.fillna(x.mean()), axis=0)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df_filled)\n    principalDf = pd.DataFrame(data=principalComponents, columns=['Component 1', 'Component 2'])\n\n    # Plot the first two principal components\n    fig, ax = plt.subplots()\n    sns.scatterplot(x='Component 1', y='Component 2', data=principalDf, ax=ax)\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return principalDf, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame):\n    # Drop non-numeric columns\n    df_numeric = df.select_dtypes(include=[np.number])\n\n    # Fill missing values with column means\n    df_filled = df_numeric.apply(lambda x: x.fillna(x.mean()), axis=0)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df_filled)\n    principalDf = pd.DataFrame(data=principalComponents, columns=['Component 1', 'Component 2'])\n\n    # Plot the first two principal components\n    fig, ax = plt.subplots()\n    sns.scatterplot(x='Component 1', y='Component 2', data=principalDf, ax=ax)\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return principalDf, ax\n\n# Example usage:\n# df = pd.DataFrame([[1,2,3],[4,5,6],[7.0,np.nan,9.0]], columns=[\"c1\",\"c2\",\"c3\"])\n# principalDf, ax = task_func(df)\n# print(principalDf)\n```\n"}
{"task_id": "BigCodeBench/46", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Replace missing values with column mean\n    df_filled = df.apply(lambda col: col.fillna(col.mean()), axis=0)\n\n    # Calculate z-scores\n    df_zscore = df_filled.apply(zscore)\n\n    # Plot histograms\n    plots = []\n    for col in df_zscore.select_dtypes(include=[np.number]).columns:\n        fig, ax = plt.subplots()\n        ax.hist(df_zscore[col], bins=10)\n        ax.set_title(f'Histogram of {col}')\n        ax.set_xlabel('Z-score')\n        ax.set_ylabel('Frequency')\n        plots.append(ax)\n\n    return df_zscore, plots", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Replace missing values with column mean\n    df_filled = df.apply(lambda col: col.fillna(col.mean()), axis=0)\n\n    # Calculate z-scores\n    df_zscore = df_filled.apply(zscore)\n\n    # Plot histograms\n    plots = []\n    for col in df_zscore.select_dtypes(include=[np.number]).columns:\n        fig, ax = plt.subplots()\n        ax.hist(df_zscore[col], bins=10)\n        ax.set_title(f'Histogram of {col}')\n        ax.set_xlabel('Z-score')\n        ax.set_ylabel('Frequency')\n        plots.append(ax)\n\n    return df_zscore, plots\n```\n"}
{"task_id": "BigCodeBench/47", "solution": "from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    # Standardizing the DataFrame\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(df)\n    standardized_df = pd.DataFrame(standardized_data, columns=df.columns)\n\n    # Calculating the correlation matrix\n    correlation_matrix = df.corr()\n\n    # Plotting the heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return standardized_df, heatmap", "raw_solution": "from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    # Standardizing the DataFrame\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(df)\n    standardized_df = pd.DataFrame(standardized_data, columns=df.columns)\n\n    # Calculating the correlation matrix\n    correlation_matrix = df.corr()\n\n    # Plotting the heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return standardized_df, heatmap\n```\n"}
{"task_id": "BigCodeBench/48", "solution": "import time\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\ndef task_func(n, output_path=None):\n    \"\"\"\n    n random Unix timestamps utpann karen aur unhen UTC DATE_FORMAT ke roop mein suroopit strings mein badlen. \n    utpann timestamps ke distribution ka ek histogram plot karen. yadi ek output path pradaan kiya gaya hai,\n    to histogram ko nirdisht path par save karen. anyatha, plot dikhaen.\n\n    Parameters:\n    - n (int): utpann karne ke liye timestamps ki number.\n    - output_path (str, optional): histogram plot ko save karne ka path. defaults roop se None.\n\n    Returns:\n        list: n suroopit timestamps ki list.\n\n    Requirements:\n    - time\n    - datetime\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> random.seed(42)\n    >>> timestamps = task_func(n=3, output_path=None)\n    >>> print(timestamps)\n    ['2013-07-06 20:56:46', '1977-07-29 23:34:23', '1971-09-14 11:29:44']\n\n    \"\"\"\n    timestamps = []\n    for _ in range(n):\n        random_timestamp = random.randint(0, int(time.time()))\n        utc_time = datetime.utcfromtimestamp(random_timestamp)\n        formatted_time = utc_time.strftime(DATE_FORMAT)\n        timestamps.append(formatted_time)\n\n    # Plotting the histogram\n    plt.hist(timestamps, bins=10)\n    plt.xlabel('Timestamps')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Random Timestamps')\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n\n    return timestamps", "raw_solution": "import time\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\n\ndef task_func(n, output_path=None):\n    \"\"\"\n    n random Unix timestamps utpann karen aur unhen UTC DATE_FORMAT ke roop mein suroopit strings mein badlen. \n    utpann timestamps ke distribution ka ek histogram plot karen. yadi ek output path pradaan kiya gaya hai,\n    to histogram ko nirdisht path par save karen. anyatha, plot dikhaen.\n\n    Parameters:\n    - n (int): utpann karne ke liye timestamps ki number.\n    - output_path (str, optional): histogram plot ko save karne ka path. defaults roop se None.\n\n    Returns:\n        list: n suroopit timestamps ki list.\n\n    Requirements:\n    - time\n    - datetime\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> random.seed(42)\n    >>> timestamps = task_func(n=3, output_path=None)\n    >>> print(timestamps)\n    ['2013-07-06 20:56:46', '1977-07-29 23:34:23', '1971-09-14 11:29:44']\n\n    \"\"\"\n    timestamps = []\n    for _ in range(n):\n        random_timestamp = random.randint(0, int(time.time()))\n        utc_time = datetime.utcfromtimestamp(random_timestamp)\n        formatted_time = utc_time.strftime(DATE_FORMAT)\n        timestamps.append(formatted_time)\n\n    # Plotting the histogram\n    plt.hist(timestamps, bins=10)\n    plt.xlabel('Timestamps')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Random Timestamps')\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n\n    return timestamps\n```\n"}
{"task_id": "BigCodeBench/49", "solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"input timestamps ki list empty hai.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetime_objects = [datetime.fromtimestamp(ts) for ts in timestamps]\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        'Timestamp': timestamps,\n        'Datetime': datetime_objects\n    })\n\n    # Plot a histogram of the datetime objects\n    ax = df['Datetime'].plot(kind='hist', bins=10, title='Distribution of Datetime Objects')\n    ax.set_xlabel('Datetime')\n    ax.set_ylabel('Frequency')\n\n    return df, ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\n\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"input timestamps ki list empty hai.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetime_objects = [datetime.fromtimestamp(ts) for ts in timestamps]\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        'Timestamp': timestamps,\n        'Datetime': datetime_objects\n    })\n\n    # Plot a histogram of the datetime objects\n    ax = df['Datetime'].plot(kind='hist', bins=10, title='Distribution of Datetime Objects')\n    ax.set_xlabel('Datetime')\n    ax.set_ylabel('Frequency')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/50", "solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\ndef task_func(timestamp):\n    \"\"\"\n    Convert a UNIX timestamp to different timezones and create a pandas DataFrame and a bar chart.\n\n    Parameters:\n    - timestamp (int): The Unix timestamp.\n\n    Returns:\n        tuple: A tuple containing:\n        DataFrame: A pandas DataFrame containing the datetime in different timezones.\n        Axes: A matplotlib Axes object for the generated bar chart.\n\n    Requirements:\n    - datetime\n    - pandas\n    - pytz\n    - matplotlib.pyplot\n\n    Example:\n    >>> df, ax = task_func(1347517370)\n    >>> print(df)\n    Timezone            Datetime\n    0  America/New_York 2012-09-13 02:22:50\n    1     Europe/London 2012-09-13 07:22:50\n    2     Asia/Shanghai 2012-09-13 14:22:50\n    3        Asia/Tokyo 2012-09-13 15:22:50\n    4  Australia/Sydney 2012-09-13 16:22:50\n    \"\"\"\n    # Convert the timestamp to a naive datetime object\n    naive_datetime = datetime.fromtimestamp(timestamp)\n\n    # Convert the naive datetime object to aware datetime objects in different timezones\n    aware_datetimes = []\n    for tz in TIMEZONES:\n        timezone = pytz.timezone(tz)\n        aware_datetime = timezone.localize(naive_datetime)\n        aware_datetimes.append(aware_datetime)\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        \"Timezone\": TIMEZONES,\n        \"Datetime\": aware_datetimes\n    })\n\n    # Plot the data\n    ax = df.plot(x=\"Timezone\", y=\"Datetime\", kind=\"bar\", legend=False)\n    ax.set_xlabel(\"Timezone\")\n    ax.set_ylabel(\"Datetime\")\n    ax.set_title(\"Datetime = f(Timezones)\")\n\n    return df, ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\n\n\ndef task_func(timestamp):\n    \"\"\"\n    Convert a UNIX timestamp to different timezones and create a pandas DataFrame and a bar chart.\n\n    Parameters:\n    - timestamp (int): The Unix timestamp.\n\n    Returns:\n        tuple: A tuple containing:\n        DataFrame: A pandas DataFrame containing the datetime in different timezones.\n        Axes: A matplotlib Axes object for the generated bar chart.\n\n    Requirements:\n    - datetime\n    - pandas\n    - pytz\n    - matplotlib.pyplot\n\n    Example:\n    >>> df, ax = task_func(1347517370)\n    >>> print(df)\n    Timezone            Datetime\n    0  America/New_York 2012-09-13 02:22:50\n    1     Europe/London 2012-09-13 07:22:50\n    2     Asia/Shanghai 2012-09-13 14:22:50\n    3        Asia/Tokyo 2012-09-13 15:22:50\n    4  Australia/Sydney 2012-09-13 16:22:50\n    \"\"\"\n    # Convert the timestamp to a naive datetime object\n    naive_datetime = datetime.fromtimestamp(timestamp)\n\n    # Convert the naive datetime object to aware datetime objects in different timezones\n    aware_datetimes = []\n    for tz in TIMEZONES:\n        timezone = pytz.timezone(tz)\n        aware_datetime = timezone.localize(naive_datetime)\n        aware_datetimes.append(aware_datetime)\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        \"Timezone\": TIMEZONES,\n        \"Datetime\": aware_datetimes\n    })\n\n    # Plot the data\n    ax = df.plot(x=\"Timezone\", y=\"Datetime\", kind=\"bar\", legend=False)\n    ax.set_xlabel(\"Timezone\")\n    ax.set_ylabel(\"Datetime\")\n    ax.set_title(\"Datetime = f(Timezones)\")\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/51", "solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, age: int, height: int):\n    # Filter the dataframe based on conditions\n    filtered_df = df[(df['Age'] > age) & (df['Height'] < height)]\n\n    # Check if the filtered dataframe has less than 3 columns\n    if len(filtered_df.columns) < 3:\n        # Add a 'Cluster' column with 0\n        filtered_df['Cluster'] = 0\n        return filtered_df, None\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    filtered_df['Cluster'] = kmeans.fit_predict(filtered_df[['Age', 'Height']])\n\n    # Plot the data\n    ax = filtered_df.plot.scatter(x='Age', y='Height', c='Cluster', colormap='viridis')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Height')\n    ax.set_title('KMeans Clustering based on Age and Height')\n\n    return filtered_df, ax", "raw_solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, age: int, height: int):\n    # Filter the dataframe based on conditions\n    filtered_df = df[(df['Age'] > age) & (df['Height'] < height)]\n\n    # Check if the filtered dataframe has less than 3 columns\n    if len(filtered_df.columns) < 3:\n        # Add a 'Cluster' column with 0\n        filtered_df['Cluster'] = 0\n        return filtered_df, None\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    filtered_df['Cluster'] = kmeans.fit_predict(filtered_df[['Age', 'Height']])\n\n    # Plot the data\n    ax = filtered_df.plot.scatter(x='Age', y='Height', c='Cluster', colormap='viridis')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Height')\n    ax.set_title('KMeans Clustering based on Age and Height')\n\n    return filtered_df, ax\n\n# Example usage"}
{"task_id": "BigCodeBench/52", "solution": "import pandas as pd\nimport regex as re\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\ndef task_func(text):\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Split text into words\n    words = text.split()\n\n    # Filter out stopwords\n    filtered_words = [word for word in words if word not in STOPWORDS]\n\n    # Create a pandas Series with word frequencies\n    word_counts = pd.Series(filtered_words).value_counts()\n\n    return word_counts", "raw_solution": "import pandas as pd\nimport regex as re\n\n# Constants\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\n\n\ndef task_func(text):\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Split text into words\n    words = text.split()\n\n    # Filter out stopwords\n    filtered_words = [word for word in words if word not in STOPWORDS]\n\n    # Create a pandas Series with word frequencies\n    word_counts = pd.Series(filtered_words).value_counts()\n\n    return word_counts\n\n# Example usage"}
{"task_id": "BigCodeBench/53", "solution": "import pandas as pd\nimport regex as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\ndef task_func(text):\n    pattern = r\"name: (.*?), Email: (.*?), Age: (.*?), country: (.*?)($|\\n)\"\n    matches = re.findall(pattern, text)\n    data = [match for match in matches]\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Convert 'Age' column to integer\n    df['Age'] = df['Age'].astype(int)\n\n    # Plot age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df['Age'], bins=10, kde=True)\n    plt.title('Age Distribution')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return df\ntext = 'name: John Doe, Email: john.doe@example.com, Age: 30, country: USA\\nname: Jane Doe, Email: jane.doe@example.com, Age: 25, country: UK'", "raw_solution": "import pandas as pd\nimport regex as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\n\n\ndef task_func(text):\n    pattern = r\"name: (.*?), Email: (.*?), Age: (.*?), country: (.*?)($|\\n)\"\n    matches = re.findall(pattern, text)\n    data = [match for match in matches]\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Convert 'Age' column to integer\n    df['Age'] = df['Age'].astype(int)\n\n    # Plot age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df['Age'], bins=10, kde=True)\n    plt.title('Age Distribution')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return df\n\n# Example usage:\ntext = 'name: John Doe, Email: john.doe@example.com, Age: 30, country: USA\\nname: Jane Doe, Email: jane.doe@example.com, Age: 25, country: UK'\ndf = task_func(text)"}
{"task_id": "BigCodeBench/54", "solution": "import pandas as pd\nimport regex as re\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(text):\n    # Remove empty sentences\n    sentences = [sentence for sentence in re.split(r'[.!?]', text) if sentence.strip()]\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    dtm = vectorizer.fit_transform(sentences)\n\n    # Convert the result to a pandas DataFrame\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\ntext = \"This is a sample sentence. This sentence contains sample words.\"", "raw_solution": "import pandas as pd\nimport regex as re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef task_func(text):\n    # Remove empty sentences\n    sentences = [sentence for sentence in re.split(r'[.!?]', text) if sentence.strip()]\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    dtm = vectorizer.fit_transform(sentences)\n\n    # Convert the result to a pandas DataFrame\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\n\n# Example usage:\ntext = \"This is a sample sentence. This sentence contains sample words.\"\ndtm = task_func(text)"}
{"task_id": "BigCodeBench/55", "solution": "import re\nimport pandas as pd\nSTOPWORDS = [\"Those\", \"are\", \"the\", \"words\", \"to\", \"ignore\"]\ndef task_func(text):\n    sentences = re.split(r'[.!?]', text)\n    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n\n    sentence_data = []\n    for i, sentence in enumerate(sentences, start=1):\n        words = re.findall(r'\\b\\w+\\b', sentence)\n        filtered_words = [word for word in words if word not in STOPWORDS]\n        sentence_data.append({'Sentence {}'.format(i): len(filtered_words)})\n\n    df = pd.DataFrame(sentence_data)\n    return df\ntext = \"yah ek sample sentence hai. is sentence mein sample shabd hain.\"", "raw_solution": "import re\nimport pandas as pd\n\nSTOPWORDS = [\"Those\", \"are\", \"the\", \"words\", \"to\", \"ignore\"]\n\ndef task_func(text):\n    sentences = re.split(r'[.!?]', text)\n    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n\n    sentence_data = []\n    for i, sentence in enumerate(sentences, start=1):\n        words = re.findall(r'\\b\\w+\\b', sentence)\n        filtered_words = [word for word in words if word not in STOPWORDS]\n        sentence_data.append({'Sentence {}'.format(i): len(filtered_words)})\n\n    df = pd.DataFrame(sentence_data)\n    return df\n\n# Example usage:\ntext = \"yah ek sample sentence hai. is sentence mein sample shabd hain.\"\ndf = task_func(text)"}
{"task_id": "BigCodeBench/56", "solution": "import pandas as pd\nimport regex as re\ndef task_func(text):\n    # Extracting the scores and categories using regex\n    pattern = r'Score: (\\d+), Category: (.*?)\\\\'\n    matches = re.findall(pattern, text)\n\n    # Creating a list of dictionaries to store the data\n    data = [{'Score': int(score), 'Category': category} for score, category in matches]\n\n    # Creating a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n\n    return df\ntext = \"Score: 85, Category: Math\\\\nScore: 90, Category: Science\\\\nScore: 80, Category: Math\"", "raw_solution": "import pandas as pd\nimport regex as re\n\ndef task_func(text):\n    # Extracting the scores and categories using regex\n    pattern = r'Score: (\\d+), Category: (.*?)\\\\'\n    matches = re.findall(pattern, text)\n\n    # Creating a list of dictionaries to store the data\n    data = [{'Score': int(score), 'Category': category} for score, category in matches]\n\n    # Creating a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n\n    return df\n\n# Example usage\ntext = \"Score: 85, Category: Math\\\\nScore: 90, Category: Science\\\\nScore: 80, Category: Math\"\ndf = task_func(text)"}
{"task_id": "BigCodeBench/57", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(csv_file_path: str, title: str):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Calculate the correlation matrix\n    corr_matrix = df.corr()\n\n    # Round the correlation values to 2 decimal places\n    corr_matrix = corr_matrix.round(2)\n\n    # Create a heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar_kws={\"shrink\": 0.8})\n    plt.title(title)\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=45)\n    plt.tight_layout()\n\n    return corr_matrix, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(csv_file_path: str, title: str):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Calculate the correlation matrix\n    corr_matrix = df.corr()\n\n    # Round the correlation values to 2 decimal places\n    corr_matrix = corr_matrix.round(2)\n\n    # Create a heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar_kws={\"shrink\": 0.8})\n    plt.title(title)\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=45)\n    plt.tight_layout()\n\n    return corr_matrix, ax\n```\n"}
{"task_id": "BigCodeBench/58", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, num_samples):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create a histogram of the samples\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(\"Normal Distribution\")\n\n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, num_samples):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create a histogram of the samples\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(\"Normal Distribution\")\n\n    return fig\n\n# Example usage\n# fig = task_func(0, 1, 1000)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/59", "solution": "import wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(page_title):\n    try:\n        # Fetch the content of the Wikipedia page\n        page_content = wikipedia.page(page_title).content\n\n        # Generate a word cloud\n        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(page_content)\n\n        # Plot the word cloud\n        fig, ax = plt.subplots(figsize=(10, 5))\n        ax.imshow(wordcloud, interpolation='bilinear')\n        ax.axis('off')  # Turn off the axis grid and labels\n\n        return ax\n    except wikipedia.exceptions.PageError:\n        # Return None if the page does not exist\n        return None", "raw_solution": "import wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(page_title):\n    try:\n        # Fetch the content of the Wikipedia page\n        page_content = wikipedia.page(page_title).content\n\n        # Generate a word cloud\n        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(page_content)\n\n        # Plot the word cloud\n        fig, ax = plt.subplots(figsize=(10, 5))\n        ax.imshow(wordcloud, interpolation='bilinear')\n        ax.axis('off')  # Turn off the axis grid and labels\n\n        return ax\n    except wikipedia.exceptions.PageError:\n        # Return None if the page does not exist\n        return None\n```\n"}
{"task_id": "BigCodeBench/60", "solution": "import json\nimport pandas as pd\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(result)\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n    \n    # Save the list of dictionaries to a JSON file\n    with open(json_file_path, 'w') as json_file:\n        json.dump(result, json_file)\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {1: 2, 3: 4, 5: 6}]", "raw_solution": "import json\nimport pandas as pd\n\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(result)\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n    \n    # Save the list of dictionaries to a JSON file\n    with open(json_file_path, 'w') as json_file:\n        json.dump(result, json_file)\n\n# Example usage:\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {1: 2, 3: 4, 5: 6}]\ntask_func(result, 'test.csv', 'test.json')\n```\n"}
{"task_id": "BigCodeBench/61", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(result):\n    # Extract 'from_user' values\n    from_user_values = [item['from_user'] for item in result]\n\n    # Calculate square roots\n    square_roots = np.sqrt(from_user_values)\n    rounded_square_roots = np.round(square_roots, 2)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(from_user_values, rounded_square_roots, marker='o')\n    ax.set_title(PLOT_TITLE)\n    ax.set_xlabel(X_LABEL)\n    ax.set_ylabel(Y_LABEL)\n\n    # Annotate with current date and time\n    current_time = datetime.now().strftime(TIME_FORMAT)\n    ax.annotate(current_time, xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10, color='gray')\n\n    return rounded_square_roots, ax\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 16}, {\"some_key\": 2, \"another_key\": 4, \"from_user\": 9}]", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Constants\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(result):\n    # Extract 'from_user' values\n    from_user_values = [item['from_user'] for item in result]\n\n    # Calculate square roots\n    square_roots = np.sqrt(from_user_values)\n    rounded_square_roots = np.round(square_roots, 2)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(from_user_values, rounded_square_roots, marker='o')\n    ax.set_title(PLOT_TITLE)\n    ax.set_xlabel(X_LABEL)\n    ax.set_ylabel(Y_LABEL)\n\n    # Annotate with current date and time\n    current_time = datetime.now().strftime(TIME_FORMAT)\n    ax.annotate(current_time, xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10, color='gray')\n\n    return rounded_square_roots, ax\n\n# Example usage\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 16}, {\"some_key\": 2, \"another_key\": 4, \"from_user\": 9}]\nsquare_roots, ax = task_func(result)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/62", "solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    \"\"\"\n    Diye gaye result mein \"from_user\" values ka ek histogram banaen. Histogram bar ka color diye gaye colors list se random roop se chuna jaata hai.\n\n    Parameters:\n    - result (list): Ek list jismein \"from_user\" key wali dictionaries shaamil hain.\n    - colors (list, optional): Histogram bar ke liye chunne ke liye colors ki ek list. Defaults hai ['b', 'g', 'r', 'c', 'm', 'y', 'k'].\n\n    Returns:\n        None: Function histogram pradarshit karta hai and koi value vaapas nahi karta hai.\n\n    Requirements:\n    - random\n    - matplotlib\n    - seaborn\n\n    Example:\n    >>> result = [{\"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]\n    >>> task_func(result)\n\n    \"\"\"\n    # Extract 'from_user' values\n    from_user_values = [item['from_user'] for item in result]\n\n    # Randomly select a color from the colors list\n    selected_color = random.choice(colors)\n\n    # Create a histogram using seaborn\n    sns.histplot(from_user_values, color=selected_color, bins=5, kde=False)\n\n    # Show the plot\n    plt.show()", "raw_solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    \"\"\"\n    Diye gaye result mein \"from_user\" values ka ek histogram banaen. Histogram bar ka color diye gaye colors list se random roop se chuna jaata hai.\n\n    Parameters:\n    - result (list): Ek list jismein \"from_user\" key wali dictionaries shaamil hain.\n    - colors (list, optional): Histogram bar ke liye chunne ke liye colors ki ek list. Defaults hai ['b', 'g', 'r', 'c', 'm', 'y', 'k'].\n\n    Returns:\n        None: Function histogram pradarshit karta hai and koi value vaapas nahi karta hai.\n\n    Requirements:\n    - random\n    - matplotlib\n    - seaborn\n\n    Example:\n    >>> result = [{\"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]\n    >>> task_func(result)\n\n    \"\"\"\n    # Extract 'from_user' values\n    from_user_values = [item['from_user'] for item in result]\n\n    # Randomly select a color from the colors list\n    selected_color = random.choice(colors)\n\n    # Create a histogram using seaborn\n    sns.histplot(from_user_values, color=selected_color, bins=5, kde=False)\n\n    # Show the plot\n    plt.show()\n```\n"}
{"task_id": "BigCodeBench/63", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(car_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n    \n    # Plot the distribution of vehicle colors\n    ax = df['Color'].value_counts().plot(kind='bar', color='skyblue')\n    ax.set_title('Distribution of Vehicle Colors')\n    ax.set_xlabel('Color')\n    ax.set_ylabel('Count')\n    \n    return df, ax\ncar_dict = {'Ford': 'Red', 'Toyota': 'Blue', 'Mercedes': 'Black', 'Tesla': 'White', 'BMW': 'Silver'}", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(car_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n    \n    # Plot the distribution of vehicle colors\n    ax = df['Color'].value_counts().plot(kind='bar', color='skyblue')\n    ax.set_title('Distribution of Vehicle Colors')\n    ax.set_xlabel('Color')\n    ax.set_ylabel('Count')\n    \n    return df, ax\n\n# Example usage:\ncar_dict = {'Ford': 'Red', 'Toyota': 'Blue', 'Mercedes': 'Black', 'Tesla': 'White', 'BMW': 'Silver'}\ndf, ax = task_func(car_dict)"}
{"task_id": "BigCodeBench/64", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Pivot the DataFrame to get the count of 'col3' values for each combination of 'col1' and 'col2'\n    pivot_df = df.pivot_table(index='col1', columns='col2', aggfunc=lambda x: x.value_counts().to_dict(), fill_value=0)\n    \n    # Create a heatmap to visualize the distribution of 'col3' values\n    plt.figure(figsize=(10, 6))\n    ax = sns.heatmap(pivot_df, annot=True, fmt='d', cmap='Blues')\n    ax.set_title('Distribution of col3 values')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Pivot the DataFrame to get the count of 'col3' values for each combination of 'col1' and 'col2'\n    pivot_df = df.pivot_table(index='col1', columns='col2', aggfunc=lambda x: x.value_counts().to_dict(), fill_value=0)\n    \n    # Create a heatmap to visualize the distribution of 'col3' values\n    plt.figure(figsize=(10, 6))\n    ax = sns.heatmap(pivot_df, annot=True, fmt='d', cmap='Blues')\n    ax.set_title('Distribution of col3 values')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/65", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df = df.drop_duplicates()\n\n    last_col = COLUMNS[-1]\n    unique_values = df[last_col].unique()\n    df[last_col] = df[last_col].map(lambda x: f\"{last_col}={x}\")\n\n    ax = df.plot(x=last_col, y=COLUMNS[:-1], kind='line', marker='o')\n    ax.set_xlabel('-'.join(COLUMNS[:-1]))\n    ax.set_ylabel(last_col)\n\n    return df, ax\ndata = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df = df.drop_duplicates()\n\n    last_col = COLUMNS[-1]\n    unique_values = df[last_col].unique()\n    df[last_col] = df[last_col].map(lambda x: f\"{last_col}={x}\")\n\n    ax = df.plot(x=last_col, y=COLUMNS[:-1], kind='line', marker='o')\n    ax.set_xlabel('-'.join(COLUMNS[:-1]))\n    ax.set_ylabel(last_col)\n\n    return df, ax\n\n# Example usage:\ndata = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\nanalyzed_df, ax = task_func(data)"}
{"task_id": "BigCodeBench/66", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Create a DataFrame from the given data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Create a new DataFrame with 'col1' and 'col2' columns\n    df_grouped = df.groupby(['col1', 'col2'])['col3'].sum().reset_index()\n    \n    # Create a seaborn plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.countplot(data=df_grouped, x='col3', hue='col2')\n    ax.set_xlabel('col3')\n    \n    return df_grouped, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    # Create a DataFrame from the given data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Create a new DataFrame with 'col1' and 'col2' columns\n    df_grouped = df.groupby(['col1', 'col2'])['col3'].sum().reset_index()\n    \n    # Create a seaborn plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.countplot(data=df_grouped, x='col3', hue='col2')\n    ax.set_xlabel('col3')\n    \n    return df_grouped, ax\n```\n"}
{"task_id": "BigCodeBench/67", "solution": "import pandas as pd\nimport re\nimport os\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    # Initialize an empty list to store file information\n    file_info = []\n\n    # Iterate over all files in the directory\n    for file_name in os.listdir(dir_path):\n        # Check if the file name matches the pattern\n        if re.match(pattern, file_name):\n            # Get the file size\n            file_size = os.path.getsize(os.path.join(dir_path, file_name))\n            # Append the file name and size to the list\n            file_info.append({'File': file_name, 'Size': file_size})\n\n    # Create a pandas DataFrame from the list\n    df = pd.DataFrame(file_info)\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport os\n\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    # Initialize an empty list to store file information\n    file_info = []\n\n    # Iterate over all files in the directory\n    for file_name in os.listdir(dir_path):\n        # Check if the file name matches the pattern\n        if re.match(pattern, file_name):\n            # Get the file size\n            file_size = os.path.getsize(os.path.join(dir_path, file_name))\n            # Append the file name and size to the list\n            file_info.append({'File': file_name, 'Size': file_size})\n\n    # Create a pandas DataFrame from the list\n    df = pd.DataFrame(file_info)\n\n    return df\n\n# Example usage:\n# report = task_func('/path/to/directory')\n# print(report)\n```\n"}
{"task_id": "BigCodeBench/68", "solution": "import pandas as pd\nimport seaborn as sns\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(data)\n\n    # Filter the DataFrame to include only rows with employee IDs starting with the specified prefix\n    filtered_df = df[df['Employee ID'].str.startswith(emp_prefix)]\n\n    # Create a histogram of the 'Age' column\n    ax = sns.histplot(filtered_df['Age'], bins=10, kde=True)\n\n    return filtered_df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\n\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(data)\n\n    # Filter the DataFrame to include only rows with employee IDs starting with the specified prefix\n    filtered_df = df[df['Employee ID'].str.startswith(emp_prefix)]\n\n    # Create a histogram of the 'Age' column\n    ax = sns.histplot(filtered_df['Age'], bins=10, kde=True)\n\n    return filtered_df, ax\n```\n"}
{"task_id": "BigCodeBench/69", "solution": "import random\nimport matplotlib.pyplot as plt\nSALARY_RANGE = (20000, 100000)\ndef task_func(dict1):\n    # Filter only 'EMPXX' department\n    emp_dept = dict1.get('EMPXX', 0)\n    \n    # Generate random salaries\n    salaries = [random.randint(*SALARY_RANGE) for _ in range(emp_dept)]\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(salaries, bins=10, color='skyblue', edgecolor='black')\n    ax.set_title('EMPXX department mein salary distribution')\n    ax.set_xlabel('salary')\n    ax.set_ylabel('employees ki number')\n    \n    return ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\n# Constants\nSALARY_RANGE = (20000, 100000)\n\ndef task_func(dict1):\n    # Filter only 'EMPXX' department\n    emp_dept = dict1.get('EMPXX', 0)\n    \n    # Generate random salaries\n    salaries = [random.randint(*SALARY_RANGE) for _ in range(emp_dept)]\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(salaries, bins=10, color='skyblue', edgecolor='black')\n    ax.set_title('EMPXX department mein salary distribution')\n    ax.set_xlabel('salary')\n    ax.set_ylabel('employees ki number')\n    \n    return ax\n\n# Example usage"}
{"task_id": "BigCodeBench/70", "solution": "import pandas as pd\nimport json\nimport numpy as np\ndef task_func(json_file):\n    # Load JSON data\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n\n    # Plotting\n    ax = None\n    if not df.empty:\n        ax = df.plot(x='email', y=['sum', 'mean'], kind='bar')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport json\nimport numpy as np\n\n# Constants\nCOLUMNS = ['email', 'list', 'sum', 'mean']\n\ndef task_func(json_file):\n    # Load JSON data\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n\n    # Plotting\n    ax = None\n    if not df.empty:\n        ax = df.plot(x='email', y=['sum', 'mean'], kind='bar')\n\n    return df, ax\n\n# Example usage:\n# df, ax = task_func('data/task_func/json_1.json')\n# print(df)\n```\nThis script reads a JSON file, processes the data to calculate the sum and mean of the lists, and then plots the results. If the JSON file is empty, it returns an empty DataFrame and None for the plot.\n"}
{"task_id": "BigCodeBench/71", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\ndef task_func(csv_file):\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(csv_file)\n\n    # Ensure the 'list' column contains valid Python lists\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and standard deviation for each list\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['std'] = df['list'].apply(np.std)\n\n    # Create a histogram of the mean values\n    ax = sns.histplot(df['mean'], kde=True)\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\n\ndef task_func(csv_file):\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(csv_file)\n\n    # Ensure the 'list' column contains valid Python lists\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and standard deviation for each list\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['std'] = df['list'].apply(np.std)\n\n    # Create a histogram of the mean values\n    ax = sns.histplot(df['mean'], kde=True)\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/72", "solution": "import pandas as pd\nimport os\nimport numpy as np\nimport ast\nimport matplotlib.pyplot as plt\ndef task_func(directory):\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # List to store filenames\n    filenames = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        if filename.endswith('.csv'):\n            filenames.append(filename)\n\n    # If there are no csv files, return the empty DataFrame and None\n    if not filenames:\n        return df, None\n\n    # Sort filenames by length in descending order\n    filenames.sort(key=len, reverse=True)\n\n    # Load the longest filename csv file into a DataFrame\n    df = pd.read_csv(os.path.join(directory, filenames[0]))\n\n    # Check if required columns exist\n    if 'email' not in df.columns or 'list' not in df.columns:\n        return df, None\n\n    # Convert 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and median for each list\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['median'] = df['list'].apply(np.median)\n\n    # Plot the histogram of the median values\n    ax = df['median'].plot(kind='hist', bins=10, title='Median Histogram')\n    plt.xlabel('Median Value')\n    plt.ylabel('Frequency')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport os\nimport numpy as np\nimport ast\nimport matplotlib.pyplot as plt\n\ndef task_func(directory):\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # List to store filenames\n    filenames = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        if filename.endswith('.csv'):\n            filenames.append(filename)\n\n    # If there are no csv files, return the empty DataFrame and None\n    if not filenames:\n        return df, None\n\n    # Sort filenames by length in descending order\n    filenames.sort(key=len, reverse=True)\n\n    # Load the longest filename csv file into a DataFrame\n    df = pd.read_csv(os.path.join(directory, filenames[0]))\n\n    # Check if required columns exist\n    if 'email' not in df.columns or 'list' not in df.columns:\n        return df, None\n\n    # Convert 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and median for each list\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['median'] = df['list'].apply(np.median)\n\n    # Plot the histogram of the median values\n    ax = df['median'].plot(kind='hist', bins=10, title='Median Histogram')\n    plt.xlabel('Median Value')\n    plt.ylabel('Frequency')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/73", "solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT email, list FROM EmailData\"\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    # Convert string representation of list to actual list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and variance for each email\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    # Plotting the sum, mean, and variance\n    fig, ax = plt.subplots()\n    df.plot(x='email', y=['sum', 'mean', 'var'], ax=ax)\n    ax.set_title('Sum, Mean, and Variance of Lists per Email')\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Value')\n    ax.legend(['Sum', 'Mean', 'Variance'])\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\n\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT email, list FROM EmailData\"\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    # Convert string representation of list to actual list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and variance for each email\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    # Plotting the sum, mean, and variance\n    fig, ax = plt.subplots()\n    df.plot(x='email', y=['sum', 'mean', 'var'], ax=ax)\n    ax.set_title('Sum, Mean, and Variance of Lists per Email')\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Value')\n    ax.legend(['Sum', 'Mean', 'Variance'])\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/74", "solution": "import socket\nimport requests\ndef task_func(host):\n    if not host:\n        raise ValueError(\"host ek non-khali string hona chahiye.\")\n\n    try:\n        # Get the IP address of the host\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        raise ConnectionError(f\"Unable to resolve the host: {host}\")\n\n    try:\n        # Get geolocation information using ipinfo.io API\n        response = requests.get(f\"http://ipinfo.io/{ip_address}\")\n        response.raise_for_status()\n        geolocation_info = response.json()\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Error fetching geolocation information: {e}\")\n\n    return {\n        \"ip_address\": ip_address,\n        \"geolocation\": geolocation_info\n    }", "raw_solution": "import socket\nimport requests\n\ndef task_func(host):\n    if not host:\n        raise ValueError(\"host ek non-khali string hona chahiye.\")\n\n    try:\n        # Get the IP address of the host\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        raise ConnectionError(f\"Unable to resolve the host: {host}\")\n\n    try:\n        # Get geolocation information using ipinfo.io API\n        response = requests.get(f\"http://ipinfo.io/{ip_address}\")\n        response.raise_for_status()\n        geolocation_info = response.json()\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Error fetching geolocation information: {e}\")\n\n    return {\n        \"ip_address\": ip_address,\n        \"geolocation\": geolocation_info\n    }\n\n# Example usage:\n# result = task_func('google.com')\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/75", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise TypeError(\"Input must be a non-empty pandas DataFrame.\")\n    if sales_lower_bound >= sales_upper_bound:\n        raise ValueError(\"sales_lower_bound must be less than sales_upper_bound.\")\n\n    if fruits is None:\n        fruits = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n    if days is None:\n        days = [datetime(2024, 1, 1) + timedelta(days=i) for i in range(7)]\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    sales_data = []\n    for fruit, day in itertools.product(fruits, days):\n        sales = np.random.randint(sales_lower_bound, sales_upper_bound)\n        sales_data.append({'Fruit': fruit, 'Day': day, 'Sales': sales})\n\n    report_df = pd.DataFrame(sales_data)\n\n    plot = sns.boxplot(x='Fruit', y='Sales', data=report_df)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return report_df, plot", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise TypeError(\"Input must be a non-empty pandas DataFrame.\")\n    if sales_lower_bound >= sales_upper_bound:\n        raise ValueError(\"sales_lower_bound must be less than sales_upper_bound.\")\n\n    if fruits is None:\n        fruits = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n    if days is None:\n        days = [datetime(2024, 1, 1) + timedelta(days=i) for i in range(7)]\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    sales_data = []\n    for fruit, day in itertools.product(fruits, days):\n        sales = np.random.randint(sales_lower_bound, sales_upper_bound)\n        sales_data.append({'Fruit': fruit, 'Day': day, 'Sales': sales})\n\n    report_df = pd.DataFrame(sales_data)\n\n    plot = sns.boxplot(x='Fruit', y='Sales', data=report_df)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return report_df, plot\n```\n"}
{"task_id": "BigCodeBench/76", "solution": "import random\nimport string\nfrom django.http import HttpResponse\ndef task_func(request, session_expire_time):\n    # Generate a random session key of length 20\n    session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    # Validate the session key\n    if not (all(c.isalnum() for c in session_key) and len(session_key) == 20):\n        raise ValueError(\"Session key must be alphanumeric and of length 20.\")\n\n    # Create a HttpResponse object with a cookie containing the session key\n    response = HttpResponse(\"Session key successfully generated.\")\n    response.set_cookie('session_key', session_key, max_age=session_expire_time)\n\n    return response", "raw_solution": "import random\nimport string\nfrom django.http import HttpResponse\n\n\ndef task_func(request, session_expire_time):\n    # Generate a random session key of length 20\n    session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    # Validate the session key\n    if not (all(c.isalnum() for c in session_key) and len(session_key) == 20):\n        raise ValueError(\"Session key must be alphanumeric and of length 20.\")\n\n    # Create a HttpResponse object with a cookie containing the session key\n    response = HttpResponse(\"Session key successfully generated.\")\n    response.set_cookie('session_key', session_key, max_age=session_expire_time)\n\n    return response\n```\n"}
{"task_id": "BigCodeBench/77", "solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\ndef task_func(data):\n    \"\"\"\n    This function is designed to authenticate users in a web application context. It expects a dictionary with 'username' and 'password' keys.\n    The password is expected to be a base64-encoded SHA-256 hash. The function decodes and verifies the credentials, returning an appropriate HTTP response.\n    \"\"\"\n    try:\n        username = data['username']\n        password = data['password']\n    except KeyError:\n        return HttpResponseBadRequest('Bad Request.')\n\n    try:\n        # Decode the base64 encoded password\n        decoded_password = base64.b64decode(password).decode()\n    except (binascii.Error, UnicodeDecodeError):\n        return HttpResponseBadRequest('Bad Request.')\n\n    # Expected username and password hash\n    expected_username = 'admin'\n    expected_password_hash = hashlib.sha256('password'.encode()).hexdigest()\n\n    # Check if the credentials match\n    if username == expected_username and decoded_password == expected_password_hash:\n        return HttpResponse('Login successful.', status=200)\n    else:\n        return HttpResponse('Login failed.', status=401)", "raw_solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\n\ndef task_func(data):\n    \"\"\"\n    This function is designed to authenticate users in a web application context. It expects a dictionary with 'username' and 'password' keys.\n    The password is expected to be a base64-encoded SHA-256 hash. The function decodes and verifies the credentials, returning an appropriate HTTP response.\n    \"\"\"\n    try:\n        username = data['username']\n        password = data['password']\n    except KeyError:\n        return HttpResponseBadRequest('Bad Request.')\n\n    try:\n        # Decode the base64 encoded password\n        decoded_password = base64.b64decode(password).decode()\n    except (binascii.Error, UnicodeDecodeError):\n        return HttpResponseBadRequest('Bad Request.')\n\n    # Expected username and password hash\n    expected_username = 'admin'\n    expected_password_hash = hashlib.sha256('password'.encode()).hexdigest()\n\n    # Check if the credentials match\n    if username == expected_username and decoded_password == expected_password_hash:\n        return HttpResponse('Login successful.', status=200)\n    else:\n        return HttpResponse('Login failed.', status=401)\n```\n"}
{"task_id": "BigCodeBench/78", "solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse\ndef task_func(request, header, csv_data):\n    # Create an in-memory output stream\n    output = io.StringIO()\n    \n    # Create a CSV writer object\n    writer = csv.writer(output)\n    \n    # Write the header\n    writer.writerow(header)\n    \n    # Write the data\n    for row in csv_data:\n        writer.writerow(row)\n    \n    # Reset the stream position to the beginning\n    output.seek(0)\n    \n    # Create a FileResponse with the CSV data\n    response = FileResponse(output, as_attachment=True, filename='data.csv')\n    \n    return response", "raw_solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse\n\ndef task_func(request, header, csv_data):\n    # Create an in-memory output stream\n    output = io.StringIO()\n    \n    # Create a CSV writer object\n    writer = csv.writer(output)\n    \n    # Write the header\n    writer.writerow(header)\n    \n    # Write the data\n    for row in csv_data:\n        writer.writerow(row)\n    \n    # Reset the stream position to the beginning\n    output.seek(0)\n    \n    # Create a FileResponse with the CSV data\n    response = FileResponse(output, as_attachment=True, filename='data.csv')\n    \n    return response\n```\n"}
{"task_id": "BigCodeBench/79", "solution": "import zipfile\nimport io\nfrom django.http import FileResponse, HttpRequest\nfrom django.conf import settings\ndef task_func(request, file_paths):\n    \"\"\"\n    Django HttpRequest ke liye ek ZIP file response banata hai, diye gaye files ko zip karta hai. Yeh function useful hai\n    un scenarios ke liye jahaan web request ke response mein multiple file downloads ki zaroorat hoti hai. actual HttpRequest\n    ka prayog function ke andar nahi kiya jaata but Django view structures ke saath compatibility ke liye zaroori hai.\n\n    Parameters:\n    - request (HttpRequest): incoming Django HttpRequest, function mein istemal nahi hota hai.\n    - file_paths (list of str): Zip mein import kiye jaane wale file_paths or file contents ki list.\n\n    Returns:\n        fileresponse: Ek Django fileresponse object jismein ZIP file ek attachment ke taur par import hota hai.\n\n    Requirements:\n    - django.http\n    - django.conf\n    - zipfile\n    - io\n\n    Example:\n    >>> from django.conf import settings\n    >>> if not settings.configured:\n    ...     settings.configure()  # Add minimal necessary settings\n    >>> from django.http import HttpRequest\n    >>> request = HttpRequest()\n    >>> response = task_func(request)\n    >>> response['Content-Type']\n    'application/zip'\n    >>> request = HttpRequest()\n    >>> response = task_func(request)\n    >>> response['Content-Disposition']\n    'attachment; filename=\"files.zip\"'\n\n    \"\"\"\n    # Create an in-memory bytes buffer\n    buffer = io.BytesIO()\n    \n    # Create a zip file in the buffer\n    with zipfile.ZipFile(buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for file_path in file_paths:\n            # Add each file to the zip file\n            zip_file.writestr(file_path, 'This is a placeholder content for the file')\n\n    # Reset buffer position to the beginning\n    buffer.seek(0)\n\n    # Create a FileResponse with the zip file\n    response = FileResponse(buffer, as_attachment=True, filename='files.zip')\n\n    # Set the appropriate headers\n    response['Content-Type'] = 'application/zip'\n    response['Content-Disposition'] = 'attachment; filename=\"files.zip\"'\n\n    return response", "raw_solution": "import zipfile\nimport io\nfrom django.http import FileResponse, HttpRequest\nfrom django.conf import settings\n\ndef task_func(request, file_paths):\n    \"\"\"\n    Django HttpRequest ke liye ek ZIP file response banata hai, diye gaye files ko zip karta hai. Yeh function useful hai\n    un scenarios ke liye jahaan web request ke response mein multiple file downloads ki zaroorat hoti hai. actual HttpRequest\n    ka prayog function ke andar nahi kiya jaata but Django view structures ke saath compatibility ke liye zaroori hai.\n\n    Parameters:\n    - request (HttpRequest): incoming Django HttpRequest, function mein istemal nahi hota hai.\n    - file_paths (list of str): Zip mein import kiye jaane wale file_paths or file contents ki list.\n\n    Returns:\n        fileresponse: Ek Django fileresponse object jismein ZIP file ek attachment ke taur par import hota hai.\n\n    Requirements:\n    - django.http\n    - django.conf\n    - zipfile\n    - io\n\n    Example:\n    >>> from django.conf import settings\n    >>> if not settings.configured:\n    ...     settings.configure()  # Add minimal necessary settings\n    >>> from django.http import HttpRequest\n    >>> request = HttpRequest()\n    >>> response = task_func(request)\n    >>> response['Content-Type']\n    'application/zip'\n    >>> request = HttpRequest()\n    >>> response = task_func(request)\n    >>> response['Content-Disposition']\n    'attachment; filename=\"files.zip\"'\n\n    \"\"\"\n    # Create an in-memory bytes buffer\n    buffer = io.BytesIO()\n    \n    # Create a zip file in the buffer\n    with zipfile.ZipFile(buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for file_path in file_paths:\n            # Add each file to the zip file\n            zip_file.writestr(file_path, 'This is a placeholder content for the file')\n\n    # Reset buffer position to the beginning\n    buffer.seek(0)\n\n    # Create a FileResponse with the zip file\n    response = FileResponse(buffer, as_attachment=True, filename='files.zip')\n\n    # Set the appropriate headers\n    response['Content-Type'] = 'application/zip'\n    response['Content-Disposition'] = 'attachment; filename=\"files.zip\"'\n\n    return response\n```\n"}
{"task_id": "BigCodeBench/80", "solution": "from flask import Flask, render_template, request\nimport json\nimport logging\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['GET', 'POST'])\n    def root():\n        if request.method == 'POST':\n            data = request.get_json()\n            logging.info(f\"Received POST data: {json.dumps(data)}\")\n            return render_template('index.html', data=data)\n        return render_template('index.html')\n\n    return app", "raw_solution": "from flask import Flask, render_template, request\nimport json\nimport logging\n\nlogging.basicConfig(filename=\"out.log\", level=logging.INFO)\n\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['GET', 'POST'])\n    def root():\n        if request.method == 'POST':\n            data = request.get_json()\n            logging.info(f\"Received POST data: {json.dumps(data)}\")\n            return render_template('index.html', data=data)\n        return render_template('index.html')\n\n    return app\n```\nThis script creates a Flask application that handles both GET and POST requests at the root URL. It logs the POST request data in JSON format to a file named \"out.log\" and renders an 'index.html' template with the received data. The `render_template` function is used to render the HTML template, and the `request.get_json()` function is used to parse the JSON data from the POST request.\n"}
{"task_id": "BigCodeBench/81", "solution": "from flask import Flask, render_template\nfrom flask_restful import Resource, Api\nimport requests\ndef task_func(api_url, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class Data(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return response.json()\n\n    api.add_resource(Data, '/data')\n\n    return app", "raw_solution": "from flask import Flask, render_template\nfrom flask_restful import Resource, Api\nimport requests\n\ndef task_func(api_url, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class Data(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return response.json()\n\n    api.add_resource(Data, '/data')\n\n    return app\n\n# Example usage:\n# app = task_func('https://api.example.com/data', 'templates')\n# app.run(debug=True)\n```\nThis script creates a Flask application with a single endpoint `/data` that fetches data from an external API and returns it as JSON. The `render_template` function is used to render templates from the specified `template_folder`.\n\nPlease note that the `requests` library is used to make HTTP requests to the external API, and the `flask_restful` library is used to create a RESTful API with Flask. Make sure to install these libraries using pip if they are not already installed:\n\n```bash\npip install flask flask_restful requests\n```\n\nThe example usage at the end of the script shows how to call the `task_func` function and run the Flask application. The `debug=True` argument in `app.run()` enables debug mode, which is useful for development but should be turned off for production.\n"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_FOLDER'] = template_folder\n\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    class User(UserMixin):\n        def __init__(self, username, password):\n            self.id = username\n            self.password = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        # Dummy user database\n        users = {'user1': User('user1', 'password1')}\n        if user_id in users:\n            return users[user_id]\n        return None\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = load_user(form.username.data)\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n            return 'Invalid username or password'\n        return render_template('login.html', form=form)\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello, {current_user.id}!'\n\n    @app.route('/logout')\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    return app", "raw_solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_FOLDER'] = template_folder\n\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    class User(UserMixin):\n        def __init__(self, username, password):\n            self.id = username\n            self.password = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        # Dummy user database\n        users = {'user1': User('user1', 'password1')}\n        if user_id in users:\n            return users[user_id]\n        return None\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = load_user(form.username.data)\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n            return 'Invalid username or password'\n        return render_template('login.html', form=form)\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello, {current_user.id}!'\n\n    @app.route('/logout')\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    return app\n```\n"}
{"task_id": "BigCodeBench/83", "solution": "from flask import Flask\nfrom flask_mail import Mail, Message\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__)\n    app.config['MAIL_SERVER'] = smtp_server\n    app.config['MAIL_PORT'] = smtp_port\n    app.config['MAIL_USERNAME'] = smtp_user\n    app.config['MAIL_PASSWORD'] = smtp_password\n    app.config['MAIL_USE_TLS'] = True\n    app.config['MAIL_USE_SSL'] = False\n    app.config['MAIL_TEMPLATE_FOLDER'] = template_folder\n\n    mail = Mail(app)\n\n    @app.route('/send_email')\n    def send_email():\n        msg = Message(subject=\"Hello from Flask\",\n                      sender=smtp_user,\n                      recipients=[\"recipient@example.com\"],\n                      body=\"This is a test email sent from a Flask app using Flask-Mail.\")\n        mail.send(msg)\n        return \"Email sent!\"\n\n    return app", "raw_solution": "from flask import Flask\nfrom flask_mail import Mail, Message\n\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__)\n    app.config['MAIL_SERVER'] = smtp_server\n    app.config['MAIL_PORT'] = smtp_port\n    app.config['MAIL_USERNAME'] = smtp_user\n    app.config['MAIL_PASSWORD'] = smtp_password\n    app.config['MAIL_USE_TLS'] = True\n    app.config['MAIL_USE_SSL'] = False\n    app.config['MAIL_TEMPLATE_FOLDER'] = template_folder\n\n    mail = Mail(app)\n\n    @app.route('/send_email')\n    def send_email():\n        msg = Message(subject=\"Hello from Flask\",\n                      sender=smtp_user,\n                      recipients=[\"recipient@example.com\"],\n                      body=\"This is a test email sent from a Flask app using Flask-Mail.\")\n        mail.send(msg)\n        return \"Email sent!\"\n\n    return app\n\n# Example usage:\n# app = task_func('smtp.example.com', 587, 'user@example.com', 'password', 'templates')\n# app.run(debug=True)\n```\nThis script sets up a Flask application with the necessary configuration for sending emails using Flask-Mail. It also defines a route that sends a test email when accessed. The example usage at the end demonstrates how to run the application.\n"}
{"task_id": "BigCodeBench/84", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise TypeError(\"products must be a list of strings.\")\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer.\")\n    if not (isinstance(sales_lower, int) and isinstance(sales_upper, int)) or sales_lower > sales_upper:\n        raise ValueError(\"sales_lower must be less than or equal to sales_upper.\")\n    if not (isinstance(profit_margin_min, float) and isinstance(profit_margin_max, float)) or profit_margin_min > profit_margin_max:\n        raise ValueError(\"profit_margin_min must be less than or equal to profit_margin_max.\")\n\n    np.random.seed(random_seed)\n    sales_data = {\n        \"Product\": np.random.choice(products, n_samples),\n        \"Sales\": np.random.randint(sales_lower, sales_upper, n_samples),\n        \"Profit\": np.random.uniform(profit_margin_min, profit_margin_max, n_samples) * np.random.randint(sales_lower, sales_upper, n_samples)\n    }\n    df = pd.DataFrame(sales_data)\n    report = df.groupby(\"Product\").agg({\"Sales\": \"sum\", \"Profit\": \"sum\"}).reset_index()\n    report = report.sort_values(by=\"Profit\", ascending=False)\n    return report\nproducts = [\"iPhone\", \"ipad\", \"Macbook\", \"airpods\", \"Apple Watch\"]", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise TypeError(\"products must be a list of strings.\")\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer.\")\n    if not (isinstance(sales_lower, int) and isinstance(sales_upper, int)) or sales_lower > sales_upper:\n        raise ValueError(\"sales_lower must be less than or equal to sales_upper.\")\n    if not (isinstance(profit_margin_min, float) and isinstance(profit_margin_max, float)) or profit_margin_min > profit_margin_max:\n        raise ValueError(\"profit_margin_min must be less than or equal to profit_margin_max.\")\n\n    np.random.seed(random_seed)\n    sales_data = {\n        \"Product\": np.random.choice(products, n_samples),\n        \"Sales\": np.random.randint(sales_lower, sales_upper, n_samples),\n        \"Profit\": np.random.uniform(profit_margin_min, profit_margin_max, n_samples) * np.random.randint(sales_lower, sales_upper, n_samples)\n    }\n    df = pd.DataFrame(sales_data)\n    report = df.groupby(\"Product\").agg({\"Sales\": \"sum\", \"Profit\": \"sum\"}).reset_index()\n    report = report.sort_values(by=\"Profit\", ascending=False)\n    return report\n\n# Example usage:\nproducts = [\"iPhone\", \"ipad\", \"Macbook\", \"airpods\", \"Apple Watch\"]\nreport = task_func(products, n_samples=50, sales_lower=100, sales_upper=150, profit_margin_min=0.2, profit_margin_max=0.4, random_seed=42)"}
{"task_id": "BigCodeBench/85", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, random_seed=42):\n    if end_date < start_date:\n        raise ValueError(\"End date must be after start date.\")\n\n    np.random.seed(random_seed)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    num_days = len(date_range)\n\n    temperature = np.random.uniform(low=-10, high=40, size=num_days)\n    humidity = np.random.uniform(low=20, high=100, size=num_days)\n    wind_speed = np.random.uniform(low=0, high=20, size=num_days)\n\n    data = pd.DataFrame({\n        'Date': date_range,\n        'Temperature': temperature,\n        'Humidity': humidity,\n        'Wind Speed': wind_speed\n    })\n\n    fig, ax = plt.subplots()\n    ax.plot(data['Date'], data['Temperature'], label='Temperature')\n    ax.plot(data['Date'], data['Humidity'], label='Humidity')\n    ax.plot(data['Date'], data['Wind Speed'], label='Wind Speed')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Value')\n    ax.set_title('Weather Data')\n    ax.legend()\n\n    return data, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date, end_date, random_seed=42):\n    if end_date < start_date:\n        raise ValueError(\"End date must be after start date.\")\n\n    np.random.seed(random_seed)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    num_days = len(date_range)\n\n    temperature = np.random.uniform(low=-10, high=40, size=num_days)\n    humidity = np.random.uniform(low=20, high=100, size=num_days)\n    wind_speed = np.random.uniform(low=0, high=20, size=num_days)\n\n    data = pd.DataFrame({\n        'Date': date_range,\n        'Temperature': temperature,\n        'Humidity': humidity,\n        'Wind Speed': wind_speed\n    })\n\n    fig, ax = plt.subplots()\n    ax.plot(data['Date'], data['Temperature'], label='Temperature')\n    ax.plot(data['Date'], data['Humidity'], label='Humidity')\n    ax.plot(data['Date'], data['Wind Speed'], label='Wind Speed')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Value')\n    ax.set_title('Weather Data')\n    ax.legend()\n\n    return data, ax\n```\n"}
{"task_id": "BigCodeBench/86", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    np.random.seed(seed)\n    scores = np.random.randint(0, 100, size=len(students))\n    df = pd.DataFrame({'Student': students, 'Score': scores})\n    df = df.sort_values(by='Score')\n\n    ax = df.plot.bar(x='Student', y='Score', legend=False)\n    ax.set_ylabel('Score')\n    ax.set_title('Student Scores')\n\n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    np.random.seed(seed)\n    scores = np.random.randint(0, 100, size=len(students))\n    df = pd.DataFrame({'Student': students, 'Score': scores})\n    df = df.sort_values(by='Score')\n\n    ax = df.plot.bar(x='Student', y='Score', legend=False)\n    ax.set_ylabel('Score')\n    ax.set_title('Student Scores')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/87", "solution": "import pandas as pd\nfrom random import choices, seed\ndef task_func(products, ratings, weights, random_seed=42):\n    seed(random_seed)\n    selected_ratings = choices(ratings, weights, k=len(products))\n    df = pd.DataFrame({'Product': products, 'Rating': selected_ratings})\n    df = df.sort_values(by='Rating', ascending=False)\n    return df\nproducts = [\"iPhone\", \"iPad\", \"Macbook\", \"Airpods\", \"Apple Watch\"]\nratings = [1, 2, 3, 4, 5]\nweights = [0.05, 0.1, 0.2, 0.3, 0.35]", "raw_solution": "import pandas as pd\nfrom random import choices, seed\n\ndef task_func(products, ratings, weights, random_seed=42):\n    seed(random_seed)\n    selected_ratings = choices(ratings, weights, k=len(products))\n    df = pd.DataFrame({'Product': products, 'Rating': selected_ratings})\n    df = df.sort_values(by='Rating', ascending=False)\n    return df\n\n# Example usage:\nproducts = [\"iPhone\", \"iPad\", \"Macbook\", \"Airpods\", \"Apple Watch\"]\nratings = [1, 2, 3, 4, 5]\nweights = [0.05, 0.1, 0.2, 0.3, 0.35]\ndf = task_func(products, ratings, weights, 42)"}
{"task_id": "BigCodeBench/88", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\ndef task_func(start_date, end_date, seed=42):\n    np.random.seed(seed)\n    \n    # Generate date range\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random sales data\n    sales_data = np.random.randint(0, 500, size=len(date_range))\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Date': date_range,\n        'Sales': sales_data\n    })\n    \n    # Plot sales data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax\nstart_date = datetime(2021, 1, 1)\nend_date = datetime(2021, 12, 31)", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\ndef task_func(start_date, end_date, seed=42):\n    np.random.seed(seed)\n    \n    # Generate date range\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random sales data\n    sales_data = np.random.randint(0, 500, size=len(date_range))\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Date': date_range,\n        'Sales': sales_data\n    })\n    \n    # Plot sales data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax\n\n# Example usage\nstart_date = datetime(2021, 1, 1)\nend_date = datetime(2021, 12, 31)\ndata, plot = task_func(start_date, end_date)"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Calculate z-scores for the specified column\n    z_scores = np.abs(stats.zscore(standardized_data[:, column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers from the data\n    filtered_data = np.delete(standardized_data, outliers, axis=0)\n\n    # Plotting the original data with outliers\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.scatter(standardized_data[:, column], z_scores, label='Data with Outliers')\n    plt.axhline(outlier_z_score, color='r', linestyle='--', label=f'Z-score Threshold = {outlier_z_score}')\n    plt.xlabel('Column Value')\n    plt.ylabel('Z-score')\n    plt.title('Data with Outliers')\n    plt.legend()\n\n    # Plotting the data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(filtered_data[:, column], np.zeros_like(filtered_data[:, column]), color='g', label='Data without Outliers')\n    plt.xlabel('Column Value')\n    plt.title('Data without Outliers')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    return standardized_data, filtered_data, outliers", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Calculate z-scores for the specified column\n    z_scores = np.abs(stats.zscore(standardized_data[:, column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers from the data\n    filtered_data = np.delete(standardized_data, outliers, axis=0)\n\n    # Plotting the original data with outliers\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.scatter(standardized_data[:, column], z_scores, label='Data with Outliers')\n    plt.axhline(outlier_z_score, color='r', linestyle='--', label=f'Z-score Threshold = {outlier_z_score}')\n    plt.xlabel('Column Value')\n    plt.ylabel('Z-score')\n    plt.title('Data with Outliers')\n    plt.legend()\n\n    # Plotting the data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(filtered_data[:, column], np.zeros_like(filtered_data[:, column]), color='g', label='Data without Outliers')\n    plt.xlabel('Column Value')\n    plt.title('Data without Outliers')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    return standardized_data, filtered_data, outliers\n```\n"}
{"task_id": "BigCodeBench/90", "solution": "import numpy as np\nimport math\ndef task_func(data, target, k):\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n\n    # Calculate distances from the target point to all points in the dataset\n    distances = []\n    for point in data.itertuples(index=False):\n        distance = math.sqrt((point[0] - target[0])**2 + (point[1] - target[1])**2)\n        distances.append((distance, point))\n\n    # Sort the distances and select the first k points\n    k_nearest_neighbors = [point for _, point in sorted(distances)[:k]]\n\n    return k_nearest_neighbors", "raw_solution": "import numpy as np\nimport math\n\ndef task_func(data, target, k):\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n\n    # Calculate distances from the target point to all points in the dataset\n    distances = []\n    for point in data.itertuples(index=False):\n        distance = math.sqrt((point[0] - target[0])**2 + (point[1] - target[1])**2)\n        distances.append((distance, point))\n\n    # Sort the distances and select the first k points\n    k_nearest_neighbors = [point for _, point in sorted(distances)[:k]]\n\n    return k_nearest_neighbors\n\n# Example usage:\n# data = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Latitude', 'Longitude'])\n# target = [10, 15]\n# k = 2\n# print(task_func(data, target, k))  # Output: [[7, 8], [14, 25]]\n```\n"}
{"task_id": "BigCodeBench/91", "solution": "import pandas as pd\nfrom scipy.stats import linregress\nimport matplotlib.pyplot as plt\ndef task_func(data, column1, column2):\n    if column1 not in data.columns or column2 not in data.columns:\n        raise ValueError(\"Specified columns must exist in the DataFrame\")\n\n    x = data[column1]\n    y = data[column2]\n\n    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n\n    fig, ax = plt.subplots()\n    ax.scatter(x, y, label='Data points')\n    ax.plot(x, slope*x + intercept, color='red', label='Fitted line')\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.set_title('Linear Regression')\n    ax.legend()\n\n    return (slope, intercept, r_value, p_value, std_err), ax\ndata = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])", "raw_solution": "import pandas as pd\nfrom scipy.stats import linregress\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column1, column2):\n    if column1 not in data.columns or column2 not in data.columns:\n        raise ValueError(\"Specified columns must exist in the DataFrame\")\n\n    x = data[column1]\n    y = data[column2]\n\n    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n\n    fig, ax = plt.subplots()\n    ax.scatter(x, y, label='Data points')\n    ax.plot(x, slope*x + intercept, color='red', label='Fitted line')\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.set_title('Linear Regression')\n    ax.legend()\n\n    return (slope, intercept, r_value, p_value, std_err), ax\n\n# Example usage:\ndata = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])\nresult, ax = task_func(data, 'Column1', 'Column2')\n```\n"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data should be a pandas DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"n_clusters should be an integer greater than 1.\")\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    labels = kmeans.fit_predict(data)\n    \n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis')\n    centers = kmeans.cluster_centers_\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n    \n    ax.set_title('K-Means Clustering')\n    ax.set_xlabel('Feature1')\n    ax.set_ylabel('Feature2')\n    \n    return labels, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data should be a pandas DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"n_clusters should be an integer greater than 1.\")\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    labels = kmeans.fit_predict(data)\n    \n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis')\n    centers = kmeans.cluster_centers_\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n    \n    ax.set_title('K-Means Clustering')\n    ax.set_xlabel('Feature1')\n    ax.set_ylabel('Feature2')\n    \n    return labels, ax\n```\n"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n    \n    # Initialize PCA\n    pca = PCA(n_components=n_components)\n    \n    # Fit and transform the data\n    principal_components = pca.fit_transform(data)\n    \n    # Create a DataFrame with the principal components\n    pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n    \n    # Create a scatter plot of the first two principal components\n    fig, ax = plt.subplots()\n    ax.scatter(pca_df['PC1'], pca_df['PC2'])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA - Principal Component Analysis')\n    \n    return pca_df, ax\ndata = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n    \n    # Initialize PCA\n    pca = PCA(n_components=n_components)\n    \n    # Fit and transform the data\n    principal_components = pca.fit_transform(data)\n    \n    # Create a DataFrame with the principal components\n    pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n    \n    # Create a scatter plot of the first two principal components\n    fig, ax = plt.subplots()\n    ax.scatter(pca_df['PC1'], pca_df['PC2'])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA - Principal Component Analysis')\n    \n    return pca_df, ax\n\n# Example usage:\ndata = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])\ntransformed_data, plot = task_func(data)\n```\n"}
{"task_id": "BigCodeBench/94", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, num_samples):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, num_samples)\n\n    # Create a histogram of the samples\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the PDF of the normal distribution\n    pdf = norm.pdf(bins, mean, std_dev)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(f\"Fit results: mean = {mean:.2f},  std = {std_dev:.2f}\")\n\n    return samples, fig", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, num_samples):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, num_samples)\n\n    # Create a histogram of the samples\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the PDF of the normal distribution\n    pdf = norm.pdf(bins, mean, std_dev)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(f\"Fit results: mean = {mean:.2f},  std = {std_dev:.2f}\")\n\n    return samples, fig\n```\n"}
{"task_id": "BigCodeBench/95", "solution": "import pandas as pd\nfrom random import randint, uniform, seed\ndef task_func(categories=None, months=None, random_seed=42):\n    if categories is None:\n        categories = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Beauty & Personal Care']\n    if months is None:\n        months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n    \n    if not isinstance(categories, list) or not isinstance(months, list):\n        raise ValueError(\"Both 'categories' and 'months' must be lists.\")\n    \n    if not all(isinstance(item, str) for item in categories) or not all(isinstance(item, str) for item in months):\n        raise ValueError(\"All items in 'categories' and 'months' must be strings.\")\n    \n    seed(random_seed)\n    data = []\n    for month in months:\n        for category in categories:\n            sales = randint(100, 500) + uniform(0, 1)\n            data.append([month, category, sales])\n    \n    df = pd.DataFrame(data, columns=['Month', 'Category', 'Sales'])\n    return df", "raw_solution": "import pandas as pd\nfrom random import randint, uniform, seed\n\ndef task_func(categories=None, months=None, random_seed=42):\n    if categories is None:\n        categories = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Beauty & Personal Care']\n    if months is None:\n        months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n    \n    if not isinstance(categories, list) or not isinstance(months, list):\n        raise ValueError(\"Both 'categories' and 'months' must be lists.\")\n    \n    if not all(isinstance(item, str) for item in categories) or not all(isinstance(item, str) for item in months):\n        raise ValueError(\"All items in 'categories' and 'months' must be strings.\")\n    \n    seed(random_seed)\n    data = []\n    for month in months:\n        for category in categories:\n            sales = randint(100, 500) + uniform(0, 1)\n            data.append([month, category, sales])\n    \n    df = pd.DataFrame(data, columns=['Month', 'Category', 'Sales'])\n    return df\n\n# Example usage:\n# report = task_func()\n# print(report.head())\n```\n"}
{"task_id": "BigCodeBench/96", "solution": "import csv\nfrom collections import Counter\nimport operator\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, newline='') as csvfile:\n        reader = csv.reader(csvfile, delimiter=csv_delimiter)\n        words = []\n        for row in reader:\n            words.extend(row)\n        \n        word_counts = Counter(words)\n        sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n        \n        return sorted_word_counts", "raw_solution": "import csv\nfrom collections import Counter\nimport operator\n\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, newline='') as csvfile:\n        reader = csv.reader(csvfile, delimiter=csv_delimiter)\n        words = []\n        for row in reader:\n            words.extend(row)\n        \n        word_counts = Counter(words)\n        sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n        \n        return sorted_word_counts\n\n# Example usage:\n# Assuming 'temp_data.csv' is a valid CSV file in the same directory as this script.\n# result = task_func('temp_data.csv', ',')\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/97", "solution": "import math\nimport itertools\nfrom functools import reduce\ndef task_func(numbers):\n    \"\"\"\n    This function generates all possible combinations of the given numbers with different lengths,\n    calculates the product of each combination, computes the logarithm of each product, and finally\n    sums up all the logarithms to get the final result.\n\n    Parameters:\n    - numbers (list of int): A list of integers from which combinations are generated.\n\n    Returns:\n        float: The sum of the logarithms of all combinations' products.\n\n    Requirements:\n    - math\n    - itertools\n    - functools\n\n    Example:\n    >>> numbers = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n    >>> type(task_func(numbers)) == float\n    True\n    >>> isinstance(task_func(numbers), float)\n    True\n    \"\"\"\n    total_log_sum = 0.0\n    for r in range(1, len(numbers) + 1):\n        for combination in itertools.combinations(numbers, r):\n            product = reduce(lambda x, y: x * y, combination)\n            log_sum = sum(math.log(product) for _ in combination)\n            total_log_sum += log_sum\n    return total_log_sum", "raw_solution": "import math\nimport itertools\nfrom functools import reduce\n\ndef task_func(numbers):\n    \"\"\"\n    This function generates all possible combinations of the given numbers with different lengths,\n    calculates the product of each combination, computes the logarithm of each product, and finally\n    sums up all the logarithms to get the final result.\n\n    Parameters:\n    - numbers (list of int): A list of integers from which combinations are generated.\n\n    Returns:\n        float: The sum of the logarithms of all combinations' products.\n\n    Requirements:\n    - math\n    - itertools\n    - functools\n\n    Example:\n    >>> numbers = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n    >>> type(task_func(numbers)) == float\n    True\n    >>> isinstance(task_func(numbers), float)\n    True\n    \"\"\"\n    total_log_sum = 0.0\n    for r in range(1, len(numbers) + 1):\n        for combination in itertools.combinations(numbers, r):\n            product = reduce(lambda x, y: x * y, combination)\n            log_sum = sum(math.log(product) for _ in combination)\n            total_log_sum += log_sum\n    return total_log_sum\n```\n"}
{"task_id": "BigCodeBench/98", "solution": "import random\nimport string\nfrom collections import Counter\ndef task_func(num_strings, string_length):\n    random_strings = [''.join(random.choices(string.ascii_lowercase, k=string_length)) for _ in range(num_strings)]\n    \n    all_characters = ''.join(random_strings)\n    character_count = Counter(all_characters)\n    \n    sorted_character_count = sorted(character_count.items(), key=lambda item: item[1], reverse=True)\n    \n    return sorted_character_count", "raw_solution": "import random\nimport string\nfrom collections import Counter\n\ndef task_func(num_strings, string_length):\n    random_strings = [''.join(random.choices(string.ascii_lowercase, k=string_length)) for _ in range(num_strings)]\n    \n    all_characters = ''.join(random_strings)\n    character_count = Counter(all_characters)\n    \n    sorted_character_count = sorted(character_count.items(), key=lambda item: item[1], reverse=True)\n    \n    return sorted_character_count\n\n# Test cases"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the Iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Set the global font to Arial\n    plt.rc('font', family='Arial')\n\n    # Create a pair plot using seaborn\n    pair_plot = sns.pairplot(df, hue='species', palette='Set2')\n\n    # Set the title and labels\n    pair_plot.fig.suptitle('Iris dataset pair plot', y=1.02)\n    pair_plot.set(xlabel='sepal length (cm)', ylabel='sepal width (cm)')\n    pair_plot.set(xlabel='petal length (cm)', ylabel='petal width (cm)')\n\n    return pair_plot.fig", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load the Iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Set the global font to Arial\n    plt.rc('font', family='Arial')\n\n    # Create a pair plot using seaborn\n    pair_plot = sns.pairplot(df, hue='species', palette='Set2')\n\n    # Set the title and labels\n    pair_plot.fig.suptitle('Iris dataset pair plot', y=1.02)\n    pair_plot.set(xlabel='sepal length (cm)', ylabel='sepal width (cm)')\n    pair_plot.set(xlabel='petal length (cm)', ylabel='petal width (cm)')\n\n    return pair_plot.fig\n\n# Example usage\nfig = task_func()"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        random.seed(seed)\n        dates = [datetime.now() - timedelta(days=i) for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df = df.sort_values('Date')\n\n        fig, ax = plt.subplots()\n        ax.plot(df['Date'], df['Value'], marker='o')\n        ax.set_title('Random Time Series Data')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.grid(True)\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the plot: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    try:\n        random.seed(seed)\n        dates = [datetime.now() - timedelta(days=i) for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df = df.sort_values('Date')\n\n        fig, ax = plt.subplots()\n        ax.plot(df['Date'], df['Value'], marker='o')\n        ax.set_title('Random Time Series Data')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.grid(True)\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the plot: {e}\")\n```\n"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport os\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    try:\n        # Load the Boston Housing dataset\n        boston = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.loadtxt(boston)\n\n        # Create a DataFrame\n        df = pd.DataFrame(data, columns=[f'Feature_{i}' for i in range(data.shape[1])])\n\n        # Set the seed for reproducibility\n        np.random.seed(seed)\n\n        # Create a correlation matrix\n        corr = df.corr()\n\n        # Set the font to Arial\n        plt.rc('font', family='Arial')\n\n        # Create a heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n\n        # Save the plot\n        plt.savefig('boston_corr_heatmap.png')\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while creating the heatmap: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport os\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    try:\n        # Load the Boston Housing dataset\n        boston = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.loadtxt(boston)\n\n        # Create a DataFrame\n        df = pd.DataFrame(data, columns=[f'Feature_{i}' for i in range(data.shape[1])])\n\n        # Set the seed for reproducibility\n        np.random.seed(seed)\n\n        # Create a correlation matrix\n        corr = df.corr()\n\n        # Set the font to Arial\n        plt.rc('font', family='Arial')\n\n        # Create a heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n\n        # Save the plot\n        plt.savefig('boston_corr_heatmap.png')\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while creating the heatmap: {e}\")\n```\n"}
{"task_id": "BigCodeBench/102", "solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\ndef task_func():\n    # Set the font to Arial\n    plt.rcParams['font.sans-serif'] = 'Arial'\n    plt.rcParams['font.family'] = 'sans-serif'\n\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = diabetes.target\n\n    # Create a seaborn pairplot\n    pairplot = sns.pairplot(df)\n\n    # Return the pairplot and the DataFrame\n    return pairplot.fig, df", "raw_solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\n\ndef task_func():\n    # Set the font to Arial\n    plt.rcParams['font.sans-serif'] = 'Arial'\n    plt.rcParams['font.family'] = 'sans-serif'\n\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = diabetes.target\n\n    # Create a seaborn pairplot\n    pairplot = sns.pairplot(df)\n\n    # Return the pairplot and the DataFrame\n    return pairplot.fig, df\n```\n"}
{"task_id": "BigCodeBench/103", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\ndef task_func(temperatures):\n    if not isinstance(temperatures, pd.DataFrame) or temperatures.empty or 'date' not in temperatures.columns or 'temperature' not in temperatures.columns:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame with 'date' and 'temperature' columns.\")\n\n    # Set the font to Arial\n    plt.rcParams['font.sans-serif'] = 'Arial'\n    plt.rcParams['font.family'] = 'sans-serif'\n\n    # Plotting the temperatures\n    fig, ax = plt.subplots()\n    ax.plot(temperatures.index, temperatures['temperature'], label='Temperature')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Temperature (degC)')\n    ax.set_title('Daily Temperatures in New York')\n    ax.legend()\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\n\ndef task_func(temperatures):\n    if not isinstance(temperatures, pd.DataFrame) or temperatures.empty or 'date' not in temperatures.columns or 'temperature' not in temperatures.columns:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame with 'date' and 'temperature' columns.\")\n\n    # Set the font to Arial\n    plt.rcParams['font.sans-serif'] = 'Arial'\n    plt.rcParams['font.family'] = 'sans-serif'\n\n    # Plotting the temperatures\n    fig, ax = plt.subplots()\n    ax.plot(temperatures.index, temperatures['temperature'], label='Temperature')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Temperature (degC)')\n    ax.set_title('Daily Temperatures in New York')\n    ax.legend()\n\n    return ax\n\n# Example usage:\n# temperatures = pd.DataFrame({\n#     'temperature': [random.randint(-10, 30) for _ in range(365)],\n#     'date': pd.date_range(start='01-01-2023', periods=365, tz='America/New_York')\n# }).set_index('date')\n# ax = task_func(temperatures)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/104", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n\n    # Ensure the 'group' column is categorical\n    df['group'] = df['group'].astype('category')\n\n    # Create a cycle of colors for each group\n    colors = cycle(plt.cm.tab10.colors)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    for group in groups:\n        if group not in df['group'].cat.categories:\n            continue\n        group_data = df[df['group'] == group]\n        group_data['date_ordinal'] = group_data['date'].map(pd.Timestamp.toordinal)\n        ax.scatter(group_data['date_ordinal'], group_data['value'], label=group, color=next(colors))\n\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.legend(title='Group')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n\n    # Ensure the 'group' column is categorical\n    df['group'] = df['group'].astype('category')\n\n    # Create a cycle of colors for each group\n    colors = cycle(plt.cm.tab10.colors)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    for group in groups:\n        if group not in df['group'].cat.categories:\n            continue\n        group_data = df[df['group'] == group]\n        group_data['date_ordinal'] = group_data['date'].map(pd.Timestamp.toordinal)\n        ax.scatter(group_data['date_ordinal'], group_data['value'], label=group, color=next(colors))\n\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.legend(title='Group')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/105", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df):\n    if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame must be non-empty and contain 'group', 'date', and 'value' columns.\")\n    \n    try:\n        df['date'] = pd.to_datetime(df['date'])\n    except Exception as e:\n        raise ValueError(f\"Date column must be in datetime format. Error: {e}\")\n\n    df['ordinal'] = pd.to_datetime(df['date']).map(pd.Timestamp.toordinal)\n\n    correlation_matrix = df.corr()\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, ax=ax)\n    ax.set_title('Correlation Matrix')\n\n    pairplot_grid = sns.pairplot(df, hue='group')\n\n    return fig, pairplot_grid", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df):\n    if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame must be non-empty and contain 'group', 'date', and 'value' columns.\")\n    \n    try:\n        df['date'] = pd.to_datetime(df['date'])\n    except Exception as e:\n        raise ValueError(f\"Date column must be in datetime format. Error: {e}\")\n\n    df['ordinal'] = pd.to_datetime(df['date']).map(pd.Timestamp.toordinal)\n\n    correlation_matrix = df.corr()\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, ax=ax)\n    ax.set_title('Correlation Matrix')\n\n    pairplot_grid = sns.pairplot(df, hue='group')\n\n    return fig, pairplot_grid\n\n# Example usage:\n# df = pd.DataFrame({\n#     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n#     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n#     \"value\": [10, 20, 16, 31, 56],\n# })\n# heatmap_fig, pairplot_grid = task_func(df)\n```\n"}
{"task_id": "BigCodeBench/106", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"Invalid 'df': 'date' column must be in datetime format.\")\n    \n    df['date_ordinal'] = df['date'].map(pd.Timestamp.toordinal)\n    \n    # Group data by 'group' and perform linear regression on each group\n    grouped = df.groupby('group')\n    models = []\n    predictions = []\n    \n    fig, ax = plt.subplots()\n    \n    for name, group in grouped:\n        X = group[['date_ordinal']].values\n        y = group['value'].values\n        \n        model = LinearRegression()\n        model.fit(X, y)\n        models.append(model)\n        \n        # Predict values\n        X_new = pd.DataFrame({'date_ordinal': pd.to_datetime(group['date']).map(pd.Timestamp.toordinal).values})\n        y_pred = model.predict(X_new)\n        \n        # Plot original and predicted values\n        ax.scatter(group['date_ordinal'], group['value'], label=f'Group {name} Original')\n        ax.plot(group['date_ordinal'], y_pred, color='red', label=f'Group {name} Predicted')\n        \n        predictions.append(y_pred)\n    \n    ax.set_title('Value vs date (Linear Regression Prediction)')\n    ax.set_xlabel('date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    return models, predictions, ax", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"Invalid 'df': 'date' column must be in datetime format.\")\n    \n    df['date_ordinal'] = df['date'].map(pd.Timestamp.toordinal)\n    \n    # Group data by 'group' and perform linear regression on each group\n    grouped = df.groupby('group')\n    models = []\n    predictions = []\n    \n    fig, ax = plt.subplots()\n    \n    for name, group in grouped:\n        X = group[['date_ordinal']].values\n        y = group['value'].values\n        \n        model = LinearRegression()\n        model.fit(X, y)\n        models.append(model)\n        \n        # Predict values\n        X_new = pd.DataFrame({'date_ordinal': pd.to_datetime(group['date']).map(pd.Timestamp.toordinal).values})\n        y_pred = model.predict(X_new)\n        \n        # Plot original and predicted values\n        ax.scatter(group['date_ordinal'], group['value'], label=f'Group {name} Original')\n        ax.plot(group['date_ordinal'], y_pred, color='red', label=f'Group {name} Predicted')\n        \n        predictions.append(y_pred)\n    \n    ax.set_title('Value vs date (Linear Regression Prediction)')\n    ax.set_xlabel('date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    return models, predictions, ax\n```\n"}
{"task_id": "BigCodeBench/107", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(df, n_clusters=3, random_state=0):\n    if df.empty or not all(col in df.columns for col in [\"group\", \"date\", \"value\"]):\n        raise ValueError(\"DataFrame should not be empty and should contain 'group', 'date', and 'value' columns.\")\n\n    df['date'] = pd.to_datetime(df['date'])\n    df['date_ordinal'] = df['date'].map(pd.Timestamp.toordinal)\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    df['cluster'] = kmeans.fit_predict(df[['date_ordinal', 'value']])\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['date_ordinal'], df['value'], c=df['cluster'], cmap='viridis')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df, n_clusters=3, random_state=0):\n    if df.empty or not all(col in df.columns for col in [\"group\", \"date\", \"value\"]):\n        raise ValueError(\"DataFrame should not be empty and should contain 'group', 'date', and 'value' columns.\")\n\n    df['date'] = pd.to_datetime(df['date'])\n    df['date_ordinal'] = df['date'].map(pd.Timestamp.toordinal)\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    df['cluster'] = kmeans.fit_predict(df[['date_ordinal', 'value']])\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['date_ordinal'], df['value'], c=df['cluster'], cmap='viridis')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n#     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n#     \"value\": [10, 20, 16, 31, 56]\n# })\n# ax = task_func(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'date' and 'value' columns.\")\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"The 'date' column must be of datetime type.\")\n    if freq not in ['D', 'W', 'M', 'Q', 'Y']:\n        raise ValueError(\"Invalid frequency string.\")\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Decomposition model must be either 'additive' or 'multiplicative'.\")\n\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, period=freq)\n    fig, ax = plt.subplots()\n    decomposition.plot(ax=ax)\n    ax.set_title('Time Series Decomposition')\n    ax.set_ylabel('Value')\n\n    return decomposition, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'date' and 'value' columns.\")\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"The 'date' column must be of datetime type.\")\n    if freq not in ['D', 'W', 'M', 'Q', 'Y']:\n        raise ValueError(\"Invalid frequency string.\")\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Decomposition model must be either 'additive' or 'multiplicative'.\")\n\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, period=freq)\n    fig, ax = plt.subplots()\n    decomposition.plot(ax=ax)\n    ax.set_title('Time Series Decomposition')\n    ax.set_ylabel('Value')\n\n    return decomposition, ax\n```\n"}
{"task_id": "BigCodeBench/109", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, items=None, locations=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df is not a DataFrame.\")\n    \n    if 'items' not in df.columns or 'location' not in df.columns:\n        raise ValueError(\"The DataFrame must contain 'items' and 'location' columns.\")\n    \n    if items is None:\n        items = ['apple', 'banana', 'grape', 'orange', 'pineapple']\n    \n    if locations is None:\n        locations = ['store1', 'store2', 'store3', 'store4', 'store5']\n    \n    # Filter the DataFrame based on the provided items and locations\n    df_filtered = df[df['items'].isin(items) & df['location'].isin(locations)]\n    \n    # Create a pivot table to get the count of items per location\n    pivot_table = df_filtered.pivot_table(index='location', columns='items', aggfunc='size', fill_value=0)\n    \n    # Plot the pivot table\n    ax = pivot_table.plot(kind='bar', stacked=True, figsize=(10, 6))\n    ax.set_title('Item Distribution by Location')\n    ax.set_xlabel('Location')\n    ax.set_ylabel('Count')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, items=None, locations=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df is not a DataFrame.\")\n    \n    if 'items' not in df.columns or 'location' not in df.columns:\n        raise ValueError(\"The DataFrame must contain 'items' and 'location' columns.\")\n    \n    if items is None:\n        items = ['apple', 'banana', 'grape', 'orange', 'pineapple']\n    \n    if locations is None:\n        locations = ['store1', 'store2', 'store3', 'store4', 'store5']\n    \n    # Filter the DataFrame based on the provided items and locations\n    df_filtered = df[df['items'].isin(items) & df['location'].isin(locations)]\n    \n    # Create a pivot table to get the count of items per location\n    pivot_table = df_filtered.pivot_table(index='location', columns='items', aggfunc='size', fill_value=0)\n    \n    # Plot the pivot table\n    ax = pivot_table.plot(kind='bar', stacked=True, figsize=(10, 6))\n    ax.set_title('Item Distribution by Location')\n    ax.set_xlabel('Location')\n    ax.set_ylabel('Count')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Item': ['apple', 'banana', 'apple', 'orange'],\n#     'Location': ['store1', 'store2', 'store3', 'store1']\n# })\n# ax = task_func(df)\n# ax.get_title()\n```\n"}
{"task_id": "BigCodeBench/110", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'sales']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date' and 'sales' columns.\")\n\n    df = df.sort_values('Date')\n    df['turnover'] = df['sales'].cumsum()\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['turnover'], label='Turnover')\n    ax.set_title('Daily Turnover')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('sales')\n    ax.legend()\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'sales']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date' and 'sales' columns.\")\n\n    df = df.sort_values('Date')\n    df['turnover'] = df['sales'].cumsum()\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['turnover'], label='Turnover')\n    ax.set_title('Daily Turnover')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('sales')\n    ax.legend()\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Date': pd.date_range(start='1/1/2021', end='12/31/2021'),\n#     'sales': np.random.randint(100, 2000, size=365)\n# })\n# ax = task_func(df)\n# print(ax.get_title())  # Expected: 'Daily Turnover'\n# print(ax.get_ylabel())  # Expected: 'sales'\n```\n"}
{"task_id": "BigCodeBench/111", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'Time', 'Temperature']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date', 'Time', and 'Temperature' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index(['Date', 'Time'], inplace=True)\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.heatmap(df.unstack(level=1)['Temperature'].unstack(level=0), cmap='coolwarm', cbar_kws={'label': 'Temperature (\u00b0C)'})\n    ax.set_title('Temperature Heatmap')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'Time', 'Temperature']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date', 'Time', and 'Temperature' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index(['Date', 'Time'], inplace=True)\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.heatmap(df.unstack(level=1)['Temperature'].unstack(level=0), cmap='coolwarm', cbar_kws={'label': 'Temperature (\u00b0C)'})\n    ax.set_title('Temperature Heatmap')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/112", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'Status' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a pandas DataFrame with a 'Status' column.\")\n\n    status_counts = df['Status'].value_counts()\n    ax = status_counts.plot.pie(autopct='%1.1f%%', title='Status Distribution')\n    plt.show()\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'Status' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a pandas DataFrame with a 'Status' column.\")\n\n    status_counts = df['Status'].value_counts()\n    ax = status_counts.plot.pie(autopct='%1.1f%%', title='Status Distribution')\n    plt.show()\n    return ax\n```\n"}
{"task_id": "BigCodeBench/113", "solution": "import json\nfrom collections import Counter\nimport random\ndef task_func(my_dict, keys):\n    if len(keys) != 10:\n        raise ValueError(\"The 'keys' list must contain exactly 10 elements.\")\n\n    # Adding 10 random elements to the dictionary\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    # Saving the updated dictionary to a JSON file\n    json_filename = 'updated_dictionary.json'\n    with open(json_filename, 'w') as json_file:\n        json.dump(my_dict, json_file)\n\n    # Counting the frequency of each key in the dictionary\n    key_counter = Counter(my_dict.keys())\n\n    # Saving the key frequencies to a text file\n    txt_filename = 'key_frequencies.txt'\n    with open(txt_filename, 'w') as txt_file:\n        for key, count in key_counter.items():\n            txt_file.write(f\"{key}: {count}\\n\")\n\n    return my_dict, json_filename, txt_filename", "raw_solution": "import json\nfrom collections import Counter\nimport random\n\ndef task_func(my_dict, keys):\n    if len(keys) != 10:\n        raise ValueError(\"The 'keys' list must contain exactly 10 elements.\")\n\n    # Adding 10 random elements to the dictionary\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    # Saving the updated dictionary to a JSON file\n    json_filename = 'updated_dictionary.json'\n    with open(json_filename, 'w') as json_file:\n        json.dump(my_dict, json_file)\n\n    # Counting the frequency of each key in the dictionary\n    key_counter = Counter(my_dict.keys())\n\n    # Saving the key frequencies to a text file\n    txt_filename = 'key_frequencies.txt'\n    with open(txt_filename, 'w') as txt_file:\n        for key, count in key_counter.items():\n            txt_file.write(f\"{key}: {count}\\n\")\n\n    return my_dict, json_filename, txt_filename\n\n# Example usage:\n# result, json_path, txt_path = task_func({'first_key': 1, 'second_key': 2}, ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'])\n# print(isinstance(result, dict))  # Should print True\n# print(len(result) > 2)  # Should print True\n```\n"}
{"task_id": "BigCodeBench/114", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(my_dict):\n    if 'array' not in my_dict:\n        raise KeyError(\"The dictionary must contain an 'array' key.\")\n    \n    if not isinstance(my_dict['array'], np.ndarray):\n        raise TypeError(\"The value of 'array' key must be a numpy array.\")\n    \n    scaler = MinMaxScaler()\n    my_dict['normalized_array'] = scaler.fit_transform(my_dict['array'].reshape(-1, 1)).flatten()\n    \n    return my_dict", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(my_dict):\n    if 'array' not in my_dict:\n        raise KeyError(\"The dictionary must contain an 'array' key.\")\n    \n    if not isinstance(my_dict['array'], np.ndarray):\n        raise TypeError(\"The value of 'array' key must be a numpy array.\")\n    \n    scaler = MinMaxScaler()\n    my_dict['normalized_array'] = scaler.fit_transform(my_dict['array'].reshape(-1, 1)).flatten()\n    \n    return my_dict\n\n# Example usage:\nexample_dict = {'array': np.array([1, 2, 3, 4, 5])}\nresult = task_func(example_dict)"}
{"task_id": "BigCodeBench/115", "solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list cannot be empty\")\n\n    # Convert list to numpy array\n    num_array = np.array(numbers)\n\n    # Calculate mode\n    mode_value = mode(num_array)[0][0]\n\n    # Calculate entropy\n    entropy_value = entropy(num_array, base=2)\n\n    # Create and return the dictionary\n    result = {\n        'mode': mode_value,\n        'entropy': entropy_value\n    }\n\n    return result", "raw_solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\n\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list cannot be empty\")\n\n    # Convert list to numpy array\n    num_array = np.array(numbers)\n\n    # Calculate mode\n    mode_value = mode(num_array)[0][0]\n\n    # Calculate entropy\n    entropy_value = entropy(num_array, base=2)\n\n    # Create and return the dictionary\n    result = {\n        'mode': mode_value,\n        'entropy': entropy_value\n    }\n\n    return result\n\n# Example usage:\n# result = task_func([1, 2, 2, 3, 3, 3])\n# print(result)  # Output: {'mode': 3, 'entropy': 1.4591479170272448}\n```\n"}
{"task_id": "BigCodeBench/116", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, sample_size):\n    # Generate random samples from a normal distribution\n    data = np.random.normal(mu, sigma, sample_size)\n\n    # Plot histogram of the samples\n    plt.hist(data, bins=30, alpha=0.75, color='blue')\n    plt.xlabel('Sample Mean')\n    plt.ylabel('Frequency')\n    plt.title('Sample Histogram')\n    plt.show()\n\n    return data", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, sample_size):\n    # Generate random samples from a normal distribution\n    data = np.random.normal(mu, sigma, sample_size)\n\n    # Plot histogram of the samples\n    plt.hist(data, bins=30, alpha=0.75, color='blue')\n    plt.xlabel('Sample Mean')\n    plt.ylabel('Frequency')\n    plt.title('Sample Histogram')\n    plt.show()\n\n    return data\n\n# Example usage:\n# data = task_func(0, 1, 1000)\n# print(len(data))  # Should print 1000\n# print(isinstance(data, np.ndarray))  # Should print True\n```\n"}
{"task_id": "BigCodeBench/117", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if not isinstance(num_of_students, int) or num_of_students <= 0:\n        raise ValueError(\"num_of_students must be a positive integer.\")\n\n    set_seed(seed)\n    name_list = name_list or ['john', 'mike', 'saara', 'emma', 'nik']\n    gender_list = gender_list or ['purush', 'female']\n\n    data = {\n        'Name': [choice(name_list) for _ in range(num_of_students)],\n        'Age': [np.random.randint(age_range[0], age_range[1] + 1) for _ in range(num_of_students)],\n        'Gender': [choice(gender_list) for _ in range(num_of_students)],\n        'Score': [np.random.randint(score_range[0], score_range[1] + 1) for _ in range(num_of_students)]\n    }\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\n\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if not isinstance(num_of_students, int) or num_of_students <= 0:\n        raise ValueError(\"num_of_students must be a positive integer.\")\n\n    set_seed(seed)\n    name_list = name_list or ['john', 'mike', 'saara', 'emma', 'nik']\n    gender_list = gender_list or ['purush', 'female']\n\n    data = {\n        'Name': [choice(name_list) for _ in range(num_of_students)],\n        'Age': [np.random.randint(age_range[0], age_range[1] + 1) for _ in range(num_of_students)],\n        'Gender': [choice(gender_list) for _ in range(num_of_students)],\n        'Score': [np.random.randint(score_range[0], score_range[1] + 1) for _ in range(num_of_students)]\n    }\n\n    df = pd.DataFrame(data)\n    return df\n```\n"}
{"task_id": "BigCodeBench/118", "solution": "import os\nimport shutil\ndef task_func(directory, backup_directory):\n    # Check if backup directory exists, if not, create it\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n\n    # List to store paths of copied files\n    copied_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file is a json file\n        if filename.endswith('.json'):\n            # Define source and destination paths\n            source_path = os.path.join(directory, filename)\n            destination_path = os.path.join(backup_directory, filename)\n\n            # Copy the file to the backup directory\n            shutil.copy(source_path, destination_path)\n\n            # Add the path to the copied files list\n            copied_files.append(destination_path)\n\n    return copied_files", "raw_solution": "import os\nimport shutil\n\ndef task_func(directory, backup_directory):\n    # Check if backup directory exists, if not, create it\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n\n    # List to store paths of copied files\n    copied_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file is a json file\n        if filename.endswith('.json'):\n            # Define source and destination paths\n            source_path = os.path.join(directory, filename)\n            destination_path = os.path.join(backup_directory, filename)\n\n            # Copy the file to the backup directory\n            shutil.copy(source_path, destination_path)\n\n            # Add the path to the copied files list\n            copied_files.append(destination_path)\n\n    return copied_files\n```\n"}
{"task_id": "BigCodeBench/119", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    \"\"\"\n    Ek parabola ka ek diagram banata hai aur pradarshit karta hai jise equation y = x^2 dwara darshaya gaya hai.\n    function matplotlib ka upyog karke parabola ko plot karta hai, title ko 'y = x^2' ke roop mein set karta hai, axes ko 'x' aur 'y' ke roop mein label karta hai,\n    aur grid ko saksham karta hai. Yah 400 points ke saath -10 se 10 tak x values ki ek nishchit range ka upyog karta hai.\n    Yah function buniyaadii plotting capabilities ko pradarshit karne aur quadratic functions ko visualize karne ke liye upyog kiya jaata hai.\n    function koi parameters nahin leta hai aur koi maan vaapas nahin karta hai.\n\n    Parameters:\n    - None\n\n    Returns:\n        None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    Examples:\n    >>> task_func() # This will display the plot of the parabola y = x^2\n    >>> type(task_func())\n    <class 'NoneType'>\n    \"\"\"\n\n    # Generate 400 points between -10 and 10\n    x = np.linspace(-10, 10, 400)\n    y = x**2\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, label='y = x^2')\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.legend()\n    plt.show()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    \"\"\"\n    Ek parabola ka ek diagram banata hai aur pradarshit karta hai jise equation y = x^2 dwara darshaya gaya hai.\n    function matplotlib ka upyog karke parabola ko plot karta hai, title ko 'y = x^2' ke roop mein set karta hai, axes ko 'x' aur 'y' ke roop mein label karta hai,\n    aur grid ko saksham karta hai. Yah 400 points ke saath -10 se 10 tak x values ki ek nishchit range ka upyog karta hai.\n    Yah function buniyaadii plotting capabilities ko pradarshit karne aur quadratic functions ko visualize karne ke liye upyog kiya jaata hai.\n    function koi parameters nahin leta hai aur koi maan vaapas nahin karta hai.\n\n    Parameters:\n    - None\n\n    Returns:\n        None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    Examples:\n    >>> task_func() # This will display the plot of the parabola y = x^2\n    >>> type(task_func())\n    <class 'NoneType'>\n    \"\"\"\n\n    # Generate 400 points between -10 and 10\n    x = np.linspace(-10, 10, 400)\n    y = x**2\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, label='y = x^2')\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n# Call the function to display the plot\ntask_func()\n```\n"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date.\")\n\n    random_seed(seed)\n    num_days = (end_date - start_date).days + 1\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n\n    return pd.Series(random_dates)", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date.\")\n\n    random_seed(seed)\n    num_days = (end_date - start_date).days + 1\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n\n    return pd.Series(random_dates)\n\n# Example usage:\n# dates = task_func(seed=123)\n# print(dates.head())\n```\n"}
{"task_id": "BigCodeBench/121", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(my_list, seed=42):\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    np.random.seed(seed)\n    sales_data = np.random.randint(1000, 5000, size=len(categories))\n\n    df = pd.DataFrame({\n        'Category': categories,\n        'Sales': sales_data\n    })\n\n    ax = df.plot(x='Category', y='Sales', kind='bar', legend=False)\n    ax.set_title('Category-wise Sales Data')\n    ax.set_ylabel('Sales')\n    ax.set_xlabel('Category')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, seed=42):\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    np.random.seed(seed)\n    sales_data = np.random.randint(1000, 5000, size=len(categories))\n\n    df = pd.DataFrame({\n        'Category': categories,\n        'Sales': sales_data\n    })\n\n    ax = df.plot(x='Category', y='Sales', kind='bar', legend=False)\n    ax.set_title('Category-wise Sales Data')\n    ax.set_ylabel('Sales')\n    ax.set_xlabel('Category')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/122", "solution": "import numpy as np\nimport random\ndef task_func(my_list):\n    # Calculate the sum of the list\n    total_sum = sum(my_list)\n    \n    # Generate a random number between 0 and 100\n    random_number = random.randint(0, 100)\n    \n    # Append the random number to the list\n    my_list.append(random_number)\n    \n    # Generate a random floating-point number array with the length equal to the sum of the list\n    random_array = np.random.rand(total_sum)\n    \n    # Return the array\n    return random_array", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(my_list):\n    # Calculate the sum of the list\n    total_sum = sum(my_list)\n    \n    # Generate a random number between 0 and 100\n    random_number = random.randint(0, 100)\n    \n    # Append the random number to the list\n    my_list.append(random_number)\n    \n    # Generate a random floating-point number array with the length equal to the sum of the list\n    random_array = np.random.rand(total_sum)\n    \n    # Return the array\n    return random_array\n```\n"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n    \n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(f\"No directory found at {file_dir}\")\n\n    csv_files = glob.glob(os.path.join(file_dir, f'*{file_ext}'))\n    if not csv_files:\n        raise FileNotFoundError(f\"No CSV files found in directory {file_dir}\")\n\n    df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n    df['sum'] = df[my_list].sum(axis=1)\n    \n    return df", "raw_solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n    \n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(f\"No directory found at {file_dir}\")\n\n    csv_files = glob.glob(os.path.join(file_dir, f'*{file_ext}'))\n    if not csv_files:\n        raise FileNotFoundError(f\"No CSV files found in directory {file_dir}\")\n\n    df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n    df['sum'] = df[my_list].sum(axis=1)\n    \n    return df\n\n# Example usage:\n# Assuming there are CSV files in the 'data_files' directory with columns specified in 'my_list'\n# my_list = ['A', 'B']\n# df = task_func(my_list)\n# print(df.head())\n```\n"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"my_list must contain only numeric elements.\")\n\n    random_seed(seed)\n    start_time = time.time()\n    total_sum = sum(my_list)\n    random_numbers = [randint(1, 100) for _ in range(total_sum)]\n    random_numbers = random_numbers[:size]\n    end_time = time.time()\n\n    plt.hist(random_numbers, bins=range(1, 102), align='left', rwidth=0.8)\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    ax = plt.gca()\n\n    return end_time - start_time, ax", "raw_solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"my_list must contain only numeric elements.\")\n\n    random_seed(seed)\n    start_time = time.time()\n    total_sum = sum(my_list)\n    random_numbers = [randint(1, 100) for _ in range(total_sum)]\n    random_numbers = random_numbers[:size]\n    end_time = time.time()\n\n    plt.hist(random_numbers, bins=range(1, 102), align='left', rwidth=0.8)\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    ax = plt.gca()\n\n    return end_time - start_time, ax\n```\n"}
{"task_id": "BigCodeBench/125", "solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\ndef task_func(LETTERS, n):\n    # Generate all possible combinations of length 'n' from the given letters\n    combinations = list(itertools.product(LETTERS, repeat=n))\n    \n    # Count occurrences of each letter in each combination\n    letter_counts = defaultdict(int)\n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n    \n    # Generate a random number between 0 and 100\n    random_number = random.randint(0, 100)\n    \n    # Create a dictionary to store the results\n    results = {\n        \"letter_combinations\": letter_counts,\n        \"random_number\": random_number\n    }\n    \n    # Create a filename with a prefix and the random number\n    filename = f\"prefix_{random_number}.json\"\n    \n    # Write the results to a JSON file\n    with open(filename, 'w') as file:\n        json.dump(results, file)\n    \n    return filename", "raw_solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\n\ndef task_func(LETTERS, n):\n    # Generate all possible combinations of length 'n' from the given letters\n    combinations = list(itertools.product(LETTERS, repeat=n))\n    \n    # Count occurrences of each letter in each combination\n    letter_counts = defaultdict(int)\n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n    \n    # Generate a random number between 0 and 100\n    random_number = random.randint(0, 100)\n    \n    # Create a dictionary to store the results\n    results = {\n        \"letter_combinations\": letter_counts,\n        \"random_number\": random_number\n    }\n    \n    # Create a filename with a prefix and the random number\n    filename = f\"prefix_{random_number}.json\"\n    \n    # Write the results to a JSON file\n    with open(filename, 'w') as file:\n        json.dump(results, file)\n    \n    return filename\n\n# Example usage:\n# print(task_func(['a', 'b', 'c', 'd', 'e'], 3))\n```\n"}
{"task_id": "BigCodeBench/126", "solution": "import pandas as pd\nfrom random import randint, seed as random_seed\nimport statistics\nimport numpy as np\ndef task_func(animals=None, seed=42):\n    if animals is None:\n        animals = ['sher', 'haathi', 'baagh', 'jiraaf', 'paandaa']\n    \n    random_seed(seed)\n    data = []\n\n    for animal in animals:\n        counts = [randint(1, 100) for _ in range(10)]\n        mean = statistics.mean(counts)\n        median = statistics.median(counts)\n        std_dev = statistics.stdev(counts)\n        data.append([animal, mean, median, std_dev])\n\n    df = pd.DataFrame(data, columns=['Animal', 'Mean', 'Median', 'Standard Deviation'])\n\n    return df", "raw_solution": "import pandas as pd\nfrom random import randint, seed as random_seed\nimport statistics\nimport numpy as np\n\ndef task_func(animals=None, seed=42):\n    if animals is None:\n        animals = ['sher', 'haathi', 'baagh', 'jiraaf', 'paandaa']\n    \n    random_seed(seed)\n    data = []\n\n    for animal in animals:\n        counts = [randint(1, 100) for _ in range(10)]\n        mean = statistics.mean(counts)\n        median = statistics.median(counts)\n        std_dev = statistics.stdev(counts)\n        data.append([animal, mean, median, std_dev])\n\n    df = pd.DataFrame(data, columns=['Animal', 'Mean', 'Median', 'Standard Deviation'])\n\n    return df\n\n# Example usage:\nreport = task_func()"}
{"task_id": "BigCodeBench/127", "solution": "import os\nimport shutil\nimport glob\nimport hashlib\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    if not os.path.exists(ROOT_DIR):\n        raise FileNotFoundError(f\"Root directory '{ROOT_DIR}' does not exist.\")\n    \n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n    \n    moved_files_count = 0\n    for file_path in glob.glob(os.path.join(ROOT_DIR, '*')):\n        if os.path.isfile(file_path):\n            file_hash = hashlib.md5()\n            with open(file_path, 'rb') as f:\n                while chunk := f.read(8192):\n                    file_hash.update(chunk)\n            if file_hash.hexdigest() == SPECIFIC_HASH:\n                shutil.move(file_path, DEST_DIR)\n                moved_files_count += 1\n    \n    return moved_files_count", "raw_solution": "import os\nimport shutil\nimport glob\nimport hashlib\n\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    if not os.path.exists(ROOT_DIR):\n        raise FileNotFoundError(f\"Root directory '{ROOT_DIR}' does not exist.\")\n    \n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n    \n    moved_files_count = 0\n    for file_path in glob.glob(os.path.join(ROOT_DIR, '*')):\n        if os.path.isfile(file_path):\n            file_hash = hashlib.md5()\n            with open(file_path, 'rb') as f:\n                while chunk := f.read(8192):\n                    file_hash.update(chunk)\n            if file_hash.hexdigest() == SPECIFIC_HASH:\n                shutil.move(file_path, DEST_DIR)\n                moved_files_count += 1\n    \n    return moved_files_count\n```\n"}
{"task_id": "BigCodeBench/128", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\ndef task_func(POINTS=100):\n    x, y = [0], [0]  # Initial position\n\n    for _ in range(POINTS):\n        angle = math.radians(randint(0, 360))  # Random angle in degrees\n        x_new = x[-1] + math.cos(angle)\n        y_new = y[-1] + math.sin(angle)\n        x.append(x_new)\n        y.append(y_new)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, marker='o')\n    ax.set_title('Random Walk')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    plt.grid(True)\n    plt.show()\n\n    return fig", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\n\ndef task_func(POINTS=100):\n    x, y = [0], [0]  # Initial position\n\n    for _ in range(POINTS):\n        angle = math.radians(randint(0, 360))  # Random angle in degrees\n        x_new = x[-1] + math.cos(angle)\n        y_new = y[-1] + math.sin(angle)\n        x.append(x_new)\n        y.append(y_new)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, marker='o')\n    ax.set_title('Random Walk')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    plt.grid(True)\n    plt.show()\n\n    return fig\n```\n"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        raise requests.exceptions.HTTPError(f\"HTTP Error: {errh}\")\n    except requests.exceptions.ConnectionError as errc:\n        raise requests.exceptions.ConnectionError(f\"Error Connecting: {errc}\")\n    except requests.exceptions.Timeout as errt:\n        raise requests.exceptions.Timeout(f\"Timeout Error: {errt}\")\n    except requests.exceptions.RequestException as err:\n        raise requests.exceptions.RequestException(f\"Something went wrong: {err}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table')\n    if table is None:\n        raise ValueError(\"No table found on the webpage.\")\n\n    headers = []\n    rows = []\n    for header in table.find_all('th'):\n        headers.append(header.text.strip())\n    for row in table.find_all('tr'):\n        cells = row.find_all('td')\n        if cells:\n            row_data = [cell.text.strip() for cell in cells]\n            rows.append(row_data)\n\n    df = pd.DataFrame(rows, columns=headers if headers else None)\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        raise requests.exceptions.HTTPError(f\"HTTP Error: {errh}\")\n    except requests.exceptions.ConnectionError as errc:\n        raise requests.exceptions.ConnectionError(f\"Error Connecting: {errc}\")\n    except requests.exceptions.Timeout as errt:\n        raise requests.exceptions.Timeout(f\"Timeout Error: {errt}\")\n    except requests.exceptions.RequestException as err:\n        raise requests.exceptions.RequestException(f\"Something went wrong: {err}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table')\n    if table is None:\n        raise ValueError(\"No table found on the webpage.\")\n\n    headers = []\n    rows = []\n    for header in table.find_all('th'):\n        headers.append(header.text.strip())\n    for row in table.find_all('tr'):\n        cells = row.find_all('td')\n        if cells:\n            row_data = [cell.text.strip() for cell in cells]\n            rows.append(row_data)\n\n    df = pd.DataFrame(rows, columns=headers if headers else None)\n\n    return df\n\n# Example usage:\n# df = task_func('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/130", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_data = binascii.unhexlify(hex_str)\n\n    # Generate a random salt\n    salt = os.urandom(salt_size)\n\n    # Combine salt and data\n    combined_data = salt + byte_data\n\n    # Create SHA256 hash of the combined data\n    hash_object = hashlib.sha256(combined_data)\n    hash_hex = hash_object.hexdigest()\n\n    # Encode the salt in base64\n    base64_salt = base64.b64encode(salt).decode('utf-8')\n\n    return base64_salt, hash_hex", "raw_solution": "import base64\nimport binascii\nimport os\nimport hashlib\n\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_data = binascii.unhexlify(hex_str)\n\n    # Generate a random salt\n    salt = os.urandom(salt_size)\n\n    # Combine salt and data\n    combined_data = salt + byte_data\n\n    # Create SHA256 hash of the combined data\n    hash_object = hashlib.sha256(combined_data)\n    hash_hex = hash_object.hexdigest()\n\n    # Encode the salt in base64\n    base64_salt = base64.b64encode(salt).decode('utf-8')\n\n    return base64_salt, hash_hex\n\n# Example usage:\n# result = task_func(\"F3BE8080\", 16)\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/131", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_str = binascii.unhexlify(hex_str)\n\n    # Generate random salt\n    salt = os.urandom(salt_size)\n    \n    # Combine salt and original bytes\n    salted_data = salt + byte_str\n\n    # Create SHA256 hash of salted data\n    hash_obj = hashlib.sha256(salted_data)\n    hash_hex = hash_obj.hexdigest()\n\n    # Encode salt in base64\n    salt_b64 = base64.b64encode(salt).decode('utf-8')\n\n    return salt_b64, hash_hex", "raw_solution": "import base64\nimport binascii\nimport os\nimport hashlib\n\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_str = binascii.unhexlify(hex_str)\n\n    # Generate random salt\n    salt = os.urandom(salt_size)\n    \n    # Combine salt and original bytes\n    salted_data = salt + byte_str\n\n    # Create SHA256 hash of salted data\n    hash_obj = hashlib.sha256(salted_data)\n    hash_hex = hash_obj.hexdigest()\n\n    # Encode salt in base64\n    salt_b64 = base64.b64encode(salt).decode('utf-8')\n\n    return salt_b64, hash_hex\n\n# Example usage:\n# result = task_func(\"F3BE8080\", 16)\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/132", "solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(hex_str):\n    # Remove the '\\\\x' prefix if present\n    if hex_str.startswith('\\\\x'):\n        hex_str = hex_str[2:]\n\n    # Convert the hex string to bytes\n    try:\n        byte_array = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"Invalid hex string\")\n\n    # Calculate the frequency of each byte value\n    unique, counts = np.unique(byte_array, return_counts=True)\n    frequency = dict(zip(unique, counts))\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(frequency.items()), columns=['Byte Value', 'Frequency'])\n\n    # Create a matplotlib plot\n    ax = df.plot(x='Byte Value', y='Frequency', kind='bar', legend=False)\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Byte Value Frequency')\n\n    return df, ax", "raw_solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(hex_str):\n    # Remove the '\\\\x' prefix if present\n    if hex_str.startswith('\\\\x'):\n        hex_str = hex_str[2:]\n\n    # Convert the hex string to bytes\n    try:\n        byte_array = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"Invalid hex string\")\n\n    # Calculate the frequency of each byte value\n    unique, counts = np.unique(byte_array, return_counts=True)\n    frequency = dict(zip(unique, counts))\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(frequency.items()), columns=['Byte Value', 'Frequency'])\n\n    # Create a matplotlib plot\n    ax = df.plot(x='Byte Value', y='Frequency', kind='bar', legend=False)\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Byte Value Frequency')\n\n    return df, ax\n\n# Example usage:\n# df, ax = task_func('f3be8080')\n# print(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/133", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    scaler = MinMaxScaler()\n    df_normalized = df.copy()\n    df_normalized.iloc[:, -1] = scaler.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n\n    ax = df_normalized.iloc[:, -1].plot(kind='hist', bins=20, title=f\"{df.columns[-1]} ka samanyikrit data\")\n    ax.set_xlabel('index')\n    ax.set_ylabel('Normalized value')\n\n    return df_normalized, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    scaler = MinMaxScaler()\n    df_normalized = df.copy()\n    df_normalized.iloc[:, -1] = scaler.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n\n    ax = df_normalized.iloc[:, -1].plot(kind='hist', bins=20, title=f\"{df.columns[-1]} ka samanyikrit data\")\n    ax.set_xlabel('index')\n    ax.set_ylabel('Normalized value')\n\n    return df_normalized, ax\n\n# Example usage:\n# df = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list('ABCD'))\n# normalized_df, ax = task_func(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/134", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=20):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    if df.shape[1] < 1:\n        raise ValueError(\"DataFrame must have at least one column.\")\n    \n    last_column = df.columns[-1]\n    ax = df[last_column].plot(kind='hist', bins=bins, title=f'Histogram of {last_column}', figsize=(10, 6))\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=20):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    if df.shape[1] < 1:\n        raise ValueError(\"DataFrame must have at least one column.\")\n    \n    last_column = df.columns[-1]\n    ax = df[last_column].plot(kind='hist', bins=bins, title=f'Histogram of {last_column}', figsize=(10, 6))\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax\n\n# Example usage:\n# df = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list('ABCD'))\n# ax = task_func(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/135", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    if df.shape[1] < 2:\n        raise ValueError(\"DataFrame must have at least two columns.\")\n    \n    # Mean Imputation\n    imputer = SimpleImputer(strategy='mean')\n    df.iloc[:, -1] = imputer.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n    \n    # Boxplot\n    fig, ax = plt.subplots()\n    sns.boxplot(x=df.iloc[:, -1], ax=ax)\n    ax.set_title('Boxplot of Last Column')\n    ax.set_xlabel(df.columns[-1])\n    \n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    if df.shape[1] < 2:\n        raise ValueError(\"DataFrame must have at least two columns.\")\n    \n    # Mean Imputation\n    imputer = SimpleImputer(strategy='mean')\n    df.iloc[:, -1] = imputer.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n    \n    # Boxplot\n    fig, ax = plt.subplots()\n    sns.boxplot(x=df.iloc[:, -1], ax=ax)\n    ax.set_title('Boxplot of Last Column')\n    ax.set_xlabel(df.columns[-1])\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/136", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n    pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n\n    # Plotting the principal components\n    fig, ax = plt.subplots()\n    ax.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return pca_df, ax", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n    pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n\n    # Plotting the principal components\n    fig, ax = plt.subplots()\n    ax.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return pca_df, ax\n```\n"}
{"task_id": "BigCodeBench/137", "solution": "import pandas as pd\nfrom scipy.stats import skew\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    last_column = df.iloc[:, -1]\n    skewness = skew(last_column)\n\n    return skewness", "raw_solution": "import pandas as pd\nfrom scipy.stats import skew\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    last_column = df.iloc[:, -1]\n    skewness = skew(last_column)\n\n    return skewness\n\n# Example usage:"}
{"task_id": "BigCodeBench/138", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    if not isinstance(df, pd.DataFrame) or 'letters' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a DataFrame with a 'letters' column.\")\n    \n    # Calculate frequency of each letter\n    frequency = df['letters'].value_counts().reindex(letters, fill_value=0)\n    \n    # Plot the frequency\n    fig, ax = plt.subplots()\n    frequency.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n    \n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    if not isinstance(df, pd.DataFrame) or 'letters' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a DataFrame with a 'letters' column.\")\n    \n    # Calculate frequency of each letter\n    frequency = df['letters'].value_counts().reindex(letters, fill_value=0)\n    \n    # Plot the frequency\n    fig, ax = plt.subplots()\n    frequency.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n    \n    return ax\n\n# Example usage:\n# import random\n# random.seed(42)\n# df = pd.DataFrame({'letters': random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ', k=100)})\n# ax = task_func(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if len(numeric_cols) == 0:\n        raise ValueError(\"DataFrame must contain at least one numeric column.\")\n    \n    axes = []\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        ax.hist(df[col], bins=10, color='skyblue', edgecolor='black')\n        ax.set_title(f'Histogram of {col}')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if len(numeric_cols) == 0:\n        raise ValueError(\"DataFrame must contain at least one numeric column.\")\n    \n    axes = []\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        ax.hist(df[col], bins=10, color='skyblue', edgecolor='black')\n        ax.set_title(f'Histogram of {col}')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes\n\n# Example usage:\n# df = pd.DataFrame({'A': np.random.normal(0, 1, 100), 'B': np.random.exponential(1, 100)})\n# axes = task_func(df)\n# for ax in axes:\n#     plt.show()\n```\n"}
{"task_id": "BigCodeBench/140", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, cols):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df is not a DataFrame.\")\n    if not isinstance(cols, list) or not all(isinstance(col, str) for col in cols):\n        raise ValueError(\"cols should be a list of column names.\")\n    if not all(col in df.columns for col in cols):\n        raise ValueError(\"All columns in cols should exist in the DataFrame.\")\n\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, cols):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df is not a DataFrame.\")\n    if not isinstance(cols, list) or not all(isinstance(col, str) for col in cols):\n        raise ValueError(\"cols should be a list of column names.\")\n    if not all(col in df.columns for col in cols):\n        raise ValueError(\"All columns in cols should exist in the DataFrame.\")\n\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n\n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/141", "solution": "import numpy as np\nimport pandas as pd\nimport statistics\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if rows <= 0:\n        raise ValueError(\"Number of rows must be a positive integer.\")\n    \n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n    \n    stats = {}\n    for col in columns:\n        col_data = df[col]\n        stats[col] = {\n            'mean': statistics.mean(col_data),\n            'median': statistics.median(col_data)\n        }\n    \n    return df, stats", "raw_solution": "import numpy as np\nimport pandas as pd\nimport statistics\n\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if rows <= 0:\n        raise ValueError(\"Number of rows must be a positive integer.\")\n    \n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n    \n    stats = {}\n    for col in columns:\n        col_data = df[col]\n        stats[col] = {\n            'mean': statistics.mean(col_data),\n            'median': statistics.median(col_data)\n        }\n    \n    return df, stats\n\n# Example usage:\n# df, stats = task_func(10)\n# print(df)\n# print(stats)\n```\n"}
{"task_id": "BigCodeBench/142", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(0, 2 * np.pi, 100)\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n\n    axs[0].plot(x, y_sin)\n    axs[0].set_title('Sine function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n\n    axs[1].plot(x, y_cos)\n    axs[1].set_title('Cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n\n    plt.tight_layout()\n\n    return fig, axs", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    x = np.linspace(0, 2 * np.pi, 100)\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n\n    axs[0].plot(x, y_sin)\n    axs[0].set_title('Sine function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n\n    axs[1].plot(x, y_cos)\n    axs[1].set_title('Cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n\n    plt.tight_layout()\n\n    return fig, axs\n```\n"}
{"task_id": "BigCodeBench/143", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = 2 * x + 1\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'r', label='y = 2x + 1')\n    ax.scatter(2, 5, color='blue', label='Solution (x=2, y=5)')\n\n    ax.set_title('x=2 par equation y=2x+1 ka hal')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = 2 * x + 1\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'r', label='y = 2x + 1')\n    ax.scatter(2, 5, color='blue', label='Solution (x=2, y=5)')\n\n    ax.set_title('x=2 par equation y=2x+1 ka hal')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n\n    return ax\n\n# Example usage:\n# ax = task_func()\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/144", "solution": "import ipaddress\nimport requests\ndef task_func(ip_range, timeout):\n    try:\n        ip_network = ipaddress.ip_network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided.\")\n\n    active_ips = []\n\n    for ip in ip_network.hosts():\n        try:\n            response = requests.get(f\"http://{ip}\", timeout=timeout)\n            if response.status_code == 200:\n                active_ips.append(str(ip))\n        except requests.RequestException:\n            continue\n\n    return active_ips", "raw_solution": "import ipaddress\nimport requests\n\ndef task_func(ip_range, timeout):\n    try:\n        ip_network = ipaddress.ip_network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided.\")\n\n    active_ips = []\n\n    for ip in ip_network.hosts():\n        try:\n            response = requests.get(f\"http://{ip}\", timeout=timeout)\n            if response.status_code == 200:\n                active_ips.append(str(ip))\n        except requests.RequestException:\n            continue\n\n    return active_ips\n\n# Example usage:\n# print(task_func('192.168.0.0/16', 5))\n```\n"}
{"task_id": "BigCodeBench/145", "solution": "import csv\nfrom ipaddress import IPv4Network\ndef task_func(ip_range, csv_path):\n    network = IPv4Network(ip_range)\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for ip in network.hosts():\n            writer.writerow([str(ip)])\n\n    return csv_path", "raw_solution": "import csv\nfrom ipaddress import IPv4Network\n\ndef task_func(ip_range, csv_path):\n    network = IPv4Network(ip_range)\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for ip in network.hosts():\n            writer.writerow([str(ip)])\n\n    return csv_path\n```\n"}
{"task_id": "BigCodeBench/146", "solution": "import subprocess\nfrom ipaddress import IPv4Network\ndef task_func(ip_range):\n    result = {}\n    network = IPv4Network(ip_range)\n\n    for ip in network.hosts():\n        try:\n            response = subprocess.run(\n                ['ping', '-c', '1', str(ip)],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE\n            )\n            result[str(ip)] = response.returncode == 0\n        except subprocess.CalledProcessError as e:\n            raise subprocess.CalledProcessError(e.returncode, e.cmd)\n\n    return result", "raw_solution": "import subprocess\nfrom ipaddress import IPv4Network\n\ndef task_func(ip_range):\n    result = {}\n    network = IPv4Network(ip_range)\n\n    for ip in network.hosts():\n        try:\n            response = subprocess.run(\n                ['ping', '-c', '1', str(ip)],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE\n            )\n            result[str(ip)] = response.returncode == 0\n        except subprocess.CalledProcessError as e:\n            raise subprocess.CalledProcessError(e.returncode, e.cmd)\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    \"\"\"\n    Ek nishchit **IP address range** scan karta hai and checks hai ki kya ek nirdisht **port** pratyek **IP** par open hai.\n    **function** ek **dictionary** lautata hai jis mein **IP addresses** **keys** ke roop mein hain and ek **boolean** indicating hai,\n    **port** ki **status** (**true** agar open hai, otherwise **false**).\n\n    Parameters:\n    - ip_range (str): Scan karne ke liye **IP address range**, CIDR notation mein.\n    - port (int): Range mein pratyek **IP** par jaanchne ke liye **port** number.\n\n    Returns:\n        dict: Ek **dictionary** jo **IP addresses** ko unke **port status** se mapping karta hai (**true** agar open hai).\n\n    Requirements:\n    - socket\n    - ipaddress.IPv4Network\n    - threading.Thread\n\n    Example:\n    >>> result = task_func('192.168.0.0/24', 80)\n    >>> isinstance(result, dict)\n    true\n    >>> all(isinstance(key, str) and isinstance(value, bool) for key, value in result.items())\n    true\n\n    \"\"\"\n    result = {}\n    network = IPv4Network(ip_range)\n    threads = []\n\n    def scan_ip(ip):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(1)\n            try:\n                s.connect((str(ip), port))\n                result[str(ip)] = True\n            except (socket.timeout, socket.error):\n                result[str(ip)] = False\n\n    for ip in network.hosts():\n        thread = Thread(target=scan_ip, args=(ip,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return result", "raw_solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    \"\"\"\n    Ek nishchit **IP address range** scan karta hai and checks hai ki kya ek nirdisht **port** pratyek **IP** par open hai.\n    **function** ek **dictionary** lautata hai jis mein **IP addresses** **keys** ke roop mein hain and ek **boolean** indicating hai,\n    **port** ki **status** (**true** agar open hai, otherwise **false**).\n\n    Parameters:\n    - ip_range (str): Scan karne ke liye **IP address range**, CIDR notation mein.\n    - port (int): Range mein pratyek **IP** par jaanchne ke liye **port** number.\n\n    Returns:\n        dict: Ek **dictionary** jo **IP addresses** ko unke **port status** se mapping karta hai (**true** agar open hai).\n\n    Requirements:\n    - socket\n    - ipaddress.IPv4Network\n    - threading.Thread\n\n    Example:\n    >>> result = task_func('192.168.0.0/24', 80)\n    >>> isinstance(result, dict)\n    true\n    >>> all(isinstance(key, str) and isinstance(value, bool) for key, value in result.items())\n    true\n\n    \"\"\"\n    result = {}\n    network = IPv4Network(ip_range)\n    threads = []\n\n    def scan_ip(ip):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(1)\n            try:\n                s.connect((str(ip), port))\n                result[str(ip)] = True\n            except (socket.timeout, socket.error):\n                result[str(ip)] = False\n\n    for ip in network.hosts():\n        thread = Thread(target=scan_ip, args=(ip,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/148", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Use a LabelEncoder to encrypt categorical data in a specific column of a df.\n\n    Parameters:\n    - df (pd.df): The df which includes data.\n    - column_name (str): The name of the column to encode.\n\n    Returns:\n        pd.df: The df with the encoded column.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> df = pd.DataFrame({'fruit': ['apple', 'banana', 'cherry', 'apple', 'banana']})\n    >>> encoded_df = task_func(df, 'fruit')\n    >>> encoded_df['fruit'].tolist()\n    [0, 1, 2, 0, 1]\n\n    \"\"\"\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in DataFrame.\")\n    \n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    \n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Use a LabelEncoder to encrypt categorical data in a specific column of a df.\n\n    Parameters:\n    - df (pd.df): The df which includes data.\n    - column_name (str): The name of the column to encode.\n\n    Returns:\n        pd.df: The df with the encoded column.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> df = pd.DataFrame({'fruit': ['apple', 'banana', 'cherry', 'apple', 'banana']})\n    >>> encoded_df = task_func(df, 'fruit')\n    >>> encoded_df['fruit'].tolist()\n    [0, 1, 2, 0, 1]\n\n    \"\"\"\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in DataFrame.\")\n    \n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/149", "solution": "import pandas as pd\nimport numpy as np\nDEFAULT_COLUMNS = ['Element', 'Count']\ndef task_func(elements, include_index=False):\n    data = []\n    for i, element in enumerate(elements):\n        data.append([i, element, len(element)])\n    \n    columns = DEFAULT_COLUMNS\n    if include_index:\n        columns = ['Index'] + columns\n    \n    df = pd.DataFrame(data, columns=columns)\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\nDEFAULT_COLUMNS = ['Element', 'Count']\n\ndef task_func(elements, include_index=False):\n    data = []\n    for i, element in enumerate(elements):\n        data.append([i, element, len(element)])\n    \n    columns = DEFAULT_COLUMNS\n    if include_index:\n        columns = ['Index'] + columns\n    \n    df = pd.DataFrame(data, columns=columns)\n    return df\n\n# Example usage:\n# result = task_func(['abc', 'def'], include_index=True)\n# print(result.to_string(index=False))\n```\n"}
{"task_id": "BigCodeBench/150", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(product_dict, product_keys):\n    report_data = []\n    for product in product_keys:\n        if product in product_dict:\n            quantity, price = product_dict[product]\n            profit = quantity * price\n            report_data.append([product, quantity, price, profit])\n    \n    report_df = pd.DataFrame(report_data, columns=['Product', 'Quantity', 'Price', 'Profit'])\n    \n    if not report_df.empty:\n        average_price = report_df['Price'].mean()\n        average_profit = report_df['Profit'].mean()\n        report_df['Average Price'] = average_price\n        report_df['Average Profit'] = average_profit\n\n        ax = report_df.plot(x='Product', y='Profit', kind='bar', yerr='Profit', capsize=5, legend=False)\n        ax.set_ylabel('Profit')\n        ax.set_title('Profit per Product')\n    else:\n        ax = None\n\n    return report_df, ax\nproduct_dict = {'Apple': [100, 2.5], 'Orange': [80, 3.5], 'Banana': [120, 1.5]}\nproduct_keys = ['Apple', 'Banana']", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(product_dict, product_keys):\n    report_data = []\n    for product in product_keys:\n        if product in product_dict:\n            quantity, price = product_dict[product]\n            profit = quantity * price\n            report_data.append([product, quantity, price, profit])\n    \n    report_df = pd.DataFrame(report_data, columns=['Product', 'Quantity', 'Price', 'Profit'])\n    \n    if not report_df.empty:\n        average_price = report_df['Price'].mean()\n        average_profit = report_df['Profit'].mean()\n        report_df['Average Price'] = average_price\n        report_df['Average Profit'] = average_profit\n\n        ax = report_df.plot(x='Product', y='Profit', kind='bar', yerr='Profit', capsize=5, legend=False)\n        ax.set_ylabel('Profit')\n        ax.set_title('Profit per Product')\n    else:\n        ax = None\n\n    return report_df, ax\n\n# Example usage:\nproduct_dict = {'Apple': [100, 2.5], 'Orange': [80, 3.5], 'Banana': [120, 1.5]}\nproduct_keys = ['Apple', 'Banana']\nreport, ax = task_func(product_dict, product_keys)"}
{"task_id": "BigCodeBench/151", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data_dict, data_keys):\n    if not all(key in data_dict for key in data_keys):\n        raise ValueError(\"All keys in data_keys must be present in data_dict.\")\n\n    df = pd.DataFrame(data_dict)\n    scaler = MinMaxScaler()\n    df[data_keys] = scaler.fit_transform(df[data_keys])\n\n    ax = df.plot(kind='line', figsize=(10, 6))\n    ax.set_title('Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Normalized Value')\n\n    return df, ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict, data_keys):\n    if not all(key in data_dict for key in data_keys):\n        raise ValueError(\"All keys in data_keys must be present in data_dict.\")\n\n    df = pd.DataFrame(data_dict)\n    scaler = MinMaxScaler()\n    df[data_keys] = scaler.fit_transform(df[data_keys])\n\n    ax = df.plot(kind='line', figsize=(10, 6))\n    ax.set_title('Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Normalized Value')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/152", "solution": "import pandas as pd\nimport numpy as np\nfrom random import randint\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\ndef task_func():\n    # Initialize a dictionary to store student grades\n    grades_dict = {'Name': STUDENTS}\n    \n    # Generate random grades for each course\n    for course in COURSES:\n        grades_dict[course] = [randint(0, 100) for _ in STUDENTS]\n    \n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(grades_dict)\n    \n    # Calculate the average grade for each student\n    df['Average Grade'] = df[COURSES].mean(axis=1)\n    \n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\n\ndef task_func():\n    # Initialize a dictionary to store student grades\n    grades_dict = {'Name': STUDENTS}\n    \n    # Generate random grades for each course\n    for course in COURSES:\n        grades_dict[course] = [randint(0, 100) for _ in STUDENTS]\n    \n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(grades_dict)\n    \n    # Calculate the average grade for each student\n    df['Average Grade'] = df[COURSES].mean(axis=1)\n    \n    return df\n\n# Example usage"}
{"task_id": "BigCodeBench/153", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=['category'])\n    \n    # Initialize the LabelEncoder\n    le = LabelEncoder()\n    \n    # Encode the categories\n    df['Encoded'] = le.fit_transform(df['category'])\n    \n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=['category'])\n    \n    # Initialize the LabelEncoder\n    le = LabelEncoder()\n    \n    # Encode the categories\n    df['Encoded'] = le.fit_transform(df['category'])\n    \n    return df\n\n# Example usage:\n# df = task_func(['A', 'B', 'C', 'A', 'D', 'E', 'B', 'C'])\n# print(df.to_string(index=False))\n```\n"}
{"task_id": "BigCodeBench/154", "solution": "import re\nimport os\nimport glob\nimport mimetypes\ndef task_func(directory, file_pattern, suffix):\n    # Initialize an empty dictionary to store file names and their MIME types\n    file_mime_dict = {}\n\n    # Use glob to find all files matching the pattern in the directory\n    for file_path in glob.glob(os.path.join(directory, file_pattern)):\n        # Extract the file name from the file path\n        file_name = os.path.basename(file_path)\n        \n        # Check if the file name ends with the given suffix\n        if file_name.endswith(suffix):\n            # Get the MIME type of the file\n            mime_type, _ = mimetypes.guess_type(file_path)\n            \n            # Add the file name and its MIME type to the dictionary\n            file_mime_dict[file_name] = mime_type\n\n    return file_mime_dict", "raw_solution": "import re\nimport os\nimport glob\nimport mimetypes\n\ndef task_func(directory, file_pattern, suffix):\n    # Initialize an empty dictionary to store file names and their MIME types\n    file_mime_dict = {}\n\n    # Use glob to find all files matching the pattern in the directory\n    for file_path in glob.glob(os.path.join(directory, file_pattern)):\n        # Extract the file name from the file path\n        file_name = os.path.basename(file_path)\n        \n        # Check if the file name ends with the given suffix\n        if file_name.endswith(suffix):\n            # Get the MIME type of the file\n            mime_type, _ = mimetypes.guess_type(file_path)\n            \n            # Add the file name and its MIME type to the dictionary\n            file_mime_dict[file_name] = mime_type\n\n    return file_mime_dict\n\n# Example usage:\n# Assuming the directory 'dir' contains files like 'example_suff.txt', 'another_file.txt'\n# print(task_func(r'dir', '*_suff.txt', '_suff'))\n# Output: {'example_suff.txt': 'text/plain'}\n```\n"}
{"task_id": "BigCodeBench/155", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    # Convert the numpy array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    \n    # Calculate the average for each row and add it as a new column\n    df['Average'] = df.mean(axis=1)\n    \n    # Plot the average values\n    ax = df['Average'].plot(kind='line', marker='o', title='Average Values')\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average')\n    \n    return df, ax\ndata = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef task_func(data):\n    # Convert the numpy array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    \n    # Calculate the average for each row and add it as a new column\n    df['Average'] = df.mean(axis=1)\n    \n    # Plot the average values\n    ax = df['Average'].plot(kind='line', marker='o', title='Average Values')\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average')\n    \n    return df, ax\n\n# Example usage:\ndata = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\ndf, ax = task_func(data)"}
{"task_id": "BigCodeBench/156", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=[f'Feature_{i}' for i in range(1, 9)])\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Apply MinMax scaling to the DataFrame\n    scaled_data = scaler.fit_transform(df)\n    scaled_df = pd.DataFrame(scaled_data, columns=[f'Feature_{i}' for i in range(1, 9)])\n    \n    # Calculate the average of each row\n    scaled_df['Average'] = scaled_df.mean(axis=1)\n    \n    # Plot the average values\n    ax = scaled_df['Average'].plot(kind='bar', title='Average Values')\n    ax.set_xlabel('Sample Index')\n    ax.set_ylabel('Average Value')\n    \n    return scaled_df, ax\ndata = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=[f'Feature_{i}' for i in range(1, 9)])\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Apply MinMax scaling to the DataFrame\n    scaled_data = scaler.fit_transform(df)\n    scaled_df = pd.DataFrame(scaled_data, columns=[f'Feature_{i}' for i in range(1, 9)])\n    \n    # Calculate the average of each row\n    scaled_df['Average'] = scaled_df.mean(axis=1)\n    \n    # Plot the average values\n    ax = scaled_df['Average'].plot(kind='bar', title='Average Values')\n    ax.set_xlabel('Sample Index')\n    ax.set_ylabel('Average Value')\n    \n    return scaled_df, ax\n\n# Example usage:\nimport numpy as np\ndata = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\ndf, ax = task_func(data)"}
{"task_id": "BigCodeBench/157", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, np.ndarray) or data.ndim != 2:\n        raise ValueError(\"Input data must be a 2D numpy array.\")\n    \n    if not np.issubdtype(data.dtype, np.number):\n        raise ValueError(\"Input data must contain only numeric values.\")\n    \n    # Calculate the average for each row\n    averages = np.mean(data, axis=1)\n    \n    # Create a DataFrame with the averages\n    df = pd.DataFrame(data, columns=[f'Feature_{i+1}' for i in range(data.shape[1])])\n    df['Average'] = averages\n    \n    # Calculate the correlation matrix\n    correlation_matrix = np.corrcoef(data, rowvar=False)\n    \n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not isinstance(data, np.ndarray) or data.ndim != 2:\n        raise ValueError(\"Input data must be a 2D numpy array.\")\n    \n    if not np.issubdtype(data.dtype, np.number):\n        raise ValueError(\"Input data must contain only numeric values.\")\n    \n    # Calculate the average for each row\n    averages = np.mean(data, axis=1)\n    \n    # Create a DataFrame with the averages\n    df = pd.DataFrame(data, columns=[f'Feature_{i+1}' for i in range(data.shape[1])])\n    df['Average'] = averages\n    \n    # Calculate the correlation matrix\n    correlation_matrix = np.corrcoef(data, rowvar=False)\n    \n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/158", "solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\ndef task_func(url_str, file_path):\n    # Parse the URL\n    parsed_url = urllib.parse.urlparse(url_str)\n    \n    # Open the URL and read the JSON data\n    with urllib.request.urlopen(parsed_url) as response:\n        data = json.load(response)\n    \n    # Convert the JSON data to a byte object\n    json_bytes = json.dumps(data).encode('utf-8')\n    \n    # Compress the byte object using gzip\n    with gzip.open(file_path, 'wb') as gz_file:\n        gz_file.write(json_bytes)\n    \n    return file_path", "raw_solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\n\ndef task_func(url_str, file_path):\n    # Parse the URL\n    parsed_url = urllib.parse.urlparse(url_str)\n    \n    # Open the URL and read the JSON data\n    with urllib.request.urlopen(parsed_url) as response:\n        data = json.load(response)\n    \n    # Convert the JSON data to a byte object\n    json_bytes = json.dumps(data).encode('utf-8')\n    \n    # Compress the byte object using gzip\n    with gzip.open(file_path, 'wb') as gz_file:\n        gz_file.write(json_bytes)\n    \n    return file_path\n\n# Example usage:\n# print(task_func('http://example.com/data.json', '/path/to/file.json.gz'))\n```\n"}
{"task_id": "BigCodeBench/159", "solution": "import struct\nimport io\nimport gzip\ndef task_func(newArray):\n    # Convert the array into bytes using struct\n    byte_array = struct.pack(f'{len(newArray)}d', *newArray)\n\n    # Use io.BytesIO to create a bytes buffer\n    byte_buffer = io.BytesIO(byte_array)\n\n    # Compress the bytes buffer using gzip\n    with gzip.GzipFile(fileobj=byte_buffer, mode='wb') as gz_file:\n        gzipped_data = gz_file.read()\n\n    return gzipped_data", "raw_solution": "import struct\nimport io\nimport gzip\n\ndef task_func(newArray):\n    # Convert the array into bytes using struct\n    byte_array = struct.pack(f'{len(newArray)}d', *newArray)\n\n    # Use io.BytesIO to create a bytes buffer\n    byte_buffer = io.BytesIO(byte_array)\n\n    # Compress the bytes buffer using gzip\n    with gzip.GzipFile(fileobj=byte_buffer, mode='wb') as gz_file:\n        gzipped_data = gz_file.read()\n\n    return gzipped_data\n\n# Example usage:\n# newArray = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n# compressed_data = task_func(newArray)\n# print(isinstance(compressed_data, bytes))  # Should print: True\n# print(len(compressed_data) > 0)  # Should print: True\n```\n"}
{"task_id": "BigCodeBench/160", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    if data.shape[1] != 8:\n        raise ValueError(\"Input data must have exactly 8 columns.\")\n\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Calculate the average for each row and add it as a new column\n    df['Average'] = df.mean(axis=1)\n\n    # Plot the distribution of the averages\n    ax = sns.distplot(df['Average'], kde=False)\n\n    # Perform the normality test on the averages\n    if len(df['Average']) >= 20:\n        _, p_value = stats.normaltest(df['Average'])\n    else:\n        p_value = None\n\n    return df, ax, p_value", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef task_func(data):\n    if data.shape[1] != 8:\n        raise ValueError(\"Input data must have exactly 8 columns.\")\n\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Calculate the average for each row and add it as a new column\n    df['Average'] = df.mean(axis=1)\n\n    # Plot the distribution of the averages\n    ax = sns.distplot(df['Average'], kde=False)\n\n    # Perform the normality test on the averages\n    if len(df['Average']) >= 20:\n        _, p_value = stats.normaltest(df['Average'])\n    else:\n        p_value = None\n\n    return df, ax, p_value\n```\n"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    log_pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)'\n    log_data = []\n\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = re.match(log_pattern, line)\n            if match:\n                log_type, timestamp, message = match.groups()\n                try:\n                    datetime_object = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                    log_data.append({'Type': log_type, 'Timestamp': timestamp, 'Message': message})\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp format in line: {line}\")\n\n    if not log_data:\n        raise ValueError(\"No valid log entries found.\")\n\n    df = pd.DataFrame(log_data)\n    output_file = 'log_data.csv'\n    df.to_csv(output_file, index=False)\n\n    return output_file", "raw_solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    log_pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)'\n    log_data = []\n\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = re.match(log_pattern, line)\n            if match:\n                log_type, timestamp, message = match.groups()\n                try:\n                    datetime_object = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                    log_data.append({'Type': log_type, 'Timestamp': timestamp, 'Message': message})\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp format in line: {line}\")\n\n    if not log_data:\n        raise ValueError(\"No valid log entries found.\")\n\n    df = pd.DataFrame(log_data)\n    output_file = 'log_data.csv'\n    df.to_csv(output_file, index=False)\n\n    return output_file\n```\n"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate lengths of words\n    lengths = np.array([len(word) for word in words])\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(lengths, bins=np.arange(1, max(lengths)+2)-0.5, rwidth=rwidth)\n    \n    # Set labels and title\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    \n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate lengths of words\n    lengths = np.array([len(word) for word in words])\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(lengths, bins=np.arange(1, max(lengths)+2)-0.5, rwidth=rwidth)\n    \n    # Set labels and title\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    \n    return ax\n\n# Example usage:\n# ax = task_func('Hello world, this is a test sentence.')\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/163", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(rows=5, cols=5):\n    if cols > len(\"ABCDE\"):\n        raise ValueError(\"Number of columns exceeds available categories.\")\n\n    # Generate random data\n    data = np.random.rand(rows, cols)\n    categories = list(\"ABCDE\"[:cols])\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=categories)\n\n    # Plot a stacked bar chart\n    ax = df.plot(kind='bar', stacked=True, figsize=(10, 6))\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Value')\n\n    return ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(rows=5, cols=5):\n    if cols > len(\"ABCDE\"):\n        raise ValueError(\"Number of columns exceeds available categories.\")\n\n    # Generate random data\n    data = np.random.rand(rows, cols)\n    categories = list(\"ABCDE\"[:cols])\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=categories)\n\n    # Plot a stacked bar chart\n    ax = df.plot(kind='bar', stacked=True, figsize=(10, 6))\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Value')\n\n    return ax\n\n# Example usage:\n# ax = task_func(3, 3)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/164", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_labels=5, data_range=(0, 1)):\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.rand(num_labels, 2) * np.array(data_range) + np.array(data_range) / 2\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['Category1', 'Category2'])\n    df.index = [f'Label{i}' for i in range(num_labels)]\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Labels')\n    ax.set_ylabel('Values')\n    ax.legend(title='Categories')\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(num_labels=5, data_range=(0, 1)):\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.rand(num_labels, 2) * np.array(data_range) + np.array(data_range) / 2\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['Category1', 'Category2'])\n    df.index = [f'Label{i}' for i in range(num_labels)]\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Labels')\n    ax.set_ylabel('Values')\n    ax.legend(title='Categories')\n\n    return fig\n\n# Example usage\n# fig = task_func()\n# fig.show()\n```\n"}
{"task_id": "BigCodeBench/165", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    # Create a DataFrame with random integers\n    data = {\n        'A': [randint(*rand_range) for _ in range(num_rows)],\n        'B': [randint(*rand_range) for _ in range(num_rows)],\n        'C': [randint(*rand_range) for _ in range(num_rows)],\n        'D': [randint(*rand_range) for _ in range(num_rows)],\n        'E': [randint(*rand_range) for _ in range(num_rows)],\n    }\n    df = pd.DataFrame(data)\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    # Create a DataFrame with random integers\n    data = {\n        'A': [randint(*rand_range) for _ in range(num_rows)],\n        'B': [randint(*rand_range) for _ in range(num_rows)],\n        'C': [randint(*rand_range) for _ in range(num_rows)],\n        'D': [randint(*rand_range) for _ in range(num_rows)],\n        'E': [randint(*rand_range) for _ in range(num_rows)],\n    }\n    df = pd.DataFrame(data)\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    return fig\n```\n"}
{"task_id": "BigCodeBench/166", "solution": "import pandas as pd\nfrom datetime import datetime\nimport holidays\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime objects.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be before or equal to end_date.\")\n    \n    # Generate a list of all business days between start_date and end_date\n    all_days = pd.date_range(start=start_date, end=end_date, freq='B')\n    \n    # Get the public holidays for the specified country\n    country_holidays = holidays.CountryHoliday(country)\n    \n    # Filter out the holidays from the list of business days\n    business_days = [day for day in all_days if day not in country_holidays]\n    \n    return business_days", "raw_solution": "import pandas as pd\nfrom datetime import datetime\nimport holidays\n\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime objects.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be before or equal to end_date.\")\n    \n    # Generate a list of all business days between start_date and end_date\n    all_days = pd.date_range(start=start_date, end=end_date, freq='B')\n    \n    # Get the public holidays for the specified country\n    country_holidays = holidays.CountryHoliday(country)\n    \n    # Filter out the holidays from the list of business days\n    business_days = [day for day in all_days if day not in country_holidays]\n    \n    return business_days\n\n# Example usage:\n# business_days = task_func()\n# print(business_days[0])\n```\n"}
{"task_id": "BigCodeBench/167", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_types=5, integer_range=(0, 100)):\n    # Create a DataFrame with random integers\n    data = {f'Type{i+1}': [randint(*integer_range) for _ in range(num_types)] for i in range(num_types)}\n    df = pd.DataFrame(data)\n\n    # Plot the DataFrame\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_xlabel('Random Integer')\n    ax.set_ylabel('Category')\n    ax.set_title('Random Integer Distribution by Category')\n\n    return fig, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(num_types=5, integer_range=(0, 100)):\n    # Create a DataFrame with random integers\n    data = {f'Type{i+1}': [randint(*integer_range) for _ in range(num_types)] for i in range(num_types)}\n    df = pd.DataFrame(data)\n\n    # Plot the DataFrame\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_xlabel('Random Integer')\n    ax.set_ylabel('Category')\n    ax.set_title('Random Integer Distribution by Category')\n\n    return fig, ax\n```\n"}
{"task_id": "BigCodeBench/168", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_groups=5, data_size=5, labels=None):\n    if labels is None:\n        labels = [f'Group{i}' for i in range(1, num_groups + 1)]\n\n    # Generate random data\n    data = np.random.rand(data_size, num_groups)\n    df = pd.DataFrame(data, columns=labels)\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Values')\n\n    # Save the plot\n    plot_filename = 'test_plot.png'\n    fig.savefig(plot_filename)\n\n    return fig, df, plot_filename", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(num_groups=5, data_size=5, labels=None):\n    if labels is None:\n        labels = [f'Group{i}' for i in range(1, num_groups + 1)]\n\n    # Generate random data\n    data = np.random.rand(data_size, num_groups)\n    df = pd.DataFrame(data, columns=labels)\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Values')\n\n    # Save the plot\n    plot_filename = 'test_plot.png'\n    fig.savefig(plot_filename)\n\n    return fig, df, plot_filename\n```\n"}
{"task_id": "BigCodeBench/169", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\ndef task_func(image, sigma=2):\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input image must be a numpy.ndarray.\")\n    if sigma <= 0:\n        raise ValueError(\"Sigma must be a positive float.\")\n\n    filtered_image = gaussian_filter(image, sigma=sigma)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image, cmap='gray')\n    ax[0].set_title('Original')\n    ax[1].imshow(filtered_image, cmap='gray')\n    ax[1].set_title('Filtered')\n\n    return ax, filtered_image", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\n\ndef task_func(image, sigma=2):\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input image must be a numpy.ndarray.\")\n    if sigma <= 0:\n        raise ValueError(\"Sigma must be a positive float.\")\n\n    filtered_image = gaussian_filter(image, sigma=sigma)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image, cmap='gray')\n    ax[0].set_title('Original')\n    ax[1].imshow(filtered_image, cmap='gray')\n    ax[1].set_title('Filtered')\n\n    return ax, filtered_image\n\n# Example usage:\n# from skimage import data\n# ax, filtered_image = task_func(data.coins())\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/170", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url, sort_by_column=\"title\"):\n    response = requests.get(csv_url)\n    if response.status_code != 200:\n        raise Exception(\"Failed to fetch data from the URL.\")\n    \n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    df = df.sort_values(by=sort_by_column)\n    \n    return df", "raw_solution": "import pandas as pd\nimport requests\nfrom io import StringIO\n\ndef task_func(csv_url, sort_by_column=\"title\"):\n    response = requests.get(csv_url)\n    if response.status_code != 200:\n        raise Exception(\"Failed to fetch data from the URL.\")\n    \n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    df = df.sort_values(by=sort_by_column)\n    \n    return df\n\n# Example usage:\n# df = task_func(\"http://example.com/data.csv\", sort_by_column=\"price\")\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/171", "solution": "import random\nimport pandas as pd\nimport collections\nVEGETABLES = ['Carrot', 'Potato', 'Tomato', 'Cabbage', 'Spinach']\ndef task_func(vegetable_dict, seed=0):\n    random.seed(seed)\n    vegetable_counter = collections.Counter(vegetable_dict.values())\n    vegetable_counts = {veg: random.randint(1, 10) for veg in VEGETABLES}\n    \n    # Calculate percentage occurrence\n    total_counts = sum(vegetable_counts.values())\n    percentage_occurrence = {veg: (count / total_counts) * 100 if total_counts > 0 else 0 for veg, count in vegetable_counts.items()}\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Vegetable': list(percentage_occurrence.keys()),\n        'Count': list(vegetable_counts.values()),\n        'Percentage': list(percentage_occurrence.values())\n    })\n    \n    return df\nvegetable_dict = {'John': 'Carrot', 'Alice': 'Potato', 'Bob': 'Tomato'}", "raw_solution": "import random\nimport pandas as pd\nimport collections\n\n# Constants\nVEGETABLES = ['Carrot', 'Potato', 'Tomato', 'Cabbage', 'Spinach']\n\ndef task_func(vegetable_dict, seed=0):\n    random.seed(seed)\n    vegetable_counter = collections.Counter(vegetable_dict.values())\n    vegetable_counts = {veg: random.randint(1, 10) for veg in VEGETABLES}\n    \n    # Calculate percentage occurrence\n    total_counts = sum(vegetable_counts.values())\n    percentage_occurrence = {veg: (count / total_counts) * 100 if total_counts > 0 else 0 for veg, count in vegetable_counts.items()}\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Vegetable': list(percentage_occurrence.keys()),\n        'Count': list(vegetable_counts.values()),\n        'Percentage': list(percentage_occurrence.values())\n    })\n    \n    return df\n\n# Example usage\nvegetable_dict = {'John': 'Carrot', 'Alice': 'Potato', 'Bob': 'Tomato'}"}
{"task_id": "BigCodeBench/172", "solution": "import json\nfrom datetime import datetime\ndef task_func(json_data):\n    \"\"\"\n    NirDhaarit karain ki di gayee datetime weekend hai or nahi.\n\n    Notes:\n        Nikaale jaane wala datetime JSON data mein 'utc_datetime' kunji mein sthit hai.\n\n    Parameters:\n    - json_data (str): JSON string containing the datetime in UTC format.\n\n    Returns:\n        bool: True if the date is a weekend (Saturday or Sunday), False otherwise.\n\n    Requirements:\n    - json\n    - datetime\n\n    Example:\n    >>> json_data = '{\"utc_datetime\": \"2024-04-19T12:00:00\"}'\n    >>> task_func(json_data)\n    False\n\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Convert the UTC datetime string to a datetime object\n    utc_datetime = datetime.strptime(data['utc_datetime'], \"%Y-%m-%dT%H:%M:%S\")\n    \n    # Check if the date is a weekend\n    return utc_datetime.weekday() >= 5  # 5 for Saturday and 6 for Sunday", "raw_solution": "import json\nfrom datetime import datetime\n\ndef task_func(json_data):\n    \"\"\"\n    NirDhaarit karain ki di gayee datetime weekend hai or nahi.\n\n    Notes:\n        Nikaale jaane wala datetime JSON data mein 'utc_datetime' kunji mein sthit hai.\n\n    Parameters:\n    - json_data (str): JSON string containing the datetime in UTC format.\n\n    Returns:\n        bool: True if the date is a weekend (Saturday or Sunday), False otherwise.\n\n    Requirements:\n    - json\n    - datetime\n\n    Example:\n    >>> json_data = '{\"utc_datetime\": \"2024-04-19T12:00:00\"}'\n    >>> task_func(json_data)\n    False\n\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Convert the UTC datetime string to a datetime object\n    utc_datetime = datetime.strptime(data['utc_datetime'], \"%Y-%m-%dT%H:%M:%S\")\n    \n    # Check if the date is a weekend\n    return utc_datetime.weekday() >= 5  # 5 for Saturday and 6 for Sunday\n```\n"}
{"task_id": "BigCodeBench/173", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(country_dict):\n    countries = ['USA', 'UK', 'China', 'Japan', 'Australia']\n    gdp_values = np.random.randint(1000000000, 10000000000, size=len(countries))\n    df = pd.DataFrame(gdp_values, index=countries, columns=['GDP'])\n\n    return df\ncountry_dict = {'John': 'USA', 'Alice': 'UK', 'Bob': 'China', 'Charlie': 'Japan', 'David': 'Australia'}", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(country_dict):\n    countries = ['USA', 'UK', 'China', 'Japan', 'Australia']\n    gdp_values = np.random.randint(1000000000, 10000000000, size=len(countries))\n    df = pd.DataFrame(gdp_values, index=countries, columns=['GDP'])\n\n    return df\n\n# Example usage:\nnp.random.seed(0)\ncountry_dict = {'John': 'USA', 'Alice': 'UK', 'Bob': 'China', 'Charlie': 'Japan', 'David': 'Australia'}\ndf = task_func(country_dict)"}
{"task_id": "BigCodeBench/174", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, key, min_value, max_value):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    data[key] = np.random.randint(min_value, max_value, size=len(data))\n    return data", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data, key, min_value, max_value):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    data[key] = np.random.randint(min_value, max_value, size=len(data))\n    return data\n\n# Example usage:"}
{"task_id": "BigCodeBench/175", "solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    # Check if the DataFrame contains the necessary columns\n    if not {'Title', 'Views', 'Likes'}.issubset(df.columns):\n        fig, ax = plt.subplots()\n        return ax\n\n    # Check if the DataFrame is empty\n    if df.empty:\n        fig, ax = plt.subplots()\n        return ax\n\n    # Check if there are any rows matching the search criteria\n    search_pattern = re.compile(r'how|what', re.IGNORECASE)\n    matching_rows = df['Title'].apply(lambda x: bool(search_pattern.search(x)))\n    if not matching_rows.any():\n        fig, ax = plt.subplots()\n        return ax\n\n    # Filter the DataFrame to include only the matching rows\n    filtered_df = df[matching_rows]\n\n    # Calculate the like ratio\n    filtered_df['Like Ratio'] = filtered_df['Likes'] / filtered_df['Views']\n\n    # Plot the like ratio\n    fig, ax = plt.subplots()\n    ax.bar(filtered_df['Title'], filtered_df['Like Ratio'])\n    ax.set_xlabel('Title')\n    ax.set_ylabel('Like Ratio')\n    ax.set_title('Like Ratio of Videos')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax\ndata = {'Title': ['How to code', 'What is Python', 'Tutorial'], 'Views': [1500, 1200, 1000], 'Likes': [150, 300, 100]}\ndf = pd.DataFrame(data)", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df):\n    # Check if the DataFrame contains the necessary columns\n    if not {'Title', 'Views', 'Likes'}.issubset(df.columns):\n        fig, ax = plt.subplots()\n        return ax\n\n    # Check if the DataFrame is empty\n    if df.empty:\n        fig, ax = plt.subplots()\n        return ax\n\n    # Check if there are any rows matching the search criteria\n    search_pattern = re.compile(r'how|what', re.IGNORECASE)\n    matching_rows = df['Title'].apply(lambda x: bool(search_pattern.search(x)))\n    if not matching_rows.any():\n        fig, ax = plt.subplots()\n        return ax\n\n    # Filter the DataFrame to include only the matching rows\n    filtered_df = df[matching_rows]\n\n    # Calculate the like ratio\n    filtered_df['Like Ratio'] = filtered_df['Likes'] / filtered_df['Views']\n\n    # Plot the like ratio\n    fig, ax = plt.subplots()\n    ax.bar(filtered_df['Title'], filtered_df['Like Ratio'])\n    ax.set_xlabel('Title')\n    ax.set_ylabel('Like Ratio')\n    ax.set_title('Like Ratio of Videos')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:\ndata = {'Title': ['How to code', 'What is Python', 'Tutorial'], 'Views': [1500, 1200, 1000], 'Likes': [150, 300, 100]}\ndf = pd.DataFrame(data)\nax = task_func(df)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/176", "solution": "import re\nimport socket\ndef task_func(ip_addresses: list) -> dict:\n    \"\"\"\n    Dee gayee IP addresses kee ek 'list' ke saath, yah 'function' ek 'dictionary' lautata hai jo har 'valid' IP addresses ko uske\n    'respective' 'hostname' ke saath 'mapping' karta hai. Agar 'hostname' nirdhaarit nahin kiya jaa sakata hai, to 'value' 'None' hoga.\n\n    Parameters:\n    - ip_addresses (list): IP addresses kee ek 'list'.\n\n    Returns:\n        dict: Ek 'dictionary' jisamen IP addresses 'keys' ke roop mein hain aur unake 'hostnames' 'values' ke roop mein hain. Agar 'hostname' nirdhaarit nahin kiya jaa sakata hai,\n        'value' 'None' hoga.\n\n    Requirements:\n    - re\n    - socket\n\n    Example:\n    >>> task_func(['8.8.8.8', '8.8.4.4'])\n    {'8.8.8.8': 'dns.google', '8.8.4.4': 'dns.google'}\n\n    \"\"\"\n    ip_pattern = re.compile(r'^(\\d{1,3}\\.){3}\\d{1,3}$')\n    result = {}\n\n    for ip in ip_addresses:\n        if ip_pattern.match(ip):\n            try:\n                hostname = socket.gethostbyaddr(ip)[0]\n                result[ip] = hostname\n            except socket.herror:\n                result[ip] = None\n\n    return result", "raw_solution": "import re\nimport socket\n\ndef task_func(ip_addresses: list) -> dict:\n    \"\"\"\n    Dee gayee IP addresses kee ek 'list' ke saath, yah 'function' ek 'dictionary' lautata hai jo har 'valid' IP addresses ko uske\n    'respective' 'hostname' ke saath 'mapping' karta hai. Agar 'hostname' nirdhaarit nahin kiya jaa sakata hai, to 'value' 'None' hoga.\n\n    Parameters:\n    - ip_addresses (list): IP addresses kee ek 'list'.\n\n    Returns:\n        dict: Ek 'dictionary' jisamen IP addresses 'keys' ke roop mein hain aur unake 'hostnames' 'values' ke roop mein hain. Agar 'hostname' nirdhaarit nahin kiya jaa sakata hai,\n        'value' 'None' hoga.\n\n    Requirements:\n    - re\n    - socket\n\n    Example:\n    >>> task_func(['8.8.8.8', '8.8.4.4'])\n    {'8.8.8.8': 'dns.google', '8.8.4.4': 'dns.google'}\n\n    \"\"\"\n    ip_pattern = re.compile(r'^(\\d{1,3}\\.){3}\\d{1,3}$')\n    result = {}\n\n    for ip in ip_addresses:\n        if ip_pattern.match(ip):\n            try:\n                hostname = socket.gethostbyaddr(ip)[0]\n                result[ip] = hostname\n            except socket.herror:\n                result[ip] = None\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\nimport pandas as pd\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Title' and 'Content' columns.\")\n    \n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    \n    keyword = r'\\b(like|vhaatt)\\b'\n    title_content = df['Title'] + df['Content']\n    \n    # Combine all text data into a single string\n    all_text = ' '.join(title_content)\n    \n    # Remove punctuation and tokenize words\n    words = nltk.word_tokenize(all_text)\n    words = [word.lower() for word in words if word.isalpha()]\n    \n    # Filter words that match the keyword\n    matched_words = [word for word in words if re.match(keyword, word)]\n    \n    # Count frequency of matched words\n    word_freq = Counter(matched_words)\n    \n    return dict(word_freq)", "raw_solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\nimport pandas as pd\n\nnltk.download('punkt')\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Title' and 'Content' columns.\")\n    \n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    \n    keyword = r'\\b(like|vhaatt)\\b'\n    title_content = df['Title'] + df['Content']\n    \n    # Combine all text data into a single string\n    all_text = ' '.join(title_content)\n    \n    # Remove punctuation and tokenize words\n    words = nltk.word_tokenize(all_text)\n    words = [word.lower() for word in words if word.isalpha()]\n    \n    # Filter words that match the keyword\n    matched_words = [word for word in words if re.match(keyword, word)]\n    \n    # Count frequency of matched words\n    word_freq = Counter(matched_words)\n    \n    return dict(word_freq)\n```\n"}
{"task_id": "BigCodeBench/178", "solution": "import re\nimport json\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(ip_address):\n    \"\"\"\n    Get the public IP address from the JSON response that includes the IP address.\n\n    Notes:\n        The function needs to check if the provided IP address is valid.\n        If the IP address is not valid, the function will return 'Invalid IP address received'.\n\n    Parameters:\n    - ip_address (str): IP address, a JSON-formatted string.\n\n    Returns:\n        str: The public IP address.\n\n    Requirements:\n    - re\n    - urllib.request\n    - json\n\n    Example:\n    >>> ip_address = '{\"ip\": \"192.168.1.1\"}'\n    >>> task_func(ip_address)\n    '192.168.1.1'\n\n    \"\"\"\n    try:\n        # Parse the JSON string\n        ip_data = json.loads(ip_address)\n        \n        # Extract the IP address\n        ip = ip_data.get('ip', '')\n        \n        # Validate the IP address\n        if re.match(IP_REGEX, ip):\n            return ip\n        else:\n            return 'Invalid IP address received'\n    except json.JSONDecodeError:\n        return 'Invalid JSON received'", "raw_solution": "import re\nimport json\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef task_func(ip_address):\n    \"\"\"\n    Get the public IP address from the JSON response that includes the IP address.\n\n    Notes:\n        The function needs to check if the provided IP address is valid.\n        If the IP address is not valid, the function will return 'Invalid IP address received'.\n\n    Parameters:\n    - ip_address (str): IP address, a JSON-formatted string.\n\n    Returns:\n        str: The public IP address.\n\n    Requirements:\n    - re\n    - urllib.request\n    - json\n\n    Example:\n    >>> ip_address = '{\"ip\": \"192.168.1.1\"}'\n    >>> task_func(ip_address)\n    '192.168.1.1'\n\n    \"\"\"\n    try:\n        # Parse the JSON string\n        ip_data = json.loads(ip_address)\n        \n        # Extract the IP address\n        ip = ip_data.get('ip', '')\n        \n        # Validate the IP address\n        if re.match(IP_REGEX, ip):\n            return ip\n        else:\n            return 'Invalid IP address received'\n    except json.JSONDecodeError:\n        return 'Invalid JSON received'\n\n# Test cases"}
{"task_id": "BigCodeBench/179", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ndef task_func(df):\n    # Check if required columns exist\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        fig, ax = plt.subplots()\n        ax.set_title('TF-IDF scores')\n        ax.set_xlabel('Features')\n        ax.set_ylabel('Scores')\n        return ax\n\n    # Check if any article has 'kaise' or 'kya' in title\n    if not any(re.search(r'\\b(kaise|kya)\\b', title) for title in df['Title']):\n        fig, ax = plt.subplots()\n        ax.set_title('TF-IDF scores')\n        ax.set_xlabel('Features')\n        ax.set_ylabel('Scores')\n        return ax\n\n    # Combine title and content\n    text_data = df['Title'] + ' ' + df['Content']\n\n    # Initialize TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the data\n    tfidf_matrix = vectorizer.fit_transform(text_data)\n\n    # Extract feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Check if any feature name contains 'kaise' or 'kya'\n    relevant_features = [name for name in feature_names if re.search(r'\\b(kaise|kya)\\b', name)]\n\n    # Plot only relevant features\n    if relevant_features:\n        scores = tfidf_matrix.mean(axis=0).toarray()[0]\n        relevant_scores = [scores[feature_names.tolist().index(name)] for name in relevant_features]\n\n        fig, ax = plt.subplots()\n        ax.barh(relevant_features, relevant_scores, color='skyblue')\n        ax.set_title('TF-IDF scores')\n        ax.set_xlabel('Scores')\n        ax.set_ylabel('Features')\n        ax.set_yticks(np.arange(len(relevant_features)))\n        ax.set_yticklabels(relevant_features, rotation='vertical')\n        return ax\n    else:\n        fig, ax = plt.subplots()\n        ax.set_title('TF-IDF scores')\n        ax.set_xlabel('Features')\n        ax.set_ylabel('Scores')\n        return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ndef task_func(df):\n    # Check if required columns exist\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        fig, ax = plt.subplots()\n        ax.set_title('TF-IDF scores')\n        ax.set_xlabel('Features')\n        ax.set_ylabel('Scores')\n        return ax\n\n    # Check if any article has 'kaise' or 'kya' in title\n    if not any(re.search(r'\\b(kaise|kya)\\b', title) for title in df['Title']):\n        fig, ax = plt.subplots()\n        ax.set_title('TF-IDF scores')\n        ax.set_xlabel('Features')\n        ax.set_ylabel('Scores')\n        return ax\n\n    # Combine title and content\n    text_data = df['Title'] + ' ' + df['Content']\n\n    # Initialize TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the data\n    tfidf_matrix = vectorizer.fit_transform(text_data)\n\n    # Extract feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Check if any feature name contains 'kaise' or 'kya'\n    relevant_features = [name for name in feature_names if re.search(r'\\b(kaise|kya)\\b', name)]\n\n    # Plot only relevant features\n    if relevant_features:\n        scores = tfidf_matrix.mean(axis=0).toarray()[0]\n        relevant_scores = [scores[feature_names.tolist().index(name)] for name in relevant_features]\n\n        fig, ax = plt.subplots()\n        ax.barh(relevant_features, relevant_scores, color='skyblue')\n        ax.set_title('TF-IDF scores')\n        ax.set_xlabel('Scores')\n        ax.set_ylabel('Features')\n        ax.set_yticks(np.arange(len(relevant_features)))\n        ax.set_yticklabels(relevant_features, rotation='vertical')\n        return ax\n    else:\n        fig, ax = plt.subplots()\n        ax.set_title('TF-IDF scores')\n        ax.set_xlabel('Features')\n        ax.set_ylabel('Scores')\n        return ax\n```\n"}
{"task_id": "BigCodeBench/180", "solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The file {img_path} does not exist.\")\n    \n    # Load the image\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    results = []\n    for scale_factor in scale_factors:\n        # Resize the image\n        scaled_img = resize(img_array, (int(img_array.shape[0] * scale_factor), int(img_array.shape[1] * scale_factor)), anti_aliasing=True)\n        \n        # Plot the image\n        fig, ax = plt.subplots()\n        ax.imshow(scaled_img)\n        ax.set_title(f'Scale factor: {scale_factor} ({scaled_img.shape[0]}, {scaled_img.shape[1]}, {scaled_img.shape[2]})')\n        results.append((ax, scaled_img))\n    \n    return results", "raw_solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The file {img_path} does not exist.\")\n    \n    # Load the image\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    results = []\n    for scale_factor in scale_factors:\n        # Resize the image\n        scaled_img = resize(img_array, (int(img_array.shape[0] * scale_factor), int(img_array.shape[1] * scale_factor)), anti_aliasing=True)\n        \n        # Plot the image\n        fig, ax = plt.subplots()\n        ax.imshow(scaled_img)\n        ax.set_title(f'Scale factor: {scale_factor} ({scaled_img.shape[0]}, {scaled_img.shape[1]}, {scaled_img.shape[2]})')\n        results.append((ax, scaled_img))\n    \n    return results\n\n# Example usage:\n# dummy_img_path = \"sample.png\"\n# Image.fromarray(np.random.randint(0, 255, (20, 20, 3), dtype=np.uint8)).save(dummy_img_path)\n# result = task_func('sample.png')\n# os.remove(dummy_img_path)\n# for ax, img in result:\n#     print(ax.get_title(), img.shape)\n```\n"}
{"task_id": "BigCodeBench/181", "solution": "from django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\ndef task_func(data, min_delay, max_delay):\n    \"\"\"\n    Ek yaadrcchik **delay** ke baad, network ki **latency** ka anukaran karne ke lie **json data** ke saath ek django HttpResponse generate karem.\n\n    Parameters:\n    - data (str): response **body** mem samil kie jane vala **data** / \n    - min_delay (int): int mem min_delay **delay** / \n    - max_delay (int): int mem maximum **delay** / \n\n    Returns:\n        HttpResponse: **json data** ke saath ek django HttpResponse / \n\n    Requirements:\n    - django\n    - random\n    - time\n\n    Example:\n    >>> import json\n    >>> random.seed(0)\n    >>> response = task_func(json.dumps({\"Sample-Key\": \"Sample-Value\"}), 1, 5)\n    >>> response.status_code\n    200\n    >>> json.loads(response.content)\n    {\"Sample-Key\": \"Sample-Value\"}\n\n    \"\"\"\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n    return HttpResponse(data, content_type='application/json')", "raw_solution": "from django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\n\ndef task_func(data, min_delay, max_delay):\n    \"\"\"\n    Ek yaadrcchik **delay** ke baad, network ki **latency** ka anukaran karne ke lie **json data** ke saath ek django HttpResponse generate karem.\n\n    Parameters:\n    - data (str): response **body** mem samil kie jane vala **data** / \n    - min_delay (int): int mem min_delay **delay** / \n    - max_delay (int): int mem maximum **delay** / \n\n    Returns:\n        HttpResponse: **json data** ke saath ek django HttpResponse / \n\n    Requirements:\n    - django\n    - random\n    - time\n\n    Example:\n    >>> import json\n    >>> random.seed(0)\n    >>> response = task_func(json.dumps({\"Sample-Key\": \"Sample-Value\"}), 1, 5)\n    >>> response.status_code\n    200\n    >>> json.loads(response.content)\n    {\"Sample-Key\": \"Sample-Value\"}\n\n    \"\"\"\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n    return HttpResponse(data, content_type='application/json')\n```\n"}
{"task_id": "BigCodeBench/182", "solution": "import re\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\ndef task_func(df):\n    # Extract titles and content\n    titles = df['Title'].tolist()\n    content = df['Content'].tolist()\n\n    # Combine titles and content\n    combined_text = [f\"{title} {content}\" for title, content in zip(titles, content)]\n\n    # Use CountVectorizer to convert text to vectors\n    vectorizer = CountVectorizer()\n    vectors = vectorizer.fit_transform(combined_text)\n\n    # Use KMeans clustering\n    kmeans = KMeans(n_clusters=2, random_state=0)\n    kmeans.fit(vectors)\n\n    # Get cluster labels\n    labels = kmeans.labels_\n\n    return labels.tolist()", "raw_solution": "import re\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ndef task_func(df):\n    # Extract titles and content\n    titles = df['Title'].tolist()\n    content = df['Content'].tolist()\n\n    # Combine titles and content\n    combined_text = [f\"{title} {content}\" for title, content in zip(titles, content)]\n\n    # Use CountVectorizer to convert text to vectors\n    vectorizer = CountVectorizer()\n    vectors = vectorizer.fit_transform(combined_text)\n\n    # Use KMeans clustering\n    kmeans = KMeans(n_clusters=2, random_state=0)\n    kmeans.fit(vectors)\n\n    # Get cluster labels\n    labels = kmeans.labels_\n\n    return labels.tolist()\n\n# Example usage"}
{"task_id": "BigCodeBench/183", "solution": "from django.http import HttpResponse\nimport uuid\ndef task_func(data):\n    \"\"\"\n    json data ke saath ek django HttpResponse response, and requests ko track karne ke liye HTTP header mein ek uuid body karein.\n\n    Parameters:\n    - data (str): The json-formatted data to be included in the response body.\n\n    Returns:\n        HttpResponse: json data and uuid ke saath ek django HttpResponse.\n\n    Requirements:\n    - django\n    - uuid\n\n    Example:\n    >>> import json\n    >>> response = task_func(json.dumps({\"Sample-Key\": \"Sample-Value\"}))\n    >>> response.has_key('uuid')\n    true\n\n    \"\"\"\n    # Generate a UUID\n    unique_id = str(uuid.uuid4())\n\n    # Create an HttpResponse with the data and the unique_id in the header\n    response = HttpResponse(data, content_type='application/json')\n    response['uuid'] = unique_id\n\n    return response", "raw_solution": "from django.http import HttpResponse\nimport uuid\n\ndef task_func(data):\n    \"\"\"\n    json data ke saath ek django HttpResponse response, and requests ko track karne ke liye HTTP header mein ek uuid body karein.\n\n    Parameters:\n    - data (str): The json-formatted data to be included in the response body.\n\n    Returns:\n        HttpResponse: json data and uuid ke saath ek django HttpResponse.\n\n    Requirements:\n    - django\n    - uuid\n\n    Example:\n    >>> import json\n    >>> response = task_func(json.dumps({\"Sample-Key\": \"Sample-Value\"}))\n    >>> response.has_key('uuid')\n    true\n\n    \"\"\"\n    # Generate a UUID\n    unique_id = str(uuid.uuid4())\n\n    # Create an HttpResponse with the data and the unique_id in the header\n    response = HttpResponse(data, content_type='application/json')\n    response['uuid'] = unique_id\n\n    return response\n```\n"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    # Remove stopwords and numbers from the text\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in re.sub(r'\\d+', '', x).lower().split() if word not in STOPWORDS]))\n\n    # Vectorize the text\n    vectorizer = CountVectorizer()\n    vectorized_data = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert the vectorized data to a DataFrame\n    vectorized_df = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return vectorized_df", "raw_solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Remove stopwords and numbers from the text\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in re.sub(r'\\d+', '', x).lower().split() if word not in STOPWORDS]))\n\n    # Vectorize the text\n    vectorizer = CountVectorizer()\n    vectorized_data = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert the vectorized data to a DataFrame\n    vectorized_df = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return vectorized_df\n```\n"}
{"task_id": "BigCodeBench/185", "solution": "import pandas as pd\nimport numpy as np\nimport folium\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"The dictionary must contain 'Lon' and 'Lat' keys.\")\n    if not all(isinstance(value, tuple) and len(value) == 2 for value in dic.values()):\n        raise ValueError(\"The values of 'Lon' and 'Lat' must be tuples of two numbers each.\")\n\n    city_data = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        city_data.append({'City': city, 'longitude': lon, 'latitude': lat})\n\n    df = pd.DataFrame(city_data)\n\n    map_obj = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=2)\n    for _, row in df.iterrows():\n        folium.Marker([row['latitude'], row['longitude']], tooltip=row['City']).add_to(map_obj)\n\n    return map_obj, df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport folium\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"The dictionary must contain 'Lon' and 'Lat' keys.\")\n    if not all(isinstance(value, tuple) and len(value) == 2 for value in dic.values()):\n        raise ValueError(\"The values of 'Lon' and 'Lat' must be tuples of two numbers each.\")\n\n    city_data = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        city_data.append({'City': city, 'longitude': lon, 'latitude': lat})\n\n    df = pd.DataFrame(city_data)\n\n    map_obj = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=2)\n    for _, row in df.iterrows():\n        folium.Marker([row['latitude'], row['longitude']], tooltip=row['City']).add_to(map_obj)\n\n    return map_obj, df\n```\nThis script creates a map with markers for each city with randomly generated coordinates within the specified ranges. It also returns a pandas DataFrame containing the city names, longitudes, and latitudes.\n"}
{"task_id": "BigCodeBench/186", "solution": "from geopy.distance import geodesic\nimport folium\ndef task_func(dic):\n    if not dic:\n        raise ValueError(\"Input dictionary is empty.\")\n    \n    # Initialize the folium map\n    folium_map = folium.Map(location=[0, 0], zoom_start=2)\n\n    # Add markers to the map\n    location_data = {}\n    for name, coords in dic.items():\n        folium.Marker([coords['Lat'], coords['Lon']], tooltip=name).add_to(folium_map)\n        location_data[name] = coords\n\n    # Calculate distances between all pairs of locations\n    distances = {}\n    for name1, coords1 in location_data.items():\n        for name2, coords2 in location_data.items():\n            if name1 != name2:\n                dist = geodesic((coords1['Lat'], coords1['Lon']), (coords2['Lat'], coords2['Lon'])).kilometers\n                distances[(name1, name2)] = dist\n\n    return folium_map, distances", "raw_solution": "from geopy.distance import geodesic\nimport folium\n\ndef task_func(dic):\n    if not dic:\n        raise ValueError(\"Input dictionary is empty.\")\n    \n    # Initialize the folium map\n    folium_map = folium.Map(location=[0, 0], zoom_start=2)\n\n    # Add markers to the map\n    location_data = {}\n    for name, coords in dic.items():\n        folium.Marker([coords['Lat'], coords['Lon']], tooltip=name).add_to(folium_map)\n        location_data[name] = coords\n\n    # Calculate distances between all pairs of locations\n    distances = {}\n    for name1, coords1 in location_data.items():\n        for name2, coords2 in location_data.items():\n            if name1 != name2:\n                dist = geodesic((coords1['Lat'], coords1['Lon']), (coords2['Lat'], coords2['Lon'])).kilometers\n                distances[(name1, name2)] = dist\n\n    return folium_map, distances\n\n# Example usage:\n# result = task_func({'Place1': {'Lat': 0, 'Lon': 0}, 'Place2': {'Lat': 0, 'Lon': 1}})\n# print(isinstance(result, tuple) and len(result) == 2)\n# print(isinstance(result[0], folium.folium.Map) and isinstance(result[1], dict))\n```\n"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Ek GeoPandas DataFrame banaen **city** kee ek **list** ke lie, yaadrchhik roop se utpann **coordinates** ke saath, nirdisht **ranges** ke aadhaar par.\n\n    Parameters:\n    - dic (dict): **dictionary** 'Lon' aur 'Lat' kunjeeon ke saath, pratyek ek tupal (min, max) **coordinates** seema ke lie.\n    - Default: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - cities (list): **city** ke naamon keee **list** . Default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n    Returns:\n        GeoDataFrame: Ek GeoPandas DataFrame jisamen 'City' aur 'Coordinates' (**bindu** **objects**) shaamil hain.\n\n    Requirements:\n    - numpy\n    - geopandas\n    - shapely.geometry\n\n    Raises:\n    - ValueError: Agar **dictionary** mein 'Lon' ya 'Lat' **keys** gaayab hain, ya agar unake **values** **tupals** nahin hain.\n\n    Example:\n    >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    >>> gdf = task_func(dic)\n\n    \"\"\"\n    if not all(isinstance(value, tuple) and len(value) == 2 for value in dic.values()):\n        raise ValueError(\"Values in the dictionary must be tuples of length 2.\")\n    \n    data = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        point = Point(lon, lat)\n        data.append({'City': city, 'Coordinates': point})\n    \n    gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n    return gdf", "raw_solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Ek GeoPandas DataFrame banaen **city** kee ek **list** ke lie, yaadrchhik roop se utpann **coordinates** ke saath, nirdisht **ranges** ke aadhaar par.\n\n    Parameters:\n    - dic (dict): **dictionary** 'Lon' aur 'Lat' kunjeeon ke saath, pratyek ek tupal (min, max) **coordinates** seema ke lie.\n    - Default: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - cities (list): **city** ke naamon keee **list** . Default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n    Returns:\n        GeoDataFrame: Ek GeoPandas DataFrame jisamen 'City' aur 'Coordinates' (**bindu** **objects**) shaamil hain.\n\n    Requirements:\n    - numpy\n    - geopandas\n    - shapely.geometry\n\n    Raises:\n    - ValueError: Agar **dictionary** mein 'Lon' ya 'Lat' **keys** gaayab hain, ya agar unake **values** **tupals** nahin hain.\n\n    Example:\n    >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    >>> gdf = task_func(dic)\n\n    \"\"\"\n    if not all(isinstance(value, tuple) and len(value) == 2 for value in dic.values()):\n        raise ValueError(\"Values in the dictionary must be tuples of length 2.\")\n    \n    data = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        point = Point(lon, lat)\n        data.append({'City': city, 'Coordinates': point})\n    \n    gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n    return gdf\n```\n"}
{"task_id": "BigCodeBench/188", "solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\ndef task_func(dic):\n    geolocator = Photon(user_agent=\"geoapiExercises\")\n    map_ = folium.Map(location=[0, 0], zoom_start=2)\n\n    for key, value in dic.items():\n        if isinstance(value, dict) and 'Lat' in value and 'Lon' in value:\n            folium.Marker([value['Lat'], value['Lon']], tooltip=key).add_to(map_)\n        elif isinstance(value, str):\n            location = geolocator.geocode(value)\n            if location:\n                folium.Marker([location.latitude, location.longitude], tooltip=key).add_to(map_)\n    \n    return map_", "raw_solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\n\ndef task_func(dic):\n    geolocator = Photon(user_agent=\"geoapiExercises\")\n    map_ = folium.Map(location=[0, 0], zoom_start=2)\n\n    for key, value in dic.items():\n        if isinstance(value, dict) and 'Lat' in value and 'Lon' in value:\n            folium.Marker([value['Lat'], value['Lon']], tooltip=key).add_to(map_)\n        elif isinstance(value, str):\n            location = geolocator.geocode(value)\n            if location:\n                folium.Marker([location.latitude, location.longitude], tooltip=key).add_to(map_)\n    \n    return map_\n\n# Example usage:\nlocations = {'Place1': {'Lat': 0, 'Lon': 0}, 'Place2': 'New York, USA'}\nresult = task_func(locations)"}
{"task_id": "BigCodeBench/189", "solution": "import re\nimport json\nimport requests\ndef task_func(data_url: str) -> list:\n    try:\n        response = requests.get(data_url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(\"Invalid URL input\") from e\n\n    data = response.json()\n    names = data.get(\"names\", [])\n    \n    # Using regex to filter out names within square brackets\n    pattern = re.compile(r'\\[.*?\\]')\n    filtered_names = [name for name in names if not pattern.search(name)]\n\n    return filtered_names", "raw_solution": "import re\nimport json\nimport requests\n\ndef task_func(data_url: str) -> list:\n    try:\n        response = requests.get(data_url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(\"Invalid URL input\") from e\n\n    data = response.json()\n    names = data.get(\"names\", [])\n    \n    # Using regex to filter out names within square brackets\n    pattern = re.compile(r'\\[.*?\\]')\n    filtered_names = [name for name in names if not pattern.search(name)]\n\n    return filtered_names\n\n# Example usage:\n# Assuming the URL returns a JSON response with the structure {\"names\": [\"John\", \"[Adam]\", \"Eve\"]}\n# print(task_func(\"https://api.example.com/other_data\"))\n```\n"}
{"task_id": "BigCodeBench/190", "solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\ndef task_func(csv_input):\n    # Connect to the SQLite database (or create it if it doesn't exist)\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    # Create a new table\n    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {TABLE_NAME} (id INTEGER PRIMARY KEY, name TEXT)\")\n\n    # Read the CSV data\n    if isinstance(csv_input, StringIO):\n        df = pd.read_csv(csv_input)\n    else:\n        df = pd.read_csv(csv_input)\n\n    # Insert data into the table\n    for _, row in df.iterrows():\n        cursor.execute(f\"INSERT INTO {TABLE_NAME} (id, name) VALUES (?, ?)\", (row['id'], row['name']))\n\n    # Commit the changes\n    conn.commit()\n\n    # Query the data back into a DataFrame\n    query = f\"SELECT * FROM {TABLE_NAME}\"\n    df_result = pd.read_sql_query(query, conn)\n\n    # Close the connection\n    conn.close()\n\n    return df_result", "raw_solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\n\n# Constants\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\n\n\ndef task_func(csv_input):\n    # Connect to the SQLite database (or create it if it doesn't exist)\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    # Create a new table\n    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {TABLE_NAME} (id INTEGER PRIMARY KEY, name TEXT)\")\n\n    # Read the CSV data\n    if isinstance(csv_input, StringIO):\n        df = pd.read_csv(csv_input)\n    else:\n        df = pd.read_csv(csv_input)\n\n    # Insert data into the table\n    for _, row in df.iterrows():\n        cursor.execute(f\"INSERT INTO {TABLE_NAME} (id, name) VALUES (?, ?)\", (row['id'], row['name']))\n\n    # Commit the changes\n    conn.commit()\n\n    # Query the data back into a DataFrame\n    query = f\"SELECT * FROM {TABLE_NAME}\"\n    df_result = pd.read_sql_query(query, conn)\n\n    # Close the connection\n    conn.close()\n\n    return df_result\n```\n"}
{"task_id": "BigCodeBench/191", "solution": "import random\nfrom scipy import stats\ndef task_func(animals, mean):\n    sales = {animal: 0 for animal in animals}\n    for _ in range(random.randint(50, 200)):  # Random number of customers\n        chosen_animal = random.choice(animals)\n        sales[chosen_animal] += 1\n\n    # Validate sales data\n    assert all(animal in animals for animal in sales.keys()), \"All animals should be in the sales dictionary.\"\n    assert sum(sales.values()) >= 0, \"Sales should be non-negative.\"\n\n    # Generate random sales data based on a Poisson distribution\n    random.seed(42)  # For reproducibility\n    for animal in sales:\n        sales[animal] = stats.poisson.rvs(mean, size=1)[0]\n\n    return sales\nANIMALS = ['Dog', 'Cat', 'Bird', 'Fish', 'Hamster']", "raw_solution": "import random\nfrom scipy import stats\n\ndef task_func(animals, mean):\n    sales = {animal: 0 for animal in animals}\n    for _ in range(random.randint(50, 200)):  # Random number of customers\n        chosen_animal = random.choice(animals)\n        sales[chosen_animal] += 1\n\n    # Validate sales data\n    assert all(animal in animals for animal in sales.keys()), \"All animals should be in the sales dictionary.\"\n    assert sum(sales.values()) >= 0, \"Sales should be non-negative.\"\n\n    # Generate random sales data based on a Poisson distribution\n    random.seed(42)  # For reproducibility\n    for animal in sales:\n        sales[animal] = stats.poisson.rvs(mean, size=1)[0]\n\n    return sales\n\n# Example usage\nANIMALS = ['Dog', 'Cat', 'Bird', 'Fish', 'Hamster']\nsales = task_func(ANIMALS, 120)"}
{"task_id": "BigCodeBench/192", "solution": "import re\nimport smtplib\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    \"\"\"\n    Ek string se, jo square brackets mein band nahi hain, sabhi names ko nikaalen and unhein ek email mein bhejen.\n\n    Notes:\n        Email mein message \"Subject: Extracted Names\\n\\n\" ke roop mein swaroopit hai, jismein nikala gaya name \"\\nJosie Smith\\nMugsy Dog Smith\" hai.\n\n    Parameters:\n    - text (str): Vah re jisse name nikaalne hain.\n    - smtp_server (str): email bhejne ke liye upayog kiya jaane vaala SMTP server.\n    - smtp_port (int): SMTP server ke liye upayog kiya jaane vaala int.\n    - email_address (str): Vah email address jisse email bhejna hai.\n    - email_password (str): email pate ka password.\n    - recepient_address (str): recepient ka email address.\n\n    Returns:\n        list: Nikale gaye names ki ek list.\n\n    Requirements:\n    - re\n    - smtplib\n\n    Example:\n    >>> from unittest.mock import MagicMock\n    >>> mock_smtp_instance = MagicMock()\n    >>> mock_smtp = MagicMock(return_value=mock_smtp_instance)\n    >>> task_func(text=\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\", smtp=mock_smtp)\n    ['Josie Smith', 'Mugsy Dog Smith']\n\n    \"\"\"\n    # Extract names from the text\n    names = re.findall(r'\\b\\w+\\s\\w+\\b', text)\n\n    # Create the email message\n    email_message = \"Subject: Extracted Names\\n\\n\" + \"\\n\".join(names)\n\n    # Send the email\n    if smtp:\n        smtp_instance = smtp(smtp_server, smtp_port)\n        smtp_instance.starttls()\n        smtp_instance.login(email_address, email_password)\n        smtp_instance.sendmail(email_address, recepient_address, email_message)\n        smtp_instance.quit()\n    else:\n        with smtplib.SMTP(smtp_server, smtp_port) as server:\n            server.starttls()\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recepient_address, email_message)\n\n    return names", "raw_solution": "import re\nimport smtplib\n\n# Constants\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\n\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    \"\"\"\n    Ek string se, jo square brackets mein band nahi hain, sabhi names ko nikaalen and unhein ek email mein bhejen.\n\n    Notes:\n        Email mein message \"Subject: Extracted Names\\n\\n\" ke roop mein swaroopit hai, jismein nikala gaya name \"\\nJosie Smith\\nMugsy Dog Smith\" hai.\n\n    Parameters:\n    - text (str): Vah re jisse name nikaalne hain.\n    - smtp_server (str): email bhejne ke liye upayog kiya jaane vaala SMTP server.\n    - smtp_port (int): SMTP server ke liye upayog kiya jaane vaala int.\n    - email_address (str): Vah email address jisse email bhejna hai.\n    - email_password (str): email pate ka password.\n    - recepient_address (str): recepient ka email address.\n\n    Returns:\n        list: Nikale gaye names ki ek list.\n\n    Requirements:\n    - re\n    - smtplib\n\n    Example:\n    >>> from unittest.mock import MagicMock\n    >>> mock_smtp_instance = MagicMock()\n    >>> mock_smtp = MagicMock(return_value=mock_smtp_instance)\n    >>> task_func(text=\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\", smtp=mock_smtp)\n    ['Josie Smith', 'Mugsy Dog Smith']\n\n    \"\"\"\n    # Extract names from the text\n    names = re.findall(r'\\b\\w+\\s\\w+\\b', text)\n\n    # Create the email message\n    email_message = \"Subject: Extracted Names\\n\\n\" + \"\\n\".join(names)\n\n    # Send the email\n    if smtp:\n        smtp_instance = smtp(smtp_server, smtp_port)\n        smtp_instance.starttls()\n        smtp_instance.login(email_address, email_password)\n        smtp_instance.sendmail(email_address, recepient_address, email_message)\n        smtp_instance.quit()\n    else:\n        with smtplib.SMTP(smtp_server, smtp_port) as server:\n            server.starttls()\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recepient_address, email_message)\n\n    return names\n```\n"}
{"task_id": "BigCodeBench/193", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\ndef task_func(rows, columns):\n    data = {f'col{i}': [] for i in range(columns)}\n    \n    for _ in range(rows):\n        for col in data:\n            data_type = choice(DATA_TYPES)\n            if data_type == str:\n                data[col].append(''.join(choice('abcdefghijklmnopqrstuvwxyz') for _ in range(5)))\n            elif data_type == int:\n                data[col].append(np.random.randint(0, 10))\n            elif data_type == float:\n                data[col].append(np.random.randint(0, 10) + np.random.rand())\n            elif data_type == list:\n                data[col].append([np.random.randint(0, 10) for _ in range(np.random.randint(1, 6))])\n            elif data_type == tuple:\n                data[col].append(tuple(np.random.randint(0, 10, np.random.randint(1, 6))))\n            elif data_type == dict:\n                data[col].append({f'key{i}': np.random.randint(0, 10) for i in range(np.random.randint(1, 6))})\n            elif data_type == set:\n                data[col].append(set(np.random.randint(0, 10, np.random.randint(1, 6))))\n    \n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import choice\n\n# Constants\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\n\ndef task_func(rows, columns):\n    data = {f'col{i}': [] for i in range(columns)}\n    \n    for _ in range(rows):\n        for col in data:\n            data_type = choice(DATA_TYPES)\n            if data_type == str:\n                data[col].append(''.join(choice('abcdefghijklmnopqrstuvwxyz') for _ in range(5)))\n            elif data_type == int:\n                data[col].append(np.random.randint(0, 10))\n            elif data_type == float:\n                data[col].append(np.random.randint(0, 10) + np.random.rand())\n            elif data_type == list:\n                data[col].append([np.random.randint(0, 10) for _ in range(np.random.randint(1, 6))])\n            elif data_type == tuple:\n                data[col].append(tuple(np.random.randint(0, 10, np.random.randint(1, 6))))\n            elif data_type == dict:\n                data[col].append({f'key{i}': np.random.randint(0, 10) for i in range(np.random.randint(1, 6))})\n            elif data_type == set:\n                data[col].append(set(np.random.randint(0, 10, np.random.randint(1, 6))))\n    \n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\n# df = task_func(2, 3)\n# print(df)\n# print(df.shape)\n# print(isinstance(df, pd.DataFrame))\n```\n"}
{"task_id": "BigCodeBench/194", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\ndef task_func(data_size):\n    # Generate random data\n    data = np.random.rand(data_size)\n    \n    # Select a random color from the list\n    color = np.random.choice(BAR_COLOR)\n    \n    # Plot the histogram\n    plt.hist(data, bins=10, color=color, alpha=0.7)\n    plt.title('Histogram')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return data, color", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\n\ndef task_func(data_size):\n    # Generate random data\n    data = np.random.rand(data_size)\n    \n    # Select a random color from the list\n    color = np.random.choice(BAR_COLOR)\n    \n    # Plot the histogram\n    plt.hist(data, bins=10, color=color, alpha=0.7)\n    plt.title('Histogram')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return data, color\n```\n"}
{"task_id": "BigCodeBench/195", "solution": "import subprocess\nimport platform\nimport time\ndef task_func(url):\n    \"\"\"\n    default web browser mein, ek background process mein ek web page kholen.\n\n    Parameters:\n    - url (str): webpage ka URL.\n\n    Returns:\n        int: subprocess ka return code.\n\n    Requirements:\n    - subprocess\n    - platform\n    - time\n\n    Example:\n    >>> task_func('https://www.google.com')\n    0\n\n    \"\"\"\n    # Check the operating system\n    if platform.system() == \"Windows\":\n        # For Windows, use start command\n        command = [\"start\", url]\n    else:\n        # For Unix-based systems, use open command\n        command = [\"open\", url]\n\n    # Start the process\n    process = subprocess.Popen(command, shell=True)\n\n    # Wait for the process to complete\n    process.wait()\n\n    # Return the return code of the process\n    return process.returncode", "raw_solution": "import subprocess\nimport platform\nimport time\n\ndef task_func(url):\n    \"\"\"\n    default web browser mein, ek background process mein ek web page kholen.\n\n    Parameters:\n    - url (str): webpage ka URL.\n\n    Returns:\n        int: subprocess ka return code.\n\n    Requirements:\n    - subprocess\n    - platform\n    - time\n\n    Example:\n    >>> task_func('https://www.google.com')\n    0\n\n    \"\"\"\n    # Check the operating system\n    if platform.system() == \"Windows\":\n        # For Windows, use start command\n        command = [\"start\", url]\n    else:\n        # For Unix-based systems, use open command\n        command = [\"open\", url]\n\n    # Start the process\n    process = subprocess.Popen(command, shell=True)\n\n    # Wait for the process to complete\n    process.wait()\n\n    # Return the return code of the process\n    return process.returncode\n```\n"}
{"task_id": "BigCodeBench/196", "solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef task_func(length, range_limit=100, seed=0):\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random.seed(seed)\n    data = [random.randint(1, range_limit) for _ in range(length)]\n\n    sns.set(style=\"whitegrid\")\n    ax = sns.histplot(data, bins=30, kde=True)\n    ax.set_title('Random Numbers Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    plt.show()\n\n    return ax, data", "raw_solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef task_func(length, range_limit=100, seed=0):\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random.seed(seed)\n    data = [random.randint(1, range_limit) for _ in range(length)]\n\n    sns.set(style=\"whitegrid\")\n    ax = sns.histplot(data, bins=30, kde=True)\n    ax.set_title('Random Numbers Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    plt.show()\n\n    return ax, data\n\n# Example usage:\n# ax, data = task_func(1000, 100, 24)\n# print(isinstance(ax, plt.Axes))  # Should print True\n```\n"}
{"task_id": "BigCodeBench/197", "solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(l1, l2, N=10):\n    if len(l1) != len(l2):\n        raise ValueError(\"Both lists must have the same length.\")\n\n    differences = []\n    for i in range(len(l1)):\n        diff = abs(l1[i] - l2[i])\n        heapq.heappush(differences, diff)\n\n    largest_diffs = heapq.nlargest(N, differences)\n\n    squared_diffs = [math.sqrt(diff) for diff in largest_diffs]\n\n    fig, ax = plt.subplots()\n    ax.plot(squared_diffs, marker='o')\n    ax.set_title('Square Root of Largest Differences')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Square Root of Difference')\n\n    return ax", "raw_solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(l1, l2, N=10):\n    if len(l1) != len(l2):\n        raise ValueError(\"Both lists must have the same length.\")\n\n    differences = []\n    for i in range(len(l1)):\n        diff = abs(l1[i] - l2[i])\n        heapq.heappush(differences, diff)\n\n    largest_diffs = heapq.nlargest(N, differences)\n\n    squared_diffs = [math.sqrt(diff) for diff in largest_diffs]\n\n    fig, ax = plt.subplots()\n    ax.plot(squared_diffs, marker='o')\n    ax.set_title('Square Root of Largest Differences')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Square Root of Difference')\n\n    return ax\n\n# Example usage:\n# l1 = [99, 86, 90, 70, 86, 95, 56, 98, 80, 81]\n# l2 = [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n# ax = task_func(l1, l2)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/198", "solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n\n    # Sort the data\n    data.sort()\n\n    # Calculate the average\n    avg = statistics.mean(data)\n\n    # Find numbers greater than the average\n    greater_avg = np.array([x for x in data if x > avg])\n\n    # Count numbers greater than the specified value\n    count = sum(1 for x in data if x > value)\n\n    # Plot histogram\n    plt.hist(data, bins='auto', color='skyblue', alpha=0.7)\n    plt.title('Histogram of Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return greater_avg, count", "raw_solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n\n    # Sort the data\n    data.sort()\n\n    # Calculate the average\n    avg = statistics.mean(data)\n\n    # Find numbers greater than the average\n    greater_avg = np.array([x for x in data if x > avg])\n\n    # Count numbers greater than the specified value\n    count = sum(1 for x in data if x > value)\n\n    # Plot histogram\n    plt.hist(data, bins='auto', color='skyblue', alpha=0.7)\n    plt.title('Histogram of Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return greater_avg, count\n\n# Example usage:\ngreater_avg, count = task_func([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 5)"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime) or utc_datetime.tzinfo is None or utc_datetime.tzinfo.utcoffset(utc_datetime) is None:\n        raise ValueError(\"utc_datetime must be a datetime object with timezone information.\")\n\n    set_seed(seed)\n\n    report_data = []\n\n    for city in cities:\n        local_tz = pytz.timezone(timezones[city])\n        local_time = utc_datetime.astimezone(local_tz)\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        report_data.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_condition\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df", "raw_solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime) or utc_datetime.tzinfo is None or utc_datetime.tzinfo.utcoffset(utc_datetime) is None:\n        raise ValueError(\"utc_datetime must be a datetime object with timezone information.\")\n\n    set_seed(seed)\n\n    report_data = []\n\n    for city in cities:\n        local_tz = pytz.timezone(timezones[city])\n        local_time = utc_datetime.astimezone(local_tz)\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        report_data.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_condition\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df\n```\n"}
{"task_id": "BigCodeBench/200", "solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    numbers = [random.random() for _ in range(n)]\n    \n    # Calculate the average of the numbers\n    avg = statistics.mean(numbers)\n    \n    # Find numbers greater than the average\n    greater_avg = [num for num in numbers if num > avg]\n    \n    # Count how many numbers are greater or equal to the specified value\n    count = sum(1 for num in numbers if num >= value)\n    \n    # Sort the numbers and plot them\n    sorted_numbers = sorted(numbers)\n    plt.plot(sorted_numbers, label='Sorted Numbers')\n    plt.axhline(y=avg, color='r', linestyle='--', label='Average')\n    plt.axhline(y=value, color='g', linestyle='--', label='Specified Value')\n    plt.legend()\n    plt.show()\n    \n    return greater_avg, count", "raw_solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    numbers = [random.random() for _ in range(n)]\n    \n    # Calculate the average of the numbers\n    avg = statistics.mean(numbers)\n    \n    # Find numbers greater than the average\n    greater_avg = [num for num in numbers if num > avg]\n    \n    # Count how many numbers are greater or equal to the specified value\n    count = sum(1 for num in numbers if num >= value)\n    \n    # Sort the numbers and plot them\n    sorted_numbers = sorted(numbers)\n    plt.plot(sorted_numbers, label='Sorted Numbers')\n    plt.axhline(y=avg, color='r', linestyle='--', label='Average')\n    plt.axhline(y=value, color='g', linestyle='--', label='Specified Value')\n    plt.legend()\n    plt.show()\n    \n    return greater_avg, count\n\n# Example usage:\n# greater_avg, count = task_func(10, 0.5)\n# print(greater_avg)\n# print(count)\n```\n"}
{"task_id": "BigCodeBench/201", "solution": "import bisect\nimport statistics\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, column, value):\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number.\")\n\n    data = df[column].to_list()\n    data.sort()\n\n    avg = statistics.mean(data)\n    greater_avg = [x for x in data if x > avg]\n\n    index = bisect.bisect_left(data, value)\n    num_greater_value = len([x for x in data if x > value])\n\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=10, color='blue', alpha=0.7)\n    ax.axvline(avg, color='red', linestyle='dashed', linewidth=1)\n    ax.set_title('Data Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return greater_avg, num_greater_value, ax", "raw_solution": "import bisect\nimport statistics\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, column, value):\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number.\")\n\n    data = df[column].to_list()\n    data.sort()\n\n    avg = statistics.mean(data)\n    greater_avg = [x for x in data if x > avg]\n\n    index = bisect.bisect_left(data, value)\n    num_greater_value = len([x for x in data if x > value])\n\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=10, color='blue', alpha=0.7)\n    ax.axvline(avg, color='red', linestyle='dashed', linewidth=1)\n    ax.set_title('Data Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return greater_avg, num_greater_value, ax\n\n# Example usage:\n# df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n# greater_avg, num_greater_value, ax = task_func(df, 'A', 5)\n```\n"}
{"task_id": "BigCodeBench/202", "solution": "import re\nimport json\nfrom collections import Counter\ndef task_func(json_str, top_n=10):\n    # Extract URLs from the JSON string\n    urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', json_str)\n    \n    # Count occurrences of each URL\n    url_counter = Counter(urls)\n    \n    # Get the top N URLs\n    top_urls = dict(url_counter.most_common(top_n))\n    \n    return top_urls\njson_str = '{\"name\": \"John\", \"website\": \"https://www.example.com\"}'", "raw_solution": "import re\nimport json\nfrom collections import Counter\n\ndef task_func(json_str, top_n=10):\n    # Extract URLs from the JSON string\n    urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', json_str)\n    \n    # Count occurrences of each URL\n    url_counter = Counter(urls)\n    \n    # Get the top N URLs\n    top_urls = dict(url_counter.most_common(top_n))\n    \n    return top_urls\n\n# Example usage:\njson_str = '{\"name\": \"John\", \"website\": \"https://www.example.com\"}'"}
{"task_id": "BigCodeBench/203", "solution": "import json\nimport smtplib\nimport re\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    if not input_data:\n        raise ValueError(\"Input data is required\")\n\n    # Parse the input data\n    data = json.loads(input_data)\n    recipient = data.get('recipient')\n    names = data.get('names')\n\n    if not recipient or not names:\n        raise ValueError(\"Recipient and names are required\")\n\n    # Create the email content\n    email_content = \"Names:\\n\" + \"\\n\".join(names)\n\n    # Send the email\n    if smtp:\n        with smtp(smtp_server, smtp_port) as server:\n            server.starttls()\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recipient, email_content)\n    else:\n        with smtplib.SMTP(smtp_server, smtp_port) as server:\n            server.starttls()\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recipient, email_content)\n\n    # Extract and return the names\n    return names", "raw_solution": "import json\nimport smtplib\nimport re\n\n# Constants\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\n\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    if not input_data:\n        raise ValueError(\"Input data is required\")\n\n    # Parse the input data\n    data = json.loads(input_data)\n    recipient = data.get('recipient')\n    names = data.get('names')\n\n    if not recipient or not names:\n        raise ValueError(\"Recipient and names are required\")\n\n    # Create the email content\n    email_content = \"Names:\\n\" + \"\\n\".join(names)\n\n    # Send the email\n    if smtp:\n        with smtp(smtp_server, smtp_port) as server:\n            server.starttls()\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recipient, email_content)\n    else:\n        with smtplib.SMTP(smtp_server, smtp_port) as server:\n            server.starttls()\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recipient, email_content)\n\n    # Extract and return the names\n    return names\n\n# Example usage"}
{"task_id": "BigCodeBench/204", "solution": "import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Calculate mean\n    mean = np.mean(L)\n\n    # Calculate median\n    median = np.median(L)\n\n    # Calculate mode\n    mode_data = Counter(L)\n    mode = mode_data.most_common(1)[0][0]\n\n    # Calculate standard deviation\n    std_dev = np.std(L)\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(L, bins='auto', alpha=0.7, color='blue')\n    ax.set_title('Histogram of L')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"mode\": mode,\n        \"std_dev\": std_dev,\n        \"plot\": ax\n    }\nL = [1, 2, 3, 4, 5, 6, 7, 8, 9]", "raw_solution": "import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    # Calculate mean\n    mean = np.mean(L)\n\n    # Calculate median\n    median = np.median(L)\n\n    # Calculate mode\n    mode_data = Counter(L)\n    mode = mode_data.most_common(1)[0][0]\n\n    # Calculate standard deviation\n    std_dev = np.std(L)\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(L, bins='auto', alpha=0.7, color='blue')\n    ax.set_title('Histogram of L')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"mode\": mode,\n        \"std_dev\": std_dev,\n        \"plot\": ax\n    }\n\n# Example usage\nL = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nstats = task_func(L)"}
{"task_id": "BigCodeBench/205", "solution": "import subprocess\nfrom multiprocessing import Pool\ndef execute_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\ndef task_func(commands):\n    if not commands:\n        return []\n\n    with Pool() as pool:\n        results = pool.map(execute_command, commands)\n\n    return results", "raw_solution": "import subprocess\nfrom multiprocessing import Pool\n\ndef execute_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\n\ndef task_func(commands):\n    if not commands:\n        return []\n\n    with Pool() as pool:\n        results = pool.map(execute_command, commands)\n\n    return results\n\n# Test cases"}
{"task_id": "BigCodeBench/206", "solution": "import csv\nimport json\nimport os\ndef task_func(file_name):\n    if not os.path.exists(file_name):\n        raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n\n    json_file_name = os.path.splitext(file_name)[0] + '.json'\n\n    with open(file_name, mode='r', newline='', encoding='utf-8') as csvfile:\n        reader = csv.DictReader(csvfile)\n        data = [row for row in reader]\n\n    with open(json_file_name, mode='w', encoding='utf-8') as jsonfile:\n        json.dump(data, jsonfile, indent=4)\n\n    return json_file_name", "raw_solution": "import csv\nimport json\nimport os\n\ndef task_func(file_name):\n    if not os.path.exists(file_name):\n        raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n\n    json_file_name = os.path.splitext(file_name)[0] + '.json'\n\n    with open(file_name, mode='r', newline='', encoding='utf-8') as csvfile:\n        reader = csv.DictReader(csvfile)\n        data = [row for row in reader]\n\n    with open(json_file_name, mode='w', encoding='utf-8') as jsonfile:\n        json.dump(data, jsonfile, indent=4)\n\n    return json_file_name\n```\n"}
{"task_id": "BigCodeBench/207", "solution": "import re\nimport requests\nimport json\ndef task_func(input):\n    \"\"\"\n    input string se ek api endpoint nikaalen, endpoint par ek GET request bhejen, aur json format mein pratikriya data lautaen.\n\n    Parameters:\n    - input (str): input string jismein ek api endpoint shaamil hai.\n\n    Returns:\n        dict: pratikriya data.\n\n    Requirements:\n    - re\n    - json\n    - requests\n\n    Example:\n    >>> task_func('Fetch data from https://api.example.com/data')\n    {'key': 'value'}\n\n    \"\"\"\n    # Extract the API endpoint from the input string\n    match = re.search(r'https?://[^\\s]+', input)\n    if not match:\n        raise ValueError(\"No valid API endpoint found in the input string.\")\n    \n    api_endpoint = match.group(0)\n    \n    # Make a GET request to the API endpoint\n    response = requests.get(api_endpoint)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch data from {api_endpoint}. Status code: {response.status_code}\")\n    \n    # Parse the JSON data\n    data = response.json()\n    \n    return data", "raw_solution": "import re\nimport requests\nimport json\n\ndef task_func(input):\n    \"\"\"\n    input string se ek api endpoint nikaalen, endpoint par ek GET request bhejen, aur json format mein pratikriya data lautaen.\n\n    Parameters:\n    - input (str): input string jismein ek api endpoint shaamil hai.\n\n    Returns:\n        dict: pratikriya data.\n\n    Requirements:\n    - re\n    - json\n    - requests\n\n    Example:\n    >>> task_func('Fetch data from https://api.example.com/data')\n    {'key': 'value'}\n\n    \"\"\"\n    # Extract the API endpoint from the input string\n    match = re.search(r'https?://[^\\s]+', input)\n    if not match:\n        raise ValueError(\"No valid API endpoint found in the input string.\")\n    \n    api_endpoint = match.group(0)\n    \n    # Make a GET request to the API endpoint\n    response = requests.get(api_endpoint)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch data from {api_endpoint}. Status code: {response.status_code}\")\n    \n    # Parse the JSON data\n    data = response.json()\n    \n    return data\n\n# Example usage:\n# Assuming the API endpoint returns a JSON response with the structure {'key': 'value'}\n# print(task_func('Fetch data from https://api.example.com/data'))\n```\n"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n    \n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    walk = np.cumsum(steps)\n    \n    stats = {\n        'count': elements,\n        'mean': np.mean(walk),\n        'std': np.std(walk),\n        'min': np.min(walk),\n        '5%': np.percentile(walk, 5),\n        '25%': np.percentile(walk, 25),\n        '50%': np.percentile(walk, 50),\n        '75%': np.percentile(walk, 75),\n        '95%': np.percentile(walk, 95),\n        'max': np.max(walk)\n    }\n    \n    fig, ax = plt.subplots()\n    ax.plot(walk, label='Random Walk')\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    ax.legend()\n    \n    return stats, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n    \n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    walk = np.cumsum(steps)\n    \n    stats = {\n        'count': elements,\n        'mean': np.mean(walk),\n        'std': np.std(walk),\n        'min': np.min(walk),\n        '5%': np.percentile(walk, 5),\n        '25%': np.percentile(walk, 25),\n        '50%': np.percentile(walk, 50),\n        '75%': np.percentile(walk, 75),\n        '95%': np.percentile(walk, 95),\n        'max': np.max(walk)\n    }\n    \n    fig, ax = plt.subplots()\n    ax.plot(walk, label='Random Walk')\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    ax.legend()\n    \n    return stats, ax\n```\n"}
{"task_id": "BigCodeBench/209", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    tuples ka ek scatter graph banaen aur index 1 par maximum maan vaale tuple ko highlight karen.\n\n    Parameters:\n    - data (list of tuple): tuples ki ek data jahaan pratyek tuple mein do integers hote hain.\n\n    Returns:\n        maplotlib.matplotlib.axes.matplotlib.axes: plot ka matplotlib.axes object aage ki manipulation aur testing ke liye, 'max tuple highlighted' title, x-axis 'x' label, y-axis 'y' label, aur ek legend ke saath.\n\n    Requirements:\n    - numpy\n    - operator\n    - matplotlib.pyplot\n\n    Example:\n    >>> ax = task_func([(10, 20), (30, 40), (25, 50)])\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Extracting x and y values from the data\n    x_values, y_values = zip(*data)\n    \n    # Plotting the scatter graph\n    fig, ax = plt.subplots()\n    ax.scatter(x_values, y_values, label='Data Points')\n    \n    # Finding the maximum y value and its corresponding x value\n    max_y_value = max(y_values)\n    max_y_index = y_values.index(max_y_value)\n    max_x_value = x_values[max_y_index]\n    \n    # Highlighting the tuple with the maximum y value\n    ax.scatter(max_x_value, max_y_value, color='red', label='Max Tuple Highlighted')\n    \n    # Adding labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot with Max Tuple Highlighted')\n    \n    # Adding legend\n    ax.legend()\n    \n    return ax", "raw_solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    tuples ka ek scatter graph banaen aur index 1 par maximum maan vaale tuple ko highlight karen.\n\n    Parameters:\n    - data (list of tuple): tuples ki ek data jahaan pratyek tuple mein do integers hote hain.\n\n    Returns:\n        maplotlib.matplotlib.axes.matplotlib.axes: plot ka matplotlib.axes object aage ki manipulation aur testing ke liye, 'max tuple highlighted' title, x-axis 'x' label, y-axis 'y' label, aur ek legend ke saath.\n\n    Requirements:\n    - numpy\n    - operator\n    - matplotlib.pyplot\n\n    Example:\n    >>> ax = task_func([(10, 20), (30, 40), (25, 50)])\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Extracting x and y values from the data\n    x_values, y_values = zip(*data)\n    \n    # Plotting the scatter graph\n    fig, ax = plt.subplots()\n    ax.scatter(x_values, y_values, label='Data Points')\n    \n    # Finding the maximum y value and its corresponding x value\n    max_y_value = max(y_values)\n    max_y_index = y_values.index(max_y_value)\n    max_x_value = x_values[max_y_index]\n    \n    # Highlighting the tuple with the maximum y value\n    ax.scatter(max_x_value, max_y_value, color='red', label='Max Tuple Highlighted')\n    \n    # Adding labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot with Max Tuple Highlighted')\n    \n    # Adding legend\n    ax.legend()\n    \n    return ax\n\n# Example usage"}
{"task_id": "BigCodeBench/210", "solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Count the frequency of each letter\n    letter_counts = collections.Counter(itemgetter(0)(d) for d in data)\n    \n    # Find the maximum count\n    max_count = max(letter_counts.values())\n    \n    # Create a bar plot for the letter counts\n    fig, ax = plt.subplots()\n    ax.bar(letter_counts.keys(), letter_counts.values(), label='Letter Counts')\n    \n    # Highlight the letter with the maximum count\n    max_letter = max(letter_counts.items(), key=itemgetter(1))[0]\n    ax.bar(max_letter, max_count, color='red', label='Max Value Letter')\n    \n    # Set labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n    ax.legend()\n    \n    return ax\ndataset = [('a', 10), ('b', 15), ('a', 5), ('c', 20)]", "raw_solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Count the frequency of each letter\n    letter_counts = collections.Counter(itemgetter(0)(d) for d in data)\n    \n    # Find the maximum count\n    max_count = max(letter_counts.values())\n    \n    # Create a bar plot for the letter counts\n    fig, ax = plt.subplots()\n    ax.bar(letter_counts.keys(), letter_counts.values(), label='Letter Counts')\n    \n    # Highlight the letter with the maximum count\n    max_letter = max(letter_counts.items(), key=itemgetter(1))[0]\n    ax.bar(max_letter, max_count, color='red', label='Max Value Letter')\n    \n    # Set labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n    ax.legend()\n    \n    return ax\n\n# Example usage:\ndataset = [('a', 10), ('b', 15), ('a', 5), ('c', 20)]\nax = task_func(dataset)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    if headers is None:\n        headers = {'accept': 'application/octet-stream'}\n    \n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n    \n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    zip_file_path = os.path.join(destination_directory, 'temp.zip')\n    \n    with open(zip_file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        extracted_files = zip_ref.namelist()\n    \n    # Clean up the zip file\n    os.remove(zip_file_path)\n    \n    return extracted_files", "raw_solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    if headers is None:\n        headers = {'accept': 'application/octet-stream'}\n    \n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n    \n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    zip_file_path = os.path.join(destination_directory, 'temp.zip')\n    \n    with open(zip_file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        extracted_files = zip_ref.namelist()\n    \n    # Clean up the zip file\n    os.remove(zip_file_path)\n    \n    return extracted_files\n\n# Example usage:\n# extracted_files = task_func(\"https://example.com/data.zip\", \"/path/to/destination\")\n# print(extracted_files)\n```\n"}
{"task_id": "BigCodeBench/212", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert data to numpy array for easy manipulation\n    data_array = np.array(data)\n    \n    # Find the maximum y-value and its corresponding x-value\n    max_y_point = max(data_array, key=itemgetter(1))\n    \n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(data_array[:, 0], data_array[:, 1], label='Data Points')\n    \n    # Highlight the point with the maximum y-value\n    ax.scatter(max_y_point[0], max_y_point[1], color='red', label='Max Y Point')\n    \n    # Set labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Max Y Point Highlighted')\n    ax.legend()\n    \n    return ax, max_y_point", "raw_solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert data to numpy array for easy manipulation\n    data_array = np.array(data)\n    \n    # Find the maximum y-value and its corresponding x-value\n    max_y_point = max(data_array, key=itemgetter(1))\n    \n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(data_array[:, 0], data_array[:, 1], label='Data Points')\n    \n    # Highlight the point with the maximum y-value\n    ax.scatter(max_y_point[0], max_y_point[1], color='red', label='Max Y Point')\n    \n    # Set labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Max Y Point Highlighted')\n    ax.legend()\n    \n    return ax, max_y_point\n```\n"}
{"task_id": "BigCodeBench/213", "solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\ndef task_func(intervals=100, seed=0):\n    random.seed(seed)\n    numbers = []\n\n    # Generate random numbers with a delay between each interval\n    for _ in range(intervals):\n        time.sleep(1)\n        numbers.append(random.random())\n\n    # Plot the numbers\n    fig, ax = plt.subplots()\n    ax.plot(numbers, label='Random Numbers')\n    ax.set_title('Random Numbers Over Time')\n    ax.set_xlabel('Interval')\n    ax.set_ylabel('Random Value')\n    ax.legend()\n\n    # Calculate the kurtosis of the generated numbers\n    kurt = kurtosis(numbers)\n\n    return ax, kurt", "raw_solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\n\ndef task_func(intervals=100, seed=0):\n    random.seed(seed)\n    numbers = []\n\n    # Generate random numbers with a delay between each interval\n    for _ in range(intervals):\n        time.sleep(1)\n        numbers.append(random.random())\n\n    # Plot the numbers\n    fig, ax = plt.subplots()\n    ax.plot(numbers, label='Random Numbers')\n    ax.set_title('Random Numbers Over Time')\n    ax.set_xlabel('Interval')\n    ax.set_ylabel('Random Value')\n    ax.legend()\n\n    # Calculate the kurtosis of the generated numbers\n    kurt = kurtosis(numbers)\n\n    return ax, kurt\n\n# Example usage:\n# ax, kurtosis_value = task_func(5)\n# print(f\"Kurtosis value: {kurtosis_value}\")\n```\n"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate a random image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Display the image using matplotlib\n    fig, ax = plt.subplots()\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.set_title('Random Image')\n    ax.axis('off')\n\n    return ax, image", "raw_solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate a random image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Display the image using matplotlib\n    fig, ax = plt.subplots()\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.set_title('Random Image')\n    ax.axis('off')\n\n    return ax, image\n\n# Example usage:\n# ax, image = task_func()\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/215", "solution": "import requests\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nHEADERS = {\n    'accept': 'application/json'\n}\ndef task_func(url, parameters):\n    try:\n        response = requests.get(url, headers=HEADERS, params=parameters)\n        response.raise_for_status()\n        data = response.json()\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Error fetching data from {url}: {e}\")\n\n    if not data:\n        raise Exception(\"No data received from the API.\")\n\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        raise Exception(\"DataFrame is empty.\")\n\n    # Calculate correlation matrix\n    corr = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Heatmap')\n    plt.show()\n\n    return df, ax", "raw_solution": "import requests\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nHEADERS = {\n    'accept': 'application/json'\n}\n\ndef task_func(url, parameters):\n    try:\n        response = requests.get(url, headers=HEADERS, params=parameters)\n        response.raise_for_status()\n        data = response.json()\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Error fetching data from {url}: {e}\")\n\n    if not data:\n        raise Exception(\"No data received from the API.\")\n\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        raise Exception(\"DataFrame is empty.\")\n\n    # Calculate correlation matrix\n    corr = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Heatmap')\n    plt.show()\n\n    return df, ax\n```\nThis script fetches data from a given URL with specified parameters, converts it into a pandas DataFrame, and then generates a heatmap of the correlation matrix using seaborn. The heatmap is displayed using matplotlib. If any step fails, an appropriate exception is raised.\n"}
{"task_id": "BigCodeBench/216", "solution": "import pandas as pd\nimport os\nimport json\nfrom collections import Counter\ndef task_func(json_dir_path, word_count):\n    all_text = \"\"\n    for filename in os.listdir(json_dir_path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(json_dir_path, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                all_text += data.get('text', '') + \" \"\n\n    words = all_text.split()\n    word_counts = Counter(words)\n    most_common_words = word_counts.most_common(word_count)\n\n    return most_common_words", "raw_solution": "import pandas as pd\nimport os\nimport json\nfrom collections import Counter\n\ndef task_func(json_dir_path, word_count):\n    all_text = \"\"\n    for filename in os.listdir(json_dir_path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(json_dir_path, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                all_text += data.get('text', '') + \" \"\n\n    words = all_text.split()\n    word_counts = Counter(words)\n    most_common_words = word_counts.most_common(word_count)\n\n    return most_common_words\n\n# Example usage:\n# Assuming you have a directory 'data' with two json files 'fake_data_1.json' and 'fake_data_2.json'\n# with the content as provided in the example.\n# result = task_func('data', 2)\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/217", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    \n    mean, std = np.mean(sample), np.std(sample)\n    \n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n    \n    pdf = stats.norm.pdf(bins, mu, sigma)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n    \n    ax.set_title(f'$\\\\mu = {mu:.2f}, \\\\sigma = {sigma:.2f}$')\n    \n    return ax, mean, std", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    \n    mean, std = np.mean(sample), np.std(sample)\n    \n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n    \n    pdf = stats.norm.pdf(bins, mu, sigma)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n    \n    ax.set_title(f'$\\\\mu = {mu:.2f}, \\\\sigma = {sigma:.2f}$')\n    \n    return ax, mean, std\n```\n"}
{"task_id": "BigCodeBench/218", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\ndef task_func(df, dict_mapping, plot_histogram=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    \n    if not all(feature in df.columns for feature in FEATURES + [TARGET]):\n        raise ValueError(\"FEATURES and TARGET columns are not in the DataFrame\")\n    \n    # Standardizing the features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    \n    # Mapping the values in the DataFrame\n    df = df.replace(dict_mapping)\n    \n    # Plotting the histogram if required\n    ax = None\n    if plot_histogram:\n        ax = df[TARGET].plot(kind='hist', title='Target Variable Distribution')\n        plt.xlabel(TARGET)\n    \n    return df, ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\n\ndef task_func(df, dict_mapping, plot_histogram=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    \n    if not all(feature in df.columns for feature in FEATURES + [TARGET]):\n        raise ValueError(\"FEATURES and TARGET columns are not in the DataFrame\")\n    \n    # Standardizing the features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    \n    # Mapping the values in the DataFrame\n    df = df.replace(dict_mapping)\n    \n    # Plotting the histogram if required\n    ax = None\n    if plot_histogram:\n        ax = df[TARGET].plot(kind='hist', title='Target Variable Distribution')\n        plt.xlabel(TARGET)\n    \n    return df, ax\n\n# Example usage:\n# df = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6], 'feature3': [7, 8, 9],'feature4': [10, 11, 12], 'feature5': [13, 14, 15], 'target': [0, 1, 1]})\n# dict_mapping = {1: 11, 0: 22}\n# df, ax = task_func(df, dict_mapping, plot_histogram=True)\n# print(isinstance(ax, plt.Axes))\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/219", "solution": "import math\nimport statistics\nimport numpy as np\ndef task_func(input_list):\n    # Sort the input list\n    sorted_list = sorted(input_list)\n    \n    # Calculate mean, median, and mode\n    mean_value = statistics.mean(sorted_list)\n    median_value = statistics.median(sorted_list)\n    mode_value = statistics.mode(sorted_list)\n    \n    # Calculate FFT magnitude\n    fft_values = np.fft.fft(sorted_list)\n    fft_magnitude = np.abs(fft_values)\n    \n    # Calculate mean, median, and mode of FFT magnitude\n    fft_mean = statistics.mean(fft_magnitude)\n    fft_median = statistics.median(fft_magnitude)\n    fft_mode = statistics.mode(fft_magnitude)\n    \n    # Round the values to the nearest integer\n    mean_value = round(mean_value)\n    median_value = round(median_value)\n    mode_value = round(mode_value)\n    fft_mean = round(fft_mean)\n    fft_median = round(fft_median)\n    fft_mode = round(fft_mode)\n    \n    return mean_value, median_value, mode_value, fft_mean, fft_median, fft_mode\ninput_list = [30, 45, 60, 90, 180]", "raw_solution": "import math\nimport statistics\nimport numpy as np\n\ndef task_func(input_list):\n    # Sort the input list\n    sorted_list = sorted(input_list)\n    \n    # Calculate mean, median, and mode\n    mean_value = statistics.mean(sorted_list)\n    median_value = statistics.median(sorted_list)\n    mode_value = statistics.mode(sorted_list)\n    \n    # Calculate FFT magnitude\n    fft_values = np.fft.fft(sorted_list)\n    fft_magnitude = np.abs(fft_values)\n    \n    # Calculate mean, median, and mode of FFT magnitude\n    fft_mean = statistics.mean(fft_magnitude)\n    fft_median = statistics.median(fft_magnitude)\n    fft_mode = statistics.mode(fft_magnitude)\n    \n    # Round the values to the nearest integer\n    mean_value = round(mean_value)\n    median_value = round(median_value)\n    mode_value = round(mode_value)\n    fft_mean = round(fft_mean)\n    fft_median = round(fft_median)\n    fft_mode = round(fft_mode)\n    \n    return mean_value, median_value, mode_value, fft_mean, fft_median, fft_mode\n\n# Example usage\ninput_list = [30, 45, 60, 90, 180]\nstats = task_func(input_list)"}
{"task_id": "BigCodeBench/220", "solution": "from random import choice\nimport turtle\nimport time\ndef task_func(colors):\n    \"\"\"\n    Turtle graphics ka upayog karke random colors ke paanch square banaen. Pratyek square ko banaya jaata hai\n    sequentially second ke pause ke saath.\n    function ko input ke roop mein colors ki ek list ki aavashyakta hoti hai aur ek turtle graphics window set karta hai,\n    ek turtle object banaata hai, aur pradaan ki gayi list se colors ke saath squares ko banaane ke liye iska upayog karta hai.\n    Drawing ke baad window khuli rehti hai.\n\n    Parameters:\n    - colors (list): squares ko banaane ke liye upayog karne ke liye colors ke naam (strings ke roop mein) ki ek list.\n\n    Returns:\n        none.\n\n    Requirements:\n    - random.choice\n    - turtle\n    - time\n\n    Example:\n    udhaaran:\n    >>> task_func(['red', 'blue', 'green', 'yellow', 'purple'])  # yeh ek turtle graphics window open aur square banaega\n    >>> turtle.TurtleScreen._RUNNING\n    True  # jaanchen ki turtle graphics screen chal rahi hai ya nahi.\n\n    \"\"\"\n    screen = turtle.Screen()\n    screen.title(\"Random Color Squares\")\n    screen.bgcolor(\"black\")\n\n    square_turtle = turtle.Turtle()\n    square_turtle.speed(1)\n    square_turtle.hideturtle()\n\n    for color in colors:\n        square_turtle.color(color)\n        square_turtle.begin_fill()\n        for _ in range(4):\n            square_turtle.forward(100)\n            square_turtle.right(90)\n        square_turtle.end_fill()\n        time.sleep(2)\n\n    screen.mainloop()", "raw_solution": "from random import choice\nimport turtle\nimport time\n\ndef task_func(colors):\n    \"\"\"\n    Turtle graphics ka upayog karke random colors ke paanch square banaen. Pratyek square ko banaya jaata hai\n    sequentially second ke pause ke saath.\n    function ko input ke roop mein colors ki ek list ki aavashyakta hoti hai aur ek turtle graphics window set karta hai,\n    ek turtle object banaata hai, aur pradaan ki gayi list se colors ke saath squares ko banaane ke liye iska upayog karta hai.\n    Drawing ke baad window khuli rehti hai.\n\n    Parameters:\n    - colors (list): squares ko banaane ke liye upayog karne ke liye colors ke naam (strings ke roop mein) ki ek list.\n\n    Returns:\n        none.\n\n    Requirements:\n    - random.choice\n    - turtle\n    - time\n\n    Example:\n    udhaaran:\n    >>> task_func(['red', 'blue', 'green', 'yellow', 'purple'])  # yeh ek turtle graphics window open aur square banaega\n    >>> turtle.TurtleScreen._RUNNING\n    True  # jaanchen ki turtle graphics screen chal rahi hai ya nahi.\n\n    \"\"\"\n    screen = turtle.Screen()\n    screen.title(\"Random Color Squares\")\n    screen.bgcolor(\"black\")\n\n    square_turtle = turtle.Turtle()\n    square_turtle.speed(1)\n    square_turtle.hideturtle()\n\n    for color in colors:\n        square_turtle.color(color)\n        square_turtle.begin_fill()\n        for _ in range(4):\n            square_turtle.forward(100)\n            square_turtle.right(90)\n        square_turtle.end_fill()\n        time.sleep(2)\n\n    screen.mainloop()\n\n# Example usage:\n# task_func(['red', 'blue', 'green', 'yellow', 'purple'])\n```\nThis script will open a turtle graphics window and draw squares with random colors, pausing for 2 seconds between each square.\n"}
{"task_id": "BigCodeBench/221", "solution": "import numpy as np\nfrom scipy import stats\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\ndef task_func(df, dct):\n    result = {}\n    for feature in FEATURES:\n        if feature not in df.columns:\n            return \"Invalid input\"\n        try:\n            data = df[feature].replace(dct)\n            mean = np.mean(data)\n            median = np.median(data)\n            mode = stats.mode(data)[0][0]\n            variance = np.var(data)\n            result[feature] = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance}\n        except Exception as e:\n            return \"Invalid input\"\n    return result", "raw_solution": "import numpy as np\nfrom scipy import stats\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n\ndef task_func(df, dct):\n    result = {}\n    for feature in FEATURES:\n        if feature not in df.columns:\n            return \"Invalid input\"\n        try:\n            data = df[feature].replace(dct)\n            mean = np.mean(data)\n            median = np.median(data)\n            mode = stats.mode(data)[0][0]\n            variance = np.var(data)\n            result[feature] = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance}\n        except Exception as e:\n            return \"Invalid input\"\n    return result\n\n# Example usage:\n# df = pd.DataFrame({'feature1': [1, 2, 3, 4, 5], 'feature2': [5, 4, 3, 2, 1], 'feature3': [2, 2, 2, 2, 2], 'feature4': [1, 1, 3, 3, 5], 'feature5': [0, 1, 1, 1, 1]})\n# dct = {}\n# print(task_func(df, dct))\n```\n"}
{"task_id": "BigCodeBench/222", "solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(list_input):\n    # Sort the list in degree value order\n    sorted_list = sorted(list_input, key=math.degrees)\n\n    # Calculate the cumulative sum\n    cumsum = np.cumsum(sorted_list)\n\n    # Plot the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumsum, label='Cumulative Sum')\n    ax.set_title('Cumulative Sum Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.legend()\n\n    return cumsum, ax", "raw_solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(list_input):\n    # Sort the list in degree value order\n    sorted_list = sorted(list_input, key=math.degrees)\n\n    # Calculate the cumulative sum\n    cumsum = np.cumsum(sorted_list)\n\n    # Plot the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumsum, label='Cumulative Sum')\n    ax.set_title('Cumulative Sum Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.legend()\n\n    return cumsum, ax\n```\n"}
{"task_id": "BigCodeBench/223", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df, dct, columns=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Replace values in DataFrame\n    df = df.replace(dct)\n\n    # Encode categorical columns\n    if columns is None:\n        columns = df.select_dtypes(include=['object']).columns\n\n    le = LabelEncoder()\n    for col in columns:\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n\n    # Standardize numerical columns\n    numerical_cols = df.select_dtypes(include=['number']).columns\n    for col in numerical_cols:\n        if col in df.columns:\n            df[col] = (df[col] - df[col].mean()) / df[col].std()\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(df, dct, columns=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Replace values in DataFrame\n    df = df.replace(dct)\n\n    # Encode categorical columns\n    if columns is None:\n        columns = df.select_dtypes(include=['object']).columns\n\n    le = LabelEncoder()\n    for col in columns:\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n\n    # Standardize numerical columns\n    numerical_cols = df.select_dtypes(include=['number']).columns\n    for col in numerical_cols:\n        if col in df.columns:\n            df[col] = (df[col] - df[col].mean()) / df[col].std()\n\n    return df\n\n# Example usage:\n# df = pd.DataFrame({'col1': ['a', 'b', 'c'], 'col2': [1, 2, 3]})\n# dct = {'a': 'x', 'b': 'y'}\n# result = task_func(df, dct)\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/224", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    x = np.arange(range_start, range_end, step)\n    sin_x = np.sin(x)\n    cos_x = np.cos(x)\n    abs_diff = np.abs(sin_x - cos_x)\n    \n    fig, ax = plt.subplots()\n    ax.plot(x, sin_x, label='sin(x)')\n    ax.plot(x, cos_x, label='cos(x)')\n    ax.plot(x, abs_diff, label='abs(sin(x) - cos(x))')\n    ax.legend()\n    \n    fft_values = fft(abs_diff)\n    fft_mean = np.mean(np.abs(fft_values))\n    fft_median = np.median(np.abs(fft_values))\n    \n    data_gen = zip(x, sin_x, cos_x, abs_diff)\n    \n    return data_gen, ax, fft_mean, fft_median", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    x = np.arange(range_start, range_end, step)\n    sin_x = np.sin(x)\n    cos_x = np.cos(x)\n    abs_diff = np.abs(sin_x - cos_x)\n    \n    fig, ax = plt.subplots()\n    ax.plot(x, sin_x, label='sin(x)')\n    ax.plot(x, cos_x, label='cos(x)')\n    ax.plot(x, abs_diff, label='abs(sin(x) - cos(x))')\n    ax.legend()\n    \n    fft_values = fft(abs_diff)\n    fft_mean = np.mean(np.abs(fft_values))\n    fft_median = np.median(np.abs(fft_values))\n    \n    data_gen = zip(x, sin_x, cos_x, abs_diff)\n    \n    return data_gen, ax, fft_mean, fft_median\n\n# Example usage:\n# data, ax, fft_mean, fft_median = task_func()\n# print(next(data))\n```\n"}
{"task_id": "BigCodeBench/225", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n    \n    # Replace values in the DataFrame\n    df = df.replace(dct)\n    \n    # Plot histograms if specified\n    if plot_histograms and columns:\n        for column in columns:\n            if column in df.columns:\n                plt.figure()\n                df[column].hist()\n                plt.title(f'Histogram of {column}')\n                plt.xlabel(column)\n                plt.ylabel('Frequency')\n                plt.show()\n    \n    return df", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n    \n    # Replace values in the DataFrame\n    df = df.replace(dct)\n    \n    # Plot histograms if specified\n    if plot_histograms and columns:\n        for column in columns:\n            if column in df.columns:\n                plt.figure()\n                df[column].hist()\n                plt.title(f'Histogram of {column}')\n                plt.xlabel(column)\n                plt.ylabel('Frequency')\n                plt.show()\n    \n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/226", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(range_start=0, range_end=10, step=0.1):\n    x_values = np.arange(range_start, range_end, step)\n    y_values = np.exp(x_values)\n\n    plt.figure()\n    plt.plot(x_values, y_values, label='e^x')\n    plt.title('Exponential Function Plot')\n    plt.xlabel('x')\n    plt.ylabel('e^x')\n    plt.legend()\n    ax = plt.gca()\n\n    return ((x, y) for x, y in zip(x_values, y_values)), ax", "raw_solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(range_start=0, range_end=10, step=0.1):\n    x_values = np.arange(range_start, range_end, step)\n    y_values = np.exp(x_values)\n\n    plt.figure()\n    plt.plot(x_values, y_values, label='e^x')\n    plt.title('Exponential Function Plot')\n    plt.xlabel('x')\n    plt.ylabel('e^x')\n    plt.legend()\n    ax = plt.gca()\n\n    return ((x, y) for x, y in zip(x_values, y_values)), ax\n```\n"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file {audio_file} does not exist.\")\n\n    # Load audio file\n    y, sr = librosa.load(audio_file, sr=None)\n\n    # Calculate sound pressure level\n    spl = 20 * np.log10(np.sqrt(np.mean(y**2)))\n\n    # Create MxN matrix\n    matrix = np.array(L).reshape(M, N)\n\n    # Create spectrogram\n    fig, ax = plt.subplots()\n    img = librosa.display.specshow(librosa.amplitude_to_db(matrix, ref=np.max), sr=sr, x_axis='time', y_axis='log', ax=ax)\n    ax.set_title('Spectrogram')\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n\n    return matrix, fig", "raw_solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file {audio_file} does not exist.\")\n\n    # Load audio file\n    y, sr = librosa.load(audio_file, sr=None)\n\n    # Calculate sound pressure level\n    spl = 20 * np.log10(np.sqrt(np.mean(y**2)))\n\n    # Create MxN matrix\n    matrix = np.array(L).reshape(M, N)\n\n    # Create spectrogram\n    fig, ax = plt.subplots()\n    img = librosa.display.specshow(librosa.amplitude_to_db(matrix, ref=np.max), sr=sr, x_axis='time', y_axis='log', ax=ax)\n    ax.set_title('Spectrogram')\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n\n    return matrix, fig\n\n# Example usage:\n# matrix, fig = task_func([i for i in range(100)], 10, 10, 'audio.wav')\n# plt.show()\n```\nThis script loads an audio file, calculates the sound pressure level, reshapes a list into a matrix, and then creates a spectrogram using the matrix data. The function returns the matrix and the figure object for the spectrogram.\n"}
{"task_id": "BigCodeBench/228", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(df, dct):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Replace values in DataFrame using the provided dictionary\n    df = df.replace(dct)\n\n    # Calculate the correlation matrix\n    correlation_matrix = df.corr()\n\n    return correlation_matrix", "raw_solution": "import pandas as pd\nimport numpy as np\n\n# Constants\nCOLUMNS = ['column1', 'column2', 'column3', 'column4', 'column5']\n\ndef task_func(df, dct):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Replace values in DataFrame using the provided dictionary\n    df = df.replace(dct)\n\n    # Calculate the correlation matrix\n    correlation_matrix = df.corr()\n\n    return correlation_matrix\n\n# Example usage:"}
{"task_id": "BigCodeBench/229", "solution": "import json\nimport random\nfrom datetime import datetime, timedelta\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\ndef task_func(file_path, num_entries, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    log_entries = []\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(['login', 'logout', 'view', 'edit'])\n        timestamp = datetime.now() - timedelta(days=random.randint(1, 30), hours=random.randint(0, 23), minutes=random.randint(0, 59))\n        log_entries.append({'user': user, 'action': action, 'timestamp': timestamp.isoformat()})\n\n    with open(file_path, 'w') as f:\n        json.dump(log_entries, f, indent=4)\n\n    return file_path", "raw_solution": "import json\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\n\ndef task_func(file_path, num_entries, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    log_entries = []\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(['login', 'logout', 'view', 'edit'])\n        timestamp = datetime.now() - timedelta(days=random.randint(1, 30), hours=random.randint(0, 23), minutes=random.randint(0, 59))\n        log_entries.append({'user': user, 'action': action, 'timestamp': timestamp.isoformat()})\n\n    with open(file_path, 'w') as f:\n        json.dump(log_entries, f, indent=4)\n\n    return file_path\n```\n"}
{"task_id": "BigCodeBench/230", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in COLUMNS):\n        return \"invalid input\"\n\n    # Remove duplicates\n    df = df.drop_duplicates(subset='Name')\n\n    # Create a figure and axes\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Plot histogram\n    sns.histplot(df['Score'], bins=10, ax=axes[0])\n    axes[0].set_title('Histogram of Scores')\n\n    # Plot boxplot\n    sns.boxplot(x='Country', y='Score', data=df, ax=axes[1])\n    axes[1].set_title('Boxplot of Scores by Country')\n\n    plt.tight_layout()\n    return fig\ndata = pd.DataFrame([{'Name': 'James', 'Age': 30, 'Country': 'USA', 'Score': 85}, {'Name': 'Nick', 'Age': 50, 'Country': 'Australia', 'Score': 80}])", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in COLUMNS):\n        return \"invalid input\"\n\n    # Remove duplicates\n    df = df.drop_duplicates(subset='Name')\n\n    # Create a figure and axes\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Plot histogram\n    sns.histplot(df['Score'], bins=10, ax=axes[0])\n    axes[0].set_title('Histogram of Scores')\n\n    # Plot boxplot\n    sns.boxplot(x='Country', y='Score', data=df, ax=axes[1])\n    axes[1].set_title('Boxplot of Scores by Country')\n\n    plt.tight_layout()\n    return fig\n\n# Example usage:\ndata = pd.DataFrame([{'Name': 'James', 'Age': 30, 'Country': 'USA', 'Score': 85}, {'Name': 'Nick', 'Age': 50, 'Country': 'Australia', 'Score': 80}])\nfig = task_func(data)\naxes = fig.get_axes()"}
{"task_id": "BigCodeBench/231", "solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\ndef task_func(obj_list) -> Axes:\n    values = [obj.value for obj in obj_list]\n    if not values:\n        values = [0]\n\n    mu, std = np.mean(values), np.std(values)\n\n    x = np.linspace(mu - 4*std, mu + 4*std, 100)\n    p = stats.norm.pdf(x, mu, std)\n\n    fig, ax = plt.subplots()\n    ax.hist(values, bins=10, density=True, alpha=0.6, color='g')\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title('Histogram and Normal Distribution Curve')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\n\nclass ValueObject:\n    value = 0\n\n    def __init__(self, mu=0, std=1, seed=77):\n        random.seed(seed)\n        self.value = random.gauss(mu, std)\n\ndef task_func(obj_list) -> Axes:\n    values = [obj.value for obj in obj_list]\n    if not values:\n        values = [0]\n\n    mu, std = np.mean(values), np.std(values)\n\n    x = np.linspace(mu - 4*std, mu + 4*std, 100)\n    p = stats.norm.pdf(x, mu, std)\n\n    fig, ax = plt.subplots()\n    ax.hist(values, bins=10, density=True, alpha=0.6, color='g')\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title('Histogram and Normal Distribution Curve')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/232", "solution": "import pandas as pd\nimport collections\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    # Calculate total sales\n    total_sales = df['sales'].sum()\n\n    # Find the most popular order\n    order_counts = collections.Counter(df['order'])\n    most_popular_order = order_counts.most_common(1)[0][0]\n\n    return {'Total Sales': total_sales, 'Most Popular Order': most_popular_order}", "raw_solution": "import pandas as pd\nimport collections\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    # Calculate total sales\n    total_sales = df['sales'].sum()\n\n    # Find the most popular order\n    order_counts = collections.Counter(df['order'])\n    most_popular_order = order_counts.most_common(1)[0][0]\n\n    return {'Total Sales': total_sales, 'Most Popular Order': most_popular_order}\n\n# Example usage:\ndata = pd.DataFrame([{'customer': 'John', 'order': 'A', 'sales': 500}, {'customer': 'Mary', 'order': 'B', 'sales': 300}])\nreport = task_func(data)"}
{"task_id": "BigCodeBench/233", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(obj_list, attr, num_bins=30, seed=0):\n    # Set the seed for reproducibility\n    random.seed(seed)\n\n    # Extract the attribute values from the objects\n    values = [getattr(obj, attr) for obj in obj_list]\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(values, bins=num_bins, color='blue', alpha=0.7)\n    ax.set_title('Attribute Value Distribution')\n    ax.set_xlabel('Attribute Value')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\n# Sample data\nclass Object:\n    value = 0\n\n    def __init__(self, value=None):\n        if value is None:\n            self.value = random.gauss(0, 1)\n        else:\n            self.value = value\n\ndef task_func(obj_list, attr, num_bins=30, seed=0):\n    # Set the seed for reproducibility\n    random.seed(seed)\n\n    # Extract the attribute values from the objects\n    values = [getattr(obj, attr) for obj in obj_list]\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(values, bins=num_bins, color='blue', alpha=0.7)\n    ax.set_title('Attribute Value Distribution')\n    ax.set_xlabel('Attribute Value')\n    ax.set_ylabel('Frequency')\n\n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/234", "solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n\n    # Remove duplicate names\n    df = df.drop_duplicates(subset='Name')\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['Age'], df['Score'])\n\n    # Create regression line\n    regression_line = slope * df['Age'] + intercept\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['Age'], df['Score'], label='Data points')\n    plt.plot(df['Age'], regression_line, color='red', label='Regression line')\n    plt.xlabel('Age')\n    plt.ylabel('Score')\n    plt.title('Linear Regression')\n    plt.legend()\n    plt.grid(True)\n\n    return plt, plt.gca()", "raw_solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n\n    # Remove duplicate names\n    df = df.drop_duplicates(subset='Name')\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['Age'], df['Score'])\n\n    # Create regression line\n    regression_line = slope * df['Age'] + intercept\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['Age'], df['Score'], label='Data points')\n    plt.plot(df['Age'], regression_line, color='red', label='Regression line')\n    plt.xlabel('Age')\n    plt.ylabel('Score')\n    plt.title('Linear Regression')\n    plt.legend()\n    plt.grid(True)\n\n    return plt, plt.gca()\n```\n"}
{"task_id": "BigCodeBench/235", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, num_samples)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, num_bins, density=True, alpha=0.6, color='blue', edgecolor='black')\n    \n    # Overlay PDF\n    pdf = (1/(sigma * np.sqrt(2 * np.pi)) * np.exp(- (bins - mu)**2 / (2 * sigma**2)))\n    ax.plot(bins, pdf, 'r--', linewidth=2, label='PDF')\n    \n    # Fit OLS regression\n    x = bins[:-1]\n    y = count\n    df = pd.DataFrame({'x': x, 'y': y})\n    model = ols('y ~ x + I(x**2)', data=df).fit()\n    ax.plot(bins, model.predict({'x': bins}), 'g--', linewidth=2, label='OLS Fit')\n    \n    ax.set_title('Histogram with PDF and OLS Fit')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    \n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\n\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, num_samples)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, num_bins, density=True, alpha=0.6, color='blue', edgecolor='black')\n    \n    # Overlay PDF\n    pdf = (1/(sigma * np.sqrt(2 * np.pi)) * np.exp(- (bins - mu)**2 / (2 * sigma**2)))\n    ax.plot(bins, pdf, 'r--', linewidth=2, label='PDF')\n    \n    # Fit OLS regression\n    x = bins[:-1]\n    y = count\n    df = pd.DataFrame({'x': x, 'y': y})\n    model = ols('y ~ x + I(x**2)', data=df).fit()\n    ax.plot(bins, model.predict({'x': bins}), 'g--', linewidth=2, label='OLS Fit')\n    \n    ax.set_title('Histogram with PDF and OLS Fit')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/236", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\ndef task_func(df, test_size=0.2, random_state=42):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    # Remove duplicate 'name' entries\n    df = df.drop_duplicates(subset='name')\n    \n    # Features and target\n    X = df[['Age', 'score']]\n    y = df['category']\n    \n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Initialize and train the model\n    model = RandomForestClassifier(random_state=random_state)\n    model.fit(X_train, y_train)\n    \n    # Predict and calculate accuracy\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy\ndata = pd.DataFrame([{'name': 'James', 'Age': 30, 'score': 85, 'category': 'Electronics'}, {'name': 'lily', 'Age': 28, 'score': 92, 'category': 'home'}])", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef task_func(df, test_size=0.2, random_state=42):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    # Remove duplicate 'name' entries\n    df = df.drop_duplicates(subset='name')\n    \n    # Features and target\n    X = df[['Age', 'score']]\n    y = df['category']\n    \n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Initialize and train the model\n    model = RandomForestClassifier(random_state=random_state)\n    model.fit(X_train, y_train)\n    \n    # Predict and calculate accuracy\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy\n\n# Example usage:\ndata = pd.DataFrame([{'name': 'James', 'Age': 30, 'score': 85, 'category': 'Electronics'}, {'name': 'lily', 'Age': 28, 'score': 92, 'category': 'home'}])\naccuracy = task_func(data)"}
{"task_id": "BigCodeBench/237", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, save_plot=False, plot_path=None):\n    # Convert data to numpy array\n    data_array = np.array([point[1:] for point in data])\n    \n    # Apply PCA to reduce dimensionality to 2D\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(data_array)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n    ax.set_title('2D PCA of Data')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    \n    # Save plot if required\n    if save_plot:\n        if plot_path is None:\n            raise ValueError(\"plot_path is required when save_plot is True\")\n        plt.savefig(plot_path)\n    \n    return coordinates_2d, ax if save_plot else None", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(data, save_plot=False, plot_path=None):\n    # Convert data to numpy array\n    data_array = np.array([point[1:] for point in data])\n    \n    # Apply PCA to reduce dimensionality to 2D\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(data_array)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n    ax.set_title('2D PCA of Data')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    \n    # Save plot if required\n    if save_plot:\n        if plot_path is None:\n            raise ValueError(\"plot_path is required when save_plot is True\")\n        plt.savefig(plot_path)\n    \n    return coordinates_2d, ax if save_plot else None\n\n# Example usage\n# task_func([('A', 1, 1, 1), ('B', 2, 2, 2)], save_plot=True, plot_path=\"temp_plot.png\")\n```\n"}
{"task_id": "BigCodeBench/238", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    # Drop duplicates based on 'name' column\n    df = df.drop_duplicates(subset='name')\n\n    # Standardize 'age' and 'score' columns\n    scaler = StandardScaler()\n    df[['age', 'score']] = scaler.fit_transform(df[['age', 'score']])\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(df['age'], df['score'])\n    ax.set_title('Scatter Plot of Standardized age and score')\n    ax.set_xlabel('age (standardized)')\n    ax.set_ylabel('score (standardized)')\n\n    return df, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    # Drop duplicates based on 'name' column\n    df = df.drop_duplicates(subset='name')\n\n    # Standardize 'age' and 'score' columns\n    scaler = StandardScaler()\n    df[['age', 'score']] = scaler.fit_transform(df[['age', 'score']])\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(df['age'], df['score'])\n    ax.set_title('Scatter Plot of Standardized age and score')\n    ax.set_xlabel('age (standardized)')\n    ax.set_ylabel('score (standardized)')\n\n    return df, ax\n\n# Example usage:\ndata = pd.DataFrame([{'name': 'James', 'age': 30, 'score': 85},{'name': 'James', 'age': 35, 'score': 90},{'name': 'lily', 'age': 28, 'score': 92},{'name': 'Sam', 'age': 40, 'score': 88},{'name': 'Nick', 'age': 50, 'score': 80}])\nmodified_df, plot_axes = task_func(data)"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extracting values from the original list of tuples\n    values = np.array([t[1] for t in original])\n    \n    # Calculating statistics\n    mean = np.mean(values)\n    std = np.std(values)\n    min_val = np.min(values)\n    max_val = np.max(values)\n    \n    # Plotting histogram\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(values, bins='auto', alpha=0.6, density=True)\n    \n    # Plotting PDF\n    pdf = stats.norm.pdf(bins, mean, std)\n    ax.plot(bins, pdf, 'r--', linewidth=2)\n    \n    # Returning the array and statistics\n    stats_dict = {'mean': mean, 'std': std, 'min': min_val, 'max': max_val}\n    return values, stats_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extracting values from the original list of tuples\n    values = np.array([t[1] for t in original])\n    \n    # Calculating statistics\n    mean = np.mean(values)\n    std = np.std(values)\n    min_val = np.min(values)\n    max_val = np.max(values)\n    \n    # Plotting histogram\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(values, bins='auto', alpha=0.6, density=True)\n    \n    # Plotting PDF\n    pdf = stats.norm.pdf(bins, mean, std)\n    ax.plot(bins, pdf, 'r--', linewidth=2)\n    \n    # Returning the array and statistics\n    stats_dict = {'mean': mean, 'std': std, 'min': min_val, 'max': max_val}\n    return values, stats_dict, ax\n```\n"}
{"task_id": "BigCodeBench/240", "solution": "import pandas as pd\nfrom random import uniform\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    data = [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[column_name])\n    return df", "raw_solution": "import pandas as pd\nfrom random import uniform\n\n\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    data = [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[column_name])\n    return df\n```\n"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the list of tuples into a numpy array\n    arr = np.array(original, dtype=float)\n    \n    # Extract the second element of each tuple (assumed to be numeric)\n    data = arr[:, 1]\n    \n    # Normalize the data using Min-Max scaling\n    norm_arr = preprocessing.minmax_scale(data)\n    \n    # Plot the original and normalized data\n    fig, ax = plt.subplots()\n    ax.plot(arr[:, 0], data, label='Original Data')\n    ax.plot(arr[:, 0], norm_arr, label='Normalized Data', linestyle='--')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Original vs Normalized Data')\n    ax.legend()\n    \n    return arr, norm_arr, ax\noriginal = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the list of tuples into a numpy array\n    arr = np.array(original, dtype=float)\n    \n    # Extract the second element of each tuple (assumed to be numeric)\n    data = arr[:, 1]\n    \n    # Normalize the data using Min-Max scaling\n    norm_arr = preprocessing.minmax_scale(data)\n    \n    # Plot the original and normalized data\n    fig, ax = plt.subplots()\n    ax.plot(arr[:, 0], data, label='Original Data')\n    ax.plot(arr[:, 0], norm_arr, label='Normalized Data', linestyle='--')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Original vs Normalized Data')\n    ax.legend()\n    \n    return arr, norm_arr, ax\n\n# Example usage:\noriginal = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\narr, norm_arr, ax = task_func(original)"}
{"task_id": "BigCodeBench/242", "solution": "import cv2\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(image_path, kernel_size):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"Kernel size must be a positive integer.\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be read as an image.\")\n\n    # Apply the blur effect\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n\n    # Plot the original and blurred images\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax1.set_title('Original')\n    ax1.axis('off')\n\n    ax2.imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n    ax2.set_title('Blurred')\n    ax2.axis('off')\n\n    plt.show()\n\n    return blurred_image, ax1, ax2", "raw_solution": "import cv2\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(image_path, kernel_size):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"Kernel size must be a positive integer.\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be read as an image.\")\n\n    # Apply the blur effect\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n\n    # Plot the original and blurred images\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax1.set_title('Original')\n    ax1.axis('off')\n\n    ax2.imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n    ax2.set_title('Blurred')\n    ax2.axis('off')\n\n    plt.show()\n\n    return blurred_image, ax1, ax2\n```\n"}
{"task_id": "BigCodeBench/243", "solution": "import pandas as pd\nimport random\nN_DATA_POINTS = 10000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\ndef task_func(n_data_points=N_DATA_POINTS):\n    if n_data_points == 0:\n        return pd.DataFrame(columns=['Value'])\n\n    data = [round(random.uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=['Value'])\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\n# Constants\nN_DATA_POINTS = 10000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef task_func(n_data_points=N_DATA_POINTS):\n    if n_data_points == 0:\n        return pd.DataFrame(columns=['Value'])\n\n    data = [round(random.uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=['Value'])\n\n    return df\n\n# Example usage:\n# random.seed(0)\n# data = task_func(20)\n# print(data.shape)  # Output: (20, 1)\n# assert MIN_VALUE <= data.iloc[0]['Value'] <= MAX_VALUE\n```\n"}
{"task_id": "BigCodeBench/244", "solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\ndef task_func(original):\n    # Extracting numeric values from the original list\n    arr = np.array([val for _, val in original])\n\n    # Performing FFT on the numeric array\n    fft_data = fft(arr)\n\n    # Plotting the magnitude of FFT data\n    fig, ax = plt.subplots()\n    ax.hist(np.abs(fft_data), bins=10, color='blue', alpha=0.7)\n    ax.set_title('Magnitude of FFT Data')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return arr, fft_data, ax\noriginal = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]", "raw_solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\n\ndef task_func(original):\n    # Extracting numeric values from the original list\n    arr = np.array([val for _, val in original])\n\n    # Performing FFT on the numeric array\n    fft_data = fft(arr)\n\n    # Plotting the magnitude of FFT data\n    fig, ax = plt.subplots()\n    ax.hist(np.abs(fft_data), bins=10, color='blue', alpha=0.7)\n    ax.set_title('Magnitude of FFT Data')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return arr, fft_data, ax\n\n# Example usage\noriginal = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\narr, fft_data, ax = task_func(original)"}
{"task_id": "BigCodeBench/245", "solution": "import pandas as pd\nimport random\nfrom scipy import stats\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate random data points\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Create a pandas Series from the data\n    series = pd.Series(data)\n    \n    # Calculate statistical measures\n    mean_value = series.mean()\n    median_value = series.median()\n    mode_value = series.mode()[0] if not series.mode().empty else None\n    \n    # Return the results as a dictionary\n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'mode': mode_value\n    }", "raw_solution": "import pandas as pd\nimport random\nfrom scipy import stats\n\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate random data points\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Create a pandas Series from the data\n    series = pd.Series(data)\n    \n    # Calculate statistical measures\n    mean_value = series.mean()\n    median_value = series.median()\n    mode_value = series.mode()[0] if not series.mode().empty else None\n    \n    # Return the results as a dictionary\n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'mode': mode_value\n    }\n\n# Example usage\nrandom.seed(0)\nstats = task_func(1000, 5.0, 5.0)"}
{"task_id": "BigCodeBench/246", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\nANGLES = np.arange(0, 2*np.pi, 0.01)\ndef task_func(n_waves, seed=0):\n    if n_waves < 1:\n        return [], np.array([]), None\n\n    np.random.seed(seed)\n    sine_waves = [np.sin(2 * np.pi * i * ANGLES) for i in range(1, n_waves + 1)]\n    mixed_signal = np.sum(sine_waves, axis=0)\n\n    fft_data = fft(mixed_signal)\n    fft_magnitude = np.abs(fft_data)\n\n    fig, ax = plt.subplots()\n    ax.hist(fft_magnitude, bins=30, color='blue', alpha=0.7)\n    ax.set_title('FFT Magnitude Histogram')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return sine_waves, fft_data, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\nANGLES = np.arange(0, 2*np.pi, 0.01)\n\ndef task_func(n_waves, seed=0):\n    if n_waves < 1:\n        return [], np.array([]), None\n\n    np.random.seed(seed)\n    sine_waves = [np.sin(2 * np.pi * i * ANGLES) for i in range(1, n_waves + 1)]\n    mixed_signal = np.sum(sine_waves, axis=0)\n\n    fft_data = fft(mixed_signal)\n    fft_magnitude = np.abs(fft_data)\n\n    fig, ax = plt.subplots()\n    ax.hist(fft_magnitude, bins=30, color='blue', alpha=0.7)\n    ax.set_title('FFT Magnitude Histogram')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return sine_waves, fft_data, ax\n```\n"}
{"task_id": "BigCodeBench/247", "solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    if max_value < min_value:\n        raise ValueError(\"max_value should be greater than or equal to min_value\")\n\n    # Generate random data points\n    data_points = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(data_points, columns=['Value'])\n\n    # Standard scaling\n    scaler = StandardScaler()\n    df['Normalized Value'] = scaler.fit_transform(df[['Value']])\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nN_DATA_POINTS = 5000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    if max_value < min_value:\n        raise ValueError(\"max_value should be greater than or equal to min_value\")\n\n    # Generate random data points\n    data_points = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(data_points, columns=['Value'])\n\n    # Standard scaling\n    scaler = StandardScaler()\n    df['Normalized Value'] = scaler.fit_transform(df[['Value']])\n\n    return df\n\n# Example usage\nrandom.seed(0)\nnormalized_data = task_func(5000, 5, 5)"}
{"task_id": "BigCodeBench/248", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"data_list cannot be empty\")\n\n    # Unzipping the data\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=np.nan))\n\n    # Plotting the numerical values\n    fig, ax = plt.subplots()\n    for i, values in enumerate(unzipped_data[1:]):  # Ignoring the first element (characters)\n        ax.plot(values, label=f'Position {i+1}')\n    \n    ax.set_title('Numerical Values Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"data_list cannot be empty\")\n\n    # Unzipping the data\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=np.nan))\n\n    # Plotting the numerical values\n    fig, ax = plt.subplots()\n    for i, values in enumerate(unzipped_data[1:]):  # Ignoring the first element (characters)\n        ax.plot(values, label=f'Position {i+1}')\n    \n    ax.set_title('Numerical Values Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return ax\n\n# Example usage:\n# plot = task_func([('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)])\n# print(type(plot))\n```\n"}
{"task_id": "BigCodeBench/249", "solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate random data\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['Value'])\n    \n    # Split the data into train and test sets\n    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n    \n    return train_df, test_df", "raw_solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate random data\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['Value'])\n    \n    # Split the data into train and test sets\n    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n    \n    return train_df, test_df\n\n# Example usage:\n# random.seed(0)\n# train_data, test_data = task_func()\n# print(train_data.shape[0])\n# print(test_data.shape[0])\n# print(test_data.iloc[0]['Value'])\n```\n"}
{"task_id": "BigCodeBench/250", "solution": "import numpy as np\nimport itertools\nimport json\nimport tempfile\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    result = {}\n    for i, (letter, *values) in enumerate(data_list, start=1):\n        mean_value = np.mean(values)\n        result[f'Position {i}'] = mean_value\n\n    if json_file_name:\n        with open(json_file_name, 'w') as json_file:\n            json.dump(result, json_file)\n\n    return result", "raw_solution": "import numpy as np\nimport itertools\nimport json\nimport tempfile\n\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    result = {}\n    for i, (letter, *values) in enumerate(data_list, start=1):\n        mean_value = np.mean(values)\n        result[f'Position {i}'] = mean_value\n\n    if json_file_name:\n        with open(json_file_name, 'w') as json_file:\n            json.dump(result, json_file)\n\n    return result\n\n# Example usage:"}
{"task_id": "BigCodeBench/251", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n\n    if 'Job' not in data.columns:\n        raise ValueError(\"DataFrame must contain 'Job' column.\")\n\n    job_counts = data['Job'].value_counts()\n    fig, ax = plt.subplots()\n    ax.pie(job_counts, labels=job_counts.index, autopct='%1.1f%%', startangle=90)\n    ax.set_title('Job Distribution')\n    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    return fig\ndata = pd.DataFrame({'Name': ['John', 'Jane', 'Joe'],\n                       'Date': ['01/03/2012', '02/05/2013', '03/08/2014'],\n                       'Job': ['Engineer', 'Doctor', 'Lawyer']})", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n\n    if 'Job' not in data.columns:\n        raise ValueError(\"DataFrame must contain 'Job' column.\")\n\n    job_counts = data['Job'].value_counts()\n    fig, ax = plt.subplots()\n    ax.pie(job_counts, labels=job_counts.index, autopct='%1.1f%%', startangle=90)\n    ax.set_title('Job Distribution')\n    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    return fig\n\n# Example usage:\ndata = pd.DataFrame({'Name': ['John', 'Jane', 'Joe'],\n                       'Date': ['01/03/2012', '02/05/2013', '03/08/2014'],\n                       'Job': ['Engineer', 'Doctor', 'Lawyer']})\nfig = task_func(data)"}
{"task_id": "BigCodeBench/252", "solution": "import matplotlib.pyplot as plt\nfrom itertools import zip_longest\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\ndef task_func(data, labels):\n    # Handle cases where data and labels have unequal lengths\n    colors = zip_longest(COLORS, data, fillvalue='black')\n    \n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot each data series with a corresponding color\n    for color, series in colors:\n        if color is not None:\n            ax.plot(series, label=labels.pop(0), color=color)\n        else:\n            ax.plot(series, label='Series', color='black')\n    \n    # Add legend\n    ax.legend()\n    \n    return ax\ndata = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]]\nlabels = ['Series 1', 'Series 2', 'Series 3']", "raw_solution": "import matplotlib.pyplot as plt\nfrom itertools import zip_longest\n\n# Constants\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\n\ndef task_func(data, labels):\n    # Handle cases where data and labels have unequal lengths\n    colors = zip_longest(COLORS, data, fillvalue='black')\n    \n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot each data series with a corresponding color\n    for color, series in colors:\n        if color is not None:\n            ax.plot(series, label=labels.pop(0), color=color)\n        else:\n            ax.plot(series, label='Series', color='black')\n    \n    # Add legend\n    ax.legend()\n    \n    return ax\n\n# Example usage\ndata = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]]\nlabels = ['Series 1', 'Series 2', 'Series 3']\nax = task_func(data, labels)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/253", "solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\ndef task_func(ax):\n    # Generate random data for sine wave\n    theta = np.linspace(0, 2 * np.pi, 100)\n    amplitude = np.sin(theta) + random.uniform(-0.1, 0.1) * np.cos(10 * theta)\n\n    # Plot the sine wave\n    color = random.choice(COLORS)\n    ax.plot(theta, amplitude, color=color, label=f'Wave {color}')\n\n    # Set random radial labels\n    ax.set_rticks([0.5, 1.0, 1.5])\n    ax.set_rlabel_position(random.uniform(0, 360))\n\n    # Return the color used for the plot\n    return color", "raw_solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef task_func(ax):\n    # Generate random data for sine wave\n    theta = np.linspace(0, 2 * np.pi, 100)\n    amplitude = np.sin(theta) + random.uniform(-0.1, 0.1) * np.cos(10 * theta)\n\n    # Plot the sine wave\n    color = random.choice(COLORS)\n    ax.plot(theta, amplitude, color=color, label=f'Wave {color}')\n\n    # Set random radial labels\n    ax.set_rticks([0.5, 1.0, 1.5])\n    ax.set_rlabel_position(random.uniform(0, 360))\n\n    # Return the color used for the plot\n    return color\n\n# Example usage"}
{"task_id": "BigCodeBench/254", "solution": "import json\nimport math\ndef task_func(decimal_value, precision=2):\n    \"\"\"\n    Diye gaye decimal value ka square ek certain precision tak nikaalen and then result ko json string ke roop mein encode karen.\n\n    Parameters:\n    - decimal_value (float): number to calculate the square root of.\n    - precision (int, Optional): number of decimal places to round the square root to. Defaults to 2.\n\n    Returns:\n        str: square root of decimal_value rounded to the specified precision, encoded as a JSON string.\n\n    Requirements:\n    - json\n    - math\n\n    Example:\n    >>> decimal_value = 3.9\n    >>> json_str = task_func(decimal_value, precision=2)\n    >>> print(json_str)\n    \"1.97\"\n\n    \"\"\"\n    square_root = math.sqrt(decimal_value)\n    rounded_square_root = round(square_root, precision)\n    json_str = json.dumps(rounded_square_root)\n    return json_str", "raw_solution": "import json\nimport math\n\ndef task_func(decimal_value, precision=2):\n    \"\"\"\n    Diye gaye decimal value ka square ek certain precision tak nikaalen and then result ko json string ke roop mein encode karen.\n\n    Parameters:\n    - decimal_value (float): number to calculate the square root of.\n    - precision (int, Optional): number of decimal places to round the square root to. Defaults to 2.\n\n    Returns:\n        str: square root of decimal_value rounded to the specified precision, encoded as a JSON string.\n\n    Requirements:\n    - json\n    - math\n\n    Example:\n    >>> decimal_value = 3.9\n    >>> json_str = task_func(decimal_value, precision=2)\n    >>> print(json_str)\n    \"1.97\"\n\n    \"\"\"\n    square_root = math.sqrt(decimal_value)\n    rounded_square_root = round(square_root, precision)\n    json_str = json.dumps(rounded_square_root)\n    return json_str\n\n# Test cases"}
{"task_id": "BigCodeBench/255", "solution": "import matplotlib\nimport numpy as np\nFUNCTIONS = [np.sin, np.cos, np.tan]\ndef task_func(ax, func_index):\n    if not isinstance(ax, matplotlib.axes._axes.Axes):\n        raise ValueError(\"Input ax must be a matplotlib axes object.\")\n\n    if func_index < 0 or func_index >= len(FUNCTIONS):\n        raise ValueError(\"func_index must be between 0 and the number of functions minus one.\")\n\n    func = FUNCTIONS[func_index]\n    t = np.linspace(0, 2 * np.pi, 100)\n    y = func(t)\n\n    ax.plot(t, y)\n    ax.set_rticks([1])  # Set radial ticks to 1\n    ax.set_rlabel_position(45)  # Set radial label position to 45 degrees\n\n    return ax", "raw_solution": "import matplotlib\nimport numpy as np\n\n# Constants\nFUNCTIONS = [np.sin, np.cos, np.tan]\n\ndef task_func(ax, func_index):\n    if not isinstance(ax, matplotlib.axes._axes.Axes):\n        raise ValueError(\"Input ax must be a matplotlib axes object.\")\n\n    if func_index < 0 or func_index >= len(FUNCTIONS):\n        raise ValueError(\"func_index must be between 0 and the number of functions minus one.\")\n\n    func = FUNCTIONS[func_index]\n    t = np.linspace(0, 2 * np.pi, 100)\n    y = func(t)\n\n    ax.plot(t, y)\n    ax.set_rticks([1])  # Set radial ticks to 1\n    ax.set_rlabel_position(45)  # Set radial label position to 45 degrees\n\n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/256", "solution": "import json\nimport random\nimport hashlib\nfrom datetime import datetime\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object.\")\n    if not isinstance(salt, str):\n        raise ValueError(\"salt must be a string.\")\n\n    random.seed(seed)\n    password = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=password_length))\n    password_hash = hashlib.sha256((password + salt).encode()).hexdigest()\n\n    json_str = json.dumps({\n        'password': password,\n        'hash': password_hash,\n        'salt': salt,\n        'datetime': utc_datetime.isoformat()\n    })\n\n    return json_str", "raw_solution": "import json\nimport random\nimport hashlib\nfrom datetime import datetime\n\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object.\")\n    if not isinstance(salt, str):\n        raise ValueError(\"salt must be a string.\")\n\n    random.seed(seed)\n    password = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=password_length))\n    password_hash = hashlib.sha256((password + salt).encode()).hexdigest()\n\n    json_str = json.dumps({\n        'password': password,\n        'hash': password_hash,\n        'salt': salt,\n        'datetime': utc_datetime.isoformat()\n    })\n\n    return json_str\n\n# Example usage:\nutc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=datetime.timezone.utc)\npassword_json_str = task_func(utc_time)"}
{"task_id": "BigCodeBench/257", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(ax, num_turns):\n    \"\"\"\n    Dhruviy diagram 'ax' par 'num_turns' turns ki specified number ke saath ek spiral banaen. \n    spiral center se shuru hota hai and pratyek turn ke saath outward failta hai.\n    plot par radial ticks ko turns ki number ko 45 degrees se guna karke intervals par rakha jaata hai.\n\n    Parameters:\n    - ax (matplotlib.axes._axes.Axes): spiral plot karne ke liye Axes object.\n    - num_turns (int): spiral ke liye turns ki number.\n\n    Returns:\n        matplotlib.axes._axes.Axes: spiral plot ke saath sanshodhit Axes object.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib\n\n    Example:\n    >>> fig, ax = plt.subplots(subplot_kw={'polar': True})\n    >>> ax = task_func(ax, 3)\n    >>> ax.get_rlabel_position()\n    135.0\n\n    \"\"\"\n    theta = np.linspace(0, 2 * np.pi, 1000)\n    r = np.linspace(0, num_turns, 1000)\n    R, Theta = np.meshgrid(r, theta)\n    Z = np.cos(Theta) * R\n\n    ax.contour(Theta, R, Z, colors='blue')\n    ax.set_rticks(np.arange(0, num_turns, 0.5))  # Set radial ticks\n    ax.set_rlabel_position(45)  # Set the radial labels position\n\n    return ax", "raw_solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(ax, num_turns):\n    \"\"\"\n    Dhruviy diagram 'ax' par 'num_turns' turns ki specified number ke saath ek spiral banaen. \n    spiral center se shuru hota hai and pratyek turn ke saath outward failta hai.\n    plot par radial ticks ko turns ki number ko 45 degrees se guna karke intervals par rakha jaata hai.\n\n    Parameters:\n    - ax (matplotlib.axes._axes.Axes): spiral plot karne ke liye Axes object.\n    - num_turns (int): spiral ke liye turns ki number.\n\n    Returns:\n        matplotlib.axes._axes.Axes: spiral plot ke saath sanshodhit Axes object.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib\n\n    Example:\n    >>> fig, ax = plt.subplots(subplot_kw={'polar': True})\n    >>> ax = task_func(ax, 3)\n    >>> ax.get_rlabel_position()\n    135.0\n\n    \"\"\"\n    theta = np.linspace(0, 2 * np.pi, 1000)\n    r = np.linspace(0, num_turns, 1000)\n    R, Theta = np.meshgrid(r, theta)\n    Z = np.cos(Theta) * R\n\n    ax.contour(Theta, R, Z, colors='blue')\n    ax.set_rticks(np.arange(0, num_turns, 0.5))  # Set radial ticks\n    ax.set_rlabel_position(45)  # Set the radial labels position\n\n    return ax\n\n# Example usage:\n# fig, ax = plt.subplots(subplot_kw={'polar': True})\n# ax = task_func(ax, 3)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/258", "solution": "import json\nimport random\nfrom datetime import datetime\nimport pytz\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\ndef task_func(utc_datetime, seed=0):\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = utc_datetime.isoformat()\n    person_json_str = json.dumps(person)\n    return person_json_str", "raw_solution": "import json\nimport random\nfrom datetime import datetime\nimport pytz\n\n# Constants\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\n\ndef task_func(utc_datetime, seed=0):\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = utc_datetime.isoformat()\n    person_json_str = json.dumps(person)\n    return person_json_str\n\n# Example usage:"}
{"task_id": "BigCodeBench/259", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, num_points):\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be a matplotlib axes object.\")\n    if num_points < 0:\n        raise ValueError(\"Number of points cannot be negative.\")\n\n    # Generate random points\n    theta = np.linspace(0, 2 * np.pi, num_points)\n    r = np.random.rand(num_points)\n    ax.scatter(theta, r)\n\n    # Adjust radial labels\n    ax.set_rlabel_position(10)\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(ax, num_points):\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be a matplotlib axes object.\")\n    if num_points < 0:\n        raise ValueError(\"Number of points cannot be negative.\")\n\n    # Generate random points\n    theta = np.linspace(0, 2 * np.pi, num_points)\n    r = np.random.rand(num_points)\n    ax.scatter(theta, r)\n\n    # Adjust radial labels\n    ax.set_rlabel_position(10)\n\n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/260", "solution": "import json\nimport os\nimport glob\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\ndef task_func(directory):\n    # Initialize the count of updated files\n    updated_files_count = 0\n\n    # Get all json files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    for json_file in json_files:\n        with open(json_file, 'r+') as file:\n            data = json.load(file)\n\n            # Check if the key already exists in the json file\n            if KEY not in data:\n                # Add the new key-value pair\n                data[KEY] = VALUE\n                file.seek(0)\n                json.dump(data, file, indent=4)\n                updated_files_count += 1\n\n    return updated_files_count", "raw_solution": "import json\nimport os\nimport glob\n\n# Constants\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\n\ndef task_func(directory):\n    # Initialize the count of updated files\n    updated_files_count = 0\n\n    # Get all json files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    for json_file in json_files:\n        with open(json_file, 'r+') as file:\n            data = json.load(file)\n\n            # Check if the key already exists in the json file\n            if KEY not in data:\n                # Add the new key-value pair\n                data[KEY] = VALUE\n                file.seek(0)\n                json.dump(data, file, indent=4)\n                updated_files_count += 1\n\n    return updated_files_count\n```\n"}
{"task_id": "BigCodeBench/261", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, radius):\n    if not isinstance(ax, plt.Axes) or not ax.get_subplotspec().is_polar():\n        raise TypeError(\"The provided ax is not a polar plot.\")\n    if radius < 0:\n        raise ValueError(\"The radius must be non-negative.\")\n    \n    theta = np.linspace(0, 2 * np.pi, 100)\n    y = np.full_like(theta, radius)\n    ax.plot(theta, y, 'r')\n    ax.set_rticks([radius])  # Set radial ticks\n    ax.set_rlabel_position(0)  # Move radial labels to the center\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(ax, radius):\n    if not isinstance(ax, plt.Axes) or not ax.get_subplotspec().is_polar():\n        raise TypeError(\"The provided ax is not a polar plot.\")\n    if radius < 0:\n        raise ValueError(\"The radius must be non-negative.\")\n    \n    theta = np.linspace(0, 2 * np.pi, 100)\n    y = np.full_like(theta, radius)\n    ax.plot(theta, y, 'r')\n    ax.set_rticks([radius])  # Set radial ticks\n    ax.set_rlabel_position(0)  # Move radial labels to the center\n\n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/262", "solution": "import collections\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(dictionary, new_key, new_value):\n    # Add new key-value pair to the dictionary\n    dictionary[new_key] = new_value\n\n    # Count the frequency of each value in the dictionary\n    value_counts = collections.Counter(dictionary.values())\n\n    # Create a bar plot of the value distribution\n    values = list(value_counts.keys())\n    counts = list(value_counts.values())\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=values, y=counts, palette='viridis')\n    ax.set_title('Value Distribution')\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n\n    return dictionary, ax", "raw_solution": "import collections\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(dictionary, new_key, new_value):\n    # Add new key-value pair to the dictionary\n    dictionary[new_key] = new_value\n\n    # Count the frequency of each value in the dictionary\n    value_counts = collections.Counter(dictionary.values())\n\n    # Create a bar plot of the value distribution\n    values = list(value_counts.keys())\n    counts = list(value_counts.values())\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=values, y=counts, palette='viridis')\n    ax.set_title('Value Distribution')\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n\n    return dictionary, ax\n```\n"}
{"task_id": "BigCodeBench/263", "solution": "import os\nimport glob\nimport shutil\nimport time\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\ndef task_func(my_path: str, days_old: int) -> str:\n    # Calculate the threshold time\n    threshold_time = time.time() - days_old * 86400  # 86400 seconds in a day\n\n    # Create the archive directory if it doesn't exist\n    archive_path = os.path.join(my_path, 'archive')\n    if not os.path.exists(archive_path):\n        os.makedirs(archive_path)\n\n    # Search for files with the specified extensions\n    for ext in FILE_EXTENSIONS:\n        for file_path in glob.glob(os.path.join(my_path, f'*{ext}')):\n            # Check if the file is older than the threshold time\n            if os.path.getmtime(file_path) < threshold_time:\n                # Move the file to the archive directory\n                shutil.move(file_path, archive_path)\n\n    return archive_path", "raw_solution": "import os\nimport glob\nimport shutil\nimport time\n\n# Constants\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\n\ndef task_func(my_path: str, days_old: int) -> str:\n    # Calculate the threshold time\n    threshold_time = time.time() - days_old * 86400  # 86400 seconds in a day\n\n    # Create the archive directory if it doesn't exist\n    archive_path = os.path.join(my_path, 'archive')\n    if not os.path.exists(archive_path):\n        os.makedirs(archive_path)\n\n    # Search for files with the specified extensions\n    for ext in FILE_EXTENSIONS:\n        for file_path in glob.glob(os.path.join(my_path, f'*{ext}')):\n            # Check if the file is older than the threshold time\n            if os.path.getmtime(file_path) < threshold_time:\n                # Move the file to the archive directory\n                shutil.move(file_path, archive_path)\n\n    return archive_path\n\n# Example usage\n# print(task_func('/usr/my_directory', 30))\n```\n"}
{"task_id": "BigCodeBench/264", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number.\")\n\n    np.random.seed(seed)\n    mean = dictionary.get(key, 0)\n    std_dev = value\n\n    data = np.random.normal(mean, std_dev, n)\n    series = pd.Series(data)\n\n    plt.hist(data, bins=bins, alpha=0.7, color='blue')\n    plt.title('Histogram of Generated Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()\n\n    dictionary[key] = value\n\n    return dictionary, series", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number.\")\n\n    np.random.seed(seed)\n    mean = dictionary.get(key, 0)\n    std_dev = value\n\n    data = np.random.normal(mean, std_dev, n)\n    series = pd.Series(data)\n\n    plt.hist(data, bins=bins, alpha=0.7, color='blue')\n    plt.title('Histogram of Generated Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()\n\n    dictionary[key] = value\n\n    return dictionary, series\n```\n"}
{"task_id": "BigCodeBench/265", "solution": "import collections\nimport json\nimport os\ndef task_func(data, json_file_name='data.json'):\n    # Calculate frequency of each value in the dictionary\n    freq = collections.Counter(data.values())\n\n    # Save the frequency dictionary to a JSON file\n    with open(json_file_name, 'w') as json_file:\n        json.dump(freq, json_file)\n\n    # Return the path of the JSON file\n    return os.path.abspath(json_file_name)", "raw_solution": "import collections\nimport json\nimport os\n\ndef task_func(data, json_file_name='data.json'):\n    # Calculate frequency of each value in the dictionary\n    freq = collections.Counter(data.values())\n\n    # Save the frequency dictionary to a JSON file\n    with open(json_file_name, 'w') as json_file:\n        json.dump(freq, json_file)\n\n    # Return the path of the JSON file\n    return os.path.abspath(json_file_name)\n\n# Example usage:"}
{"task_id": "BigCodeBench/266", "solution": "import os\nimport os.path\nimport csv\nimport collections\nFILE_NAME = 'file_sizes.csv'\ndef task_func(my_path):\n    \"\"\"\n    Ek directory mein file ke size par ek report banaen and ise ek csv file mein likhen.\n\n    Parameters:\n    - my_path (str): directory ka path.\n\n    Returns:\n        str: csv file ka path.\n\n    Requirements:\n    - os\n    - os.path\n    - csv\n    - collections\n\n    Example:\n    >>> task_func('/usr/my_directory')\n\n    \"\"\"\n    if not os.path.isdir(my_path):\n        raise ValueError(f\"{my_path} is not a valid directory.\")\n\n    file_sizes = collections.defaultdict(int)\n\n    for root, _, files in os.walk(my_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            if os.path.isfile(file_path):\n                file_sizes[file] = os.path.getsize(file_path)\n\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        fieldnames = ['File Name', 'Size (bytes)']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for file, size in file_sizes.items():\n            writer.writerow({'File Name': file, 'Size (bytes)': size})\n\n    return os.path.abspath(FILE_NAME)", "raw_solution": "import os\nimport os.path\nimport csv\nimport collections\n\n# Constants\nFILE_NAME = 'file_sizes.csv'\n\ndef task_func(my_path):\n    \"\"\"\n    Ek directory mein file ke size par ek report banaen and ise ek csv file mein likhen.\n\n    Parameters:\n    - my_path (str): directory ka path.\n\n    Returns:\n        str: csv file ka path.\n\n    Requirements:\n    - os\n    - os.path\n    - csv\n    - collections\n\n    Example:\n    >>> task_func('/usr/my_directory')\n\n    \"\"\"\n    if not os.path.isdir(my_path):\n        raise ValueError(f\"{my_path} is not a valid directory.\")\n\n    file_sizes = collections.defaultdict(int)\n\n    for root, _, files in os.walk(my_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            if os.path.isfile(file_path):\n                file_sizes[file] = os.path.getsize(file_path)\n\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        fieldnames = ['File Name', 'Size (bytes)']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for file, size in file_sizes.items():\n            writer.writerow({'File Name': file, 'Size (bytes)': size})\n\n    return os.path.abspath(FILE_NAME)\n```\n"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # Create a new dictionary 'a' with a single key-value pair\n    a = {'a': 1}\n    \n    # Extract values from the input dictionary and create a signal\n    signal = np.array(list(data.values()))\n    \n    # Perform Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    \n    # Plot the FFT result\n    fig, ax = plt.subplots()\n    ax.plot(fftpack.fftfreq(len(signal), d=1/sample_rate))\n    ax.plot(np.abs(fft_result))\n    ax.set_title('FFT of Signal')\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Amplitude')\n    \n    return fft_result, ax\ndata = {'dicty1': 1, 'dicty2': 2, 'dicty3': 3}", "raw_solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Create a new dictionary 'a' with a single key-value pair\n    a = {'a': 1}\n    \n    # Extract values from the input dictionary and create a signal\n    signal = np.array(list(data.values()))\n    \n    # Perform Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    \n    # Plot the FFT result\n    fig, ax = plt.subplots()\n    ax.plot(fftpack.fftfreq(len(signal), d=1/sample_rate))\n    ax.plot(np.abs(fft_result))\n    ax.set_title('FFT of Signal')\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Amplitude')\n    \n    return fft_result, ax\n\n# Example usage:\ndata = {'dicty1': 1, 'dicty2': 2, 'dicty3': 3}\nfft, ax = task_func(data)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/268", "solution": "import collections\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\ndef task_func(n_keys, n_values):\n    \"\"\"\n    Ek specified number mein key (keys) and maan (values) ke saath ek Python dictionary (dictionary) banaen.\n\n    Notes:\n        keyyaan (keys) letters (letters) ki poorvnirdhaarit list se randomly chuni jaati hain, and maan 1 se shuru hone vaale consecutive integers (integers) hain.\n        key selection mein randomness ke kaaran, dictionary (dictionary) mein actual kunjiyaan (keys) pratyek execution (execution) mein bhinn ho sakti hain.\n\n    Parameters:\n    - n_keys (int): Utpann karne ke liye kunjiyo (keys) ki number.\n    - n_values (int): Pratyek key (key) ke liye maanon (values) ki number (1 se shuru hone vaale consecutive integers).\n\n    Returns:\n        dict: Ek Python (Python) dictionary (dictionary) jis mein key (keys) strings (strings) ke roop mein hain and maan integerson (integers) ki lists (lists) ke roop mein hain.\n\n    Requirements:\n    - collections\n    - random\n\n    Example:\n    >>> random.seed(0)\n    >>> task_func(3, 5)\n    {'g': [1, 2, 3, 4, 5], 'a': [1, 2, 3, 4, 5]}\n    >>> result = task_func(1, 5)\n    >>> list(result)[0] in LETTERS\n    True\n\n    \"\"\"\n    # Initialize the dictionary\n    dictionary = collections.defaultdict(list)\n\n    # Generate keys and values\n    for _ in range(n_keys):\n        key = random.choice(LETTERS)\n        values = list(range(1, n_values + 1))\n        random.shuffle(values)\n        dictionary[key].extend(values)\n\n    return dict(dictionary)", "raw_solution": "import collections\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef task_func(n_keys, n_values):\n    \"\"\"\n    Ek specified number mein key (keys) and maan (values) ke saath ek Python dictionary (dictionary) banaen.\n\n    Notes:\n        keyyaan (keys) letters (letters) ki poorvnirdhaarit list se randomly chuni jaati hain, and maan 1 se shuru hone vaale consecutive integers (integers) hain.\n        key selection mein randomness ke kaaran, dictionary (dictionary) mein actual kunjiyaan (keys) pratyek execution (execution) mein bhinn ho sakti hain.\n\n    Parameters:\n    - n_keys (int): Utpann karne ke liye kunjiyo (keys) ki number.\n    - n_values (int): Pratyek key (key) ke liye maanon (values) ki number (1 se shuru hone vaale consecutive integers).\n\n    Returns:\n        dict: Ek Python (Python) dictionary (dictionary) jis mein key (keys) strings (strings) ke roop mein hain and maan integerson (integers) ki lists (lists) ke roop mein hain.\n\n    Requirements:\n    - collections\n    - random\n\n    Example:\n    >>> random.seed(0)\n    >>> task_func(3, 5)\n    {'g': [1, 2, 3, 4, 5], 'a': [1, 2, 3, 4, 5]}\n    >>> result = task_func(1, 5)\n    >>> list(result)[0] in LETTERS\n    True\n\n    \"\"\"\n    # Initialize the dictionary\n    dictionary = collections.defaultdict(list)\n\n    # Generate keys and values\n    for _ in range(n_keys):\n        key = random.choice(LETTERS)\n        values = list(range(1, n_values + 1))\n        random.shuffle(values)\n        dictionary[key].extend(values)\n\n    return dict(dictionary)\n```\n"}
{"task_id": "BigCodeBench/269", "solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    # Add 'a' key with value 1\n    data_dict['a'] = 1\n\n    # Extract values for statistical analysis\n    values = np.array(list(data_dict.values()))\n    \n    # Calculate mean, median, and mode\n    mean_value = round(values.mean(), 2)\n    median_value = values.median()\n    mode_value = stats.mode(values)[0][0]\n\n    # Create a dictionary for statistics\n    stats_dict = {\n        'mean': mean_value,\n        'median': median_value,\n        'mode': mode_value\n    }\n\n    # Normalize values using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_values = scaler.fit_transform(values.reshape(-1, 1)).flatten()\n\n    # Plot histogram of normalized values\n    fig, ax = plt.subplots()\n    ax.hist(normalized_values, bins=10, color='blue', alpha=0.7)\n    ax.set_title('Histogram of Normalized Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return data_dict, stats_dict, ax", "raw_solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    # Add 'a' key with value 1\n    data_dict['a'] = 1\n\n    # Extract values for statistical analysis\n    values = np.array(list(data_dict.values()))\n    \n    # Calculate mean, median, and mode\n    mean_value = round(values.mean(), 2)\n    median_value = values.median()\n    mode_value = stats.mode(values)[0][0]\n\n    # Create a dictionary for statistics\n    stats_dict = {\n        'mean': mean_value,\n        'median': median_value,\n        'mode': mode_value\n    }\n\n    # Normalize values using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_values = scaler.fit_transform(values.reshape(-1, 1)).flatten()\n\n    # Plot histogram of normalized values\n    fig, ax = plt.subplots()\n    ax.hist(normalized_values, bins=10, color='blue', alpha=0.7)\n    ax.set_title('Histogram of Normalized Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return data_dict, stats_dict, ax\n```\n"}
{"task_id": "BigCodeBench/270", "solution": "import re\nfrom collections import Counter\ndef task_func(sentence):\n    # Use regex to find all words in the sentence\n    words = re.findall(r'\\b\\w+\\b', sentence.lower())\n    \n    # Use Counter to count the occurrences of each word\n    word_count = Counter(words)\n    \n    return dict(word_count)", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(sentence):\n    # Use regex to find all words in the sentence\n    words = re.findall(r'\\b\\w+\\b', sentence.lower())\n    \n    # Use Counter to count the occurrences of each word\n    word_count = Counter(words)\n    \n    return dict(word_count)\n\n# Example usage:\n# print(task_func(\"apple banana apple orange orange orange\"))\n# Output: {'apple': 2, 'banana': 1, 'orange': 3}\n```\n"}
{"task_id": "BigCodeBench/271", "solution": "import random\nimport string\nimport hashlib\nimport time\ndef task_func(data_dict: dict, seed=0) -> dict:\n    random.seed(seed)\n    salt = ''.join(random.choice(string.ascii_lowercase) for _ in range(5))\n    data_dict['a'] = 1\n    data_dict['timestamp'] = int(time.time())\n\n    for key, value in data_dict.items():\n        if isinstance(value, str):\n            combined = value + salt\n            hashed = hashlib.sha256(combined.encode()).hexdigest()\n            data_dict[key] = hashed\n\n    return data_dict", "raw_solution": "import random\nimport string\nimport hashlib\nimport time\n\ndef task_func(data_dict: dict, seed=0) -> dict:\n    random.seed(seed)\n    salt = ''.join(random.choice(string.ascii_lowercase) for _ in range(5))\n    data_dict['a'] = 1\n    data_dict['timestamp'] = int(time.time())\n\n    for key, value in data_dict.items():\n        if isinstance(value, str):\n            combined = value + salt\n            hashed = hashlib.sha256(combined.encode()).hexdigest()\n            data_dict[key] = hashed\n\n    return data_dict\n\n# Example usage:\n# print(task_func({'key': 'value'})[\"key\"])\n```\n"}
{"task_id": "BigCodeBench/272", "solution": "import cgi\nimport http.server\nimport json\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Content-Type header application/json nahi hai\"}).encode())\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Koi data nahi mila\"}).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Koi data nahi mila\"}).encode())\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps({\"status\": \"success\", \"message\": \"Data successpurvak prapt hua. \"}).encode())\ndef task_func():\n    return RequestHandler", "raw_solution": "import cgi\nimport http.server\nimport json\n\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Content-Type header application/json nahi hai\"}).encode())\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Koi data nahi mila\"}).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Koi data nahi mila\"}).encode())\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps({\"status\": \"success\", \"message\": \"Data successpurvak prapt hua. \"}).encode())\n\ndef task_func():\n    return RequestHandler\n```\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\ndef task_func():\n    return RequestHandler", "raw_solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\n\ndef task_func():\n    return RequestHandler\n```\nThis script defines a custom HTTP request handler that checks the content type and the presence of a 'data' key in the JSON payload. It sends appropriate responses based on the checks. The `task_func` returns the class `RequestHandler` which can be used to handle HTTP POST requests.\n"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nclass EmailHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Content-Type must be application/json')\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            email_data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n            return\n\n        required_keys = ['subject', 'message', 'to']\n        if not all(key in email_data for key in required_keys):\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Missing required keys: subject, message, to')\n            return\n\n        try:\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = self.server.smtp_username\n            msg['To'] = email_data['to']\n\n            with smtplib.SMTP(self.server.smtp_server, self.server.smtp_port) as server:\n                server.starttls()\n                server.login(self.server.smtp_username, self.server.smtp_password)\n                server.sendmail(self.server.smtp_username, email_data['to'], msg.as_string())\n        except smtplib.SMTPAuthenticationError:\n            self.send_response(535)\n            self.end_headers()\n            self.wfile.write(b'Authentication Failed')\n            return\n\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(b'Email sent successfully')\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class RequestHandler(EmailHandler):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.server = self\n            self.server.smtp_server = smtp_server\n            self.server.smtp_port = smtp_port\n            self.server.smtp_username = smtp_username\n            self.server.smtp_password = smtp_password\n\n    return RequestHandler", "raw_solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\nclass EmailHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Content-Type must be application/json')\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            email_data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n            return\n\n        required_keys = ['subject', 'message', 'to']\n        if not all(key in email_data for key in required_keys):\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Missing required keys: subject, message, to')\n            return\n\n        try:\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = self.server.smtp_username\n            msg['To'] = email_data['to']\n\n            with smtplib.SMTP(self.server.smtp_server, self.server.smtp_port) as server:\n                server.starttls()\n                server.login(self.server.smtp_username, self.server.smtp_password)\n                server.sendmail(self.server.smtp_username, email_data['to'], msg.as_string())\n        except smtplib.SMTPAuthenticationError:\n            self.send_response(535)\n            self.end_headers()\n            self.wfile.write(b'Authentication Failed')\n            return\n\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(b'Email sent successfully')\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class RequestHandler(EmailHandler):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.server = self\n            self.server.smtp_server = smtp_server\n            self.server.smtp_port = smtp_port\n            self.server.smtp_username = smtp_username\n            self.server.smtp_password = smtp_password\n\n    return RequestHandler\n```\n"}
{"task_id": "BigCodeBench/275", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(n):\n    if n <= 1:\n        raise ValueError(\"Input n must be greater than 1.\")\n    \n    # Generate all possible pairs of integers from 1 to n\n    pairs = list(combinations(range(1, n+1), 2))\n    \n    return pairs", "raw_solution": "import numpy as np\nfrom itertools import combinations\n\ndef task_func(n):\n    if n <= 1:\n        raise ValueError(\"Input n must be greater than 1.\")\n    \n    # Generate all possible pairs of integers from 1 to n\n    pairs = list(combinations(range(1, n+1), 2))\n    \n    return pairs\n\n# Test cases"}
{"task_id": "BigCodeBench/276", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(matrix):\n    # Convert the matrix to a numpy array\n    matrix = np.array(matrix)\n    \n    # Calculate the maximum values of each row\n    max_values = np.max(matrix, axis=1)\n    \n    # Calculate skewness and kurtosis\n    skewness = stats.skew(max_values)\n    kurt = stats.kurtosis(max_values)\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(max_values, bins='auto', alpha=0.7, color='blue')\n    ax.set_title('Histogram of Maximum Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return skewness, kurt, ax", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    # Convert the matrix to a numpy array\n    matrix = np.array(matrix)\n    \n    # Calculate the maximum values of each row\n    max_values = np.max(matrix, axis=1)\n    \n    # Calculate skewness and kurtosis\n    skewness = stats.skew(max_values)\n    kurt = stats.kurtosis(max_values)\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(max_values, bins='auto', alpha=0.7, color='blue')\n    ax.set_title('Histogram of Maximum Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return skewness, kurt, ax\n\n# Example usage:\n# skew, kurt, ax = task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# print(f\"Skewness: {skew}, Kurtosis: {kurt}\")\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/277", "solution": "import random\nfrom itertools import combinations\nimport math\ndef task_func(n):\n    if n < 2:\n        return None\n\n    # Generate n random points within the unit square\n    points = [(random.random(), random.random()) for _ in range(n)]\n\n    # Calculate all combinations of two points\n    comb = combinations(points, 2)\n\n    # Initialize minimum distance to a large value\n    min_distance = float('inf')\n    closest_pair = None\n\n    # Iterate over all combinations to find the closest pair\n    for (x1, y1), (x2, y2) in comb:\n        distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n        if distance < min_distance:\n            min_distance = distance\n            closest_pair = ((x1, y1), (x2, y2))\n\n    return closest_pair", "raw_solution": "import random\nfrom itertools import combinations\nimport math\n\ndef task_func(n):\n    if n < 2:\n        return None\n\n    # Generate n random points within the unit square\n    points = [(random.random(), random.random()) for _ in range(n)]\n\n    # Calculate all combinations of two points\n    comb = combinations(points, 2)\n\n    # Initialize minimum distance to a large value\n    min_distance = float('inf')\n    closest_pair = None\n\n    # Iterate over all combinations to find the closest pair\n    for (x1, y1), (x2, y2) in comb:\n        distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n        if distance < min_distance:\n            min_distance = distance\n            closest_pair = ((x1, y1), (x2, y2))\n\n    return closest_pair\n\n# Example usage:\nrandom.seed(0)"}
{"task_id": "BigCodeBench/278", "solution": "import numpy as np\nfrom sympy import symbols, solve\ndef task_func(precision=2, seed=0):\n    np.random.seed(seed)\n    a, b, c = np.random.uniform(-10, 10, 3)\n    x = symbols('x')\n    equation = a*x**2 + b*x + c\n    solutions = solve(equation, x)\n    \n    rounded_solutions = tuple(np.round(sol, precision) for sol in solutions)\n    \n    return rounded_solutions", "raw_solution": "import numpy as np\nfrom sympy import symbols, solve\n\ndef task_func(precision=2, seed=0):\n    np.random.seed(seed)\n    a, b, c = np.random.uniform(-10, 10, 3)\n    x = symbols('x')\n    equation = a*x**2 + b*x + c\n    solutions = solve(equation, x)\n    \n    rounded_solutions = tuple(np.round(sol, precision) for sol in solutions)\n    \n    return rounded_solutions\n```\n"}
{"task_id": "BigCodeBench/279", "solution": "import random\nfrom collections import Counter\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\ndef task_func(x=1):\n    hands = []\n    for _ in range(x):\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n    \n    # Flatten the list of lists into a single list\n    all_cards = [card for hand in hands for card in hand]\n    card_counter = Counter(all_cards)\n\n    return hands, card_counter", "raw_solution": "import random\nfrom collections import Counter\n\n# Constants\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n\ndef task_func(x=1):\n    hands = []\n    for _ in range(x):\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n    \n    # Flatten the list of lists into a single list\n    all_cards = [card for hand in hands for card in hand]\n    card_counter = Counter(all_cards)\n\n    return hands, card_counter\n\n# Example usage:\nrandom.seed(0)\nresult = task_func(1)"}
{"task_id": "BigCodeBench/280", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(signal, precision=2, seed=777):\n    np.random.seed(seed)\n    original_signal = np.array(signal, dtype=np.complex128)\n    \n    # Perform FFT\n    transformed_signal = fft(original_signal)\n    \n    # Round the transformed signal to the specified precision\n    rounded_transformed_signal = np.round(transformed_signal, precision)\n    \n    # Plot the original and transformed signals\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n    ax1.plot(original_signal)\n    ax1.set_title('Original Signal')\n    ax2.plot(rounded_transformed_signal)\n    ax2.set_title('Transformed Signal')\n    \n    return rounded_transformed_signal, (ax1, ax2)", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(signal, precision=2, seed=777):\n    np.random.seed(seed)\n    original_signal = np.array(signal, dtype=np.complex128)\n    \n    # Perform FFT\n    transformed_signal = fft(original_signal)\n    \n    # Round the transformed signal to the specified precision\n    rounded_transformed_signal = np.round(transformed_signal, precision)\n    \n    # Plot the original and transformed signals\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n    ax1.plot(original_signal)\n    ax1.set_title('Original Signal')\n    ax2.plot(rounded_transformed_signal)\n    ax2.set_title('Transformed Signal')\n    \n    return rounded_transformed_signal, (ax1, ax2)\n```\n"}
{"task_id": "BigCodeBench/281", "solution": "import re\nimport os\nfrom collections import Counter\ndef task_func(folder_path: str) -> dict:\n    ip_pattern = re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b')\n    ip_counter = Counter()\n\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            if file.endswith('.log'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    content = f.read()\n                    ip_addresses = ip_pattern.findall(content)\n                    ip_counter.update(ip_addresses)\n\n    return dict(ip_counter)", "raw_solution": "import re\nimport os\nfrom collections import Counter\n\ndef task_func(folder_path: str) -> dict:\n    ip_pattern = re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b')\n    ip_counter = Counter()\n\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            if file.endswith('.log'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    content = f.read()\n                    ip_addresses = ip_pattern.findall(content)\n                    ip_counter.update(ip_addresses)\n\n    return dict(ip_counter)\n```\n"}
{"task_id": "BigCodeBench/282", "solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\ndef task_func(file_path, onpick):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Load the image\n    image = cv2.imread(file_path)\n    if image is None:\n        raise ValueError(f\"The file {file_path} is not a valid image.\")\n\n    # Convert the image to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Calculate the histogram\n    hist_values = cv2.calcHist([image_rgb], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n\n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_title('3D Histogram')\n    ax.set_xlabel('Red')\n    ax.set_ylabel('Green')\n    ax.set_zlabel('Blue')\n\n    # Plot the histogram\n    x = np.arange(0, 8)\n    y = np.arange(0, 8)\n    x, y = np.meshgrid(x, y)\n    z = hist_values\n    ax.scatter(x, y, z, picker=True)\n\n    # Connect the onpick function\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    return ax\ndef onpick(event):\n    ind = event.ind\n    print(f'You picked data point(s) {ind}')", "raw_solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\n\ndef task_func(file_path, onpick):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Load the image\n    image = cv2.imread(file_path)\n    if image is None:\n        raise ValueError(f\"The file {file_path} is not a valid image.\")\n\n    # Convert the image to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Calculate the histogram\n    hist_values = cv2.calcHist([image_rgb], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n\n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_title('3D Histogram')\n    ax.set_xlabel('Red')\n    ax.set_ylabel('Green')\n    ax.set_zlabel('Blue')\n\n    # Plot the histogram\n    x = np.arange(0, 8)\n    y = np.arange(0, 8)\n    x, y = np.meshgrid(x, y)\n    z = hist_values\n    ax.scatter(x, y, z, picker=True)\n\n    # Connect the onpick function\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    return ax\n\n# Example usage\ndef onpick(event):\n    ind = event.ind\n    print(f'You picked data point(s) {ind}')\n\n# Create a dummy image\nnp.random.seed(42)\ndummy_img_path = 'image.jpg'\ndummy_img = np.random.randint(0, 255, (20, 20, 3), dtype=np.uint8)\ncv2.imwrite(dummy_img_path, dummy_img)\n\n# Run the function\nax = task_func('image.jpg', onpick)\n\n# Clean up\nos.remove(dummy_img_path)\n```\n"}
{"task_id": "BigCodeBench/283", "solution": "import os\nimport json\nfrom collections import Counter\ndef task_func(json_files_path='./json_files/', key='name'):\n    counter = Counter()\n    \n    for filename in os.listdir(json_files_path):\n        if filename.endswith('.json'):\n            with open(os.path.join(json_files_path, filename), 'r') as file:\n                data = json.load(file)\n                if key in data:\n                    counter[data[key]] += 1\n    \n    return dict(counter)", "raw_solution": "import os\nimport json\nfrom collections import Counter\n\ndef task_func(json_files_path='./json_files/', key='name'):\n    counter = Counter()\n    \n    for filename in os.listdir(json_files_path):\n        if filename.endswith('.json'):\n            with open(os.path.join(json_files_path, filename), 'r') as file:\n                data = json.load(file)\n                if key in data:\n                    counter[data[key]] += 1\n    \n    return dict(counter)\n```\n"}
{"task_id": "BigCodeBench/284", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\ndef task_func(url):\n    # Create a browser object\n    br = mechanize.Browser()\n    br.set_handle_robots(False)  # Ignore robots.txt\n    br.addheaders = [('User-agent', 'Mozilla/5.0')]\n\n    # Open the URL\n    br.open(url)\n\n    # Read the page content\n    html = br.response().read()\n\n    # Parse the HTML with BeautifulSoup\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Extract all hyperlinks\n    links = []\n    for link in soup.find_all('a', href=True):\n        href = link['href']\n        absolute_url = urljoin(url, href)\n        links.append(absolute_url)\n\n    return links", "raw_solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef task_func(url):\n    # Create a browser object\n    br = mechanize.Browser()\n    br.set_handle_robots(False)  # Ignore robots.txt\n    br.addheaders = [('User-agent', 'Mozilla/5.0')]\n\n    # Open the URL\n    br.open(url)\n\n    # Read the page content\n    html = br.response().read()\n\n    # Parse the HTML with BeautifulSoup\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Extract all hyperlinks\n    links = []\n    for link in soup.find_all('a', href=True):\n        href = link['href']\n        absolute_url = urljoin(url, href)\n        links.append(absolute_url)\n\n    return links\n\n# Example usage:\n# print(task_func('https://www.example.com'))\n```\n"}
{"task_id": "BigCodeBench/285", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\ndef task_func(url, form_id, data):\n    # Create a browser object\n    br = mechanize.Browser()\n    \n    # Open the URL\n    br.open(url)\n    \n    # Select the form by its id\n    br.select_form(nr=form_id)\n    \n    # Fill the form with the provided data\n    for key, value in data.items():\n        br.form[key] = value\n    \n    # Submit the form\n    response = br.submit()\n    \n    # Parse the response with BeautifulSoup\n    soup = BeautifulSoup(response.read(), 'html.parser')\n    \n    # Extract the title\n    title = soup.title.string if soup.title else 'No Title'\n    \n    return title\ndata = {'username': 'admin', 'password': 'password'}", "raw_solution": "import mechanize\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, form_id, data):\n    # Create a browser object\n    br = mechanize.Browser()\n    \n    # Open the URL\n    br.open(url)\n    \n    # Select the form by its id\n    br.select_form(nr=form_id)\n    \n    # Fill the form with the provided data\n    for key, value in data.items():\n        br.form[key] = value\n    \n    # Submit the form\n    response = br.submit()\n    \n    # Parse the response with BeautifulSoup\n    soup = BeautifulSoup(response.read(), 'html.parser')\n    \n    # Extract the title\n    title = soup.title.string if soup.title else 'No Title'\n    \n    return title\n\n# Example usage:\ndata = {'username': 'admin', 'password': 'password'}\ntitle = task_func('https://www.example.com/login', 0, data)"}
{"task_id": "BigCodeBench/286", "solution": "from collections import Counter\nimport os\nimport csv\ndef task_func(output_file, test_directory):\n    total_words = 0\n    word_counter = Counter()\n\n    try:\n        for filename in os.listdir(test_directory):\n            if filename.endswith('.txt'):\n                with open(os.path.join(test_directory, filename), 'r') as file:\n                    words = file.read().split()\n                    word_counter.update(words)\n                    total_words += len(words)\n\n        with open(output_file, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['Word', 'Count'])\n            for word, count in word_counter.items():\n                writer.writerow([word, count])\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0\n\n    return total_words", "raw_solution": "from collections import Counter\nimport os\nimport csv\n\n# Constants\nFILE_DIR = './yourdictfiles/'\n\ndef task_func(output_file, test_directory):\n    total_words = 0\n    word_counter = Counter()\n\n    try:\n        for filename in os.listdir(test_directory):\n            if filename.endswith('.txt'):\n                with open(os.path.join(test_directory, filename), 'r') as file:\n                    words = file.read().split()\n                    word_counter.update(words)\n                    total_words += len(words)\n\n        with open(output_file, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['Word', 'Count'])\n            for word, count in word_counter.items():\n                writer.writerow([word, count])\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0\n\n    return total_words\n\n# Example usage:\n# print(task_func('word_counts.csv', './yourdictfiles/'))\n```\nThis script reads all `.txt` files in the specified directory, counts the words using `collections.Counter`, and writes the counts to a CSV file. It also returns the total number of words found. If any error occurs, it prints the error message and returns 0.\n"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    word_counter = Counter()\n    total_words = 0\n\n    for file in os.listdir(directory):\n        if file.endswith('.txt'):\n            with open(os.path.join(directory, file), 'r') as f:\n                words = f.read().split()\n                word_counter.update(words)\n                total_words += len(words)\n\n    with open(filename, 'w') as f:\n        json.dump(word_counter, f)\n\n    return total_words", "raw_solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    word_counter = Counter()\n    total_words = 0\n\n    for file in os.listdir(directory):\n        if file.endswith('.txt'):\n            with open(os.path.join(directory, file), 'r') as f:\n                words = f.read().split()\n                word_counter.update(words)\n                total_words += len(words)\n\n    with open(filename, 'w') as f:\n        json.dump(word_counter, f)\n\n    return total_words\n\n# Example usage:\n# Assuming the directory './testdir/' contains a file named 'single_file.txt' with the content 'hello world hello'\n# with open(\"./testdir/single_file.txt\",\"r\") as f: print(f.read())\n# count = task_func('single_file.txt', './testdir/')\n# print(count)\n```\n"}
{"task_id": "BigCodeBench/288", "solution": "import collections\nimport json\nimport os\ndef task_func(directory_path: str) -> dict:\n    key_counter = collections.Counter()\n\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory_path, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                key_counter.update(data.keys())\n\n    return dict(key_counter)", "raw_solution": "import collections\nimport json\nimport os\n\ndef task_func(directory_path: str) -> dict:\n    key_counter = collections.Counter()\n\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory_path, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                key_counter.update(data.keys())\n\n    return dict(key_counter)\n```\n"}
{"task_id": "BigCodeBench/289", "solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(X, y, n_splits, batch_size, epochs):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Initialize KFold\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    histories = []\n\n    for train_index, val_index in kfold.split(X_scaled):\n        # Split data into training and validation sets\n        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n\n        # Build a simple neural network\n        model = tf.keras.Sequential([\n            tf.keras.layers.Dense(20, activation='relu', input_shape=(X_train.shape[1],)),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n\n        # Compile the model\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n        # Train the model\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n\n        # Store the history\n        histories.append(history)\n\n    return histories", "raw_solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(X, y, n_splits, batch_size, epochs):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Initialize KFold\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    histories = []\n\n    for train_index, val_index in kfold.split(X_scaled):\n        # Split data into training and validation sets\n        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n\n        # Build a simple neural network\n        model = tf.keras.Sequential([\n            tf.keras.layers.Dense(20, activation='relu', input_shape=(X_train.shape[1],)),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n\n        # Compile the model\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n        # Train the model\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n\n        # Store the history\n        histories.append(history)\n\n    return histories\n```\n"}
{"task_id": "BigCodeBench/290", "solution": "import nltk\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(directory_path):\n    \"\"\"\n    Ek specified directory mein sabhi '.txt' files mein unique non-stop words ki number ginen.\n\n    Parameters:\n    - directory_path (str): Us directory ka path jismein '.txt' files hain.\n\n    Returns:\n        int: Sabhi files mein unique non-stop words ki total count.\n\n    Requirements:\n    - collections.Counter\n    - os\n    - nltk.corpus.stopwords\n\n    Example:\n    >>> task_func('./yourdictfiles/')\n    1500\n\n    \"\"\"\n    word_counter = Counter()\n    \n    for filename in os.listdir(directory_path):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                for line in file:\n                    words = line.split()\n                    non_stopwords = [word for word in words if word.lower() not in STOPWORDS]\n                    word_counter.update(non_stopwords)\n    \n    return len(word_counter)", "raw_solution": "import nltk\nnltk.download('stopwords')\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(directory_path):\n    \"\"\"\n    Ek specified directory mein sabhi '.txt' files mein unique non-stop words ki number ginen.\n\n    Parameters:\n    - directory_path (str): Us directory ka path jismein '.txt' files hain.\n\n    Returns:\n        int: Sabhi files mein unique non-stop words ki total count.\n\n    Requirements:\n    - collections.Counter\n    - os\n    - nltk.corpus.stopwords\n\n    Example:\n    >>> task_func('./yourdictfiles/')\n    1500\n\n    \"\"\"\n    word_counter = Counter()\n    \n    for filename in os.listdir(directory_path):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                for line in file:\n                    words = line.split()\n                    non_stopwords = [word for word in words if word.lower() not in STOPWORDS]\n                    word_counter.update(non_stopwords)\n    \n    return len(word_counter)\n```\n"}
{"task_id": "BigCodeBench/291", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\ndef task_func(mu, sigma, seed=0):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, 1000)\n    \n    sns.set(style=\"whitegrid\")\n    ax = sns.histplot(data, bins=30, kde=True)\n    ax.set(title='Normal Distribution', xlabel='Value', ylabel='Frequency')\n    \n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ndef task_func(mu, sigma, seed=0):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, 1000)\n    \n    sns.set(style=\"whitegrid\")\n    ax = sns.histplot(data, bins=30, kde=True)\n    ax.set(title='Normal Distribution', xlabel='Value', ylabel='Frequency')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/292", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(df):\n    # Check if the dataframe contains the required columns\n    if not {'id', 'age', 'income'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n\n    # Group by 'id' and apply MinMaxScaler to 'age' and 'income'\n    scaler = MinMaxScaler()\n    df_scaled = df.groupby('id').apply(lambda x: pd.DataFrame(scaler.fit_transform(x[['age', 'income']]), columns=['age', 'income'])).reset_index(drop=True)\n\n    # Generate histogram data for 'income' column\n    income_hist = df_scaled['income'].value_counts().sort_index()\n\n    return df_scaled, income_hist", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(df):\n    # Check if the dataframe contains the required columns\n    if not {'id', 'age', 'income'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n\n    # Group by 'id' and apply MinMaxScaler to 'age' and 'income'\n    scaler = MinMaxScaler()\n    df_scaled = df.groupby('id').apply(lambda x: pd.DataFrame(scaler.fit_transform(x[['age', 'income']]), columns=['age', 'income'])).reset_index(drop=True)\n\n    # Generate histogram data for 'income' column\n    income_hist = df_scaled['income'].value_counts().sort_index()\n\n    return df_scaled, income_hist\n\n# Example usage:"}
{"task_id": "BigCodeBench/293", "solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(elements, subset_size):\n    # Generate all possible combinations of the given subset size\n    combinations = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sum of each combination\n    sums = [np.sum(comb) for comb in combinations]\n    \n    # Plot the histogram of the sums\n    ax = plt.hist(sums, bins='auto', color='skyblue', alpha=0.7)\n    plt.title('Histogram of Subset Sums')\n    plt.xlabel('Sum')\n    plt.ylabel('Frequency')\n    \n    return ax, combinations, sums", "raw_solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(elements, subset_size):\n    # Generate all possible combinations of the given subset size\n    combinations = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sum of each combination\n    sums = [np.sum(comb) for comb in combinations]\n    \n    # Plot the histogram of the sums\n    ax = plt.hist(sums, bins='auto', color='skyblue', alpha=0.7)\n    plt.title('Histogram of Subset Sums')\n    plt.xlabel('Sum')\n    plt.ylabel('Frequency')\n    \n    return ax, combinations, sums\n\n# Example usage:\n# ax, combs, sums = task_func((1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 2)\n# print(type(ax))\n# print(len(combs))\n# print(len(sums))\n```\n"}
{"task_id": "BigCodeBench/294", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    if not {'id', 'age', 'income'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n    \n    scaler = StandardScaler()\n    \n    for id_value in df['id'].unique():\n        group = df[df['id'] == id_value]\n        group[['age', 'income']] = scaler.fit_transform(group[['age', 'income']])\n        df.loc[df['id'] == id_value, ['age', 'income']] = group[['age', 'income']]\n    \n    return df\ndf = pd.DataFrame({\n    'id': [1, 1, 2, 2, 3, 3],\n    'age': [25, 26, 35, 36, 28, 29],\n    'income': [50000, 60000, 70000, 80000, 90000, 100000]\n})", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    if not {'id', 'age', 'income'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n    \n    scaler = StandardScaler()\n    \n    for id_value in df['id'].unique():\n        group = df[df['id'] == id_value]\n        group[['age', 'income']] = scaler.fit_transform(group[['age', 'income']])\n        df.loc[df['id'] == id_value, ['age', 'income']] = group[['age', 'income']]\n    \n    return df\n\n# Example usage:\ndf = pd.DataFrame({\n    'id': [1, 1, 2, 2, 3, 3],\n    'age': [25, 26, 35, 36, 28, 29],\n    'income': [50000, 60000, 70000, 80000, 90000, 100000]\n})\n\ndf_standardized = task_func(df)"}
{"task_id": "BigCodeBench/295", "solution": "import itertools\nimport statistics\ndef task_func(elements, subset_size):\n    # Generate all possible subsets of the given size\n    subsets = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sum of each subset\n    subset_sums = [sum(subset) for subset in subsets]\n    \n    # Calculate statistics for the sums\n    mean_sum = statistics.mean(subset_sums)\n    median_sum = statistics.median(subset_sums)\n    mode_sum = statistics.mode(subset_sums)\n    \n    # Return the statistics in a dictionary\n    return {\n        'mean': mean_sum,\n        'median': median_sum,\n        'mode': mode_sum\n    }", "raw_solution": "import itertools\nimport statistics\n\ndef task_func(elements, subset_size):\n    # Generate all possible subsets of the given size\n    subsets = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sum of each subset\n    subset_sums = [sum(subset) for subset in subsets]\n    \n    # Calculate statistics for the sums\n    mean_sum = statistics.mean(subset_sums)\n    median_sum = statistics.median(subset_sums)\n    mode_sum = statistics.mode(subset_sums)\n    \n    # Return the statistics in a dictionary\n    return {\n        'mean': mean_sum,\n        'median': median_sum,\n        'mode': mode_sum\n    }\n\n# Example usage:\n# print(task_func((1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 2))\n```\n"}
{"task_id": "BigCodeBench/296", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'value' column.\")\n    \n    value_counts = df['value'].value_counts()\n    \n    if value_counts.empty:\n        fig, ax = plt.subplots()\n        ax.set_title(\"Value Distribution\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Count\")\n        return ax\n    \n    fig, ax = plt.subplots()\n    value_counts.plot(kind='bar', ax=ax)\n    ax.set_title(\"Value Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    \n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'value' column.\")\n    \n    value_counts = df['value'].value_counts()\n    \n    if value_counts.empty:\n        fig, ax = plt.subplots()\n        ax.set_title(\"Value Distribution\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Count\")\n        return ax\n    \n    fig, ax = plt.subplots()\n    value_counts.plot(kind='bar', ax=ax)\n    ax.set_title(\"Value Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    \n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'id': [1, 1, 2, 2, 3, 3],'value': ['A', 'B', 'A', 'B', 'A', 'B']})\n# ax = task_func(df)\n# print(len(ax.patches))  # Should print the number of bars in the plot\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/297", "solution": "import itertools\nimport collections\ndef task_func(elements, subset_size):\n    \"\"\"\n    Ek tuple ke sabhi 2-element subsets utpann karein aur subsets mein pratyek sum ki occurrences ki ganana karein.\n\n    Returns:\n        dict: sums aur unki counts ke saath ek dictionary.\n\n    Requirements:\n    - itertools\n    - random\n    - collections\n\n    Example:\n    >>> dict(task_func((1, 2, 3, 4, 5), 2))\n    {3: 1, 4: 1, 5: 2, 6: 2, 7: 2, 8: 1, 9: 1}\n\n    \"\"\"\n    # Generate all possible subsets of the given size\n    subsets = itertools.combinations(elements, subset_size)\n    \n    # Calculate the sum of each subset\n    subset_sums = [sum(subset) for subset in subsets]\n    \n    # Count the occurrences of each sum\n    sum_counts = collections.Counter(subset_sums)\n    \n    return dict(sum_counts)", "raw_solution": "import itertools\nimport collections\n\ndef task_func(elements, subset_size):\n    \"\"\"\n    Ek tuple ke sabhi 2-element subsets utpann karein aur subsets mein pratyek sum ki occurrences ki ganana karein.\n\n    Returns:\n        dict: sums aur unki counts ke saath ek dictionary.\n\n    Requirements:\n    - itertools\n    - random\n    - collections\n\n    Example:\n    >>> dict(task_func((1, 2, 3, 4, 5), 2))\n    {3: 1, 4: 1, 5: 2, 6: 2, 7: 2, 8: 1, 9: 1}\n\n    \"\"\"\n    # Generate all possible subsets of the given size\n    subsets = itertools.combinations(elements, subset_size)\n    \n    # Calculate the sum of each subset\n    subset_sums = [sum(subset) for subset in subsets]\n    \n    # Count the occurrences of each sum\n    sum_counts = collections.Counter(subset_sums)\n    \n    return dict(sum_counts)\n```\n"}
{"task_id": "BigCodeBench/298", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(\"DataFrame must contain 'Date' and 'Value' columns\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    # Convert the 'Value' column from string to list of floats\n    df['Value'] = df['Value'].apply(eval)\n\n    # Create a DataFrame with one column for each element in the 'Value' list\n    value_df = pd.DataFrame(df['Value'].to_list(), index=df.index)\n\n    # Scale the DataFrame\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(value_df)\n    scaled_df = pd.DataFrame(scaled_values, index=df.index, columns=[f'Scaled_{i}' for i in range(scaled_values.shape[1])])\n\n    if plot:\n        ax = scaled_df.plot(kind='bar', figsize=(10, 6))\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Scaled Value')\n        ax.set_title('Scaled Values Over Time')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return scaled_df, ax\n\n    return scaled_df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(\"DataFrame must contain 'Date' and 'Value' columns\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    # Convert the 'Value' column from string to list of floats\n    df['Value'] = df['Value'].apply(eval)\n\n    # Create a DataFrame with one column for each element in the 'Value' list\n    value_df = pd.DataFrame(df['Value'].to_list(), index=df.index)\n\n    # Scale the DataFrame\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(value_df)\n    scaled_df = pd.DataFrame(scaled_values, index=df.index, columns=[f'Scaled_{i}' for i in range(scaled_values.shape[1])])\n\n    if plot:\n        ax = scaled_df.plot(kind='bar', figsize=(10, 6))\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Scaled Value')\n        ax.set_title('Scaled Values Over Time')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return scaled_df, ax\n\n    return scaled_df\n```\n"}
{"task_id": "BigCodeBench/299", "solution": "import itertools\nimport math\nfrom pandas import Series\ndef task_func(elements, subset_size, top_n=2):\n    # Generate all possible subsets of the given size\n    subsets = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the product of sums of all subsets\n    subset_sums = [sum(subset) for subset in subsets]\n    product_of_sums = math.prod(subset_sums)\n    \n    # Create a Series of the top_n sums\n    top_sums = Series(sorted(subset_sums, reverse=True)[:top_n])\n    \n    # Adjust the product based on subset size and elements\n    if subset_size > len(elements):\n        product_of_sums *= 1\n    elif subset_size == 0:\n        product_of_sums *= 1\n    \n    return product_of_sums, top_sums", "raw_solution": "import itertools\nimport math\nfrom pandas import Series\n\ndef task_func(elements, subset_size, top_n=2):\n    # Generate all possible subsets of the given size\n    subsets = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the product of sums of all subsets\n    subset_sums = [sum(subset) for subset in subsets]\n    product_of_sums = math.prod(subset_sums)\n    \n    # Create a Series of the top_n sums\n    top_sums = Series(sorted(subset_sums, reverse=True)[:top_n])\n    \n    # Adjust the product based on subset size and elements\n    if subset_size > len(elements):\n        product_of_sums *= 1\n    elif subset_size == 0:\n        product_of_sums *= 1\n    \n    return product_of_sums, top_sums\n\n# Example usage:\n# prod, sums = task_func((1, 2, 3), 2)\n# print(prod)  # Output: 60\n# print(list(sums))  # Output: [5, 4]\n```\n"}
{"task_id": "BigCodeBench/300", "solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if 'date' not in df.columns or 'value' not in df.columns:\n        raise KeyError(\"DataFrame must contain 'date' and 'value' columns.\")\n\n    # Convert 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Convert 'value' lists to separate columns\n    df = pd.concat([df.drop('value', axis=1), df['value'].apply(pd.Series)], axis=1)\n\n    # Calculate z-scores\n    df_zscore = df.copy()\n    df_zscore[df.columns[1:]] = df[df.columns[1:]].apply(zscore)\n\n    # Create a box plot of z-scores over time\n    fig, ax = plt.subplots()\n    df_zscore.boxplot(column=df.columns[1:], by='date', ax=ax)\n    ax.set_title('z-Scores Over Time')\n    ax.set_xlabel('date')\n    ax.set_ylabel('z-Score')\n\n    return df_zscore, fig", "raw_solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if 'date' not in df.columns or 'value' not in df.columns:\n        raise KeyError(\"DataFrame must contain 'date' and 'value' columns.\")\n\n    # Convert 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Convert 'value' lists to separate columns\n    df = pd.concat([df.drop('value', axis=1), df['value'].apply(pd.Series)], axis=1)\n\n    # Calculate z-scores\n    df_zscore = df.copy()\n    df_zscore[df.columns[1:]] = df[df.columns[1:]].apply(zscore)\n\n    # Create a box plot of z-scores over time\n    fig, ax = plt.subplots()\n    df_zscore.boxplot(column=df.columns[1:], by='date', ax=ax)\n    ax.set_title('z-Scores Over Time')\n    ax.set_xlabel('date')\n    ax.set_ylabel('z-Score')\n\n    return df_zscore, fig\n```\n"}
{"task_id": "BigCodeBench/301", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the date string into a datetime object\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    naive_dt = parse(date_str)\n    from_dt = from_tz.localize(naive_dt)\n    to_dt = from_dt.astimezone(to_tz)\n\n    # Calculate the difference in years from the closest solar cycle year\n    current_year = to_dt.year\n    closest_year = np.min(np.abs(SOLAR_CYCLE_YEARS - current_year))\n    years_difference = (current_year - closest_year) / 11\n\n    # Calculate the solar activity using a cosine function\n    solar_activity = math.cos(math.pi * years_difference) / 2 + 0.5\n\n    return solar_activity", "raw_solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\n\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the date string into a datetime object\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    naive_dt = parse(date_str)\n    from_dt = from_tz.localize(naive_dt)\n    to_dt = from_dt.astimezone(to_tz)\n\n    # Calculate the difference in years from the closest solar cycle year\n    current_year = to_dt.year\n    closest_year = np.min(np.abs(SOLAR_CYCLE_YEARS - current_year))\n    years_difference = (current_year - closest_year) / 11\n\n    # Calculate the solar activity using a cosine function\n    solar_activity = math.cos(math.pi * years_difference) / 2 + 0.5\n\n    return solar_activity\n\n# Test cases"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    if df.empty or not all(col in df.columns for col in COLUMNS):\n        raise ValueError(\"DataFrame must be non-empty and contain 'Date' and 'Value' columns.\")\n\n    # Convert 'Value' column to list of lists\n    df['Value'] = df['Value'].apply(lambda x: [float(i) for i in x])\n\n    # Calculate correlation matrix\n    corr_matrix = df['Value'].apply(pd.Series).corr()\n\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})\n        plt.title('Correlation Heatmap')\n        plt.show()\n        return corr_matrix, ax\n\n    return corr_matrix", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    if df.empty or not all(col in df.columns for col in COLUMNS):\n        raise ValueError(\"DataFrame must be non-empty and contain 'Date' and 'Value' columns.\")\n\n    # Convert 'Value' column to list of lists\n    df['Value'] = df['Value'].apply(lambda x: [float(i) for i in x])\n\n    # Calculate correlation matrix\n    corr_matrix = df['Value'].apply(pd.Series).corr()\n\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})\n        plt.title('Correlation Heatmap')\n        plt.show()\n        return corr_matrix, ax\n\n    return corr_matrix\n\n# Example usage:\n# df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]], columns=['Date', 'Value'])\n# corr_df = task_func(df)\n# print(corr_df[0][0])\n```\n"}
{"task_id": "BigCodeBench/303", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    date aur time ke aadhaar par moon ke phase ki ganana karen, jismein 7 years ke lunar phase cycle ko account mein rakha jae. function moon phases ke lie sandarbh years ko nirdhaarit karane ke lie ek constant array `MOON_PHASES_YEARS` ka upayog karta hai.\n\n    Parameters:\n    - date_str (str): \"yyyy-mm-dd hh:mm:ss\" format mein date string.\n    - from_tz (str): di gai date string ka timezone.\n    - to_tz (str): timezone jismein di gai date aur time ko parivartit kiya jaana chahiye.\n\n    Returns:\n        float: 0 aur 1 ke beech moon ka phase. 0 ka maan new moon aur 1 ka maan full ko darshaata hai.\n\n    Requirements:\n    - pytz\n    - numpy\n    - dateutil.parser\n    - math\n\n    Example:\n    >>> task_func('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    0.9749279121818237\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    from_tz_obj = pytz.timezone(from_tz)\n    to_tz_obj = pytz.timezone(to_tz)\n    naive_dt = parse(date_str)\n    from_dt = from_tz_obj.localize(naive_dt)\n    to_dt = from_dt.astimezone(to_tz_obj)\n\n    # Calculate the moon phase\n    year = to_dt.year\n    if year < 1900 or year > 2100:\n        raise ValueError(\"Year must be between 1900 and 2100\")\n\n    # Find the closest moon phase year\n    closest_year = np.searchsorted(MOON_PHASES_YEARS, year)\n    if closest_year == 0:\n        closest_year = 1  # Avoid division by zero\n\n    # Calculate the phase based on the year\n    phase = (year - MOON_PHASES_YEARS[closest_year - 1]) / (MOON_PHASES_YEARS[closest_year] - MOON_PHASES_YEARS[closest_year - 1])\n    phase = math.fmod(phase, 1)  # Normalize phase to be between 0 and 1\n\n    return phase", "raw_solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    date aur time ke aadhaar par moon ke phase ki ganana karen, jismein 7 years ke lunar phase cycle ko account mein rakha jae. function moon phases ke lie sandarbh years ko nirdhaarit karane ke lie ek constant array `MOON_PHASES_YEARS` ka upayog karta hai.\n\n    Parameters:\n    - date_str (str): \"yyyy-mm-dd hh:mm:ss\" format mein date string.\n    - from_tz (str): di gai date string ka timezone.\n    - to_tz (str): timezone jismein di gai date aur time ko parivartit kiya jaana chahiye.\n\n    Returns:\n        float: 0 aur 1 ke beech moon ka phase. 0 ka maan new moon aur 1 ka maan full ko darshaata hai.\n\n    Requirements:\n    - pytz\n    - numpy\n    - dateutil.parser\n    - math\n\n    Example:\n    >>> task_func('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    0.9749279121818237\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    from_tz_obj = pytz.timezone(from_tz)\n    to_tz_obj = pytz.timezone(to_tz)\n    naive_dt = parse(date_str)\n    from_dt = from_tz_obj.localize(naive_dt)\n    to_dt = from_dt.astimezone(to_tz_obj)\n\n    # Calculate the moon phase\n    year = to_dt.year\n    if year < 1900 or year > 2100:\n        raise ValueError(\"Year must be between 1900 and 2100\")\n\n    # Find the closest moon phase year\n    closest_year = np.searchsorted(MOON_PHASES_YEARS, year)\n    if closest_year == 0:\n        closest_year = 1  # Avoid division by zero\n\n    # Calculate the phase based on the year\n    phase = (year - MOON_PHASES_YEARS[closest_year - 1]) / (MOON_PHASES_YEARS[closest_year] - MOON_PHASES_YEARS[closest_year - 1])\n    phase = math.fmod(phase, 1)  # Normalize phase to be between 0 and 1\n\n    return phase\n```\n"}
{"task_id": "BigCodeBench/304", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if df.empty:\n        return (0,0)\n\n    # Convert the lists in the 'Value' column to separate columns\n    df_expanded = df.explode('Value')\n\n    # Perform PCA on the 'Value' column\n    pca = PCA(n_components=2)\n    pca.fit(df_expanded['Value'].apply(pd.Series).values)\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Plot the explained variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(2), explained_variance_ratio)\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n    ax.set_title('Explained Variance Ratio of Principal Components')\n\n    return (explained_variance_ratio, ax)", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if df.empty:\n        return (0,0)\n\n    # Convert the lists in the 'Value' column to separate columns\n    df_expanded = df.explode('Value')\n\n    # Perform PCA on the 'Value' column\n    pca = PCA(n_components=2)\n    pca.fit(df_expanded['Value'].apply(pd.Series).values)\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Plot the explained variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(2), explained_variance_ratio)\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n    ax.set_title('Explained Variance Ratio of Principal Components')\n\n    return (explained_variance_ratio, ax)\n```\n"}
{"task_id": "BigCodeBench/305", "solution": "from collections import Counter\nimport itertools\nimport random\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    all_chars = list(itertools.chain.from_iterable(list_of_lists))\n    if not all_chars:\n        return Counter()\n\n    random_sample = random.sample(all_chars, len(all_chars))\n    counter = Counter(random_sample)\n    return counter", "raw_solution": "from collections import Counter\nimport itertools\nimport random\n\n# Constants\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    all_chars = list(itertools.chain.from_iterable(list_of_lists))\n    if not all_chars:\n        return Counter()\n\n    random_sample = random.sample(all_chars, len(all_chars))\n    counter = Counter(random_sample)\n    return counter\n\n# Example usage:\n# print(task_func([['a', 'b', 'c'], [], ['d', 'e', 'f']]))\n```\n"}
{"task_id": "BigCodeBench/306", "solution": "import os\nimport logging\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    removed_files = []\n    logging_filename = 'jquery_removal.log'\n    logging.basicConfig(filename=logging_filename, level=logging.INFO, format='%(asctime)s - %(message)s')\n\n    for filename in os.listdir(directory):\n        if 'jquery' in filename and filename.endswith('.js'):\n            file_path = os.path.join(directory, filename)\n            os.remove(file_path)\n            removed_files.append(filename)\n            logging.info(f\"Removed file: {filename}\")\n\n    return len(removed_files), removed_files", "raw_solution": "import os\nimport logging\n\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    removed_files = []\n    logging_filename = 'jquery_removal.log'\n    logging.basicConfig(filename=logging_filename, level=logging.INFO, format='%(asctime)s - %(message)s')\n\n    for filename in os.listdir(directory):\n        if 'jquery' in filename and filename.endswith('.js'):\n            file_path = os.path.join(directory, filename)\n            os.remove(file_path)\n            removed_files.append(filename)\n            logging.info(f\"Removed file: {filename}\")\n\n    return len(removed_files), removed_files\n\n# Example usage:\n# print(task_func(\"/path/to/directory\"))\n```\nThis script will remove all .js files in the specified directory that contain 'jquery' in their name, and log the removal in a file named 'jquery_removal.log'. It will also raise a FileNotFoundError if the specified directory does not exist.\n"}
{"task_id": "BigCodeBench/307", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    combined_data = []\n\n    for sublist in list_of_lists:\n        if not sublist:\n            sublist = [random.randint(0, 100) for _ in range(5)]\n        combined_data.extend(sublist)\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(combined_data, bins=10, kde=True)\n    ax.set_title('Histogram of Combined Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    combined_data = []\n\n    for sublist in list_of_lists:\n        if not sublist:\n            sublist = [random.randint(0, 100) for _ in range(5)]\n        combined_data.extend(sublist)\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(combined_data, bins=10, kde=True)\n    ax.set_title('Histogram of Combined Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Create a list of all fields including additional ones\n    all_fields = FIELDS + additional_fields\n\n    # Generate random grades for each student and each field\n    report_data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in all_fields}\n\n    # Create a DataFrame from the report data\n    report_df = pd.DataFrame(report_data, index=STUDENTS)\n\n    # Calculate the average grade for each student\n    report_df['Average Grade'] = report_df.apply(mean, axis=1)\n\n    # Calculate the average grade for each field\n    field_averages = report_df.mean(axis=0)\n    field_averages.name = 'Average Grade'\n    report_df = report_df.append(field_averages)\n\n    return report_df", "raw_solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    # Create a list of all fields including additional ones\n    all_fields = FIELDS + additional_fields\n\n    # Generate random grades for each student and each field\n    report_data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in all_fields}\n\n    # Create a DataFrame from the report data\n    report_df = pd.DataFrame(report_data, index=STUDENTS)\n\n    # Calculate the average grade for each student\n    report_df['Average Grade'] = report_df.apply(mean, axis=1)\n\n    # Calculate the average grade for each field\n    field_averages = report_df.mean(axis=0)\n    field_averages.name = 'Average Grade'\n    report_df = report_df.append(field_averages)\n\n    return report_df\n\n# Example usage:\nrandom.seed(0)\nreport = task_func(['Computer Science', 'Geography'])"}
{"task_id": "BigCodeBench/309", "solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_lists, seed=42):\n    random.seed(seed)\n    scaler = MinMaxScaler()\n\n    for i, inner_list in enumerate(list_of_lists):\n        if not inner_list:\n            list_of_lists[i] = [random.randint(0, 100) for _ in range(3)]\n\n    data = np.array(list_of_lists, dtype=float)\n    scaled_data = scaler.fit_transform(data)\n\n    return scaled_data.tolist()", "raw_solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(list_of_lists, seed=42):\n    random.seed(seed)\n    scaler = MinMaxScaler()\n\n    for i, inner_list in enumerate(list_of_lists):\n        if not inner_list:\n            list_of_lists[i] = [random.randint(0, 100) for _ in range(3)]\n\n    data = np.array(list_of_lists, dtype=float)\n    scaled_data = scaler.fit_transform(data)\n\n    return scaled_data.tolist()\n\n# Example usage:\n# print(task_func([[1, 2, 3], [], [4, 5, 6]]))\n```\n"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate random data for 100 people\n    people_data = []\n    for _ in range(PEOPLE_COUNT):\n        person = {\n            'Name': f'Person{random.randint(1, 1000)}',\n            'Age': random.randint(18, 70),\n            'Height': random.uniform(150, 200),\n            'Weight': random.uniform(50, 120)\n        }\n        people_data.append(person)\n\n    # Write data to CSV file\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=COLUMNS)\n        writer.writeheader()\n        writer.writerows(people_data)\n\n    # Calculate average age, height, and weight\n    age_data = [person['Age'] for person in people_data]\n    height_data = [person['Height'] for person in people_data]\n    weight_data = [person['Weight'] for person in people_data]\n\n    average_age = mean(age_data)\n    average_height = mean(height_data)\n    average_weight = mean(weight_data)\n\n    # Append averages to CSV file\n    with open(filename, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Average', average_age, average_height, average_weight])\n\n    return os.path.abspath(filename)\nfilename = 'people_report.csv'", "raw_solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate random data for 100 people\n    people_data = []\n    for _ in range(PEOPLE_COUNT):\n        person = {\n            'Name': f'Person{random.randint(1, 1000)}',\n            'Age': random.randint(18, 70),\n            'Height': random.uniform(150, 200),\n            'Weight': random.uniform(50, 120)\n        }\n        people_data.append(person)\n\n    # Write data to CSV file\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=COLUMNS)\n        writer.writeheader()\n        writer.writerows(people_data)\n\n    # Calculate average age, height, and weight\n    age_data = [person['Age'] for person in people_data]\n    height_data = [person['Height'] for person in people_data]\n    weight_data = [person['Weight'] for person in people_data]\n\n    average_age = mean(age_data)\n    average_height = mean(height_data)\n    average_weight = mean(weight_data)\n\n    # Append averages to CSV file\n    with open(filename, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Average', average_age, average_height, average_weight])\n\n    return os.path.abspath(filename)\n\n# Example usage\nrandom.seed(0)\nfilename = 'people_report.csv'\npath = task_func(filename)"}
{"task_id": "BigCodeBench/311", "solution": "import numpy as np\nimport random\nfrom scipy import stats\ndef task_func(list_of_lists, size=5, seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    def calculate_statistics(lst):\n        if not lst:\n            lst = np.random.randint(0, 101, size)\n        mean = np.mean(lst)\n        median = np.median(lst)\n        mode = stats.mode(lst)[0][0]\n        return mean, median, mode\n\n    results = []\n    for lst in list_of_lists:\n        mean, median, mode = calculate_statistics(lst)\n        results.append({'mean': mean, 'median': median, 'mode': mode})\n\n    return results", "raw_solution": "import numpy as np\nimport random\nfrom scipy import stats\n\ndef task_func(list_of_lists, size=5, seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    def calculate_statistics(lst):\n        if not lst:\n            lst = np.random.randint(0, 101, size)\n        mean = np.mean(lst)\n        median = np.median(lst)\n        mode = stats.mode(lst)[0][0]\n        return mean, median, mode\n\n    results = []\n    for lst in list_of_lists:\n        mean, median, mode = calculate_statistics(lst)\n        results.append({'mean': mean, 'median': median, 'mode': mode})\n\n    return results\n\n# Example usage:\n# print(task_func([[1, 2, 3], [], [4, 5, 6]]))\n```\nThis script defines a function `task_func` that calculates the mean, median, and mode for each list in the input list of lists. If a list is empty, it generates a new list of random integers between 0 and 100. The function returns a list of dictionaries, each containing the calculated statistics for a list.\n"}
{"task_id": "BigCodeBench/312", "solution": "import random\nimport matplotlib.pyplot as plt\nDISTRIBUTION_SIZE = 1000\ndef task_func(bins=30):\n    \"\"\"\n    Ek Gaussian distribution utpann karein and uska histogram plot karein.\n\n    Parameters:\n    - bins (int, optional): histogram ke liye bin ki sankhya. Default 30 true.\n\n    Returns:\n        tuple: Ek tuple jismein distribution list and histogram plot ka ax patch object shaamil true.\n\n    Requirements:\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> random.seed(0)\n    >>> distribution, ax = task_func()\n    >>> len(ax.patches) == 30\n    True\n    >>> len(distribution)\n    1000\n    >>> plt.close()\n\n    \"\"\"\n    # Generate a Gaussian distribution\n    distribution = [random.gauss(0, 1) for _ in range(DISTRIBUTION_SIZE)]\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(distribution, bins=bins, color='blue', alpha=0.7)\n    ax.set_title('Histogram of Gaussian Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return distribution, ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\n# Constants\nDISTRIBUTION_SIZE = 1000\n\ndef task_func(bins=30):\n    \"\"\"\n    Ek Gaussian distribution utpann karein and uska histogram plot karein.\n\n    Parameters:\n    - bins (int, optional): histogram ke liye bin ki sankhya. Default 30 true.\n\n    Returns:\n        tuple: Ek tuple jismein distribution list and histogram plot ka ax patch object shaamil true.\n\n    Requirements:\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> random.seed(0)\n    >>> distribution, ax = task_func()\n    >>> len(ax.patches) == 30\n    True\n    >>> len(distribution)\n    1000\n    >>> plt.close()\n\n    \"\"\"\n    # Generate a Gaussian distribution\n    distribution = [random.gauss(0, 1) for _ in range(DISTRIBUTION_SIZE)]\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(distribution, bins=bins, color='blue', alpha=0.7)\n    ax.set_title('Histogram of Gaussian Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return distribution, ax\n```\n"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    # Dictionary to store files in each subdirectory\n    files_dict = {}\n\n    # Traverse the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file name contains square brackets\n            if '[' in file and ']' in file:\n                # Extract the text within square brackets\n                match = re.search(r'\\[(.*?)\\]', file)\n                if match:\n                    text = match.group(1)\n                    # Create a new directory name with the current date and time\n                    new_dir_name = f\"{datetime.now().strftime('%Y%m%d%H%M%S')}_vyavasthit\"\n                    # Create a new directory\n                    new_dir_path = os.path.join(root, new_dir_name)\n                    os.makedirs(new_dir_path, exist_ok=True)\n                    # Move the file to the new directory\n                    old_file_path = os.path.join(root, file)\n                    new_file_path = os.path.join(new_dir_path, file)\n                    shutil.move(old_file_path, new_file_path)\n                    # Add the file to the dictionary\n                    if new_dir_name not in files_dict:\n                        files_dict[new_dir_name] = []\n                    files_dict[new_dir_name].append(new_file_path)\n\n    return new_dir_path, files_dict", "raw_solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    # Dictionary to store files in each subdirectory\n    files_dict = {}\n\n    # Traverse the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file name contains square brackets\n            if '[' in file and ']' in file:\n                # Extract the text within square brackets\n                match = re.search(r'\\[(.*?)\\]', file)\n                if match:\n                    text = match.group(1)\n                    # Create a new directory name with the current date and time\n                    new_dir_name = f\"{datetime.now().strftime('%Y%m%d%H%M%S')}_vyavasthit\"\n                    # Create a new directory\n                    new_dir_path = os.path.join(root, new_dir_name)\n                    os.makedirs(new_dir_path, exist_ok=True)\n                    # Move the file to the new directory\n                    old_file_path = os.path.join(root, file)\n                    new_file_path = os.path.join(new_dir_path, file)\n                    shutil.move(old_file_path, new_file_path)\n                    # Add the file to the dictionary\n                    if new_dir_name not in files_dict:\n                        files_dict[new_dir_name] = []\n                    files_dict[new_dir_name].append(new_file_path)\n\n    return new_dir_path, files_dict\n```\n"}
{"task_id": "BigCodeBench/314", "solution": "import socket\nimport ssl\nimport http.client\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    context = ssl.create_default_context()\n\n    try:\n        # Create a socket and wrap it with SSL\n        with socket.create_connection((SERVER_NAME, SERVER_PORT)) as sock:\n            with context.wrap_socket(sock, server_hostname=SERVER_NAME) as ssock:\n                # Create an HTTP connection over the secure socket\n                conn = http.client.HTTPSConnection(SERVER_NAME, context=context)\n                conn.request(\"GET\", path)\n                response = conn.getresponse()\n\n                # Read the response body\n                response_body = response.read().decode('utf-8')\n                return response_body\n\n    except ssl.SSLError as e:\n        print(f\"SSL Error: {e}\")\n        return None", "raw_solution": "import socket\nimport ssl\nimport http.client\n\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    context = ssl.create_default_context()\n\n    try:\n        # Create a socket and wrap it with SSL\n        with socket.create_connection((SERVER_NAME, SERVER_PORT)) as sock:\n            with context.wrap_socket(sock, server_hostname=SERVER_NAME) as ssock:\n                # Create an HTTP connection over the secure socket\n                conn = http.client.HTTPSConnection(SERVER_NAME, context=context)\n                conn.request(\"GET\", path)\n                response = conn.getresponse()\n\n                # Read the response body\n                response_body = response.read().decode('utf-8')\n                return response_body\n\n    except ssl.SSLError as e:\n        print(f\"SSL Error: {e}\")\n        return None\n```\n"}
{"task_id": "BigCodeBench/315", "solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\ndef task_func(dir, api_key, recipient_email):\n    if not os.path.exists(dir):\n        raise FileNotFoundError(f\"The directory {dir} does not exist.\")\n    \n    files = [f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n    \n    if not files:\n        return False\n\n    message = Mail(\n        from_email='from@example.com',\n        to_emails=recipient_email,\n        subject='Sending with SendGrid is Fun',\n        html_content='<strong>and easy to do anywhere, even with Python</strong>')\n    \n    try:\n        sg = SendGridAPIClient(api_key)\n        response = sg.send(message)\n        return response.status_code // 100 == 2\n    except HTTPError as e:\n        print(e)\n        return False\n    except Exception as e:\n        print(e)\n        return False", "raw_solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\n\ndef task_func(dir, api_key, recipient_email):\n    if not os.path.exists(dir):\n        raise FileNotFoundError(f\"The directory {dir} does not exist.\")\n    \n    files = [f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n    \n    if not files:\n        return False\n\n    message = Mail(\n        from_email='from@example.com',\n        to_emails=recipient_email,\n        subject='Sending with SendGrid is Fun',\n        html_content='<strong>and easy to do anywhere, even with Python</strong>')\n    \n    try:\n        sg = SendGridAPIClient(api_key)\n        response = sg.send(message)\n        return response.status_code // 100 == 2\n    except HTTPError as e:\n        print(e)\n        return False\n    except Exception as e:\n        print(e)\n        return False\n```\nThis script first checks if the directory exists and if it does, it lists all the files in the directory. If there are no files, it returns False. If there are files, it creates a SendGrid email message and sends it using the provided API key. It returns True if the email was sent successfully (status code in the 2xx range), and False otherwise.\n"}
{"task_id": "BigCodeBench/316", "solution": "import pandas as pd\nimport random\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\ndef task_func(value_range=(0, 100)):\n    \"\"\"\n    Ek nirdisht range mein ek category distribution utpann karein and ise ek DataFrame ke roop mein lautayein.\n\n    Parameters:\n    - value_range (tuple): category ke liye random maan utpann karne ke liye range (minute, adhiktam) nirdisht karne wala ek tuple.\n\n    Returns:\n        DataFrame: Ek pandas DataFrame jismein do columns hain: 'Category' (category ke naam) and 'Count' (pratyek category ki count).\n\n    Requirements:\n    - pandas\n    - random\n\n    Example:\n    >>> random.seed(0)\n    >>> df = task_func()\n    >>> df['Count'][0] >= 0\n    true\n\n    \"\"\"\n    # Initialize counts for each category\n    category_counts = {category: 0 for category in CATEGORIES}\n\n    # Generate random counts for each category\n    for _ in range(random.randint(*value_range)):\n        category = random.choice(CATEGORIES)\n        category_counts[category] += 1\n\n    # Create a DataFrame from the counts\n    df = pd.DataFrame(list(category_counts.items()), columns=['Category', 'Count'])\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(value_range=(0, 100)):\n    \"\"\"\n    Ek nirdisht range mein ek category distribution utpann karein and ise ek DataFrame ke roop mein lautayein.\n\n    Parameters:\n    - value_range (tuple): category ke liye random maan utpann karne ke liye range (minute, adhiktam) nirdisht karne wala ek tuple.\n\n    Returns:\n        DataFrame: Ek pandas DataFrame jismein do columns hain: 'Category' (category ke naam) and 'Count' (pratyek category ki count).\n\n    Requirements:\n    - pandas\n    - random\n\n    Example:\n    >>> random.seed(0)\n    >>> df = task_func()\n    >>> df['Count'][0] >= 0\n    true\n\n    \"\"\"\n    # Initialize counts for each category\n    category_counts = {category: 0 for category in CATEGORIES}\n\n    # Generate random counts for each category\n    for _ in range(random.randint(*value_range)):\n        category = random.choice(CATEGORIES)\n        category_counts[category] += 1\n\n    # Create a DataFrame from the counts\n    df = pd.DataFrame(list(category_counts.items()), columns=['Category', 'Count'])\n\n    return df\n\n# Example usage\nrandom.seed(0)\ndf = task_func()"}
{"task_id": "BigCodeBench/317", "solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\ndef task_func(example_str):\n    # Remove special characters and numbers\n    cleaned_str = re.sub(r'[^a-zA-Z\\s]', '', example_str)\n\n    # Convert to lowercase\n    cleaned_str = cleaned_str.lower()\n\n    # Split the string into words\n    words = cleaned_str.split()\n\n    # Initialize the TfidfVectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the words\n    tfidf_matrix = vectorizer.fit_transform([' '.join(words)])\n\n    # Convert the matrix to a dictionary\n    tfidf_dict = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[0]))\n\n    return tfidf_dict", "raw_solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\ndef task_func(example_str):\n    # Remove special characters and numbers\n    cleaned_str = re.sub(r'[^a-zA-Z\\s]', '', example_str)\n\n    # Convert to lowercase\n    cleaned_str = cleaned_str.lower()\n\n    # Split the string into words\n    words = cleaned_str.split()\n\n    # Initialize the TfidfVectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the words\n    tfidf_matrix = vectorizer.fit_transform([' '.join(words)])\n\n    # Convert the matrix to a dictionary\n    tfidf_dict = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[0]))\n\n    return tfidf_dict\n\n# Example usage:\n# tfidf_scores = task_func(\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003] Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\")\n# print(tfidf_scores)\n```\n"}
{"task_id": "BigCodeBench/318", "solution": "import random\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(points_count=1000, radius=1):\n    # Generate random points within the circle\n    points = []\n    for _ in range(points_count):\n        angle = random.uniform(0, 2 * math.pi)\n        r = random.uniform(0, radius)\n        x = r * math.cos(angle)\n        y = r * math.sin(angle)\n        points.append((x, y))\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(*zip(*points))\n    ax.set_aspect('equal', 'box')\n    ax.set_title('Random Points within a Circle')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n\n    return ax", "raw_solution": "import random\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(points_count=1000, radius=1):\n    # Generate random points within the circle\n    points = []\n    for _ in range(points_count):\n        angle = random.uniform(0, 2 * math.pi)\n        r = random.uniform(0, radius)\n        x = r * math.cos(angle)\n        y = r * math.sin(angle)\n        points.append((x, y))\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(*zip(*points))\n    ax.set_aspect('equal', 'box')\n    ax.set_title('Random Points within a Circle')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n\n    return ax\n\n# Example usage:\n# random.seed(0)\n# ax = task_func(500, 0.5)\n# print(len(ax.collections[0].get_offsets()))  # Should print 500\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/319", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\ndef task_func(example_str, top_n=30):\n    # Extract words from the string within square brackets\n    pattern = r'\\[.*?\\]'\n    words = re.findall(pattern, example_str)\n    words = [re.sub(r'\\W+', ' ', word).strip() for word in words]\n    \n    # Calculate frequency distribution\n    freq_dist = FreqDist(words)\n    \n    # Plot frequency distribution\n    fig, ax = plt.subplots()\n    freq_dist.plot(top_n, ax=ax)\n    ax.set_title('Top {} Most Common Words'.format(top_n))\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    \n    # Get top_n words\n    top_n_words = dict(freq_dist.most_common(top_n))\n    \n    return ax, top_n_words", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\n\ndef task_func(example_str, top_n=30):\n    # Extract words from the string within square brackets\n    pattern = r'\\[.*?\\]'\n    words = re.findall(pattern, example_str)\n    words = [re.sub(r'\\W+', ' ', word).strip() for word in words]\n    \n    # Calculate frequency distribution\n    freq_dist = FreqDist(words)\n    \n    # Plot frequency distribution\n    fig, ax = plt.subplots()\n    freq_dist.plot(top_n, ax=ax)\n    ax.set_title('Top {} Most Common Words'.format(top_n))\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    \n    # Get top_n words\n    top_n_words = dict(freq_dist.most_common(top_n))\n    \n    return ax, top_n_words\n\n# Example usage"}
{"task_id": "BigCodeBench/320", "solution": "import subprocess\nimport os\nimport random\ndef task_func(directory, file_list):\n    if not file_list:\n        return None\n\n    random_file = random.choice(file_list)\n    file_path = os.path.join(directory, random_file)\n\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {random_file} does not exist in the directory {directory}.\")\n\n    try:\n        process = subprocess.run(['cmd', '/c', file_path], check=True, text=True, capture_output=True)\n        return process.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode", "raw_solution": "import subprocess\nimport os\nimport random\n\ndef task_func(directory, file_list):\n    if not file_list:\n        return None\n\n    random_file = random.choice(file_list)\n    file_path = os.path.join(directory, random_file)\n\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {random_file} does not exist in the directory {directory}.\")\n\n    try:\n        process = subprocess.run(['cmd', '/c', file_path], check=True, text=True, capture_output=True)\n        return process.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode\n```\nThis script first checks if the file list is not empty. If it is, it returns `None`. It then selects a random file from the list and constructs the full path to the file. It checks if the file exists and then runs the file as a subprocess. The function returns the exit code of the subprocess. If the file does not exist, it raises a `FileNotFoundError`. If there is an error running the subprocess, it returns the return code of the error.\n"}
{"task_id": "BigCodeBench/321", "solution": "import pandas as pd\nimport re\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Extract names from the text\n    names = re.findall(r'\\[.*?\\]', text)\n    names = [name.strip('[]') for name in names]\n\n    # Create a pandas Series with frequency counts\n    name_freqs = pd.Series(names).value_counts()\n\n    # Plot the frequency distribution\n    fig, ax = plt.subplots()\n    name_freqs.plot(kind='bar', ax=ax)\n    ax.set_title('Name Frequency Distribution')\n    ax.set_xlabel('Name')\n    ax.set_ylabel('Frequency')\n\n    # Calculate skewness and kurtosis\n    skewness = name_freqs.skew()\n    kurt = name_freqs.kurt()\n\n    return name_freqs, ax, skewness, kurt", "raw_solution": "import pandas as pd\nimport re\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Extract names from the text\n    names = re.findall(r'\\[.*?\\]', text)\n    names = [name.strip('[]') for name in names]\n\n    # Create a pandas Series with frequency counts\n    name_freqs = pd.Series(names).value_counts()\n\n    # Plot the frequency distribution\n    fig, ax = plt.subplots()\n    name_freqs.plot(kind='bar', ax=ax)\n    ax.set_title('Name Frequency Distribution')\n    ax.set_xlabel('Name')\n    ax.set_ylabel('Frequency')\n\n    # Calculate skewness and kurtosis\n    skewness = name_freqs.skew()\n    kurt = name_freqs.kurt()\n\n    return name_freqs, ax, skewness, kurt\n```\n"}
{"task_id": "BigCodeBench/322", "solution": "import subprocess\nimport os\nimport shutil\nimport sys\nDIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server\\\\Backup'\ndef task_func(filename):\n    \"\"\"\n    This function performs a backup of a specified file to a backup directory and then executes it using a subprocess.\n\n    Parameters:\n    - filename (str): The name of the file to be backed up and executed.\n\n    Returns:\n    - int: The exit code of the subprocess, or -1 if the backup fails.\n\n    Requirements:\n    - subprocess\n    - shutil\n\n    Example:\n    >>> task_func('vmware-cmd.bat') # Assuming successful execution\n    0\n    >>> task_func('nonexistent.bat') # If backup fails or file doesn't exist\n    -1\n    \"\"\"\n    # Check if the file exists in the original directory\n    original_file_path = os.path.join(DIRECTORY, filename)\n    if not os.path.isfile(original_file_path):\n        return -1\n\n    # Create backup directory if it doesn't exist\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n\n    # Perform the backup\n    backup_file_path = os.path.join(BACKUP_DIRECTORY, filename)\n    try:\n        shutil.copy(original_file_path, backup_file_path)\n    except Exception as e:\n        print(f\"Backup failed: {e}\")\n        return -1\n\n    # Execute the file using a subprocess\n    try:\n        result = subprocess.run([backup_file_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        print(f\"Execution failed: {e}\")\n        return -1", "raw_solution": "import subprocess\nimport os\nimport shutil\nimport sys\n\n# Constants\nDIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server\\\\Backup'\n\ndef task_func(filename):\n    \"\"\"\n    This function performs a backup of a specified file to a backup directory and then executes it using a subprocess.\n\n    Parameters:\n    - filename (str): The name of the file to be backed up and executed.\n\n    Returns:\n    - int: The exit code of the subprocess, or -1 if the backup fails.\n\n    Requirements:\n    - subprocess\n    - shutil\n\n    Example:\n    >>> task_func('vmware-cmd.bat') # Assuming successful execution\n    0\n    >>> task_func('nonexistent.bat') # If backup fails or file doesn't exist\n    -1\n    \"\"\"\n    # Check if the file exists in the original directory\n    original_file_path = os.path.join(DIRECTORY, filename)\n    if not os.path.isfile(original_file_path):\n        return -1\n\n    # Create backup directory if it doesn't exist\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n\n    # Perform the backup\n    backup_file_path = os.path.join(BACKUP_DIRECTORY, filename)\n    try:\n        shutil.copy(original_file_path, backup_file_path)\n    except Exception as e:\n        print(f\"Backup failed: {e}\")\n        return -1\n\n    # Execute the file using a subprocess\n    try:\n        result = subprocess.run([backup_file_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        print(f\"Execution failed: {e}\")\n        return -1\n```\n"}
{"task_id": "BigCodeBench/323", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\ndef task_func(text, num_gaussians=1, seed=42):\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians must be greater than 0.\")\n    \n    # Extract names from the text\n    names = re.findall(r'\\b\\w+\\b(?: \\[.*?\\])+', text)\n    \n    # Tokenize names into words\n    words = []\n    for name in names:\n        words.extend(re.findall(r'\\b\\w+\\b', name))\n    \n    # Count the frequency of each word\n    word_freqs = Counter(words)\n    \n    # Check if num_gaussians is greater than the number of unique words\n    if num_gaussians > len(word_freqs):\n        raise Exception(\"num_gaussians cannot be greater than the number of unique words.\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Fit a Gaussian Mixture Model\n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    word_frequencies = np.array(list(word_freqs.values())).reshape(-1, 1)\n    gmm.fit(word_frequencies)\n    \n    # Extract means and variances\n    means = gmm.means_.flatten()\n    variances = gmm.covariances_.flatten()\n    \n    return word_freqs, means, variances", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\n\ndef task_func(text, num_gaussians=1, seed=42):\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians must be greater than 0.\")\n    \n    # Extract names from the text\n    names = re.findall(r'\\b\\w+\\b(?: \\[.*?\\])+', text)\n    \n    # Tokenize names into words\n    words = []\n    for name in names:\n        words.extend(re.findall(r'\\b\\w+\\b', name))\n    \n    # Count the frequency of each word\n    word_freqs = Counter(words)\n    \n    # Check if num_gaussians is greater than the number of unique words\n    if num_gaussians > len(word_freqs):\n        raise Exception(\"num_gaussians cannot be greater than the number of unique words.\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Fit a Gaussian Mixture Model\n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    word_frequencies = np.array(list(word_freqs.values())).reshape(-1, 1)\n    gmm.fit(word_frequencies)\n    \n    # Extract means and variances\n    means = gmm.means_.flatten()\n    variances = gmm.covariances_.flatten()\n    \n    return word_freqs, means, variances\n\n# Example usage:\n# freqs, means, variances = task_func(\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\")\n# print(freqs)\n# print(means)\n# print(variances)\n```\n"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    def run_process(file_path):\n        # Run the batch file as a subprocess\n        process = subprocess.Popen(file_path, shell=True)\n        process.wait()  # Wait for the process to finish\n        return process.returncode  # Return the exit code\n\n    # List to store exit codes\n    exit_codes = []\n\n    # Create and start threads for each file\n    threads = []\n    for file_path in file_list:\n        thread = threading.Thread(target=run_process, args=(file_path,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Collect exit codes from each thread\n    for thread in threads:\n        exit_codes.append(thread.exitcode)\n\n    return exit_codes", "raw_solution": "import subprocess\nimport time\nimport threading\n\ndef task_func(file_list):\n    def run_process(file_path):\n        # Run the batch file as a subprocess\n        process = subprocess.Popen(file_path, shell=True)\n        process.wait()  # Wait for the process to finish\n        return process.returncode  # Return the exit code\n\n    # List to store exit codes\n    exit_codes = []\n\n    # Create and start threads for each file\n    threads = []\n    for file_path in file_list:\n        thread = threading.Thread(target=run_process, args=(file_path,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Collect exit codes from each thread\n    for thread in threads:\n        exit_codes.append(thread.exitcode)\n\n    return exit_codes\n\n# Example usage:\n# Assuming the batch files are in the same directory as the script\n# and have the correct permissions to execute.\n# print(task_func([\"file1.bat\", \"file2.bat\"]))\n```\nThis script uses the `subprocess` module to run batch files as separate processes, the `time` module to introduce delays if needed, and the `threading` module to run the processes concurrently. The function `task_func` takes a list of batch file paths as input and returns a list of exit codes for each process.\n"}
{"task_id": "BigCodeBench/325", "solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\ndef task_func(directory_path: str, regex_pattern: str = r'\\\\(.+?\\\\)|\\\\w') -> dict:\n    matches = {}\n    for file_path in glob.glob(os.path.join(directory_path, '*.txt')):\n        with open(file_path, 'r') as file:\n            content = file.read()\n            file_matches = re.findall(regex_pattern, content)\n            matches[Path(file_path).name] = file_matches\n    return matches", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\n\ndef task_func(directory_path: str, regex_pattern: str = r'\\\\(.+?\\\\)|\\\\w') -> dict:\n    matches = {}\n    for file_path in glob.glob(os.path.join(directory_path, '*.txt')):\n        with open(file_path, 'r') as file:\n            content = file.read()\n            file_matches = re.findall(regex_pattern, content)\n            matches[Path(file_path).name] = file_matches\n    return matches\n\n# Example usage:\n# matches = task_func('/path/to/directory')\n# print(matches)\n```\nThis script reads all text files in the specified directory, applies the regular expression pattern to each file, and stores the matches in a dictionary with the file names as keys. The regular expression pattern is designed to capture both nested and standalone words.\n"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n    results = []\n\n    for bat_file in bat_files:\n        try:\n            result = subprocess.run([sys.executable, bat_file], check=False, capture_output=True)\n            results.append((os.path.basename(bat_file), result.returncode))\n        except Exception as e:\n            results.append((os.path.basename(bat_file), None))\n\n    return results", "raw_solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n    results = []\n\n    for bat_file in bat_files:\n        try:\n            result = subprocess.run([sys.executable, bat_file], check=False, capture_output=True)\n            results.append((os.path.basename(bat_file), result.returncode))\n        except Exception as e:\n            results.append((os.path.basename(bat_file), None))\n\n    return results\n\n# Example usage:\n# print(task_func(\"path/to/directory\"))\n```\nThis script uses the `glob` module to find all `.bat` files in the specified directory. It then uses the `subprocess` module to execute each `.bat` file and capture the exit code. If an error occurs during execution, the exit code is set to `None`. The function returns a list of tuples, where each tuple contains the name of a `.bat` file and its corresponding exit code.\n"}
{"task_id": "BigCodeBench/327", "solution": "import csv\nimport re\nfrom collections import Counter\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    matches = []\n\n    with open(file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            for cell in row:\n                matches.extend(re.findall(regex_pattern, cell))\n\n    counts = Counter(matches)\n    return dict(counts)", "raw_solution": "import csv\nimport re\nfrom collections import Counter\n\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    matches = []\n\n    with open(file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            for cell in row:\n                matches.extend(re.findall(regex_pattern, cell))\n\n    counts = Counter(matches)\n    return dict(counts)\n\n# Example usage:\n# import tempfile\n# temp_dir = tempfile.mkdtemp()\n# file_path = os.path.join(temp_dir, 'data.csv')\n# with open(file_path, 'w', newline='') as file:\n#     writer = csv.writer(file)\n#     writer.writerow(['a'])\n#     writer.writerow(['b'])\n#     writer.writerow(['(abc)'])\n# counts = task_func(file_path)\n# print(counts)\n```\n"}
{"task_id": "BigCodeBench/328", "solution": "import collections\nimport random\nfrom queue import PriorityQueue\ndef task_func(number_teams=5):\n    # Create a PriorityQueue to store the teams with their points\n    pq = PriorityQueue()\n\n    # Generate random teams and points\n    for i in range(1, number_teams + 1):\n        team_name = f'Team {i}'\n        points = random.randint(10, 100)\n        pq.put((-points, team_name))  # Negative points to simulate a max-heap\n\n    # Create an OrderedDict to store the sorted teams\n    sorted_teams = collections.OrderedDict()\n\n    # Pop teams from the PriorityQueue and add them to the OrderedDict\n    while not pq.empty():\n        points, team_name = pq.get()\n        sorted_teams[team_name] = -points\n\n    return sorted_teams", "raw_solution": "import collections\nimport random\nfrom queue import PriorityQueue\n\ndef task_func(number_teams=5):\n    # Create a PriorityQueue to store the teams with their points\n    pq = PriorityQueue()\n\n    # Generate random teams and points\n    for i in range(1, number_teams + 1):\n        team_name = f'Team {i}'\n        points = random.randint(10, 100)\n        pq.put((-points, team_name))  # Negative points to simulate a max-heap\n\n    # Create an OrderedDict to store the sorted teams\n    sorted_teams = collections.OrderedDict()\n\n    # Pop teams from the PriorityQueue and add them to the OrderedDict\n    while not pq.empty():\n        points, team_name = pq.get()\n        sorted_teams[team_name] = -points\n\n    return sorted_teams\n\n# Example usage"}
{"task_id": "BigCodeBench/329", "solution": "import re\nimport json\nimport os\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    matches = {}\n    for key, value in data.items():\n        found_matches = re.findall(regex_pattern, value)\n        matches[key] = found_matches\n    \n    return matches", "raw_solution": "import re\nimport json\nimport os\n\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    matches = {}\n    for key, value in data.items():\n        found_matches = re.findall(regex_pattern, value)\n        matches[key] = found_matches\n    \n    return matches\n\n# Example usage:\n# Assuming the JSON file is named 'sample_data.json' and contains a key 'content' with a sample text.\n# temp_dir = tempfile.mkdtemp()\n# file_path = os.path.join(temp_dir, 'sample_data.json')\n# with open(file_path, 'w') as file:\n#     json.dump({'content': 'This is a (sample) text with some (matches) and characters.'}, file)\n# matches = task_func(file_path)\n# print(matches)\n```\n"}
{"task_id": "BigCodeBench/330", "solution": "import heapq\nimport random\ndef task_func(list_length: int, k: int):\n    # Generate a random list of integers\n    rand_list = [random.randint(0, 100) for _ in range(list_length)]\n    \n    # Find the k largest numbers using heapq\n    top_k = heapq.nlargest(k, rand_list)\n    \n    return rand_list, top_k", "raw_solution": "import heapq\nimport random\n\n\ndef task_func(list_length: int, k: int):\n    # Generate a random list of integers\n    rand_list = [random.randint(0, 100) for _ in range(list_length)]\n    \n    # Find the k largest numbers using heapq\n    top_k = heapq.nlargest(k, rand_list)\n    \n    return rand_list, top_k\n\n\n# Example usage:\nrandom.seed(0)\nrand_list, top_k = task_func(5, 3)"}
{"task_id": "BigCodeBench/331", "solution": "import bisect\nimport random\ndef task_func(num, list_length = 5, min_value = 0, max_value = 0):\n    # Generate a random list of integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    random_list.sort()  # Sort the list\n\n    # Insert the number into the sorted list at the correct position\n    bisect.insort(random_list, num)\n\n    return random_list[:-1], [num] + random_list", "raw_solution": "import bisect\nimport random\n\ndef task_func(num, list_length = 5, min_value = 0, max_value = 0):\n    # Generate a random list of integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    random_list.sort()  # Sort the list\n\n    # Insert the number into the sorted list at the correct position\n    bisect.insort(random_list, num)\n\n    return random_list[:-1], [num] + random_list\n\n# Example usage:\n# random.seed(0)\n# print(task_func(4, 5, 100, 100))\n# print(task_func(15, 0, 10, 20))\n```\n"}
{"task_id": "BigCodeBench/332", "solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\ndef task_func(text: str) -> dict:\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    \n    # Tokenize the text\n    words = text.split()\n    \n    # Remove stopwords\n    words = [word for word in words if word not in stopwords.words('english')]\n    \n    # Count the occurrences of each word\n    word_counts = Counter(words)\n    \n    return dict(word_counts)", "raw_solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\ndef task_func(text: str) -> dict:\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    \n    # Tokenize the text\n    words = text.split()\n    \n    # Remove stopwords\n    words = [word for word in words if word not in stopwords.words('english')]\n    \n    # Count the occurrences of each word\n    word_counts = Counter(words)\n    \n    return dict(word_counts)\n\n# Example usage:"}
{"task_id": "BigCodeBench/333", "solution": "import heapq\nimport random\ndef task_func(k, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a random list of integers\n    rand_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    \n    # Find the k smallest numbers using heapq\n    least_k = heapq.nsmallest(k, rand_list)\n    \n    return rand_list, least_k", "raw_solution": "import heapq\nimport random\n\ndef task_func(k, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a random list of integers\n    rand_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    \n    # Find the k smallest numbers using heapq\n    least_k = heapq.nsmallest(k, rand_list)\n    \n    return rand_list, least_k\n\n# Example usage:\nrandom.seed(0)\nrand_list, least_k = task_func(3)\nassert least_k[0] in rand_list\n\nrand_list, least_k = task_func(3, 5, 100, 100)\nassert least_k == [100, 100, 100]\n```\n"}
{"task_id": "BigCodeBench/334", "solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\ndef task_func(documents):\n    # Tokenize the documents\n    tokenized_docs = [' '.join(word_tokenize(doc)) for doc in documents]\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the documents\n    tfidf_matrix = vectorizer.fit_transform(tokenized_docs)\n\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df", "raw_solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\ndef task_func(documents):\n    # Tokenize the documents\n    tokenized_docs = [' '.join(word_tokenize(doc)) for doc in documents]\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the documents\n    tfidf_matrix = vectorizer.fit_transform(tokenized_docs)\n\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\n\n# Example usage:\ndocs = ['Yah first document hai.', 'Yah document second document hai.', 'and yah third hai.', 'Kya yah first document hai?']\ntfidf = task_func(docs)"}
{"task_id": "BigCodeBench/335", "solution": "import collections\nfrom queue import PriorityQueue\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(string_length=100):\n    random_string = ''.join(random.choices(LETTERS, k=string_length))\n    frequency = collections.Counter(random_string)\n    \n    # Create an ordered dictionary sorted by frequency\n    ordered_dict = collections.OrderedDict(sorted(frequency.items(), key=lambda item: item[1], reverse=True))\n    \n    return ordered_dict", "raw_solution": "import collections\nfrom queue import PriorityQueue\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef task_func(string_length=100):\n    random_string = ''.join(random.choices(LETTERS, k=string_length))\n    frequency = collections.Counter(random_string)\n    \n    # Create an ordered dictionary sorted by frequency\n    ordered_dict = collections.OrderedDict(sorted(frequency.items(), key=lambda item: item[1], reverse=True))\n    \n    return ordered_dict\n\n# Example usage"}
{"task_id": "BigCodeBench/336", "solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\ndef task_func(pattern, directory, extensions):\n    matched_files = []\n\n    for ext in extensions:\n        for file_path in glob.glob(os.path.join(directory, ext)):\n            with open(file_path, 'r', encoding='utf-8') as file:\n                content = file.read()\n                if re.search(pattern, content, re.IGNORECASE):\n                    matched_files.append(Path(file_path).resolve())\n\n    return matched_files", "raw_solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\n\ndef task_func(pattern, directory, extensions):\n    matched_files = []\n\n    for ext in extensions:\n        for file_path in glob.glob(os.path.join(directory, ext)):\n            with open(file_path, 'r', encoding='utf-8') as file:\n                content = file.read()\n                if re.search(pattern, content, re.IGNORECASE):\n                    matched_files.append(Path(file_path).resolve())\n\n    return matched_files\n```\n"}
{"task_id": "BigCodeBench/337", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame.\")\n    \n    if group_col not in df.columns or value_col not in df.columns:\n        raise ValueError(f\"Columns {group_col} and {value_col} must exist in the DataFrame.\")\n    \n    if not np.issubdtype(df[value_col].dtype, np.number):\n        raise TypeError(f\"Column {value_col} must contain numeric values.\")\n    \n    grouped = df.groupby(group_col)[value_col].mean().reset_index()\n    \n    fig, ax = plt.subplots()\n    for i, row in grouped.iterrows():\n        ax.bar(row[group_col], row[value_col], color=COLORS[i % len(COLORS)])\n    \n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    ax.set_title(f\"Bar chart of {value_col} by {group_col}\")\n    \n    return ax\ndf = pd.DataFrame({'Group': ['A', 'B', 'A', 'B', 'A', 'B'], 'Value': [1, 2, 3, 4, 5, 6]})", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame.\")\n    \n    if group_col not in df.columns or value_col not in df.columns:\n        raise ValueError(f\"Columns {group_col} and {value_col} must exist in the DataFrame.\")\n    \n    if not np.issubdtype(df[value_col].dtype, np.number):\n        raise TypeError(f\"Column {value_col} must contain numeric values.\")\n    \n    grouped = df.groupby(group_col)[value_col].mean().reset_index()\n    \n    fig, ax = plt.subplots()\n    for i, row in grouped.iterrows():\n        ax.bar(row[group_col], row[value_col], color=COLORS[i % len(COLORS)])\n    \n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    ax.set_title(f\"Bar chart of {value_col} by {group_col}\")\n    \n    return ax\n\n# Example usage:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame({'Group': ['A', 'B', 'A', 'B', 'A', 'B'], 'Value': [1, 2, 3, 4, 5, 6]})\nax = task_func(df, 'Group', 'Value')\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/338", "solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\ndef task_func(elements, seed=100):\n    random.seed(seed)\n    formatted_elements = []\n    all_characters = []\n\n    for element in elements:\n        random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n        formatted_element = element % random_string\n        formatted_elements.append(formatted_element)\n        all_characters.extend(formatted_element)\n\n    counts = {char: all_characters.count(char) for char in set(all_characters)}\n\n    plt.figure(figsize=(10, 5))\n    ax = plt.gca()\n    ax.bar(counts.keys(), counts.values())\n    ax.set_xlabel('Characters')\n    ax.set_ylabel('Counts')\n    ax.set_title('Character Counts in Formatted Strings')\n\n    return formatted_elements, ax, counts", "raw_solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\n\ndef task_func(elements, seed=100):\n    random.seed(seed)\n    formatted_elements = []\n    all_characters = []\n\n    for element in elements:\n        random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n        formatted_element = element % random_string\n        formatted_elements.append(formatted_element)\n        all_characters.extend(formatted_element)\n\n    counts = {char: all_characters.count(char) for char in set(all_characters)}\n\n    plt.figure(figsize=(10, 5))\n    ax = plt.gca()\n    ax.bar(counts.keys(), counts.values())\n    ax.set_xlabel('Characters')\n    ax.set_ylabel('Counts')\n    ax.set_title('Character Counts in Formatted Strings')\n\n    return formatted_elements, ax, counts\n\n# Example usage:\n# patterns, ax, counts = task_func(['abc', 'def'])\n# print(patterns)\n# print(counts)\n```\n"}
{"task_id": "BigCodeBench/339", "solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\ndef task_func(req_data, secret_key):\n    if not isinstance(req_data, dict):\n        raise TypeError(\"req_data should be a dictionary.\")\n\n    # Convert the dictionary to a JSON string\n    json_data = json.dumps(req_data)\n\n    # Encode the secret key\n    encoded_key = secret_key.encode()\n\n    # Create an HMAC object using SHA256\n    hmac_obj = hmac.new(encoded_key, json_data.encode(), hashlib.sha256)\n\n    # Get the hexadecimal representation of the HMAC\n    signature = hmac_obj.hexdigest()\n\n    # Encode the signature using urllib\n    encoded_signature = urllib.parse.quote(signature)\n\n    # Replace '+' with '%2B' to match the standard URL encoding\n    encoded_signature = encoded_signature.replace('+', '%2B')\n\n    return encoded_signature", "raw_solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\n\ndef task_func(req_data, secret_key):\n    if not isinstance(req_data, dict):\n        raise TypeError(\"req_data should be a dictionary.\")\n\n    # Convert the dictionary to a JSON string\n    json_data = json.dumps(req_data)\n\n    # Encode the secret key\n    encoded_key = secret_key.encode()\n\n    # Create an HMAC object using SHA256\n    hmac_obj = hmac.new(encoded_key, json_data.encode(), hashlib.sha256)\n\n    # Get the hexadecimal representation of the HMAC\n    signature = hmac_obj.hexdigest()\n\n    # Encode the signature using urllib\n    encoded_signature = urllib.parse.quote(signature)\n\n    # Replace '+' with '%2B' to match the standard URL encoding\n    encoded_signature = encoded_signature.replace('+', '%2B')\n\n    return encoded_signature\n\n# Test cases"}
{"task_id": "BigCodeBench/340", "solution": "import json\nimport hashlib\nimport blake3\ndef task_func(req_data):\n    # Convert the dictionary to a JSON string\n    json_data = json.dumps(req_data, sort_keys=True)\n    \n    # Generate BLAKE3 hash\n    blake3_hash = blake3.blake3(json_data.encode('utf-8')).hexdigest()\n    \n    # Generate MD5 hash of the BLAKE3 hash\n    md5_hash = hashlib.md5(blake3_hash.encode('utf-8')).hexdigest()\n    \n    return blake3_hash, md5_hash", "raw_solution": "import json\nimport hashlib\nimport blake3\n\ndef task_func(req_data):\n    # Convert the dictionary to a JSON string\n    json_data = json.dumps(req_data, sort_keys=True)\n    \n    # Generate BLAKE3 hash\n    blake3_hash = blake3.blake3(json_data.encode('utf-8')).hexdigest()\n    \n    # Generate MD5 hash of the BLAKE3 hash\n    md5_hash = hashlib.md5(blake3_hash.encode('utf-8')).hexdigest()\n    \n    return blake3_hash, md5_hash\n\n# Test cases"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame) or df.empty or col not in df.columns:\n        raise ValueError(\"The DataFrame is empty or the specified column does not exist.\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(df[col], kde=True, ax=axes[0])\n        axes[0].set_title(f'Histogram and KDE for {col}')\n        sns.boxplot(x=df[col], ax=axes[1])\n        axes[1].set_title(f'Box Plot for {col}')\n    elif pd.api.types.is_categorical_dtype(df[col]):\n        sns.countplot(x=df[col], ax=axes[0])\n        axes[0].set_title(f'Count Plot for {col}')\n        sns.boxplot(x=df[col], ax=axes[1])\n        axes[1].set_title(f'Box Plot for {col}')\n    else:\n        raise ValueError(\"The specified column is neither numeric nor categorical.\")\n    \n    plt.tight_layout()\n    return fig", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame) or df.empty or col not in df.columns:\n        raise ValueError(\"The DataFrame is empty or the specified column does not exist.\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(df[col], kde=True, ax=axes[0])\n        axes[0].set_title(f'Histogram and KDE for {col}')\n        sns.boxplot(x=df[col], ax=axes[1])\n        axes[1].set_title(f'Box Plot for {col}')\n    elif pd.api.types.is_categorical_dtype(df[col]):\n        sns.countplot(x=df[col], ax=axes[0])\n        axes[0].set_title(f'Count Plot for {col}')\n        sns.boxplot(x=df[col], ax=axes[1])\n        axes[1].set_title(f'Box Plot for {col}')\n    else:\n        raise ValueError(\"The specified column is neither numeric nor categorical.\")\n    \n    plt.tight_layout()\n    return fig\n\n# Example usage:\n# df = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n# fig = task_func(df, 'value')\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/342", "solution": "import string\nimport random\nimport re\ndef task_func(elements, pattern, seed=100):\n    random.seed(seed)\n    replaced_elements = []\n    for element in elements:\n        random_char = random.choice(string.ascii_letters)\n        formatted_element = pattern % random_char\n        replaced_elements.append(formatted_element)\n    \n    combined_string = ' '.join(replaced_elements)\n    search_result = bool(re.search(pattern, combined_string))\n    \n    return replaced_elements, search_result\npattern = \".*\"", "raw_solution": "import string\nimport random\nimport re\n\ndef task_func(elements, pattern, seed=100):\n    random.seed(seed)\n    replaced_elements = []\n    for element in elements:\n        random_char = random.choice(string.ascii_letters)\n        formatted_element = pattern % random_char\n        replaced_elements.append(formatted_element)\n    \n    combined_string = ' '.join(replaced_elements)\n    search_result = bool(re.search(pattern, combined_string))\n    \n    return replaced_elements, search_result\n\n# Example usage:\nELEMENTS = [\"abc\", \"def\"]\npattern = \".*\"\nreplaced_elements, result = task_func(ELEMENTS, pattern, 234)"}
{"task_id": "BigCodeBench/343", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b', 'y', 'm']\ndef task_func(df, col, title=None):\n    if not isinstance(df, pd.DataFrame) or df.empty or col not in df.columns:\n        raise ValueError(\"Input df must be a non-empty pandas DataFrame with the specified column.\")\n\n    # Calculate the value counts\n    value_counts = df[col].value_counts()\n\n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(value_counts, labels=value_counts.index, colors=COLORS, autopct='%1.1f%%', startangle=90)\n    ax.set_title(title if title else f'Distribution of {col}')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants for pie chart colors\nCOLORS = ['r', 'g', 'b', 'y', 'm']\n\ndef task_func(df, col, title=None):\n    if not isinstance(df, pd.DataFrame) or df.empty or col not in df.columns:\n        raise ValueError(\"Input df must be a non-empty pandas DataFrame with the specified column.\")\n\n    # Calculate the value counts\n    value_counts = df[col].value_counts()\n\n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(value_counts, labels=value_counts.index, colors=COLORS, autopct='%1.1f%%', startangle=90)\n    ax.set_title(title if title else f'Distribution of {col}')\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'fruit': ['apple', 'banana', 'orange', 'apple', 'banana', 'banana']})\n# ax = task_func(df, 'fruit', title='Fruit Distribution')\n# print(ax.get_title())\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/344", "solution": "import os\nimport shutil\ndef task_func(src_folder, backup_dir):\n    if not os.path.exists(src_folder):\n        raise ValueError(f\"Source folder '{src_folder}' does not exist.\")\n    \n    try:\n        # Create a backup of the source folder\n        shutil.copytree(src_folder, os.path.join(backup_dir, os.path.basename(src_folder)))\n        \n        # Remove the source folder\n        shutil.rmtree(src_folder)\n        \n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False", "raw_solution": "import os\nimport shutil\n\ndef task_func(src_folder, backup_dir):\n    if not os.path.exists(src_folder):\n        raise ValueError(f\"Source folder '{src_folder}' does not exist.\")\n    \n    try:\n        # Create a backup of the source folder\n        shutil.copytree(src_folder, os.path.join(backup_dir, os.path.basename(src_folder)))\n        \n        # Remove the source folder\n        shutil.rmtree(src_folder)\n        \n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n\n# Example usage:\n# import tempfile\n# src_folder = tempfile.mkdtemp()\n# backup_dir = tempfile.mkdtemp()\n# with open(os.path.join(src_folder, 'sample.txt'), 'w') as f:\n#     _ = f.write('This is a sample file.')\n# print(task_func(src_folder, backup_dir))  # Output: True\n```\n"}
{"task_id": "BigCodeBench/345", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col1, col2):\n    if not isinstance(df, pd.DataFrame) or df.empty or col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a non-empty DataFrame containing the specified columns.\")\n    \n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise TypeError(\"Columns must be numeric.\")\n\n    ax = sns.scatterplot(data=df, x=col1, y=col2)\n    plt.show()\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col1, col2):\n    if not isinstance(df, pd.DataFrame) or df.empty or col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a non-empty DataFrame containing the specified columns.\")\n    \n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise TypeError(\"Columns must be numeric.\")\n\n    ax = sns.scatterplot(data=df, x=col1, y=col2)\n    plt.show()\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [2, 4, 6, 8, 10]})\n# plot = task_func(df, 'X', 'Y')\n# print(len(plot.collections[0].get_offsets().data))  # Should print 5\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"The script at {script_path} does not exist.\")\n    \n    try:\n        result = subprocess.run([sys.executable, script_path] + list(args), check=True, capture_output=True)\n        if wait:\n            return result.returncode\n        else:\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output, stderr=e.stderr)", "raw_solution": "import subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"The script at {script_path} does not exist.\")\n    \n    try:\n        result = subprocess.run([sys.executable, script_path] + list(args), check=True, capture_output=True)\n        if wait:\n            return result.returncode\n        else:\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output, stderr=e.stderr)\n```\n"}
{"task_id": "BigCodeBench/347", "solution": "import pandas as pd\nimport re\nimport numpy as np\nPATTERN = r\"([a-fA-F\\d]{32})\"\ndef task_func(df, column):\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in DataFrame\")\n\n    # Extract all matches in the specified column\n    matches = df[column].apply(lambda x: re.findall(PATTERN, x))\n\n    # Flatten the list of lists and count occurrences\n    flat_matches = [item for sublist in matches for item in sublist]\n    counts = pd.Series(flat_matches).value_counts()\n\n    return counts\ndata = pd.DataFrame({\"text\": [\"6f96cfdfe5ccc627cadf24b41725caa4 gorilla\", \"6f96cfdfe5ccc627cadf24b41725caa4 banana\", \"1234567890abcdef1234567890abcdef apple\"]})", "raw_solution": "import pandas as pd\nimport re\nimport numpy as np\n\n# Constants\nPATTERN = r\"([a-fA-F\\d]{32})\"\n\ndef task_func(df, column):\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in DataFrame\")\n\n    # Extract all matches in the specified column\n    matches = df[column].apply(lambda x: re.findall(PATTERN, x))\n\n    # Flatten the list of lists and count occurrences\n    flat_matches = [item for sublist in matches for item in sublist]\n    counts = pd.Series(flat_matches).value_counts()\n\n    return counts\n\n# Example usage:\ndata = pd.DataFrame({\"text\": [\"6f96cfdfe5ccc627cadf24b41725caa4 gorilla\", \"6f96cfdfe5ccc627cadf24b41725caa4 banana\", \"1234567890abcdef1234567890abcdef apple\"]})\ncounts = task_func(data, \"text\")"}
{"task_id": "BigCodeBench/348", "solution": "import subprocess\nimport os\nimport signal\nimport time\ndef task_func(process_name: str) -> int:\n    \"\"\"\n    Kills all running processes with a given name.\n\n    Notes:\n        The function sends a termination signal to all processes with the given name and waits for 1 second.\n        There is no guarantee that all processes will be terminated within this time.\n\n    Parameters:\n    - process_name (str): The name of the processes to be killed.\n\n    Returns:\n        int: The number of processes killed. If no processes were found, it returns 0.\n\n    Requirements:\n    - subprocess\n    - os\n    - signal\n    - time\n\n    Example:\n    >>> pids = task_func('test_name') # Dummy example, should return 0\n    >>> pids\n    0\n    \"\"\"\n    # Find all PIDs of the process with the given name\n    pids = []\n    for line in os.popen(\"ps -A\"):\n        if process_name in line:\n            pids.append(int(line.split()[1]))\n\n    # Send a termination signal to each PID\n    for pid in pids:\n        os.kill(pid, signal.SIGTERM)\n        time.sleep(1)  # Wait for 1 second\n\n    return len(pids)", "raw_solution": "import subprocess\nimport os\nimport signal\nimport time\n\ndef task_func(process_name: str) -> int:\n    \"\"\"\n    Kills all running processes with a given name.\n\n    Notes:\n        The function sends a termination signal to all processes with the given name and waits for 1 second.\n        There is no guarantee that all processes will be terminated within this time.\n\n    Parameters:\n    - process_name (str): The name of the processes to be killed.\n\n    Returns:\n        int: The number of processes killed. If no processes were found, it returns 0.\n\n    Requirements:\n    - subprocess\n    - os\n    - signal\n    - time\n\n    Example:\n    >>> pids = task_func('test_name') # Dummy example, should return 0\n    >>> pids\n    0\n    \"\"\"\n    # Find all PIDs of the process with the given name\n    pids = []\n    for line in os.popen(\"ps -A\"):\n        if process_name in line:\n            pids.append(int(line.split()[1]))\n\n    # Send a termination signal to each PID\n    for pid in pids:\n        os.kill(pid, signal.SIGTERM)\n        time.sleep(1)  # Wait for 1 second\n\n    return len(pids)\n```\n"}
{"task_id": "BigCodeBench/349", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories):\n    data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity = random.randint(1, 100)\n        revenue = random.uniform(10, 100) * quantity\n        data.append([product, category, quantity, revenue])\n\n    report = pd.DataFrame(data, columns=['Product', 'Category', 'Sales Quantity', 'Revenue'])\n\n    return report", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories):\n    data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity = random.randint(1, 100)\n        revenue = random.uniform(10, 100) * quantity\n        data.append([product, category, quantity, revenue])\n\n    report = pd.DataFrame(data, columns=['Product', 'Category', 'Sales Quantity', 'Revenue'])\n\n    return report\n\n# Example usage:\nrandom.seed(0)\nreport = task_func(['Product 1', 'Product 2', 'Product 3'], ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports'])"}
{"task_id": "BigCodeBench/350", "solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\nimport gzip\ndef task_func(src_folder, dst_folder):\n    failed_files = []\n    try:\n        # Ensure destination folder exists\n        os.makedirs(dst_folder, exist_ok=True)\n\n        # Get list of all files in the source folder\n        files = glob(os.path.join(src_folder, '*'))\n\n        for file in files:\n            try:\n                # Compress the file using gzip\n                with open(file, 'rb') as f_in, gzip.open(file + '.gz', 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n\n                # Move the compressed file to the destination folder\n                shutil.move(file + '.gz', dst_folder)\n\n            except Exception as e:\n                failed_files.append(file)\n\n        if failed_files:\n            return {\n                'success': False,\n                'message': 'Some files failed to compress or move.',\n                'failed_files': failed_files\n            }\n\n        return {\n            'success': True,\n            'message': 'All files compressed and moved successfully.',\n            'failed_files': []\n        }\n\n    except Exception as e:\n        return {\n            'success': False,\n            'message': str(e),\n            'failed_files': []\n        }", "raw_solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\nimport gzip\n\ndef task_func(src_folder, dst_folder):\n    failed_files = []\n    try:\n        # Ensure destination folder exists\n        os.makedirs(dst_folder, exist_ok=True)\n\n        # Get list of all files in the source folder\n        files = glob(os.path.join(src_folder, '*'))\n\n        for file in files:\n            try:\n                # Compress the file using gzip\n                with open(file, 'rb') as f_in, gzip.open(file + '.gz', 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n\n                # Move the compressed file to the destination folder\n                shutil.move(file + '.gz', dst_folder)\n\n            except Exception as e:\n                failed_files.append(file)\n\n        if failed_files:\n            return {\n                'success': False,\n                'message': 'Some files failed to compress or move.',\n                'failed_files': failed_files\n            }\n\n        return {\n            'success': True,\n            'message': 'All files compressed and moved successfully.',\n            'failed_files': []\n        }\n\n    except Exception as e:\n        return {\n            'success': False,\n            'message': str(e),\n            'failed_files': []\n        }\n```\n"}
{"task_id": "BigCodeBench/351", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    sales_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = quantity_sold * random.randint(min_value, max_value)\n        sales_data.append([product, category, quantity_sold, revenue])\n\n    columns = ['Product', 'Category', 'Quantity Sold', 'Revenue']\n    report = pd.DataFrame(sales_data, columns=columns)\n\n    return report", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    sales_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = quantity_sold * random.randint(min_value, max_value)\n        sales_data.append([product, category, quantity_sold, revenue])\n\n    columns = ['Product', 'Category', 'Quantity Sold', 'Revenue']\n    report = pd.DataFrame(sales_data, columns=columns)\n\n    return report\n\n# Example usage:\nrandom.seed(0)\nreport = task_func(['Product 1'], ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports'], 100, 100)"}
{"task_id": "BigCodeBench/352", "solution": "import pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text_dict, word_keys, top_k=2):\n    if top_k < 0:\n        raise ValueError(\"top_k must be a non-negative integer\")\n\n    # Convert the Counter object to a DataFrame\n    df = pd.DataFrame(list(text_dict.items()), columns=['Word', 'Frequency'])\n    \n    # Create a DataFrame with the specified word keys\n    df_keys = pd.DataFrame(word_keys, columns=['Word'])\n    \n    # Merge the two DataFrames on 'Word' to get the frequencies\n    df_merged = pd.merge(df_keys, df, on='Word', how='left').fillna(0)\n    \n    # Get the top_k most common words\n    top_k_words = df_merged.nlargest(top_k, 'Frequency')\n    \n    # Create a bar chart\n    ax = top_k_words.plot(kind='bar', x='Word', y='Frequency', legend=False)\n    ax.set_title('Top {} Words Frequency'.format(top_k))\n    ax.set_ylabel('Frequency')\n    \n    # Convert the result to a dictionary\n    frequencies = dict(zip(top_k_words['Word'], top_k_words['Frequency']))\n    \n    return ax, frequencies", "raw_solution": "import pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text_dict, word_keys, top_k=2):\n    if top_k < 0:\n        raise ValueError(\"top_k must be a non-negative integer\")\n\n    # Convert the Counter object to a DataFrame\n    df = pd.DataFrame(list(text_dict.items()), columns=['Word', 'Frequency'])\n    \n    # Create a DataFrame with the specified word keys\n    df_keys = pd.DataFrame(word_keys, columns=['Word'])\n    \n    # Merge the two DataFrames on 'Word' to get the frequencies\n    df_merged = pd.merge(df_keys, df, on='Word', how='left').fillna(0)\n    \n    # Get the top_k most common words\n    top_k_words = df_merged.nlargest(top_k, 'Frequency')\n    \n    # Create a bar chart\n    ax = top_k_words.plot(kind='bar', x='Word', y='Frequency', legend=False)\n    ax.set_title('Top {} Words Frequency'.format(top_k))\n    ax.set_ylabel('Frequency')\n    \n    # Convert the result to a dictionary\n    frequencies = dict(zip(top_k_words['Word'], top_k_words['Frequency']))\n    \n    return ax, frequencies\n\n# Example usage:\n# text_dict = Counter({'the': 3, 'be': 1, 'to': 1, 'that': 2, 'and': 1, 'a': 1, 'in': 1, 'have': 1, 'I': 1})\n# word_keys = ['the', 'and', 'I']\n# ax, frequencies = task_func(text_dict, word_keys, 3)\n# print(type(ax))\n# print(frequencies)\n```\n"}
{"task_id": "BigCodeBench/353", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    sales_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = random.randint(min_value, max_value)\n        total_revenue = quantity_sold * revenue\n        sales_data.append([product, category, quantity_sold, revenue, total_revenue])\n\n    columns = ['Product', 'Category', 'Quantity Sold', 'Revenue', 'Total Revenue']\n    report = pd.DataFrame(sales_data, columns=columns)\n\n    return report", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    sales_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = random.randint(min_value, max_value)\n        total_revenue = quantity_sold * revenue\n        sales_data.append([product, category, quantity_sold, revenue, total_revenue])\n\n    columns = ['Product', 'Category', 'Quantity Sold', 'Revenue', 'Total Revenue']\n    report = pd.DataFrame(sales_data, columns=columns)\n\n    return report\n\n# Example usage:\nrandom.seed(0)\nreport = task_func(['Product 1', 'Product 2', 'Product 3'], ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports'], 100, 100)"}
{"task_id": "BigCodeBench/354", "solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(sentences_dict, word_keys):\n    # Initialize a counter to count occurrences of each word\n    word_counter = collections.Counter()\n\n    # Iterate over each sentence in the dictionary\n    for sentence in sentences_dict.values():\n        # Split the sentence into words and update the counter\n        words = sentence.split()\n        word_counter.update(words)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(word_counter.items(), columns=['Word', 'Frequency'])\n\n    # Filter the DataFrame to include only the specified words\n    df = df[df['Word'].isin(word_keys)]\n\n    # Plot the frequencies of the specified words\n    ax = df.plot(x='Word', y='Frequency', kind='bar', legend=False)\n    ax.set_title('Word Frequencies')\n    ax.set_ylabel('Frequency')\n\n    return ax\nsentences_dict = {'Sentence1': 'the quick brown fox', 'Sentence2': 'jumps over the lazy dog', 'Sentence3': 'the dog is brown'}\nword_keys = ['the', 'dog']", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nWORDS = ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I']\n\ndef task_func(sentences_dict, word_keys):\n    # Initialize a counter to count occurrences of each word\n    word_counter = collections.Counter()\n\n    # Iterate over each sentence in the dictionary\n    for sentence in sentences_dict.values():\n        # Split the sentence into words and update the counter\n        words = sentence.split()\n        word_counter.update(words)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(word_counter.items(), columns=['Word', 'Frequency'])\n\n    # Filter the DataFrame to include only the specified words\n    df = df[df['Word'].isin(word_keys)]\n\n    # Plot the frequencies of the specified words\n    ax = df.plot(x='Word', y='Frequency', kind='bar', legend=False)\n    ax.set_title('Word Frequencies')\n    ax.set_ylabel('Frequency')\n\n    return ax\n\n# Example usage:\nsentences_dict = {'Sentence1': 'the quick brown fox', 'Sentence2': 'jumps over the lazy dog', 'Sentence3': 'the dog is brown'}\nword_keys = ['the', 'dog']\ntask_func(sentences_dict, word_keys)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/355", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\ndef task_func(amplitude, frequency, time):\n    # Generate Hann window\n    window = get_window('hann', len(time))\n    \n    # Generate complex wave\n    wave = amplitude * np.exp(1j * 2 * np.pi * frequency * time) * window\n    \n    # Plot the complex wave\n    fig, ax = plt.subplots()\n    ax.plot(time, np.real(wave), label='Real part')\n    ax.plot(time, np.imag(wave), label='Imaginary part')\n    ax.set_title('Hann window ke saath complex wave')\n    ax.set_xlabel('time')\n    ax.set_ylabel('amplitude')\n    ax.legend()\n    \n    return wave, fig, ax", "raw_solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\n\ndef task_func(amplitude, frequency, time):\n    # Generate Hann window\n    window = get_window('hann', len(time))\n    \n    # Generate complex wave\n    wave = amplitude * np.exp(1j * 2 * np.pi * frequency * time) * window\n    \n    # Plot the complex wave\n    fig, ax = plt.subplots()\n    ax.plot(time, np.real(wave), label='Real part')\n    ax.plot(time, np.imag(wave), label='Imaginary part')\n    ax.set_title('Hann window ke saath complex wave')\n    ax.set_xlabel('time')\n    ax.set_ylabel('amplitude')\n    ax.legend()\n    \n    return wave, fig, ax\n\n# Example usage:\n# wave, fig, ax = task_func(1, 1, np.linspace(0, 1, 10, endpoint=False))\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/356", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\ndef task_func(x, y):\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"x and y must be numpy.ndarray objects.\")\n    if x.size != y.size:\n        raise ValueError(\"x and y must have the same length.\")\n\n    X, Y = np.meshgrid(x, y)\n    Z = np.vectorize(lambda x, y: cmath.phase(complex(x, y)))(X, Y)\n\n    fig, ax = plt.subplots()\n    c = ax.contourf(X, Y, Z, cmap='hsv')\n    fig.colorbar(c, ax=ax)\n    ax.set_title('Phase Plot')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n\n    return ax, Z", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\n\ndef task_func(x, y):\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"x and y must be numpy.ndarray objects.\")\n    if x.size != y.size:\n        raise ValueError(\"x and y must have the same length.\")\n\n    X, Y = np.meshgrid(x, y)\n    Z = np.vectorize(lambda x, y: cmath.phase(complex(x, y)))(X, Y)\n\n    fig, ax = plt.subplots()\n    c = ax.contourf(X, Y, Z, cmap='hsv')\n    fig.colorbar(c, ax=ax)\n    ax.set_title('Phase Plot')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n\n    return ax, Z\n\n# Example usage:\n# ax, Z = task_func(np.array([1, 2, 3]), np.array([1, 2, 3]))\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/357", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy.ndarray\")\n\n    real_part = norm.pdf(x, loc=0, scale=1)\n    imaginary_part = norm.pdf(x, loc=2, scale=2)\n\n    result = real_part + 1j * imaginary_part\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, np.real(result), label='Real Part')\n    plt.plot(x, np.imag(result), label='Imaginary Part')\n    plt.title('Complex Gaussian Distribution')\n    plt.xlabel('x')\n    plt.ylabel('Probability Density')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return result\nX = np.linspace(-10, 10, 1000)", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy.ndarray\")\n\n    real_part = norm.pdf(x, loc=0, scale=1)\n    imaginary_part = norm.pdf(x, loc=2, scale=2)\n\n    result = real_part + 1j * imaginary_part\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, np.real(result), label='Real Part')\n    plt.plot(x, np.imag(result), label='Imaginary Part')\n    plt.title('Complex Gaussian Distribution')\n    plt.xlabel('x')\n    plt.ylabel('Probability Density')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return result\n\n# Example usage:\nX = np.linspace(-10, 10, 1000)\nresult = task_func(X)"}
{"task_id": "BigCodeBench/358", "solution": "import itertools\nimport json\ndef task_func(json_list, r):\n    try:\n        data = json.loads(json_list)\n        number_list = data['number_list']\n    except (json.JSONDecodeError, KeyError):\n        raise Exception(\"Invalid JSON or 'number_list' key missing\")\n\n    if not number_list:\n        raise Exception(\"Number list is empty\")\n\n    combinations = list(itertools.combinations(number_list, r))\n    return combinations", "raw_solution": "import itertools\nimport json\n\n\ndef task_func(json_list, r):\n    try:\n        data = json.loads(json_list)\n        number_list = data['number_list']\n    except (json.JSONDecodeError, KeyError):\n        raise Exception(\"Invalid JSON or 'number_list' key missing\")\n\n    if not number_list:\n        raise Exception(\"Number list is empty\")\n\n    combinations = list(itertools.combinations(number_list, r))\n    return combinations\n\n\n# Example usage:\ncombinations = task_func('{\"number_list\": [1, 2, 3, 4, 5]}', 3)"}
{"task_id": "BigCodeBench/359", "solution": "from scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(data_dict, data_keys):\n    # Extract data from the dictionary\n    x = data_dict[data_keys[0]]\n    y = data_dict[data_keys[1]]\n    \n    # Calculate correlation\n    correlation, _ = stats.pearsonr(x, y)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y, label=f'Correlation: {correlation:.4f}')\n    ax.set_xlabel(data_keys[0])\n    ax.set_ylabel(data_keys[1])\n    ax.set_title('Scatter Plot of Data Series')\n    ax.legend()\n    \n    return correlation, ax", "raw_solution": "from scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict, data_keys):\n    # Extract data from the dictionary\n    x = data_dict[data_keys[0]]\n    y = data_dict[data_keys[1]]\n    \n    # Calculate correlation\n    correlation, _ = stats.pearsonr(x, y)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y, label=f'Correlation: {correlation:.4f}')\n    ax.set_xlabel(data_keys[0])\n    ax.set_ylabel(data_keys[1])\n    ax.set_title('Scatter Plot of Data Series')\n    ax.legend()\n    \n    return correlation, ax\n```\n"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n    \n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file at {file_location}.\")\n\n    stats = {}\n    for column in df.columns:\n        mean = np.mean(df[column])\n        std = np.std(df[column])\n        stats[column] = {'mean': mean, 'std': std}\n\n    fig, ax = plt.subplots()\n    columns = list(stats.keys())\n    means = [stats[col]['mean'] for col in columns]\n    stds = [stats[col]['std'] for col in columns]\n\n    ax.bar(columns, means, yerr=stds, capsize=5)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n\n    return stats, fig", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n    \n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file at {file_location}.\")\n\n    stats = {}\n    for column in df.columns:\n        mean = np.mean(df[column])\n        std = np.std(df[column])\n        stats[column] = {'mean': mean, 'std': std}\n\n    fig, ax = plt.subplots()\n    columns = list(stats.keys())\n    means = [stats[col]['mean'] for col in columns]\n    stds = [stats[col]['std'] for col in columns]\n\n    ax.bar(columns, means, yerr=stds, capsize=5)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n\n    return stats, fig\n```\n"}
{"task_id": "BigCodeBench/361", "solution": "import pandas as pd\nimport logging\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    # Check if excel file exists\n    if not os.path.exists(excel_file_location):\n        raise FileNotFoundError(f\"The file {excel_file_location} does not exist.\")\n    \n    # Load the excel file\n    try:\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"Sheet {sheet_name} not found in the file {excel_file_location}.\") from e\n\n    # Save the dataframe to a csv file\n    df.to_csv(csv_file_location, index=False)\n\n    # Calculate the sum of each column\n    column_sums = df.sum().to_dict()\n\n    # Log the result\n    logging.info(f\"Column sums: {column_sums}\")\n\n    return column_sums", "raw_solution": "import pandas as pd\nimport logging\n\n# Set up basic configuration for logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    # Check if excel file exists\n    if not os.path.exists(excel_file_location):\n        raise FileNotFoundError(f\"The file {excel_file_location} does not exist.\")\n    \n    # Load the excel file\n    try:\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"Sheet {sheet_name} not found in the file {excel_file_location}.\") from e\n\n    # Save the dataframe to a csv file\n    df.to_csv(csv_file_location, index=False)\n\n    # Calculate the sum of each column\n    column_sums = df.sum().to_dict()\n\n    # Log the result\n    logging.info(f\"Column sums: {column_sums}\")\n\n    return column_sums\n```\n"}
{"task_id": "BigCodeBench/362", "solution": "import pandas as pd\nimport os\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    if not os.path.exists(original_file_location):\n        raise FileNotFoundError(f\"The file {original_file_location} does not exist.\")\n    \n    try:\n        df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file {original_file_location}.\")\n\n    df.to_excel(new_file_location, sheet_name=sheet_name, index=False)\n\n    return df", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    if not os.path.exists(original_file_location):\n        raise FileNotFoundError(f\"The file {original_file_location} does not exist.\")\n    \n    try:\n        df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file {original_file_location}.\")\n\n    df.to_excel(new_file_location, sheet_name=sheet_name, index=False)\n\n    return df\n\n# Example usage:\n# Assuming you have a function create_dummy_excel that creates a dummy excel file with a sheet named 'Sheet1'\n# create_dummy_excel('test.xlsx', 'Sheet1')\n# df = task_func('test.xlsx', 'new_test.xlsx', 'Sheet1')\n# os.remove('test.xlsx')\n# os.remove('new_test.xlsx')\n```\n"}
{"task_id": "BigCodeBench/363", "solution": "from multiprocessing import Pool\nimport math\ndef calculate_factorial(number: int) -> tuple:\n    return number, math.factorial(number)\ndef task_func(numbers: list) -> dict:\n    if not all(isinstance(n, int) and n >= 0 for n in numbers):\n        raise ValueError(\"All elements in the list must be non-negative integers.\")\n    \n    with Pool() as pool:\n        results = pool.map(calculate_factorial, numbers)\n    \n    return dict(results)", "raw_solution": "from multiprocessing import Pool\nimport math\n\ndef calculate_factorial(number: int) -> tuple:\n    return number, math.factorial(number)\n    \ndef task_func(numbers: list) -> dict:\n    if not all(isinstance(n, int) and n >= 0 for n in numbers):\n        raise ValueError(\"All elements in the list must be non-negative integers.\")\n    \n    with Pool() as pool:\n        results = pool.map(calculate_factorial, numbers)\n    \n    return dict(results)\n\n# Example usage:\n# factorials = task_func([5, 6, 7, 8, 9])\n# print(factorials)  # Output: {5: 120, 6: 720, 7: 5040, 8: 40320, 9: 362880}\n```\nThis script uses the `multiprocessing.Pool` to parallelize the calculation of factorials. The `calculate_factorial` function is used to calculate the factorial of a number and returns a tuple containing the number and its factorial. The `task_func` function takes a list of numbers, checks if all elements are non-negative integers, and then calculates the factorials in parallel using the `Pool.map` method. The results are returned as a dictionary.\n"}
{"task_id": "BigCodeBench/364", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Splitting the data into training and testing sets\n    X = df[FEATURES]\n    y = df[TARGET]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Training the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Splitting the data into training and testing sets\n    X = df[FEATURES]\n    y = df[TARGET]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Training the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model\n\n# Example usage"}
{"task_id": "BigCodeBench/365", "solution": "from collections import Counter\nimport json\nimport random\nimport tempfile\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\ndef task_func(n, file_name, seed=77):\n    \"\"\"\n    WORDS name ek sthir list se random roop se chune gaye 'n' words ki ek number ke saath ek json file banaen.\n\n    Parameters:\n    - n (int): list se chunne ke liye words ki number.\n    - file_name (str): utpann ki jaane wali json file ka name.\n    - seed (int, Optional): random number generator ke liye seed. defaults roop se 77.\n\n    Returns:\n        str: banai gayi json file ka name.\n\n    Requirements:\n    - collections\n    - json\n    - random\n\n    Example:\n    >>> import tempfile\n    >>> temp_dir = tempfile.mkdtemp()\n    >>> file_name = temp_dir + \"/word_counts.json\"\n    >>> task_func(5, file_name, 29).endswith('word_counts.json')\n    True\n\n    \"\"\"\n    random.seed(seed)\n    selected_words = random.sample(WORDS, n)\n    word_counts = Counter(selected_words)\n    \n    with open(file_name, 'w') as f:\n        json.dump(word_counts, f)\n    \n    return file_name", "raw_solution": "from collections import Counter\nimport json\nimport random\nimport tempfile\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\n\ndef task_func(n, file_name, seed=77):\n    \"\"\"\n    WORDS name ek sthir list se random roop se chune gaye 'n' words ki ek number ke saath ek json file banaen.\n\n    Parameters:\n    - n (int): list se chunne ke liye words ki number.\n    - file_name (str): utpann ki jaane wali json file ka name.\n    - seed (int, Optional): random number generator ke liye seed. defaults roop se 77.\n\n    Returns:\n        str: banai gayi json file ka name.\n\n    Requirements:\n    - collections\n    - json\n    - random\n\n    Example:\n    >>> import tempfile\n    >>> temp_dir = tempfile.mkdtemp()\n    >>> file_name = temp_dir + \"/word_counts.json\"\n    >>> task_func(5, file_name, 29).endswith('word_counts.json')\n    True\n\n    \"\"\"\n    random.seed(seed)\n    selected_words = random.sample(WORDS, n)\n    word_counts = Counter(selected_words)\n    \n    with open(file_name, 'w') as f:\n        json.dump(word_counts, f)\n    \n    return file_name\n```\n"}
{"task_id": "BigCodeBench/366", "solution": "import matplotlib.pyplot as plt\nimport random\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\ndef task_func(number_list, bins):\n    \"\"\"\n    numbers ki ek list ka histogram subplot banaen.\n\n    Notes:\n        Yah function Matplotlib ka upayog karke ek histogram plot banata hai.\n        plot ka title 'histogram' par set hai.\n        x-axis label 'number' par set hai.\n        y-axis label 'frequency' par set hai.\n        histogram bars ka rang purvanirdharit colors ke ek set se randomly chuna jaata hai.\n\n    Parameters:\n    - number_list (list): plot kiye jaane wale numeric values ki ek list.\n    - bins (int or sequence): Yadi ek int hai, to histogram bin ki sankhya.\n    - Yadi ek sequence hai, to bin edges paribhashit karta hai.\n\n    Returns:\n        matplotlib.axes._axes.Axes: histogram plot ka pratinidhitva karne wala axis object.\n\n    Requirements:\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> random.seed(0)\n    >>> number_list = [1, 2, 2, 3, 3, 3, 4, 4, 5]\n    >>> bins = 5\n    >>> ax = task_func(number_list, bins)\n    >>> ax.patches[0].get_height()\n    1.0\n    >>> ax.patches[2].get_height() > ax.patches[0].get_height()\n    True\n    >>> plt.close()\n\n    \"\"\"\n    # Generate a random color from the COLORS list\n    random_color = random.choice(COLORS)\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(number_list, bins=bins, color=random_color)\n    \n    # Set labels and title\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram')\n    \n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\ndef task_func(number_list, bins):\n    \"\"\"\n    numbers ki ek list ka histogram subplot banaen.\n\n    Notes:\n        Yah function Matplotlib ka upayog karke ek histogram plot banata hai.\n        plot ka title 'histogram' par set hai.\n        x-axis label 'number' par set hai.\n        y-axis label 'frequency' par set hai.\n        histogram bars ka rang purvanirdharit colors ke ek set se randomly chuna jaata hai.\n\n    Parameters:\n    - number_list (list): plot kiye jaane wale numeric values ki ek list.\n    - bins (int or sequence): Yadi ek int hai, to histogram bin ki sankhya.\n    - Yadi ek sequence hai, to bin edges paribhashit karta hai.\n\n    Returns:\n        matplotlib.axes._axes.Axes: histogram plot ka pratinidhitva karne wala axis object.\n\n    Requirements:\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> random.seed(0)\n    >>> number_list = [1, 2, 2, 3, 3, 3, 4, 4, 5]\n    >>> bins = 5\n    >>> ax = task_func(number_list, bins)\n    >>> ax.patches[0].get_height()\n    1.0\n    >>> ax.patches[2].get_height() > ax.patches[0].get_height()\n    True\n    >>> plt.close()\n\n    \"\"\"\n    # Generate a random color from the COLORS list\n    random_color = random.choice(COLORS)\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(number_list, bins=bins, color=random_color)\n    \n    # Set labels and title\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n\n    # Count activities by day of the week\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime('%A')\n        activity_counts[day_of_week] += 1\n\n    # Plot the activity counts\n    days = list(activity_counts.keys())\n    counts = list(activity_counts.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity Counts')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n\n    # Count activities by day of the week\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime('%A')\n        activity_counts[day_of_week] += 1\n\n    # Plot the activity counts\n    days = list(activity_counts.keys())\n    counts = list(activity_counts.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity Counts')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:\n# ax = task_func([datetime(2023, 10, 25), datetime(2023, 10, 26), datetime(2023, 10, 27)])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    random.seed(seed)\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    if not files:\n        raise FileNotFoundError(\"No files found in the source directory.\")\n\n    selected_file = random.choice(files)\n    shutil.copy(os.path.join(src_dir, selected_file), dest_dir)\n    return selected_file", "raw_solution": "import os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    random.seed(seed)\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    if not files:\n        raise FileNotFoundError(\"No files found in the source directory.\")\n\n    selected_file = random.choice(files)\n    shutil.copy(os.path.join(src_dir, selected_file), dest_dir)\n    return selected_file\n```\n"}
{"task_id": "BigCodeBench/369", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    '''\n    Draw a histogram of the given array with a Gaussian fit.\n\n    Parameters:\n    l (numpy array): The input array.\n\n    Returns:\n    ax (matplotlib.axes._axes.Axes): Axes object with the plot.\n\n    Note:\n    - This function use \"Fit results: mu = {mean},  std = {standard deviation}\" as the title of the plot, \n    where the values are rounded to two decimal points.\n\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> l = np.array([5, 5, 5, 5, 5])\n    >>> ax = task_func(l)\n    >>> print(ax.get_title())\n    Fit results: mu = 5.00,  std = 0.00\n    >>> plt.close()\n    '''\n    mu, std = stats.norm.fit(l)\n    fig, ax = plt.subplots()\n    ax.hist(l, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n\n    return ax", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(l):\n    '''\n    Draw a histogram of the given array with a Gaussian fit.\n\n    Parameters:\n    l (numpy array): The input array.\n\n    Returns:\n    ax (matplotlib.axes._axes.Axes): Axes object with the plot.\n\n    Note:\n    - This function use \"Fit results: mu = {mean},  std = {standard deviation}\" as the title of the plot, \n    where the values are rounded to two decimal points.\n\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> l = np.array([5, 5, 5, 5, 5])\n    >>> ax = task_func(l)\n    >>> print(ax.get_title())\n    Fit results: mu = 5.00,  std = 0.00\n    >>> plt.close()\n    '''\n    mu, std = stats.norm.fit(l)\n    fig, ax = plt.subplots()\n    ax.hist(l, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/370", "solution": "import os\nimport re\nimport json\nimport glob\ndef task_func(directory_path: str) -> list:\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n    \n    json_files = glob.glob(os.path.join(directory_path, '*.json'))\n    processed_files = []\n\n    for file_path in json_files:\n        with open(file_path, 'r') as file:\n            content = file.read()\n            escaped_content = re.sub(r'\"', r'\\\\\\\\\"', content)\n            processed_files.append(escaped_content)\n    \n    return processed_files", "raw_solution": "import os\nimport re\nimport json\nimport glob\n\ndef task_func(directory_path: str) -> list:\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n    \n    json_files = glob.glob(os.path.join(directory_path, '*.json'))\n    processed_files = []\n\n    for file_path in json_files:\n        with open(file_path, 'r') as file:\n            content = file.read()\n            escaped_content = re.sub(r'\"', r'\\\\\\\\\"', content)\n            processed_files.append(escaped_content)\n    \n    return processed_files\n```\n"}
{"task_id": "BigCodeBench/371", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(l):\n    \"\"\"\n    input field ko [0, 1] range tak scale karein and ise DataFrame ke roop mein pradarshit karein.\n\n    Notes:\n        return DataFrame 'Scaled Values' ka use column naam ke roop mein karta hai.\n\n    Parameters:\n    - l (numpy array): The input array.\n\n    Returns:\n        DataFrame: scale kiye gaye array ka ek pandas DataFrame.\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing\n    - pandas\n\n    Example:\n    >>> import numpy as np\n    >>> l = np.array([10, 20, 30, 40, 50])\n    >>> df = task_func(l)\n    >>> print(int(df.iloc[0]['Scaled Values']))\n    0\n\n    \"\"\"\n    # Convert the input array to a DataFrame\n    df = pd.DataFrame(l, columns=['Original Values'])\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Scale the 'Original Values' column\n    df['Scaled Values'] = scaler.fit_transform(df[['Original Values']])\n\n    return df", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(l):\n    \"\"\"\n    input field ko [0, 1] range tak scale karein and ise DataFrame ke roop mein pradarshit karein.\n\n    Notes:\n        return DataFrame 'Scaled Values' ka use column naam ke roop mein karta hai.\n\n    Parameters:\n    - l (numpy array): The input array.\n\n    Returns:\n        DataFrame: scale kiye gaye array ka ek pandas DataFrame.\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing\n    - pandas\n\n    Example:\n    >>> import numpy as np\n    >>> l = np.array([10, 20, 30, 40, 50])\n    >>> df = task_func(l)\n    >>> print(int(df.iloc[0]['Scaled Values']))\n    0\n\n    \"\"\"\n    # Convert the input array to a DataFrame\n    df = pd.DataFrame(l, columns=['Original Values'])\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Scale the 'Original Values' column\n    df['Scaled Values'] = scaler.fit_transform(df[['Original Values']])\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/372", "solution": "import re\nimport glob\nfrom docx import Document\ndef task_func(directory_path: str) -> int:\n    # Find all .docx files in the given directory\n    docx_files = glob.glob(f\"{directory_path}/*.docx\")\n    modified_files_count = 0\n\n    for file_path in docx_files:\n        # Load the document\n        doc = Document(file_path)\n        \n        # Iterate over each paragraph in the document\n        for para in doc.paragraphs:\n            # Replace double quotes with double backslashes\n            new_text = re.sub(r'\"', r'\\\"', para.text)\n            \n            # Update the paragraph text\n            para.text = new_text\n        \n        # Save the modified document\n        doc.save(file_path)\n        modified_files_count += 1\n\n    return modified_files_count", "raw_solution": "import re\nimport glob\nfrom docx import Document\n\ndef task_func(directory_path: str) -> int:\n    # Find all .docx files in the given directory\n    docx_files = glob.glob(f\"{directory_path}/*.docx\")\n    modified_files_count = 0\n\n    for file_path in docx_files:\n        # Load the document\n        doc = Document(file_path)\n        \n        # Iterate over each paragraph in the document\n        for para in doc.paragraphs:\n            # Replace double quotes with double backslashes\n            new_text = re.sub(r'\"', r'\\\"', para.text)\n            \n            # Update the paragraph text\n            para.text = new_text\n        \n        # Save the modified document\n        doc.save(file_path)\n        modified_files_count += 1\n\n    return modified_files_count\n```\n"}
{"task_id": "BigCodeBench/373", "solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\ndef task_func(l, x_data, plot=False):\n    def func(x, a, b):\n        return a * x + b\n\n    params, _ = curve_fit(func, x_data, l)\n    fitted_values = func(x_data, *params)\n\n    if plot:\n        fig, ax = plt.subplots()\n        ax.scatter(x_data, l, label='Data')\n        ax.plot(x_data, fitted_values, label='Fitted Line', color='red')\n        ax.set_xlabel('X Data')\n        ax.set_ylabel('Y Data')\n        ax.set_title('Data and Fitted Line')\n        ax.legend()\n        plt.show()\n        return params, fitted_values, ax\n\n    return params, fitted_values", "raw_solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef task_func(l, x_data, plot=False):\n    def func(x, a, b):\n        return a * x + b\n\n    params, _ = curve_fit(func, x_data, l)\n    fitted_values = func(x_data, *params)\n\n    if plot:\n        fig, ax = plt.subplots()\n        ax.scatter(x_data, l, label='Data')\n        ax.plot(x_data, fitted_values, label='Fitted Line', color='red')\n        ax.set_xlabel('X Data')\n        ax.set_ylabel('Y Data')\n        ax.set_title('Data and Fitted Line')\n        ax.legend()\n        plt.show()\n        return params, fitted_values, ax\n\n    return params, fitted_values\n```\n"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Check if the directory exists\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n    \n    # Get all .xlsx files in the directory\n    excel_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Initialize a counter for modified files\n    modified_files_count = 0\n    \n    for file in excel_files:\n        # Load the workbook\n        workbook = load_workbook(file)\n        \n        # Iterate through all sheets\n        for sheet in workbook.worksheets:\n            # Iterate through all cells\n            for row in sheet.iter_rows():\n                for cell in row:\n                    # Check if the cell value is a string\n                    if isinstance(cell.value, str):\n                        # Replace double quotes with double backslashes\n                        cell.value = re.sub(r'\"', r'\\\\\\\\\"', cell.value)\n                        modified_files_count += 1\n        \n        # Save the workbook\n        workbook.save(file)\n    \n    return modified_files_count", "raw_solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Check if the directory exists\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n    \n    # Get all .xlsx files in the directory\n    excel_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Initialize a counter for modified files\n    modified_files_count = 0\n    \n    for file in excel_files:\n        # Load the workbook\n        workbook = load_workbook(file)\n        \n        # Iterate through all sheets\n        for sheet in workbook.worksheets:\n            # Iterate through all cells\n            for row in sheet.iter_rows():\n                for cell in row:\n                    # Check if the cell value is a string\n                    if isinstance(cell.value, str):\n                        # Replace double quotes with double backslashes\n                        cell.value = re.sub(r'\"', r'\\\\\\\\\"', cell.value)\n                        modified_files_count += 1\n        \n        # Save the workbook\n        workbook.save(file)\n    \n    return modified_files_count\n```\n"}
{"task_id": "BigCodeBench/375", "solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(l):\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(l)\n    \n    fig, ax = plt.subplots()\n    ax.scatter(principalComponents[:, 0], principalComponents[:, 1])\n    ax.set_title('PCA Result')\n    ax.set_xlabel('First Principal Component')\n    ax.set_ylabel('Second Principal Component')\n    \n    return ax\nl = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])", "raw_solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(l):\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(l)\n    \n    fig, ax = plt.subplots()\n    ax.scatter(principalComponents[:, 0], principalComponents[:, 1])\n    ax.set_title('PCA Result')\n    ax.set_xlabel('First Principal Component')\n    ax.set_ylabel('Second Principal Component')\n    \n    return ax\n\n# Example usage:\nimport numpy as np\nl = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\nax = task_func(l)"}
{"task_id": "BigCodeBench/376", "solution": "import nltk\nimport re\nfrom collections import Counter\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Remove stopwords\n    filtered_words = [word for word in words if word not in STOPWORDS]\n\n    # Count word frequencies\n    word_counts = Counter(filtered_words)\n\n    return dict(word_counts)", "raw_solution": "import nltk\nimport re\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Remove stopwords\n    filtered_words = [word for word in words if word not in STOPWORDS]\n\n    # Count word frequencies\n    word_counts = Counter(filtered_words)\n\n    return dict(word_counts)\n\n# Example usage:\n# print(task_func('This is a sample text. This text is for testing.'))\n```\n"}
{"task_id": "BigCodeBench/377", "solution": "from texttable import Texttable\nimport os\nimport psutil\ndef task_func():\n    # Create a table\n    table = Texttable()\n    table.add_row([\"Metric\", \"Value\"])\n\n    # Get system information\n    cpu_usage = psutil.cpu_percent(interval=1)\n    memory_usage = psutil.virtual_memory().percent\n    disk_usage = psutil.disk_usage(os.sep).percent\n\n    # Add information to the table\n    table.add_row([\"CPU Usage (%)\", cpu_usage])\n    table.add_row([\"Memory Usage (%)\", memory_usage])\n    table.add_row([\"Disk Usage (%)\", disk_usage])\n\n    # Return the table as a string\n    return table.draw()", "raw_solution": "from texttable import Texttable\nimport os\nimport psutil\n\ndef task_func():\n    # Create a table\n    table = Texttable()\n    table.add_row([\"Metric\", \"Value\"])\n\n    # Get system information\n    cpu_usage = psutil.cpu_percent(interval=1)\n    memory_usage = psutil.virtual_memory().percent\n    disk_usage = psutil.disk_usage(os.sep).percent\n\n    # Add information to the table\n    table.add_row([\"CPU Usage (%)\", cpu_usage])\n    table.add_row([\"Memory Usage (%)\", memory_usage])\n    table.add_row([\"Disk Usage (%)\", disk_usage])\n\n    # Return the table as a string\n    return table.draw()\n\n# Example usage"}
{"task_id": "BigCodeBench/378", "solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\ndef task_func(data_dir='./data/'):\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"The directory {data_dir} does not exist.\")\n    \n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n    \n    if not csv_files:\n        raise ValueError(f\"No CSV files found in the directory {data_dir}.\")\n    \n    table = Texttable()\n    table.add_row([\"File\", \"Rows\", \"Columns\"])\n    \n    for csv_file in csv_files:\n        try:\n            df = pd.read_csv(csv_file)\n            table.add_row([os.path.basename(csv_file), df.shape[0], df.shape[1]])\n        except pd.errors.EmptyDataError:\n            raise pd.errors.EmptyDataError(f\"The file {csv_file} is empty.\")\n    \n    return table.draw()", "raw_solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\n\ndef task_func(data_dir='./data/'):\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"The directory {data_dir} does not exist.\")\n    \n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n    \n    if not csv_files:\n        raise ValueError(f\"No CSV files found in the directory {data_dir}.\")\n    \n    table = Texttable()\n    table.add_row([\"File\", \"Rows\", \"Columns\"])\n    \n    for csv_file in csv_files:\n        try:\n            df = pd.read_csv(csv_file)\n            table.add_row([os.path.basename(csv_file), df.shape[0], df.shape[1]])\n        except pd.errors.EmptyDataError:\n            raise pd.errors.EmptyDataError(f\"The file {csv_file} is empty.\")\n    \n    return table.draw()\n```\n"}
{"task_id": "BigCodeBench/379", "solution": "import pandas as pd\nimport numpy as np\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length):\n    \"\"\"\n    Create a pandas DataFrame with random data of specified length.\n\n    Parameters:\n    - length (int): The length of the DataFrame to be generated.\n\n    Returns:\n    - DataFrame: A pandas DataFrame with random data.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    >>> np.random.seed(0)\n    >>> df = task_func(5)\n    >>> df.shape\n    (5, 5)\n    \"\"\"\n    # Generate random data\n    data = np.random.rand(length, len(COLUMNS))\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(length):\n    \"\"\"\n    Create a pandas DataFrame with random data of specified length.\n\n    Parameters:\n    - length (int): The length of the DataFrame to be generated.\n\n    Returns:\n    - DataFrame: A pandas DataFrame with random data.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    >>> np.random.seed(0)\n    >>> df = task_func(5)\n    >>> df.shape\n    (5, 5)\n    \"\"\"\n    # Generate random data\n    data = np.random.rand(length, len(COLUMNS))\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/380", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    # List all files in the directory\n    files = os.listdir(directory)\n    \n    # Regex to match file extensions\n    extension_regex = re.compile(r'\\.(\\w+)$')\n    \n    # Set to store unique file extensions\n    extensions = set()\n    \n    for file in files:\n        match = extension_regex.search(file)\n        if match:\n            extensions.add(match.group(1))\n    \n    # Create a new directory for each unique file extension\n    for ext in extensions:\n        ext_dir = os.path.join(directory, ext)\n        os.makedirs(ext_dir, exist_ok=True)\n        \n        # Move files with the current extension to the corresponding directory\n        for file in files:\n            if file.endswith(ext):\n                shutil.move(os.path.join(directory, file), os.path.join(ext_dir, file))", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(directory):\n    # List all files in the directory\n    files = os.listdir(directory)\n    \n    # Regex to match file extensions\n    extension_regex = re.compile(r'\\.(\\w+)$')\n    \n    # Set to store unique file extensions\n    extensions = set()\n    \n    for file in files:\n        match = extension_regex.search(file)\n        if match:\n            extensions.add(match.group(1))\n    \n    # Create a new directory for each unique file extension\n    for ext in extensions:\n        ext_dir = os.path.join(directory, ext)\n        os.makedirs(ext_dir, exist_ok=True)\n        \n        # Move files with the current extension to the corresponding directory\n        for file in files:\n            if file.endswith(ext):\n                shutil.move(os.path.join(directory, file), os.path.join(ext_dir, file))\n\n# Example usage:\n# import tempfile\n# temp_dir = tempfile.mkdtemp()\n# with open(temp_dir + '/file1.txt', 'w') as f:\n#     _ = f.write('This is a text file.')\n# task_func(temp_dir)\n# print(os.listdir(temp_dir))\n```\n"}
{"task_id": "BigCodeBench/381", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    df = pd.read_csv(file_path)\n    \n    if target_column not in df.columns:\n        raise ValueError(f\"The target column {target_column} is not in the DataFrame.\")\n    \n    df.dropna(inplace=True)\n    \n    if not all(df.dtypes == 'float32'):\n        raise ValueError(\"All columns in the DataFrame must be of type 'float32'.\")\n    \n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    model = RandomForestClassifier(random_state=seed)\n    model.fit(X, y)\n    \n    importances = model.feature_importances_\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=importances, y=X.columns)\n    ax.set_title('Feature Importances')\n    ax.set_xlabel('Importance')\n    ax.set_ylabel('Features')\n    \n    return ax, importances", "raw_solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    df = pd.read_csv(file_path)\n    \n    if target_column not in df.columns:\n        raise ValueError(f\"The target column {target_column} is not in the DataFrame.\")\n    \n    df.dropna(inplace=True)\n    \n    if not all(df.dtypes == 'float32'):\n        raise ValueError(\"All columns in the DataFrame must be of type 'float32'.\")\n    \n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    model = RandomForestClassifier(random_state=seed)\n    model.fit(X, y)\n    \n    importances = model.feature_importances_\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=importances, y=X.columns)\n    ax.set_title('Feature Importances')\n    ax.set_xlabel('Importance')\n    ax.set_ylabel('Features')\n    \n    return ax, importances\n\n# Example usage:\n# ax, importances = task_func('arena.csv', 'Index')\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/382", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(length):\n    # Generate a normal distribution with MU=0 and SIGMA=1\n    distribution = np.random.normal(loc=0, scale=1, size=length)\n\n    # Plot the histogram of the distribution\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(distribution, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the probability density function\n    pdf = norm.pdf(bins, loc=0, scale=1)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n\n    # Show the plot\n    plt.show()\n\n    return distribution, ax", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n    # Generate a normal distribution with MU=0 and SIGMA=1\n    distribution = np.random.normal(loc=0, scale=1, size=length)\n\n    # Plot the histogram of the distribution\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(distribution, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the probability density function\n    pdf = norm.pdf(bins, loc=0, scale=1)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n\n    # Show the plot\n    plt.show()\n\n    return distribution, ax\n```\n"}
{"task_id": "BigCodeBench/383", "solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\nimport re\ndef task_func(text, n, top_k):\n    # Preprocess the text\n    text = re.sub(r'\\W+', ' ', text.lower())\n    \n    # Tokenize the text into n-grams\n    words = text.split()\n    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n    \n    # Count the frequency of each n-gram\n    ngram_counts = Counter(ngrams)\n    \n    # Get the top k n-grams\n    top_ngrams = ngram_counts.most_common(top_k)\n    \n    # Create a DataFrame from the top n-grams\n    df = pd.DataFrame(top_ngrams, columns=['n-gram', 'count'])\n    \n    # Plot the top n-grams\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='count', y='n-gram', data=df)\n    ax.set_title(f'Top {top_k} n-grams')\n    ax.set_xlabel('Count')\n    ax.set_ylabel('n-gram')\n    \n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\nimport re\n\ndef task_func(text, n, top_k):\n    # Preprocess the text\n    text = re.sub(r'\\W+', ' ', text.lower())\n    \n    # Tokenize the text into n-grams\n    words = text.split()\n    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n    \n    # Count the frequency of each n-gram\n    ngram_counts = Counter(ngrams)\n    \n    # Get the top k n-grams\n    top_ngrams = ngram_counts.most_common(top_k)\n    \n    # Create a DataFrame from the top n-grams\n    df = pd.DataFrame(top_ngrams, columns=['n-gram', 'count'])\n    \n    # Plot the top n-grams\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='count', y='n-gram', data=df)\n    ax.set_title(f'Top {top_k} n-grams')\n    ax.set_xlabel('Count')\n    ax.set_ylabel('n-gram')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/384", "solution": "import collections\nimport random\nimport itertools\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Bear', 'Giraffe', 'Horse', 'Rabbit', 'Snake', 'Zebra']\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n    reversed_dict = {animal: [] for animal in ANIMALS}\n    animal_counter = collections.Counter()\n\n    for name, animal in animal_dict.items():\n        if animal in reversed_dict:\n            reversed_dict[animal].append(name)\n            animal_counter[animal] += random.randint(1, max_count)\n\n    return reversed_dict, animal_counter\nanimal_dict = {'John': 'Cat', 'Alice': 'Dog', 'Bob': 'Elephant', 'Charlie': 'Lion', 'David': 'Tiger', 'Sue': 'Pangolin'}", "raw_solution": "import collections\nimport random\nimport itertools\n\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Bear', 'Giraffe', 'Horse', 'Rabbit', 'Snake', 'Zebra']\n\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n    reversed_dict = {animal: [] for animal in ANIMALS}\n    animal_counter = collections.Counter()\n\n    for name, animal in animal_dict.items():\n        if animal in reversed_dict:\n            reversed_dict[animal].append(name)\n            animal_counter[animal] += random.randint(1, max_count)\n\n    return reversed_dict, animal_counter\n\n# Example usage:\nanimal_dict = {'John': 'Cat', 'Alice': 'Dog', 'Bob': 'Elephant', 'Charlie': 'Lion', 'David': 'Tiger', 'Sue': 'Pangolin'}\nreversed_dict, animal_counter = task_func(animal_dict, 15, 77)"}
{"task_id": "BigCodeBench/385", "solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit in the dictionary\n    fruit_counter = Counter(fruit_dict.values())\n\n    # Create a bar chart of the fruit counts\n    fig, ax = plt.subplots()\n    ax.bar(fruit_counter.keys(), fruit_counter.values())\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Count')\n    ax.set_title('Fruit Counts')\n\n    return fruit_counter, ax\nfruit_dict = {'John': 'Apple', 'Alice': 'Banana', 'Bob': 'Cherry', 'Charlie': 'Date', 'David': 'Apple'}", "raw_solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\n\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry', 'Fig', 'Grape', 'Honeydew', 'Indian Prune', 'Jackfruit']\n\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit in the dictionary\n    fruit_counter = Counter(fruit_dict.values())\n\n    # Create a bar chart of the fruit counts\n    fig, ax = plt.subplots()\n    ax.bar(fruit_counter.keys(), fruit_counter.values())\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Count')\n    ax.set_title('Fruit Counts')\n\n    return fruit_counter, ax\n\n# Example usage:\nfruit_dict = {'John': 'Apple', 'Alice': 'Banana', 'Bob': 'Cherry', 'Charlie': 'Date', 'David': 'Apple'}\nfreq, ax = task_func(fruit_dict)"}
{"task_id": "BigCodeBench/386", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length, min_value = 0, max_value = 100):\n    # Generate random data\n    data = np.random.randint(min_value, max_value, size=(length, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Calculate CDF\n    cdf = df.cumsum()\n\n    # Plot CDF\n    plt.figure(figsize=(10, 6))\n    for column in COLUMNS:\n        plt.plot(cdf[column], label=column)\n    plt.title('Cumulative Distribution Function (CDF)')\n    plt.xlabel('Index')\n    plt.ylabel('Cumulative Sum')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return cdf", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(length, min_value = 0, max_value = 100):\n    # Generate random data\n    data = np.random.randint(min_value, max_value, size=(length, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Calculate CDF\n    cdf = df.cumsum()\n\n    # Plot CDF\n    plt.figure(figsize=(10, 6))\n    for column in COLUMNS:\n        plt.plot(cdf[column], label=column)\n    plt.title('Cumulative Distribution Function (CDF)')\n    plt.xlabel('Index')\n    plt.ylabel('Cumulative Sum')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return cdf\n\n# Example usage"}
{"task_id": "BigCodeBench/387", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\ndef task_func(city_dict, max_range=1000000, seed=0):\n    np.random.seed(seed)\n    population_dict = {city: 0 for city in CITIES}\n    \n    for person, city in city_dict.items():\n        if city in CITIES:\n            population_dict[city] += 1\n    \n    for city in population_dict:\n        population_dict[city] = np.random.randint(1, max_range)\n    \n    city_names = list(population_dict.keys())\n    city_populations = list(population_dict.values())\n    \n    fig, ax = plt.subplots()\n    ax.bar(city_names, city_populations)\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('City Populations')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    \n    return population_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\n\ndef task_func(city_dict, max_range=1000000, seed=0):\n    np.random.seed(seed)\n    population_dict = {city: 0 for city in CITIES}\n    \n    for person, city in city_dict.items():\n        if city in CITIES:\n            population_dict[city] += 1\n    \n    for city in population_dict:\n        population_dict[city] = np.random.randint(1, max_range)\n    \n    city_names = list(population_dict.keys())\n    city_populations = list(population_dict.values())\n    \n    fig, ax = plt.subplots()\n    ax.bar(city_names, city_populations)\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('City Populations')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    \n    return population_dict, ax\n```\n"}
{"task_id": "BigCodeBench/388", "solution": "import collections\nimport pandas as pd\ndef task_func(my_tuple, path_csv_files):\n    result = {}\n    for column in my_tuple:\n        result[column] = collections.Counter()\n    \n    for file in path_csv_files:\n        df = pd.read_csv(file)\n        for column in my_tuple:\n            if column in df.columns:\n                result[column].update(df[column])\n    \n    return result", "raw_solution": "import collections\nimport pandas as pd\n\ndef task_func(my_tuple, path_csv_files):\n    result = {}\n    for column in my_tuple:\n        result[column] = collections.Counter()\n    \n    for file in path_csv_files:\n        df = pd.read_csv(file)\n        for column in my_tuple:\n            if column in df.columns:\n                result[column].update(df[column])\n    \n    return result\n\n# Example usage:\n# Assuming we have two CSV files with the following content:\n# file1.csv:\n# Country,Gender\n# USA,Male\n# Canada,Female\n# USA,Male\n# file2.csv:\n# Country,Gender\n# UK,Male\n# USA,Male\n# Germany,Female\n\n# from unittest.mock import MagicMock\n# import pandas as pd\n# df1 = pd.DataFrame({'Country': ['USA', 'Canada', 'USA'], 'Gender': ['Male', 'Female', 'Male']})\n# df2 = pd.DataFrame({'Country': ['UK', 'USA', 'Germany'], 'Gender': ['Male', 'Male', 'Female']})\n# pd.read_csv = MagicMock(side_effect=[df1, df2])\n# result = task_func(('Country', 'Gender'), ['file1.csv', 'file2.csv'])\n# print(result['Country'])\n# Output: Counter({'USA': 3, 'Canada': 1, 'UK': 1, 'Germany': 1})\n```\n"}
{"task_id": "BigCodeBench/389", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    # Define the keywords to search for\n    keywords = ['like', 'what']\n    \n    # Create a new subdirectory named \"Interesting Files\"\n    new_dir = os.path.join(directory, 'Interesting Files')\n    os.makedirs(new_dir, exist_ok=True)\n    \n    # List to store the names of files that match the criteria\n    interesting_files = []\n    \n    # Iterate over all files in the given directory\n    for filename in os.listdir(directory):\n        # Check if the file name contains any of the keywords\n        if any(keyword in filename for keyword in keywords):\n            # Move the file to the new subdirectory\n            shutil.move(os.path.join(directory, filename), new_dir)\n            interesting_files.append(filename)\n    \n    return interesting_files", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(directory):\n    # Define the keywords to search for\n    keywords = ['like', 'what']\n    \n    # Create a new subdirectory named \"Interesting Files\"\n    new_dir = os.path.join(directory, 'Interesting Files')\n    os.makedirs(new_dir, exist_ok=True)\n    \n    # List to store the names of files that match the criteria\n    interesting_files = []\n    \n    # Iterate over all files in the given directory\n    for filename in os.listdir(directory):\n        # Check if the file name contains any of the keywords\n        if any(keyword in filename for keyword in keywords):\n            # Move the file to the new subdirectory\n            shutil.move(os.path.join(directory, filename), new_dir)\n            interesting_files.append(filename)\n    \n    return interesting_files\n```\n"}
{"task_id": "BigCodeBench/390", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    if not csv_url_dict or \"url\" not in csv_url_dict:\n        raise ValueError(\"The dictionary is empty or does not contain the 'url' key.\")\n\n    csv_url = csv_url_dict[\"url\"]\n    response = requests.get(csv_url)\n    if response.status_code != 200:\n        raise ValueError(\"Failed to fetch data from the URL.\")\n\n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    df = df.sort_values(by=sort_by_column)\n\n    return df", "raw_solution": "import pandas as pd\nimport requests\nfrom io import StringIO\n\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    if not csv_url_dict or \"url\" not in csv_url_dict:\n        raise ValueError(\"The dictionary is empty or does not contain the 'url' key.\")\n\n    csv_url = csv_url_dict[\"url\"]\n    response = requests.get(csv_url)\n    if response.status_code != 200:\n        raise ValueError(\"Failed to fetch data from the URL.\")\n\n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    df = df.sort_values(by=sort_by_column)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/391", "solution": "import os\nimport glob\nimport shutil\ndef task_func(directory, archive_dir='archive'):\n    errors = []\n    try:\n        # Create the archive directory if it doesn't exist\n        if not os.path.exists(archive_dir):\n            os.makedirs(archive_dir)\n\n        # Find all JSON files in the specified directory\n        json_files = glob.glob(os.path.join(directory, '*.json'))\n\n        # Move each JSON file to the archive directory\n        for file in json_files:\n            try:\n                shutil.move(file, archive_dir)\n            except Exception as e:\n                errors.append(f\"Error moving file {file}: {str(e)}\")\n\n        # If there were no errors, return True, otherwise False\n        return (len(errors) == 0, errors)\n\n    except Exception as e:\n        return (False, [f\"Error in task_func: {str(e)}\"])", "raw_solution": "import os\nimport glob\nimport shutil\n\ndef task_func(directory, archive_dir='archive'):\n    errors = []\n    try:\n        # Create the archive directory if it doesn't exist\n        if not os.path.exists(archive_dir):\n            os.makedirs(archive_dir)\n\n        # Find all JSON files in the specified directory\n        json_files = glob.glob(os.path.join(directory, '*.json'))\n\n        # Move each JSON file to the archive directory\n        for file in json_files:\n            try:\n                shutil.move(file, archive_dir)\n            except Exception as e:\n                errors.append(f\"Error moving file {file}: {str(e)}\")\n\n        # If there were no errors, return True, otherwise False\n        return (len(errors) == 0, errors)\n\n    except Exception as e:\n        return (False, [f\"Error in task_func: {str(e)}\"])\n```\n"}
{"task_id": "BigCodeBench/392", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col, group_name):\n    if group_name not in df[group_col].values:\n        raise ValueError(f\"Group name '{group_name}' not found in DataFrame.\")\n\n    grouped_df = df[df[group_col] == group_name]\n    values = grouped_df[value_col].values\n\n    fig, ax = plt.subplots()\n    ax.bar(range(len(values)), values, color=COLORS[0])\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    ax.set_title(f'Bar chart of {value_col} for {group_name}')\n\n    return ax\ndf = pd.DataFrame({'Group': ['A', 'B', 'C'], 'Value': [10, 20, 30]})", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col, group_name):\n    if group_name not in df[group_col].values:\n        raise ValueError(f\"Group name '{group_name}' not found in DataFrame.\")\n\n    grouped_df = df[df[group_col] == group_name]\n    values = grouped_df[value_col].values\n\n    fig, ax = plt.subplots()\n    ax.bar(range(len(values)), values, color=COLORS[0])\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    ax.set_title(f'Bar chart of {value_col} for {group_name}')\n\n    return ax\n\n# Example usage:\nimport pandas as pd\ndf = pd.DataFrame({'Group': ['A', 'B', 'C'], 'Value': [10, 20, 30]})\nax = task_func(df, 'Group', 'Value', 'B')\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/393", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Histogram\n    ax[0].hist(samples, bins=30, alpha=0.7, color='blue')\n    ax[0].set_title('Histogram of Samples')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Q-Q Plot\n    sorted_samples = np.sort(samples)\n    quantiles = np.arange(1, len(sorted_samples) + 1) / len(sorted_samples)\n    norm_quantiles = stats.norm.ppf(quantiles, mu, sigma)\n    ax[1].plot(norm_quantiles, sorted_samples, 'o', alpha=0.7, color='red')\n    ax[1].set_title('Q-Q Plot')\n    ax[1].set_xlabel('Theoretical Quantiles')\n    ax[1].set_ylabel('Sample Quantiles')\n\n    plt.tight_layout()\n    return fig", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Histogram\n    ax[0].hist(samples, bins=30, alpha=0.7, color='blue')\n    ax[0].set_title('Histogram of Samples')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Q-Q Plot\n    sorted_samples = np.sort(samples)\n    quantiles = np.arange(1, len(sorted_samples) + 1) / len(sorted_samples)\n    norm_quantiles = stats.norm.ppf(quantiles, mu, sigma)\n    ax[1].plot(norm_quantiles, sorted_samples, 'o', alpha=0.7, color='red')\n    ax[1].set_title('Q-Q Plot')\n    ax[1].set_xlabel('Theoretical Quantiles')\n    ax[1].set_ylabel('Sample Quantiles')\n\n    plt.tight_layout()\n    return fig\n```\n"}
{"task_id": "BigCodeBench/394", "solution": "import collections\nimport string\nimport random\ndef task_func(length, seed=0):\n    random.seed(seed)\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n    frequency = collections.Counter(random_string)\n    return dict(frequency)", "raw_solution": "import collections\nimport string\nimport random\n\ndef task_func(length, seed=0):\n    random.seed(seed)\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n    frequency = collections.Counter(random_string)\n    return dict(frequency)\n\n# Test cases"}
{"task_id": "BigCodeBench/395", "solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    file_paths = glob.glob(os.path.join(directory, file_pattern))\n    if not file_paths:\n        raise ValueError(f\"No files matching pattern {file_pattern} found in directory {directory}.\")\n    \n    file_paths = natsort.natsorted(file_paths)\n    data = []\n\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n            numbers = re.findall(regex, content)\n            data.append({'filename': os.path.basename(file_path), 'numeric data': list(map(int, numbers))})\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\n\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    file_paths = glob.glob(os.path.join(directory, file_pattern))\n    if not file_paths:\n        raise ValueError(f\"No files matching pattern {file_pattern} found in directory {directory}.\")\n    \n    file_paths = natsort.natsorted(file_paths)\n    data = []\n\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n            numbers = re.findall(regex, content)\n            data.append({'filename': os.path.basename(file_path), 'numeric data': list(map(int, numbers))})\n\n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\n# data_dir = './data/'\n# create_dummy_files(data_dir)\n# df = task_func('./data/', '*.txt', r'([0-9]+)')\n# tear_down_files(data_dir)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/396", "solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, sample_size, seed=0):\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be a positive integer.\")\n    \n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    \n    fig, ax = plt.subplots()\n    ax.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n    \n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    ax.set_title(\"Gaussian Kernel Density Estimate\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Density\")\n    \n    return ax", "raw_solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mu, sigma, sample_size, seed=0):\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be a positive integer.\")\n    \n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    \n    fig, ax = plt.subplots()\n    ax.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n    \n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    ax.set_title(\"Gaussian Kernel Density Estimate\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Density\")\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/397", "solution": "import re\nimport urllib.request\nimport json\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(API_URL):\n    try:\n        with urllib.request.urlopen(API_URL) as response:\n            data = json.load(response)\n            ip = data.get('ip', '')\n            if re.match(IP_REGEX, ip):\n                return ip\n            else:\n                raise ValueError(\"Invalid IP address format.\")\n    except urllib.error.URLError as e:\n        raise Exception(f\"Failed to retrieve IP address: {e}\")", "raw_solution": "import re\nimport urllib.request\nimport json\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef task_func(API_URL):\n    try:\n        with urllib.request.urlopen(API_URL) as response:\n            data = json.load(response)\n            ip = data.get('ip', '')\n            if re.match(IP_REGEX, ip):\n                return ip\n            else:\n                raise ValueError(\"Invalid IP address format.\")\n    except urllib.error.URLError as e:\n        raise Exception(f\"Failed to retrieve IP address: {e}\")\n\n# Example usage:\n# print(task_func('https://api.ipify.org?format=json'))\n```\n"}
{"task_id": "BigCodeBench/398", "solution": "import json\nimport os\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return False\n\n    try:\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n    except json.JSONDecodeError:\n        return False\n\n    return isinstance(data, list)", "raw_solution": "import json\nimport os\n\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return False\n\n    try:\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n    except json.JSONDecodeError:\n        return False\n\n    return isinstance(data, list)\n```\n"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be a positive integer.\")\n\n    # Generate sample points\n    t = np.linspace(0, 1, sample_size)\n\n    # Generate sine and cosine waves\n    sine_wave = np.sin(2 * math.pi * frequency * t)\n    cosine_wave = np.cos(2 * math.pi * frequency * t)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n    ax.legend()\n\n    return fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be a positive integer.\")\n\n    # Generate sample points\n    t = np.linspace(0, 1, sample_size)\n\n    # Generate sine and cosine waves\n    sine_wave = np.sin(2 * math.pi * frequency * t)\n    cosine_wave = np.cos(2 * math.pi * frequency * t)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n    ax.legend()\n\n    return fig, ax\n```\n"}
{"task_id": "BigCodeBench/400", "solution": "import json\nfrom pathlib import Path\nfrom glob import glob\ndef task_func(directory, string):\n    matching_files = []\n    for file_path in glob(f\"{directory}/**/*.json\", recursive=True):\n        with open(file_path, \"r\") as file:\n            try:\n                data = json.load(file)\n                if string in json.dumps(data):\n                    matching_files.append(file_path)\n            except json.JSONDecodeError:\n                continue\n    return matching_files", "raw_solution": "import json\nfrom pathlib import Path\nfrom glob import glob\n\ndef task_func(directory, string):\n    matching_files = []\n    for file_path in glob(f\"{directory}/**/*.json\", recursive=True):\n        with open(file_path, \"r\") as file:\n            try:\n                data = json.load(file)\n                if string in json.dumps(data):\n                    matching_files.append(file_path)\n            except json.JSONDecodeError:\n                continue\n    return matching_files\n```\n"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail, Message\ndef task_func(app_name):\n    app = Flask(app_name)\n    mail = Mail(app)\n\n    # Fetching environment variables or using defaults\n    mail_server = os.getenv('MAIL_SERVER', 'localhost')\n    mail_port = int(os.getenv('MAIL_PORT', 25))\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'False').lower() in ['true', '1', 't']\n    mail_username = os.getenv('MAIL_USERNAME', None)\n    mail_password = os.getenv('MAIL_PASSWORD', None)\n\n    # Configuring the mail\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password\n    )\n\n    # Sending a test email\n    with app.app_context():\n        msg = Message(\"Hello from Flask-Mail\",\n                      sender=\"noreply@example.com\",\n                      recipients=[\"recipient@example.com\"])\n        msg.body = \"This is a test email sent from a Flask application using Flask-Mail.\"\n        mail.send(msg)\n\n    return mail, app.config", "raw_solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail, Message\n\ndef task_func(app_name):\n    app = Flask(app_name)\n    mail = Mail(app)\n\n    # Fetching environment variables or using defaults\n    mail_server = os.getenv('MAIL_SERVER', 'localhost')\n    mail_port = int(os.getenv('MAIL_PORT', 25))\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'False').lower() in ['true', '1', 't']\n    mail_username = os.getenv('MAIL_USERNAME', None)\n    mail_password = os.getenv('MAIL_PASSWORD', None)\n\n    # Configuring the mail\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password\n    )\n\n    # Sending a test email\n    with app.app_context():\n        msg = Message(\"Hello from Flask-Mail\",\n                      sender=\"noreply@example.com\",\n                      recipients=[\"recipient@example.com\"])\n        msg.body = \"This is a test email sent from a Flask application using Flask-Mail.\"\n        mail.send(msg)\n\n    return mail, app.config\n\n# Example usage"}
{"task_id": "BigCodeBench/402", "solution": "import re\nimport requests\nimport json\nimport csv\nimport os\nAPI_URL = 'https://api.example.com/data'\ndef task_func(pattern):\n    response = requests.get(API_URL)\n    data = response.json().get('data', [])\n\n    matches = [item for item in data if re.search(pattern, item)]\n\n    csv_file_path = 'matched_data.csv'\n    with open(csv_file_path, mode='w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Matched Data'])\n        for match in matches:\n            writer.writerow([match])\n\n    return os.path.abspath(csv_file_path)", "raw_solution": "import re\nimport requests\nimport json\nimport csv\nimport os  \n\n# Constants\nAPI_URL = 'https://api.example.com/data'\n\ndef task_func(pattern):\n    response = requests.get(API_URL)\n    data = response.json().get('data', [])\n\n    matches = [item for item in data if re.search(pattern, item)]\n\n    csv_file_path = 'matched_data.csv'\n    with open(csv_file_path, mode='w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Matched Data'])\n        for match in matches:\n            writer.writerow([match])\n\n    return os.path.abspath(csv_file_path)\n```\nThis script sends a GET request to the API, searches for matches in the data using the provided regular expression pattern, and writes the matched data to a CSV file. The function returns the absolute path to the CSV file.\n"}
{"task_id": "BigCodeBench/403", "solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\ndef task_func(img_path, blur_radius=5):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image\n    image = Image.open(img_path)\n\n    # Convert to grayscale\n    grayscale_image = image.convert('L')\n\n    # Apply blur filter\n    blurred_image = grayscale_image.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n\n    # Convert images to numpy arrays\n    original_array = np.array(image)\n    processed_array = np.array(blurred_image)\n\n    return original_array, processed_array", "raw_solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path, blur_radius=5):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image\n    image = Image.open(img_path)\n\n    # Convert to grayscale\n    grayscale_image = image.convert('L')\n\n    # Apply blur filter\n    blurred_image = grayscale_image.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n\n    # Convert images to numpy arrays\n    original_array = np.array(image)\n    processed_array = np.array(blurred_image)\n\n    return original_array, processed_array\n\n# Example usage:\n# image_path = 'sample.png'\n# create_dummy_image(image_path=image_path)\n# original, processed = task_func(image_path)\n# os.remove(image_path)\n```\n"}
{"task_id": "BigCodeBench/404", "solution": "import cv2\nimport numpy as np\nimport os\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image\n    image = cv2.imread(img_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Find contours in the grayscale image\n    contours, hierarchy = cv2.findContours(gray_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Draw contours on the original image\n    result_image = cv2.drawContours(image.copy(), contours, -1, (0, 255, 0), 2)\n\n    return result_image, contours", "raw_solution": "import cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image\n    image = cv2.imread(img_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Find contours in the grayscale image\n    contours, hierarchy = cv2.findContours(gray_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Draw contours on the original image\n    result_image = cv2.drawContours(image.copy(), contours, -1, (0, 255, 0), 2)\n\n    return result_image, contours\n```\n"}
{"task_id": "BigCodeBench/405", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(points: int):\n    random_numbers = [random.random() for _ in range(points)]\n    fig, ax = plt.subplots()\n    ax.plot(range(points), random_numbers, marker='o')\n    ax.set_title('Random Numbers Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Random Number')\n    plt.show()\n    return random_numbers, ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\ndef task_func(points: int):\n    random_numbers = [random.random() for _ in range(points)]\n    fig, ax = plt.subplots()\n    ax.plot(range(points), random_numbers, marker='o')\n    ax.set_title('Random Numbers Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Random Number')\n    plt.show()\n    return random_numbers, ax\n```\n"}
{"task_id": "BigCodeBench/406", "solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\ndef task_func(img_path, angle):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The file {img_path} does not exist.\")\n\n    # Open the image\n    original_img = Image.open(img_path)\n\n    # Rotate the image\n    rotated_img = original_img.rotate(angle)\n\n    # Convert images to numpy arrays\n    original_img_array = np.array(original_img)\n    rotated_img_array = np.array(rotated_img)\n\n    # Display the original and rotated images\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.title('Original Image')\n    plt.imshow(original_img_array)\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.title('Rotated Image')\n    plt.imshow(rotated_img_array)\n    plt.axis('off')\n\n    plt.show()\n\n    return original_img_array, rotated_img_array", "raw_solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\ndef task_func(img_path, angle):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The file {img_path} does not exist.\")\n\n    # Open the image\n    original_img = Image.open(img_path)\n\n    # Rotate the image\n    rotated_img = original_img.rotate(angle)\n\n    # Convert images to numpy arrays\n    original_img_array = np.array(original_img)\n    rotated_img_array = np.array(rotated_img)\n\n    # Display the original and rotated images\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.title('Original Image')\n    plt.imshow(original_img_array)\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.title('Rotated Image')\n    plt.imshow(rotated_img_array)\n    plt.axis('off')\n\n    plt.show()\n\n    return original_img_array, rotated_img_array\n\n# Example usage:\n# img_path = 'sample.png'\n# task_func(img_path, 45)\n# os.remove(img_path)\n```\nThis script opens an image, rotates it by a given angle, converts the images to numpy arrays, and displays both the original and rotated images using matplotlib. It also removes the image file after processing.\n"}
{"task_id": "BigCodeBench/407", "solution": "import os\nimport csv\nfrom openpyxl import load_workbook\ndef task_func(file_name, excel_file_path, csv_file_path) -> str:\n    excel_file = os.path.join(excel_file_path, file_name)\n    csv_file = os.path.join(csv_file_path, file_name.replace('.xlsx', '.csv'))\n\n    if not os.path.exists(excel_file):\n        raise FileNotFoundError(f\"[Errno 2] No such file or directory: '{excel_file}'\")\n\n    workbook = load_workbook(filename=excel_file)\n    sheet = workbook.active\n\n    with open(csv_file, 'w', newline='') as csvfile:\n        csvwriter = csv.writer(csvfile)\n        for row in sheet.iter_rows(values_only=True):\n            csvwriter.writerow(row)\n\n    return os.path.basename(csv_file)", "raw_solution": "import os\nimport csv\nfrom openpyxl import load_workbook\n\ndef task_func(file_name, excel_file_path, csv_file_path) -> str:\n    excel_file = os.path.join(excel_file_path, file_name)\n    csv_file = os.path.join(csv_file_path, file_name.replace('.xlsx', '.csv'))\n\n    if not os.path.exists(excel_file):\n        raise FileNotFoundError(f\"[Errno 2] No such file or directory: '{excel_file}'\")\n\n    workbook = load_workbook(filename=excel_file)\n    sheet = workbook.active\n\n    with open(csv_file, 'w', newline='') as csvfile:\n        csvwriter = csv.writer(csvfile)\n        for row in sheet.iter_rows(values_only=True):\n            csvwriter.writerow(row)\n\n    return os.path.basename(csv_file)\n\n# Example usage:\n# print(task_func('test.xlsx', '/path/to/excel/files', '/path/to/csv/files'))\n```\n"}
{"task_id": "BigCodeBench/408", "solution": "import sqlite3\nimport pandas as pd\ndef task_func(db_file: str, query: str) -> pd.DataFrame:\n    \"\"\"\n    Ek SQLite database se query karen and results lautayen.\n    Yeh function ek diye gaye SQLite database se judta hai, ek diya gaya SQL query nishpadit karta hai,\n    and results ko ek pandas dataframe ke roop mein lautaata hai.\n\n    Parameters:\n    - db_file (str): SQLite database file ka path.\n    - query (str): Nishpadit karne ke liye SQL query.\n\n    Returns:\n        pd.dataframe: Ek dataframe jismein nishpadit query ke results shaamil hain.\n\n    Requirements:\n    - sqlite3\n    - pandas\n\n    Example:\n    >>> db_file = 'sample_database.db'\n    >>> df = task_func(db_file, \"SELECT * FROM users WHERE name = 'John Doe'\")\n    pd.dataframe:\n    id        name  age\n    --  ----------  ---\n    ..  John Doe   ..\n    >>> df = task_func(db_file, \"SELECT age, COUNT(*) AS count FROM users GROUP BY age\")\n    pd.dataframe:\n    age  count\n    ---  -----\n    25   3\n\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    # Execute the query and load the results into a pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # Close the connection\n    conn.close()\n    \n    return df", "raw_solution": "import sqlite3\nimport pandas as pd\n\ndef task_func(db_file: str, query: str) -> pd.DataFrame:\n    \"\"\"\n    Ek SQLite database se query karen and results lautayen.\n    Yeh function ek diye gaye SQLite database se judta hai, ek diya gaya SQL query nishpadit karta hai,\n    and results ko ek pandas dataframe ke roop mein lautaata hai.\n\n    Parameters:\n    - db_file (str): SQLite database file ka path.\n    - query (str): Nishpadit karne ke liye SQL query.\n\n    Returns:\n        pd.dataframe: Ek dataframe jismein nishpadit query ke results shaamil hain.\n\n    Requirements:\n    - sqlite3\n    - pandas\n\n    Example:\n    >>> db_file = 'sample_database.db'\n    >>> df = task_func(db_file, \"SELECT * FROM users WHERE name = 'John Doe'\")\n    pd.dataframe:\n    id        name  age\n    --  ----------  ---\n    ..  John Doe   ..\n    >>> df = task_func(db_file, \"SELECT age, COUNT(*) AS count FROM users GROUP BY age\")\n    pd.dataframe:\n    age  count\n    ---  -----\n    25   3\n\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    # Execute the query and load the results into a pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # Close the connection\n    conn.close()\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    file_path = os.path.join(excel_file_path, file_name)\n    \n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist in the directory {excel_file_path}.\")\n\n    df = pd.read_excel(file_path)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the file {file_name}.\")\n\n    column_data = df[column_name]\n    \n    mean_value = np.mean(column_data)\n    median_value = np.median(column_data)\n    std_dev_value = np.std(column_data)\n    \n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'std_dev': std_dev_value\n    }", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    file_path = os.path.join(excel_file_path, file_name)\n    \n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist in the directory {excel_file_path}.\")\n\n    df = pd.read_excel(file_path)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the file {file_name}.\")\n\n    column_data = df[column_name]\n    \n    mean_value = np.mean(column_data)\n    median_value = np.median(column_data)\n    std_dev_value = np.std(column_data)\n    \n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'std_dev': std_dev_value\n    }\n\n# Example usage:\n# data_dir, file_name = './data/', 'excel_file1.xls'\n# create_dummy_file(data_dir, file_name)\n# stats = task_func(data_dir, file_name, 'Sales')\n# os.remove(data_dir + file_name)\n# os.rmdir(data_dir)\n# print(stats)\n```\n"}
{"task_id": "BigCodeBench/410", "solution": "import os\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> pd.DataFrame:\n    # Check if file exists\n    file_path = os.path.join(excel_directory, file_name)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist in the directory {excel_directory}.\")\n\n    # Load the Excel file\n    df = pd.read_excel(file_path)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n\n    # Convert the date column to datetime\n    df[column_name] = pd.to_datetime(df[column_name])\n\n    # Filter the DataFrame based on the date range\n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    filtered_df = df[(df[column_name] >= start_date) & (df[column_name] <= end_date)]\n\n    return filtered_df", "raw_solution": "import os\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> pd.DataFrame:\n    # Check if file exists\n    file_path = os.path.join(excel_directory, file_name)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist in the directory {excel_directory}.\")\n\n    # Load the Excel file\n    df = pd.read_excel(file_path)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n\n    # Convert the date column to datetime\n    df[column_name] = pd.to_datetime(df[column_name])\n\n    # Filter the DataFrame based on the date range\n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    filtered_df = df[(df[column_name] >= start_date) & (df[column_name] <= end_date)]\n\n    return filtered_df\n\n# Example usage:\n# data_dir, file_name = './excel_files/', 'excel_file1.xls'\n# test_file = create_dummy_file(data_dir, file_name)\n# filtered_df = task_func(data_dir, file_name, 'Date', '2020-01-01', '2020-12-31')\n# os.remove(test_file)\n# os.rmdir(data_dir)\n# print(filtered_df.head())\n```\n"}
{"task_id": "BigCodeBench/411", "solution": "import pandas as pd\nimport json\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    df = pd.DataFrame(data)\n    if 'c' in df.columns:\n        df = df.drop(columns=['c'])\n    df_dict = df.to_dict(orient='index')\n    with open(output_path, 'w') as f:\n        json.dump(df_dict, f)\n    return output_path", "raw_solution": "import pandas as pd\nimport json\n\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    df = pd.DataFrame(data)\n    if 'c' in df.columns:\n        df = df.drop(columns=['c'])\n    df_dict = df.to_dict(orient='index')\n    with open(output_path, 'w') as f:\n        json.dump(df_dict, f)\n    return output_path\n\n# Test cases"}
{"task_id": "BigCodeBench/412", "solution": "import json\nimport base64\nimport unicodedata\ndef task_func(json_file: str) -> dict:\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    decoded_data = {}\n    for key, value in data.items():\n        decoded_value = base64.b64decode(value).decode('utf-8')\n        normalized_value = unicodedata.normalize('NFC', decoded_value)\n        decoded_data[key] = normalized_value\n\n    return decoded_data", "raw_solution": "import json\nimport base64\nimport unicodedata\n\ndef task_func(json_file: str) -> dict:\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    decoded_data = {}\n    for key, value in data.items():\n        decoded_value = base64.b64decode(value).decode('utf-8')\n        normalized_value = unicodedata.normalize('NFC', decoded_value)\n        decoded_data[key] = normalized_value\n\n    return decoded_data\n\n# Example usage:\n# Assuming 'example.json' contains {\"key1\": \"SGVsbG8gV29ybGQ=\", \"key2\": \"UHl0aG9uIENvZGUgUmVmaW5lcg==\"}\n# print(task_func('example.json'))\n# Output: {'key1': 'Hello World', 'key2': 'Python Code Refiner'}\n```\n"}
{"task_id": "BigCodeBench/413", "solution": "import os\nfrom flask import Flask\nfrom flask_mail import Mail, Message\ndef task_func(app):\n    mail = Mail(app)\n\n    # Fetching email server details from environment variables\n    mail_config = {\n        'MAIL_SERVER': os.getenv('MAIL_SERVER', 'smtp.example.com'),\n        'MAIL_PORT': int(os.getenv('MAIL_PORT', 587)),\n        'MAIL_USE_TLS': os.getenv('MAIL_USE_TLS', 'true').lower() in ['true', '1', 't'],\n        'MAIL_USERNAME': os.getenv('MAIL_USERNAME', 'user@example.com'),\n        'MAIL_PASSWORD': os.getenv('MAIL_PASSWORD', 'password'),\n        'MAIL_DEFAULT_SENDER': os.getenv('MAIL_DEFAULT_SENDER', 'noreply@example.com')\n    }\n\n    # Setting the mail configuration\n    for key, value in mail_config.items():\n        app.config[key] = value\n\n    return mail, mail_config", "raw_solution": "import os\nfrom flask import Flask\nfrom flask_mail import Mail, Message\n\ndef task_func(app):\n    mail = Mail(app)\n\n    # Fetching email server details from environment variables\n    mail_config = {\n        'MAIL_SERVER': os.getenv('MAIL_SERVER', 'smtp.example.com'),\n        'MAIL_PORT': int(os.getenv('MAIL_PORT', 587)),\n        'MAIL_USE_TLS': os.getenv('MAIL_USE_TLS', 'true').lower() in ['true', '1', 't'],\n        'MAIL_USERNAME': os.getenv('MAIL_USERNAME', 'user@example.com'),\n        'MAIL_PASSWORD': os.getenv('MAIL_PASSWORD', 'password'),\n        'MAIL_DEFAULT_SENDER': os.getenv('MAIL_DEFAULT_SENDER', 'noreply@example.com')\n    }\n\n    # Setting the mail configuration\n    for key, value in mail_config.items():\n        app.config[key] = value\n\n    return mail, mail_config\n\n# Example usage"}
{"task_id": "BigCodeBench/414", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    df = pd.DataFrame(data)\n    \n    if column in df.columns:\n        df = df.drop(columns=[column])\n    \n    if df.select_dtypes(include=[np.number]).shape[1] > 0:\n        ax = df.plot(kind='line')\n        plt.show()\n    else:\n        ax = None\n\n    return df, ax\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column=\"c\"):\n    df = pd.DataFrame(data)\n    \n    if column in df.columns:\n        df = df.drop(columns=[column])\n    \n    if df.select_dtypes(include=[np.number]).shape[1] > 0:\n        ax = df.plot(kind='line')\n        plt.show()\n    else:\n        ax = None\n\n    return df, ax\n\n# Example usage:\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\nmodified_df, ax = task_func(data)"}
{"task_id": "BigCodeBench/415", "solution": "import pandas as pd\nimport codecs\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame.\")\n    \n    if 'UnicodeString' not in dataframe.columns:\n        raise KeyError(\"DataFrame must contain 'UnicodeString' column.\")\n    \n    def decode_unicode(unicode_str):\n        return codecs.decode(unicode_str.encode('utf-8'), 'unicode_escape').decode('utf-8')\n    \n    dataframe['UnicodeString'] = dataframe['UnicodeString'].apply(decode_unicode)\n    \n    return dataframe", "raw_solution": "import pandas as pd\nimport codecs\n\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame.\")\n    \n    if 'UnicodeString' not in dataframe.columns:\n        raise KeyError(\"DataFrame must contain 'UnicodeString' column.\")\n    \n    def decode_unicode(unicode_str):\n        return codecs.decode(unicode_str.encode('utf-8'), 'unicode_escape').decode('utf-8')\n    \n    dataframe['UnicodeString'] = dataframe['UnicodeString'].apply(decode_unicode)\n    \n    return dataframe\n\n# Example usage:"}
{"task_id": "BigCodeBench/416", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n    \n    if not pd.api.types.is_numeric_dtype(data[column]):\n        raise ValueError(f\"Column '{column}' must be numeric.\")\n    \n    corr_matrix = data.corr()\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.show()\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column=\"c\"):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n    \n    if not pd.api.types.is_numeric_dtype(data[column]):\n        raise ValueError(f\"Column '{column}' must be numeric.\")\n    \n    corr_matrix = data.corr()\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.show()\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]})\n# task_func(df)\n```\n"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Create a sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model with SGD optimizer and binary cross-entropy loss\n    model.compile(optimizer=SGD(learning_rate=0.1), loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Train the model on the training data\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='train')\n    ax.plot(history.history['val_loss'], label='test')\n    ax.set_xlabel('Epochs')\n    ax.set_ylabel('Loss')\n    ax.set_title('Model Loss')\n    ax.legend()\n\n    return model, ax", "raw_solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Create a sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model with SGD optimizer and binary cross-entropy loss\n    model.compile(optimizer=SGD(learning_rate=0.1), loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Train the model on the training data\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='train')\n    ax.plot(history.history['val_loss'], label='test')\n    ax.set_xlabel('Epochs')\n    ax.set_ylabel('Loss')\n    ax.set_title('Model Loss')\n    ax.legend()\n\n    return model, ax\n```\n"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_shape=(2,), activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.1), metrics=['accuracy'])\n\n    # Fit the model\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n    ax.set_title('ROC curve')\n    ax.set_xlabel('false positive rate')\n    ax.set_ylabel('true positive rate')\n    ax.legend()\n\n    return model, ax", "raw_solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_shape=(2,), activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.1), metrics=['accuracy'])\n\n    # Fit the model\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n    ax.set_title('ROC curve')\n    ax.set_xlabel('false positive rate')\n    ax.set_ylabel('true positive rate')\n    ax.legend()\n\n    return model, ax\n```\n"}
{"task_id": "BigCodeBench/419", "solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Splitting the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Creating a Keras Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=X.shape[1], activation='sigmoid')\n    ])\n\n    # Compiling the model\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n    # Training the model\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predicting probabilities for the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Calculating precision and recall\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred_prob)\n\n    # Plotting the precision-recall curve\n    fig, ax = plt.subplots()\n    ax.plot(recall, precision, marker='.')\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    ax.set_title('Precision-Recall Curve')\n    plt.show()\n\n    return model, ax", "raw_solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Splitting the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Creating a Keras Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=X.shape[1], activation='sigmoid')\n    ])\n\n    # Compiling the model\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n    # Training the model\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predicting probabilities for the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Calculating precision and recall\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred_prob)\n\n    # Plotting the precision-recall curve\n    fig, ax = plt.subplots()\n    ax.plot(recall, precision, marker='.')\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    ax.set_title('Precision-Recall Curve')\n    plt.show()\n\n    return model, ax\n\n# Example usage:\n# X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# Y = np.array([[0], [1], [1], [0]])\n# model, ax = task_func(X, Y)\n# print(isinstance(model, keras.models.Sequential))  # Should print True\n# print(isinstance(ax, plt.Axes))  # Should print True\n```\n"}
{"task_id": "BigCodeBench/420", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data):\n    df = pd.DataFrame(data)\n    numeric_cols = df.select_dtypes(include=['number']).columns\n    non_numeric_cols = df.select_dtypes(exclude=['number']).columns\n\n    scaler = StandardScaler()\n    try:\n        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    except ValueError:\n        df[numeric_cols] = df[numeric_cols].apply(lambda x: pd.to_numeric(x, errors='coerce')).fillna(x.mean())\n        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data):\n    df = pd.DataFrame(data)\n    numeric_cols = df.select_dtypes(include=['number']).columns\n    non_numeric_cols = df.select_dtypes(exclude=['number']).columns\n\n    scaler = StandardScaler()\n    try:\n        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    except ValueError:\n        df[numeric_cols] = df[numeric_cols].apply(lambda x: pd.to_numeric(x, errors='coerce')).fillna(x.mean())\n        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/421", "solution": "import requests\nimport os\nimport json\nimport time\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\ndef task_func(url, directory, metadata):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    if not isinstance(url, str):\n        raise TypeError(\"The URL must be a string.\")\n    \n    status_codes = []\n    \n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        \n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as file:\n                file_data = file.read()\n                payload = {\n                    'metadata': metadata,\n                    'file': (filename, file_data)\n                }\n                \n                response = requests.post(url, headers=HEADERS, data=json.dumps(payload))\n                status_codes.append(response.status_code)\n                \n                time.sleep(1)  # Wait for 1 second before the next upload\n    \n    return status_codes", "raw_solution": "import requests\nimport os\nimport json\nimport time\n\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\n\ndef task_func(url, directory, metadata):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    if not isinstance(url, str):\n        raise TypeError(\"The URL must be a string.\")\n    \n    status_codes = []\n    \n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        \n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as file:\n                file_data = file.read()\n                payload = {\n                    'metadata': metadata,\n                    'file': (filename, file_data)\n                }\n                \n                response = requests.post(url, headers=HEADERS, data=json.dumps(payload))\n                status_codes.append(response.status_code)\n                \n                time.sleep(1)  # Wait for 1 second before the next upload\n    \n    return status_codes\n```\n"}
{"task_id": "BigCodeBench/422", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    df = pd.DataFrame(df)\n    if column_to_remove in df.columns:\n        df = df.drop(columns=[column_to_remove])\n\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    return X_train, X_test, y_train, y_test", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    df = pd.DataFrame(df)\n    if column_to_remove in df.columns:\n        df = df.drop(columns=[column_to_remove])\n\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    return X_train, X_test, y_train, y_test\n\n# Example usage:\ndata = {\n    'a': [1, 2, 3, 4],\n    'b': [5, 6, 7, 8],\n    'c': [9, 10, 11, 12],\n    'target': [0, 1, 0, 1]\n}\nX_train, X_test, y_train, y_test = task_func(data, 'target')"}
{"task_id": "BigCodeBench/423", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be read as an image.\")\n\n    # Convert to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Apply threshold\n    _, binary_img_array = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    return gray_image, binary_img_array", "raw_solution": "import numpy as np\nimport cv2\nimport os\n\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be read as an image.\")\n\n    # Convert to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Apply threshold\n    _, binary_img_array = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    return gray_image, binary_img_array\n```\n"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be loaded.\")\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to a 2D array of pixels\n    pixel_values = image.reshape((-1, 3))\n    pixel_values = np.float32(pixel_values)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n\n    # Replace pixels in the original image with their cluster centers\n    segmented_image = np.zeros_like(image)\n    for i, color in enumerate(kmeans.cluster_centers_):\n        mask = np.all(image == color, axis=-1)\n        segmented_image[mask] = kmeans.cluster_centers_[i]\n\n    # Reshape the segmented image back to its original dimensions\n    segmented_image = segmented_image.reshape(image.shape)\n\n    return image, segmented_image", "raw_solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be loaded.\")\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to a 2D array of pixels\n    pixel_values = image.reshape((-1, 3))\n    pixel_values = np.float32(pixel_values)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n\n    # Replace pixels in the original image with their cluster centers\n    segmented_image = np.zeros_like(image)\n    for i, color in enumerate(kmeans.cluster_centers_):\n        mask = np.all(image == color, axis=-1)\n        segmented_image[mask] = kmeans.cluster_centers_[i]\n\n    # Reshape the segmented image back to its original dimensions\n    segmented_image = segmented_image.reshape(image.shape)\n\n    return image, segmented_image\n\n# Example usage:\n# create_dummy_image('image.jpg')\n# original_img_array, segmented_img_array = task_func('image.jpg', 3)\n# os.remove('image.jpg')\n# print(original_img_array.shape) # Example output\n# print(segmented_img_array.shape) # Example output for n_clusters > 1\n```\n"}
{"task_id": "BigCodeBench/425", "solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n\n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    \n    # Calculate the histogram\n    histogram = cv2.calcHist([image], [0], None, [256], [0, 256])\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.plot(histogram)\n    ax.set_title('Grayscale Histogram')\n    ax.set_xlabel('Pixel Value')\n    ax.set_ylabel('Frequency')\n\n    # Save the histogram plot\n    plt.savefig(histogram_path)\n\n    return ax", "raw_solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\n\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n\n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    \n    # Calculate the histogram\n    histogram = cv2.calcHist([image], [0], None, [256], [0, 256])\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.plot(histogram)\n    ax.set_title('Grayscale Histogram')\n    ax.set_xlabel('Pixel Value')\n    ax.set_ylabel('Frequency')\n\n    # Save the histogram plot\n    plt.savefig(histogram_path)\n\n    return ax\n\n# Example usage:\n# task_func('path_to_image.jpg', 'path_to_histogram.png')\n```\nThis script reads an image from the specified path, calculates its histogram, and saves the histogram plot to the specified path. It also returns the Axes object for the plot, which can be used to further manipulate the plot if needed.\n"}
{"task_id": "BigCodeBench/426", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    \n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n    \n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    \n    # Apply threshold\n    _, binary_image = cv2.threshold(image, threshold, 255, cv2.THRESH_BINARY)\n    \n    # Save the binary image\n    cv2.imwrite('binary_image.jpg', binary_image)\n    \n    return image, binary_image", "raw_solution": "import numpy as np\nimport cv2\nimport os\n\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    \n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n    \n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    \n    # Apply threshold\n    _, binary_image = cv2.threshold(image, threshold, 255, cv2.THRESH_BINARY)\n    \n    # Save the binary image\n    cv2.imwrite('binary_image.jpg', binary_image)\n    \n    return image, binary_image\n\n# Example usage:\n# create_dummy_image('image.jpg')\n# original_img_array, binary_img_array = task_func('image.jpg', 128)\n# os.remove('image.jpg')\n# print(original_img_array.shape, binary_img_array.shape)\n```\n"}
{"task_id": "BigCodeBench/427", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on=\"id\")\n\n    # Extract features and target\n    X = merged_df[features]\n    y = merged_df[target]\n\n    # Create and train the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Calculate residuals\n    residuals = y - model.predict(X)\n\n    # Plot residuals\n    fig, ax = plt.subplots()\n    ax.scatter(y, residuals)\n    ax.set_title(\"Residuals Plot\")\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Residuals\")\n\n    # Return results\n    return {\n        \"coefficients\": model.coef_,\n        \"intercept\": model.intercept_,\n        \"residuals_plot\": ax\n    }", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on=\"id\")\n\n    # Extract features and target\n    X = merged_df[features]\n    y = merged_df[target]\n\n    # Create and train the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Calculate residuals\n    residuals = y - model.predict(X)\n\n    # Plot residuals\n    fig, ax = plt.subplots()\n    ax.scatter(y, residuals)\n    ax.set_title(\"Residuals Plot\")\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Residuals\")\n\n    # Return results\n    return {\n        \"coefficients\": model.coef_,\n        \"intercept\": model.intercept_,\n        \"residuals_plot\": ax\n    }\n```\n"}
{"task_id": "BigCodeBench/428", "solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df1, df2):\n    # Merge the dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n\n    # Select only numeric columns for scaling\n    numeric_cols = merged_df.select_dtypes(include=['float64', 'int64']).columns\n\n    # Scale numeric columns\n    scaler = StandardScaler()\n    merged_df[numeric_cols] = scaler.fit_transform(merged_df[numeric_cols])\n\n    # Generate pair plot for scaled numeric columns\n    pair_plot = sns.pairplot(merged_df[numeric_cols])\n\n    return merged_df, pair_plot", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df1, df2):\n    # Merge the dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n\n    # Select only numeric columns for scaling\n    numeric_cols = merged_df.select_dtypes(include=['float64', 'int64']).columns\n\n    # Scale numeric columns\n    scaler = StandardScaler()\n    merged_df[numeric_cols] = scaler.fit_transform(merged_df[numeric_cols])\n\n    # Generate pair plot for scaled numeric columns\n    pair_plot = sns.pairplot(merged_df[numeric_cols])\n\n    return merged_df, pair_plot\n```\n"}
{"task_id": "BigCodeBench/429", "solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2):\n    # Merge the two dataframes on 'id' column\n    df = pd.merge(df1, df2, on='id')\n\n    # Select the features and target variable\n    X = df.drop(['id', 'target'], axis=1)\n    y = df['target']\n\n    # Perform feature selection\n    selector = SelectKBest(score_func=f_classif, k=2)\n    selected_features = selector.fit_transform(X, y)\n\n    # Get the names of the selected features\n    feature_names = X.columns[selector.get_support(indices=True)]\n\n    # Calculate the correlation matrix\n    corr_matrix = df[feature_names].corr()\n\n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Feature Correlation Heatmap')\n    plt.show()\n\n    return feature_names.tolist(), heatmap\ndf1 = pd.DataFrame({'id': [1, 2, 3], 'feature1': [1.2, 3.4, 5.6], 'feature2': [2.3, 4.5, 6.7], 'feature3': [3.4, 5.6, 7.8]})\ndf2 = pd.DataFrame({'id': [1, 2, 3], 'target': [4.5, 6.7, 8.9]})", "raw_solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2):\n    # Merge the two dataframes on 'id' column\n    df = pd.merge(df1, df2, on='id')\n\n    # Select the features and target variable\n    X = df.drop(['id', 'target'], axis=1)\n    y = df['target']\n\n    # Perform feature selection\n    selector = SelectKBest(score_func=f_classif, k=2)\n    selected_features = selector.fit_transform(X, y)\n\n    # Get the names of the selected features\n    feature_names = X.columns[selector.get_support(indices=True)]\n\n    # Calculate the correlation matrix\n    corr_matrix = df[feature_names].corr()\n\n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Feature Correlation Heatmap')\n    plt.show()\n\n    return feature_names.tolist(), heatmap\n\n# Example usage\ndf1 = pd.DataFrame({'id': [1, 2, 3], 'feature1': [1.2, 3.4, 5.6], 'feature2': [2.3, 4.5, 6.7], 'feature3': [3.4, 5.6, 7.8]})\ndf2 = pd.DataFrame({'id': [1, 2, 3], 'target': [4.5, 6.7, 8.9]})\nselected_features, heatmap = task_func(df1, df2)"}
{"task_id": "BigCodeBench/430", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the two dataframes on 'id' column\n    df = pd.merge(df1, df2, on='id')\n    \n    # Extract the feature columns\n    X = df[[column1, column2]]\n    \n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    labels = kmeans.fit_predict(X)\n    \n    # Plot the clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[column1], X[column2], c=labels, cmap='viridis')\n    \n    # Add legend\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n    ax.add_artist(legend1)\n    \n    return labels, ax", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the two dataframes on 'id' column\n    df = pd.merge(df1, df2, on='id')\n    \n    # Extract the feature columns\n    X = df[[column1, column2]]\n    \n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    labels = kmeans.fit_predict(X)\n    \n    # Plot the clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[column1], X[column2], c=labels, cmap='viridis')\n    \n    # Add legend\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n    ax.add_artist(legend1)\n    \n    return labels, ax\n```\n"}
{"task_id": "BigCodeBench/431", "solution": "import cv2\nimport os\nimport numpy as np\ndef task_func(image_file: str) -> np.ndarray:\n    if not os.path.exists(image_file):\n        raise FileNotFoundError(f\"The file {image_file} does not exist.\")\n    \n    image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        raise ValueError(f\"The file {image_file} is not a valid image.\")\n    \n    histogram = np.histogram(image, bins=256, range=(0, 256))[0]\n    \n    return histogram", "raw_solution": "import cv2\nimport os\nimport numpy as np\n\ndef task_func(image_file: str) -> np.ndarray:\n    if not os.path.exists(image_file):\n        raise FileNotFoundError(f\"The file {image_file} does not exist.\")\n    \n    image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        raise ValueError(f\"The file {image_file} is not a valid image.\")\n    \n    histogram = np.histogram(image, bins=256, range=(0, 256))[0]\n    \n    return histogram\n\n# Example usage:\n# dummy_image_path = 'dummy_image.png'\n# np.random.seed(48)\n# dummy_image = np.random.randint(0, 256, (10, 10), dtype=np.uint8)\n# cv2.imwrite(dummy_image_path, dummy_image)\n# histogram = task_func(dummy_image_path)\n# os.remove(dummy_image_path)\n# print(histogram.shape)  # Output: (256,)\n```\n"}
{"task_id": "BigCodeBench/432", "solution": "import seaborn as sns\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id')\n\n    # Create contingency table\n    contingency_table = pd.crosstab(merged_df[column1], merged_df[column2])\n\n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    # Create heatmap\n    plt.figure(figsize=(10, 6))\n    heatmap = sns.heatmap(contingency_table, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel(column2)\n    plt.ylabel(column1)\n    plt.title('Contingency Table Heatmap')\n\n    return p, heatmap", "raw_solution": "import seaborn as sns\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id')\n\n    # Create contingency table\n    contingency_table = pd.crosstab(merged_df[column1], merged_df[column2])\n\n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    # Create heatmap\n    plt.figure(figsize=(10, 6))\n    heatmap = sns.heatmap(contingency_table, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel(column2)\n    plt.ylabel(column1)\n    plt.title('Contingency Table Heatmap')\n\n    return p, heatmap\n```\n"}
{"task_id": "BigCodeBench/433", "solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\ndef task_func(s, signature, secret_key):\n    \"\"\"\n    Ek nirdisht secret key ka upayog karke, pradaan ki gayi signature ke viruddh base64-encoded message ke HMAC SHA-1 signature ko manya karta hai.\n    Yeh function first base64-encoded message ko decode karta hai, phir pradaan ki gayi secret key ka upayog karke iska HMAC SHA-1 hash compute karta hai,\n    and finally is computed hash ki tulna pradaan ki gayi signature se karta hai.\n    >>> task_func('SGVsbG8gV29ybGQ=', 'incorrect_signature', 'my_secret_key')\n    false\n\n    Parameters:\n    - s (str): The base64-encoded message to validate.\n    - signature (str): The HMAC SHA-1 signature to compare against.\n    - secret_key (str): The secret key used to compute the HMAC SHA-1 hash.\n\n    Returns:\n        bool: Returns true if the provided signature matches the computed signature, false otherwise.\n\n    Requirements:\n    - base64\n    - hashlib\n    - hmac\n    - binascii\n\n    Example:\n    >>> task_func('SGVsbG8gV29ybGQ=', 'c47c23299efca3c220f4c19a5f2e4ced14729322', 'my_secret_key')\n    true\n\n    \"\"\"\n    # Decode the base64-encoded message\n    decoded_message = base64.b64decode(s)\n\n    # Compute the HMAC SHA-1 hash of the decoded message\n    computed_signature = hmac.new(\n        key=secret_key.encode(),\n        msg=decoded_message,\n        digestmod=hashlib.sha1\n    ).hexdigest()\n\n    # Compare the computed signature with the provided signature\n    return computed_signature == signature", "raw_solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\n\ndef task_func(s, signature, secret_key):\n    \"\"\"\n    Ek nirdisht secret key ka upayog karke, pradaan ki gayi signature ke viruddh base64-encoded message ke HMAC SHA-1 signature ko manya karta hai.\n    Yeh function first base64-encoded message ko decode karta hai, phir pradaan ki gayi secret key ka upayog karke iska HMAC SHA-1 hash compute karta hai,\n    and finally is computed hash ki tulna pradaan ki gayi signature se karta hai.\n    >>> task_func('SGVsbG8gV29ybGQ=', 'incorrect_signature', 'my_secret_key')\n    false\n\n    Parameters:\n    - s (str): The base64-encoded message to validate.\n    - signature (str): The HMAC SHA-1 signature to compare against.\n    - secret_key (str): The secret key used to compute the HMAC SHA-1 hash.\n\n    Returns:\n        bool: Returns true if the provided signature matches the computed signature, false otherwise.\n\n    Requirements:\n    - base64\n    - hashlib\n    - hmac\n    - binascii\n\n    Example:\n    >>> task_func('SGVsbG8gV29ybGQ=', 'c47c23299efca3c220f4c19a5f2e4ced14729322', 'my_secret_key')\n    true\n\n    \"\"\"\n    # Decode the base64-encoded message\n    decoded_message = base64.b64decode(s)\n\n    # Compute the HMAC SHA-1 hash of the decoded message\n    computed_signature = hmac.new(\n        key=secret_key.encode(),\n        msg=decoded_message,\n        digestmod=hashlib.sha1\n    ).hexdigest()\n\n    # Compare the computed signature with the provided signature\n    return computed_signature == signature\n```\n"}
{"task_id": "BigCodeBench/434", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    random.seed(seed)\n    products = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\n    product_map = {}\n    data = []\n\n    # Split the input string into lines\n    lines = s.split('\\n')\n    for line in lines:\n        # Split each line into parts\n        parts = line.split()\n        if len(parts) != 5:\n            raise ValueError(\"Each line must contain exactly 5 fields: ID, Quantity, Code, Price, and Description.\")\n        \n        # Extract fields\n        id, quantity, code, price, description = parts\n        quantity, price = int(quantity), int(price)\n        \n        # Assign a product name based on the code\n        if code not in product_map:\n            product = random.choice(products)\n            product_map[code] = product\n        else:\n            product = product_map[code]\n        \n        # Append data\n        data.append([id, quantity, code, price, product, description])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['ID', 'Quantity', 'Code', 'Price', 'Product', 'Description'])\n    \n    return df\ns = '1 10 A10B 100 This is a description with spaces\\n2 20 B20C 200 Another description example'", "raw_solution": "import pandas as pd\nimport re\nimport random\n\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    random.seed(seed)\n    products = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\n    product_map = {}\n    data = []\n\n    # Split the input string into lines\n    lines = s.split('\\n')\n    for line in lines:\n        # Split each line into parts\n        parts = line.split()\n        if len(parts) != 5:\n            raise ValueError(\"Each line must contain exactly 5 fields: ID, Quantity, Code, Price, and Description.\")\n        \n        # Extract fields\n        id, quantity, code, price, description = parts\n        quantity, price = int(quantity), int(price)\n        \n        # Assign a product name based on the code\n        if code not in product_map:\n            product = random.choice(products)\n            product_map[code] = product\n        else:\n            product = product_map[code]\n        \n        # Append data\n        data.append([id, quantity, code, price, product, description])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['ID', 'Quantity', 'Code', 'Price', 'Product', 'Description'])\n    \n    return df\n\n# Example usage\ns = '1 10 A10B 100 This is a description with spaces\\n2 20 B20C 200 Another description example'\ndf = task_func(s)"}
{"task_id": "BigCodeBench/435", "solution": "import pandas as pd\nfrom random import randint\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    valid_names = ['John', 'Alice', 'Bob', 'Charlie', 'David']\n    if name not in valid_names:\n        raise ValueError(f\"Name must be one of {valid_names}\")\n\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'developer', 'Tester']\n    job_title = job_titles[randint(0, len(job_titles) - 1)]\n\n    data = {\n        'Name': [name],\n        'Age': [age],\n        'Code': [code],\n        'Salary': [salary],\n        'Bio': [bio],\n        'Job Title': [job_title]\n    }\n\n    data_df = pd.DataFrame(data)\n\n    return data_df", "raw_solution": "import pandas as pd\nfrom random import randint\n\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    valid_names = ['John', 'Alice', 'Bob', 'Charlie', 'David']\n    if name not in valid_names:\n        raise ValueError(f\"Name must be one of {valid_names}\")\n\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'developer', 'Tester']\n    job_title = job_titles[randint(0, len(job_titles) - 1)]\n\n    data = {\n        'Name': [name],\n        'Age': [age],\n        'Code': [code],\n        'Salary': [salary],\n        'Bio': [bio],\n        'Job Title': [job_title]\n    }\n\n    data_df = pd.DataFrame(data)\n\n    return data_df\n\n# Example usage:\n# df = task_func(\"John\", 30, \"A10B\", 5000.0, \"This is a bio with spaces\")\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/436", "solution": "import string\nimport matplotlib.pyplot as plt\ndef task_func(s):\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string.\")\n\n    # Convert string to lowercase\n    s = s.lower()\n\n    # Calculate frequency of each character\n    freqs = {char: s.count(char) for char in string.ascii_lowercase}\n\n    # Plot the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(freqs.keys(), freqs.values())\n    ax.set_xlabel('Character')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Character Frequency')\n\n    return freqs, ax", "raw_solution": "import string\nimport matplotlib.pyplot as plt\n\ndef task_func(s):\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string.\")\n\n    # Convert string to lowercase\n    s = s.lower()\n\n    # Calculate frequency of each character\n    freqs = {char: s.count(char) for char in string.ascii_lowercase}\n\n    # Plot the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(freqs.keys(), freqs.values())\n    ax.set_xlabel('Character')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Character Frequency')\n\n    return freqs, ax\n```\n"}
{"task_id": "BigCodeBench/437", "solution": "import pickle\nimport os\nimport pandas as pd\nimport numpy as np\ndef task_func(df, file_name=\"save.pkl\"):\n    \"\"\"\n    Dee gaee Pandas DataFrame \"df\" ko die gae name ki ek pickle file mein surakshit karen, ise padhen\n    validation ke lie back, and intermediate file ko hata den.\n\n    Parameters:\n    - df (DataFrame): Saheje jaane wala pandas DataFrame.\n    - file_name (str, optional): Us file ka name jahan DataFrame ko saheja jaega. defaults 'save.pkl' hai.\n\n    Returns:\n        loaded_df (pd.DataFrame): specified file se load kiya gaya DataFrame.\n\n    Requirements:\n    - pickle\n    - os\n\n    Example:\n    >>> import pandas as pd\n    >>> import numpy as np\n    >>> np.random.seed(0)\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('AbcD'))\n    >>> loaded_df = task_func(df, 'test_file.pkl')\n    >>> assert df.equals(loaded_df)\n    >>> type(df), type(loaded_df)\n    (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>)\n    >>> df.head(2)\n    A   b   c   D\n    0  44  47  64  67\n    1  67   9  83  21\n\n    \"\"\"\n    # Save the DataFrame to a pickle file\n    with open(file_name, 'wb') as file:\n        pickle.dump(df, file)\n\n    # Load the DataFrame from the pickle file\n    with open(file_name, 'rb') as file:\n        loaded_df = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(file_name)\n\n    return loaded_df", "raw_solution": "import pickle\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df, file_name=\"save.pkl\"):\n    \"\"\"\n    Dee gaee Pandas DataFrame \"df\" ko die gae name ki ek pickle file mein surakshit karen, ise padhen\n    validation ke lie back, and intermediate file ko hata den.\n\n    Parameters:\n    - df (DataFrame): Saheje jaane wala pandas DataFrame.\n    - file_name (str, optional): Us file ka name jahan DataFrame ko saheja jaega. defaults 'save.pkl' hai.\n\n    Returns:\n        loaded_df (pd.DataFrame): specified file se load kiya gaya DataFrame.\n\n    Requirements:\n    - pickle\n    - os\n\n    Example:\n    >>> import pandas as pd\n    >>> import numpy as np\n    >>> np.random.seed(0)\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('AbcD'))\n    >>> loaded_df = task_func(df, 'test_file.pkl')\n    >>> assert df.equals(loaded_df)\n    >>> type(df), type(loaded_df)\n    (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>)\n    >>> df.head(2)\n    A   b   c   D\n    0  44  47  64  67\n    1  67   9  83  21\n\n    \"\"\"\n    # Save the DataFrame to a pickle file\n    with open(file_name, 'wb') as file:\n        pickle.dump(df, file)\n\n    # Load the DataFrame from the pickle file\n    with open(file_name, 'rb') as file:\n        loaded_df = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(file_name)\n\n    return loaded_df\n```\n"}
{"task_id": "BigCodeBench/438", "solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list) or not all(isinstance(n, (int, float)) for n in numbers):\n        raise TypeError(\"Input must be a list of numbers.\")\n\n    # Generate a random figure\n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n    ax.set_title('Random Numbers Plot')\n\n    # Save the figure to a pickle file\n    with open(file_path, 'wb') as file:\n        pickle.dump(fig, file)\n\n    # Load the figure from the pickle file\n    with open(file_path, 'rb') as file:\n        loaded_fig = pickle.load(file)\n\n    # Clean up by deleting the pickle file\n    os.remove(file_path)\n\n    return loaded_fig", "raw_solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\nimport random\n\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list) or not all(isinstance(n, (int, float)) for n in numbers):\n        raise TypeError(\"Input must be a list of numbers.\")\n\n    # Generate a random figure\n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n    ax.set_title('Random Numbers Plot')\n\n    # Save the figure to a pickle file\n    with open(file_path, 'wb') as file:\n        pickle.dump(fig, file)\n\n    # Load the figure from the pickle file\n    with open(file_path, 'rb') as file:\n        loaded_fig = pickle.load(file)\n\n    # Clean up by deleting the pickle file\n    os.remove(file_path)\n\n    return loaded_fig\n\n# Example usage:\n# numbers = [random.random() for _ in range(100)]\n# loaded_fig = task_func(numbers)\n# print(type(loaded_fig))\n```\n"}
{"task_id": "BigCodeBench/439", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(P, T):\n    # Ensure P is a numpy array\n    P = np.array(P)\n    # Ensure T is a numpy array\n    T = np.array(T)\n    \n    # Perform matrix-tensor multiplication\n    product = np.tensordot(P, T, axes=1)\n    \n    # Visualize the result using seaborn heatmap\n    fig, ax = plt.subplots()\n    sns.heatmap(product, ax=ax, annot=True, fmt=\".2f\")\n    \n    return product, ax", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(P, T):\n    # Ensure P is a numpy array\n    P = np.array(P)\n    # Ensure T is a numpy array\n    T = np.array(T)\n    \n    # Perform matrix-tensor multiplication\n    product = np.tensordot(P, T, axes=1)\n    \n    # Visualize the result using seaborn heatmap\n    fig, ax = plt.subplots()\n    sns.heatmap(product, ax=ax, annot=True, fmt=\".2f\")\n    \n    return product, ax\n```\n"}
{"task_id": "BigCodeBench/440", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(P, T):\n    if P.size == 0 or T.size == 0:\n        raise ValueError(\"Input matrices cannot be empty.\")\n    \n    if P.shape[1] != T.shape[1]:\n        raise ValueError(\"The number of columns in P and T must be the same.\")\n    \n    # Perform matrix-tensor multiplication\n    result = np.tensordot(P, T, axes=1)\n    \n    # Standardize the result\n    scaler = StandardScaler()\n    standardized_result = scaler.fit_transform(result)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(standardized_result, columns=[f'feature_{i}' for i in range(standardized_result.shape[1])])\n    \n    return df\nP = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\nT = np.random.rand(3, 5, 5)", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(P, T):\n    if P.size == 0 or T.size == 0:\n        raise ValueError(\"Input matrices cannot be empty.\")\n    \n    if P.shape[1] != T.shape[1]:\n        raise ValueError(\"The number of columns in P and T must be the same.\")\n    \n    # Perform matrix-tensor multiplication\n    result = np.tensordot(P, T, axes=1)\n    \n    # Standardize the result\n    scaler = StandardScaler()\n    standardized_result = scaler.fit_transform(result)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(standardized_result, columns=[f'feature_{i}' for i in range(standardized_result.shape[1])])\n    \n    return df\n\n# Example usage:\nnp.random.seed(0)\nP = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\nT = np.random.rand(3, 5, 5)\nresult = task_func(P, T)"}
{"task_id": "BigCodeBench/441", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(P, T):\n    \"\"\"\n    numpy ka upayog karke matrix \"p\" and ek 3d tensor \"t\" ka product calculate kare and then matplotlib.pyplot se 3d mein result ko visualize kare. matrix and tensor ka product Einstein yog par aadhaarit hai.\n\n    Notes:\n        Yah function only numpy matrices/arrays sweekar karta hai.\n\n    Parameters:\n    - p (numpy.ndarray): input matrix jiska shape (N, 3) hai, jahan N rows ki number hai.\n    - t (numpy.ndarray): input tensor jiska shape (3, 3, 3) hai.\n\n    Returns:\n        tuple:\n        result (numpy.ndarray): matrix p and tensor t ka product jiska shape (N, 3) hai.\n        ax (mpl_toolkits.mplot3d.axes3d.Axes3d): result ka 3d visualization.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot.pyplot\n\n    Example:\n    >>> p = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1]])\n    >>> t = np.random.rand(3, 3, 3)\n    >>> result, ax = task_func(p, t)\n    >>> type(result)\n    <class 'numpy.ndarray'>\n    >>> type(ax)\n    <class 'mpl_toolkits.mplot3d.axes3d.Axes3d'>\n    \"\"\"\n    # Check if the shapes of P and T are as expected\n    if P.shape[1] != 3 or T.shape != (3, 3, 3):\n        raise ValueError(\"Invalid shapes for P and T\")\n\n    # Calculate the product of P and T using numpy's einsum function\n    result = np.einsum('ij,ijk->ik', P, T)\n\n    # Create a 3D plot of the result\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n\n    return result, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(P, T):\n    \"\"\"\n    numpy ka upayog karke matrix \"p\" and ek 3d tensor \"t\" ka product calculate kare and then matplotlib.pyplot se 3d mein result ko visualize kare. matrix and tensor ka product Einstein yog par aadhaarit hai.\n\n    Notes:\n        Yah function only numpy matrices/arrays sweekar karta hai.\n\n    Parameters:\n    - p (numpy.ndarray): input matrix jiska shape (N, 3) hai, jahan N rows ki number hai.\n    - t (numpy.ndarray): input tensor jiska shape (3, 3, 3) hai.\n\n    Returns:\n        tuple:\n        result (numpy.ndarray): matrix p and tensor t ka product jiska shape (N, 3) hai.\n        ax (mpl_toolkits.mplot3d.axes3d.Axes3d): result ka 3d visualization.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot.pyplot\n\n    Example:\n    >>> p = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1]])\n    >>> t = np.random.rand(3, 3, 3)\n    >>> result, ax = task_func(p, t)\n    >>> type(result)\n    <class 'numpy.ndarray'>\n    >>> type(ax)\n    <class 'mpl_toolkits.mplot3d.axes3d.Axes3d'>\n    \"\"\"\n    # Check if the shapes of P and T are as expected\n    if P.shape[1] != 3 or T.shape != (3, 3, 3):\n        raise ValueError(\"Invalid shapes for P and T\")\n\n    # Calculate the product of P and T using numpy's einsum function\n    result = np.einsum('ij,ijk->ik', P, T)\n\n    # Create a 3D plot of the result\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n\n    return result, ax\n```\n"}
{"task_id": "BigCodeBench/442", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    # Ensure the tensor has the correct shape\n    if T.shape != tensor_shape:\n        raise ValueError(\"Tensor T must have the same shape as tensor_shape\")\n\n    # Flatten the tensor to 2D\n    T_flat = T.reshape(-1, T.shape[-1])\n\n    # Concatenate matrix P and the flattened tensor T\n    combined_data = np.hstack((P, T_flat))\n\n    # Apply PCA to reduce dimensionality\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(combined_data)\n\n    # Plot the PCA result\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n\n    return pca_result, ax\nP = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\nT = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    # Ensure the tensor has the correct shape\n    if T.shape != tensor_shape:\n        raise ValueError(\"Tensor T must have the same shape as tensor_shape\")\n\n    # Flatten the tensor to 2D\n    T_flat = T.reshape(-1, T.shape[-1])\n\n    # Concatenate matrix P and the flattened tensor T\n    combined_data = np.hstack((P, T_flat))\n\n    # Apply PCA to reduce dimensionality\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(combined_data)\n\n    # Plot the PCA result\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n\n    return pca_result, ax\n\n# Example usage:\nP = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\nT = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\npca_result, ax = task_func(P, T)"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Calculate the product of P and T\n    result = np.dot(P, T.flatten())\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(result.reshape(-1, 1))\n\n    # Plot the clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(result, cluster_result, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Cluster Labels')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\n    return cluster_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Calculate the product of P and T\n    result = np.dot(P, T.flatten())\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(result.reshape(-1, 1))\n\n    # Plot the clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(result, cluster_result, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Cluster Labels')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\n    return cluster_result, ax\n```\n"}
{"task_id": "BigCodeBench/444", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(n_points=100, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random points in 3D space\n    points = np.random.rand(n_points, 3)\n\n    # Create a 3D scatter plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n\n    # Set plot labels\n    ax.set_xlabel('X axis')\n    ax.set_ylabel('Y axis')\n    ax.set_zlabel('Z axis')\n    ax.set_title('3D Scatter Plot of Random Points')\n\n    return points, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(n_points=100, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random points in 3D space\n    points = np.random.rand(n_points, 3)\n\n    # Create a 3D scatter plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n\n    # Set plot labels\n    ax.set_xlabel('X axis')\n    ax.set_ylabel('Y axis')\n    ax.set_zlabel('Z axis')\n    ax.set_title('3D Scatter Plot of Random Points')\n\n    return points, ax\n```\n"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    if not isinstance(points, np.ndarray) or points.shape[1] != 2:\n        raise ValueError(\"Input must be a numpy array with shape (n_points, 2)\")\n\n    vor = Voronoi(points)\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    plt.show()\n\n    return vor, ax", "raw_solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    if not isinstance(points, np.ndarray) or points.shape[1] != 2:\n        raise ValueError(\"Input must be a numpy array with shape (n_points, 2)\")\n\n    vor = Voronoi(points)\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    plt.show()\n\n    return vor, ax\n\n# Example usage:\n# points = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# vor, ax = task_func(points)\n```\n"}
{"task_id": "BigCodeBench/446", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    # Generate isotropic Gaussian blobs\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n\n    # Plot the blobs\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n    ax.set_title('Isotropic Gaussian Blobs')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax, label='Cluster Label')\n\n    return X, y, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    # Generate isotropic Gaussian blobs\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n\n    # Plot the blobs\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n    ax.set_title('Isotropic Gaussian Blobs')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax, label='Cluster Label')\n\n    return X, y, ax\n```\n"}
{"task_id": "BigCodeBench/447", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, n_components=2, random_state=None):\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    if n_components == 1:\n        ax.scatter(transformed_data, np.zeros_like(transformed_data), alpha=0.5)\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Values')\n    elif n_components == 2:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.5)\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n    else:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.5)\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n        ax.set_title('First two principal components')\n\n    return {\"transformed_data\": transformed_data, \"ax\": ax}", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(data, n_components=2, random_state=None):\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    if n_components == 1:\n        ax.scatter(transformed_data, np.zeros_like(transformed_data), alpha=0.5)\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Values')\n    elif n_components == 2:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.5)\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n    else:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.5)\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n        ax.set_title('First two principal components')\n\n    return {\"transformed_data\": transformed_data, \"ax\": ax}\n```\n"}
{"task_id": "BigCodeBench/448", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'N({mu}, {sigma}^2)')\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('x')\n    ax.set_ylabel('Probability Density')\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'N({mu}, {sigma}^2)')\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('x')\n    ax.set_ylabel('Probability Density')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/449", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Plotting histograms for each feature\n    axes_list = []\n    for column in standardized_data.columns:\n        fig, ax = plt.subplots()\n        ax.hist(standardized_data[column], bins=20, color='blue', alpha=0.7)\n        ax.set_title(f'Histogram of {column}')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n\n    return standardized_data, axes_list", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Plotting histograms for each feature\n    axes_list = []\n    for column in standardized_data.columns:\n        fig, ax = plt.subplots()\n        ax.hist(standardized_data[column], bins=20, color='blue', alpha=0.7)\n        ax.set_title(f'Histogram of {column}')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n\n    return standardized_data, axes_list\n```\n"}
{"task_id": "BigCodeBench/450", "solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate synthetic dataset\n    X, _ = make_blobs(n_samples=n_samples, centers=centers, random_state=random_seed)\n\n    # Calculate distances between all pairs of samples\n    distances = cdist(X, X, metric='euclidean')\n\n    # Plot the dataset\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c='blue', label='Samples')\n    ax.set_title('Synthetic Dataset')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.legend()\n\n    if plot_path:\n        plt.savefig(plot_path)\n        plt.close(fig)\n        return distances, None\n\n    return distances, ax", "raw_solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate synthetic dataset\n    X, _ = make_blobs(n_samples=n_samples, centers=centers, random_state=random_seed)\n\n    # Calculate distances between all pairs of samples\n    distances = cdist(X, X, metric='euclidean')\n\n    # Plot the dataset\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c='blue', label='Samples')\n    ax.set_title('Synthetic Dataset')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.legend()\n\n    if plot_path:\n        plt.savefig(plot_path)\n        plt.close(fig)\n        return distances, None\n\n    return distances, ax\n```\n"}
{"task_id": "BigCodeBench/451", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a random dataset\n    data = np.random.rand(N_SAMPLES, N_FEATURES)\n\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Calculate covariance matrix\n    covariance_matrix = np.cov(transformed_data, rowvar=False)\n\n    # Plot heatmap\n    fig, ax = plt.subplots()\n    sns.heatmap(covariance_matrix, ax=ax, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n    ax.set_title(\"Covariance Matrix Heatmap\")\n\n    return transformed_data, ax if n_components > 1 else None", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a random dataset\n    data = np.random.rand(N_SAMPLES, N_FEATURES)\n\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Calculate covariance matrix\n    covariance_matrix = np.cov(transformed_data, rowvar=False)\n\n    # Plot heatmap\n    fig, ax = plt.subplots()\n    sns.heatmap(covariance_matrix, ax=ax, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n    ax.set_title(\"Covariance Matrix Heatmap\")\n\n    return transformed_data, ax if n_components > 1 else None\n```\n"}
{"task_id": "BigCodeBench/452", "solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=random_seed)\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n    \n    # Create and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n    \n    # Get the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n    \n    # Calculate Mean Squared Error (MSE)\n    mse = np.mean((predictions - y_test) ** 2)\n    \n    return predictions, coefficients, intercept, mse", "raw_solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=random_seed)\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n    \n    # Create and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n    \n    # Get the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n    \n    # Calculate Mean Squared Error (MSE)\n    mse = np.mean((predictions - y_test) ** 2)\n    \n    return predictions, coefficients, intercept, mse\n```\n"}
{"task_id": "BigCodeBench/453", "solution": "import re\nimport string\nfrom random import choice\ndef task_func(n, pattern):\n    while True:\n        # Generate a random string of length n\n        random_string = ''.join(choice(string.ascii_uppercase + string.ascii_lowercase) for _ in range(n))\n        \n        # Check if the generated string matches the pattern\n        if re.match(pattern, random_string):\n            return random_string", "raw_solution": "import re\nimport string\nfrom random import choice\n\ndef task_func(n, pattern):\n    while True:\n        # Generate a random string of length n\n        random_string = ''.join(choice(string.ascii_uppercase + string.ascii_lowercase) for _ in range(n))\n        \n        # Check if the generated string matches the pattern\n        if re.match(pattern, random_string):\n            return random_string\n\n# Test cases"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n\n    # Find all files with the given extension in the source directory\n    search_pattern = os.path.join(src_dir, f'*.{ext}')\n    files_to_move = glob.glob(search_pattern)\n\n    moved_files = []\n\n    for file_path in files_to_move:\n        file_name = os.path.basename(file_path)\n        dest_file_path = os.path.join(dest_dir, file_name)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            shutil.move(file_path, dest_file_path)\n            moved_files.append(dest_file_path)\n\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n\n    # Find all files with the given extension in the source directory\n    search_pattern = os.path.join(src_dir, f'*.{ext}')\n    files_to_move = glob.glob(search_pattern)\n\n    moved_files = []\n\n    for file_path in files_to_move:\n        file_name = os.path.basename(file_path)\n        dest_file_path = os.path.join(dest_dir, file_name)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            shutil.move(file_path, dest_file_path)\n            moved_files.append(dest_file_path)\n\n    return moved_files\n```\n"}
{"task_id": "BigCodeBench/455", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, n):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, n)\n\n    # Plot histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the probability density function\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title(\"Histogram and PDF of Normal Distribution\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    return samples", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, n):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, n)\n\n    # Plot histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the probability density function\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title(\"Histogram and PDF of Normal Distribution\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    return samples\n\n# Example usage:\n# Generate 500 samples from a normal distribution with mean 5 and standard deviation 2\nsamples = task_func(5, 2, 500)"}
{"task_id": "BigCodeBench/456", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(normalized_data, cmap=\"YlGnBu\", cbar_kws={'label': 'Normalized Value'})\n    plt.title('Heatmap of Normalized Data')\n    plt.xlabel('Features')\n    plt.ylabel('Samples')\n\n    return normalized_data, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(normalized_data, cmap=\"YlGnBu\", cbar_kws={'label': 'Normalized Value'})\n    plt.title('Heatmap of Normalized Data')\n    plt.xlabel('Features')\n    plt.ylabel('Samples')\n\n    return normalized_data, ax\n```\n"}
{"task_id": "BigCodeBench/457", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in L):\n        raise TypeError(\"Input must be a list of lists of integers.\")\n\n    # Convert the list of lists into a single list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Create a pandas Series from the list\n    series = pd.Series(flat_list)\n\n    # Plot the histogram\n    ax = series.plot(kind='hist', bins=series.nunique(), rwidth=0.8)\n\n    # Return the axes object\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in L):\n        raise TypeError(\"Input must be a list of lists of integers.\")\n\n    # Convert the list of lists into a single list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Create a pandas Series from the list\n    series = pd.Series(flat_list)\n\n    # Plot the histogram\n    ax = series.plot(kind='hist', bins=series.nunique(), rwidth=0.8)\n\n    # Return the axes object\n    return ax\n```\n"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    def extract_numbers(value):\n        if isinstance(value, str):\n            numbers = re.findall(r'\\d+', value)\n            return [int(num) for num in numbers]\n        elif isinstance(value, list):\n            return [item for item in value if isinstance(item, (int, float))]\n        return []\n\n    def flatten_dict(d, parent_key='', sep='_'):\n        items = []\n        for k, v in d.items():\n            new_key = parent_key + sep + k if parent_key else k\n            if isinstance(v, dict):\n                items.extend(flatten_dict(v, new_key, sep=sep))\n            else:\n                items.append((new_key, v))\n        return items\n\n    flat_data = flatten_dict(data)\n    df_data = {k: extract_numbers(v) for k, v in flat_data}\n\n    df = pd.DataFrame(df_data)\n    return df\njson_str = '{\"a\": [1, 2, 3], \"b\": 4.9, \"c\": \"5\"}'", "raw_solution": "import json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    def extract_numbers(value):\n        if isinstance(value, str):\n            numbers = re.findall(r'\\d+', value)\n            return [int(num) for num in numbers]\n        elif isinstance(value, list):\n            return [item for item in value if isinstance(item, (int, float))]\n        return []\n\n    def flatten_dict(d, parent_key='', sep='_'):\n        items = []\n        for k, v in d.items():\n            new_key = parent_key + sep + k if parent_key else k\n            if isinstance(v, dict):\n                items.extend(flatten_dict(v, new_key, sep=sep))\n            else:\n                items.append((new_key, v))\n        return items\n\n    flat_data = flatten_dict(data)\n    df_data = {k: extract_numbers(v) for k, v in flat_data}\n\n    df = pd.DataFrame(df_data)\n    return df\n\n# Example usage:\njson_str = '{\"a\": [1, 2, 3], \"b\": 4.9, \"c\": \"5\"}'\ndf = task_func(json_str)"}
{"task_id": "BigCodeBench/459", "solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\ndef task_func(script_dir, scripts, delay):\n    if delay < 0:\n        raise ValueError(\"Delay cannot be negative.\")\n    if not scripts:\n        raise ValueError(\"Scripts list cannot be empty.\")\n\n    timestamps = []\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.isfile(script_path):\n            raise FileNotFoundError(f\"Script {script} not found in directory {script_dir}.\")\n\n        start_time = datetime.now()\n        timestamps.append(start_time.strftime('%Y-%m-%d %H:%M:%S'))\n\n        subprocess.run(['bash', script_path], check=True)\n        time.sleep(delay)\n\n    return timestamps", "raw_solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\n\ndef task_func(script_dir, scripts, delay):\n    if delay < 0:\n        raise ValueError(\"Delay cannot be negative.\")\n    if not scripts:\n        raise ValueError(\"Scripts list cannot be empty.\")\n\n    timestamps = []\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.isfile(script_path):\n            raise FileNotFoundError(f\"Script {script} not found in directory {script_dir}.\")\n\n        start_time = datetime.now()\n        timestamps.append(start_time.strftime('%Y-%m-%d %H:%M:%S'))\n\n        subprocess.run(['bash', script_path], check=True)\n        time.sleep(delay)\n\n    return timestamps\n```\n"}
{"task_id": "BigCodeBench/460", "solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(script_path, output_file_path):\n    try:\n        # Run the script using subprocess\n        subprocess.run([\"bash\", script_path, output_file_path], check=True)\n    except subprocess.CalledProcessError:\n        raise ValueError(\"Script execution failed or output file is invalid.\")\n\n    # Read the CSV file into a DataFrame\n    try:\n        df = pd.read_csv(output_file_path)\n    except FileNotFoundError:\n        raise ValueError(\"Output file not found.\")\n    except pd.errors.EmptyDataError:\n        raise ValueError(\"Output file is empty.\")\n    except pd.errors.ParserError:\n        raise ValueError(\"Output file is not a valid CSV.\")\n\n    # Check if the DataFrame has exactly two columns\n    if df.shape[1] != 2:\n        raise ValueError(\"Output CSV must have exactly two columns.\")\n\n    # Plot the bar graph\n    ax = df.plot(kind='bar', x=df.columns[0], y=df.columns[1], legend=False)\n    plt.xlabel(df.columns[0])\n    plt.ylabel(df.columns[1])\n    plt.title(\"Bar Graph from CSV Data\")\n    plt.show()\n\n    return df, ax", "raw_solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(script_path, output_file_path):\n    try:\n        # Run the script using subprocess\n        subprocess.run([\"bash\", script_path, output_file_path], check=True)\n    except subprocess.CalledProcessError:\n        raise ValueError(\"Script execution failed or output file is invalid.\")\n\n    # Read the CSV file into a DataFrame\n    try:\n        df = pd.read_csv(output_file_path)\n    except FileNotFoundError:\n        raise ValueError(\"Output file not found.\")\n    except pd.errors.EmptyDataError:\n        raise ValueError(\"Output file is empty.\")\n    except pd.errors.ParserError:\n        raise ValueError(\"Output file is not a valid CSV.\")\n\n    # Check if the DataFrame has exactly two columns\n    if df.shape[1] != 2:\n        raise ValueError(\"Output CSV must have exactly two columns.\")\n\n    # Plot the bar graph\n    ax = df.plot(kind='bar', x=df.columns[0], y=df.columns[1], legend=False)\n    plt.xlabel(df.columns[0])\n    plt.ylabel(df.columns[1])\n    plt.title(\"Bar Graph from CSV Data\")\n    plt.show()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} does not exist.\")\n    \n    # Start the script as a subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # Initialize CPU and memory usage\n    cpu_usage = 0.0\n    memory_usage = 0.0\n    \n    try:\n        # Wait for the script to complete or timeout\n        process.wait(timeout=timeout)\n        \n        # Get the process object by name\n        for proc in psutil.process_iter(['pid', 'name']):\n            if proc.info['name'] == 'bash':\n                bash_process = proc\n                break\n        else:\n            raise RuntimeError(\"Bash process not found.\")\n        \n        # Get the CPU and memory usage of the bash process\n        cpu_usage = bash_process.cpu_percent(interval=1)\n        memory_usage = bash_process.memory_info().rss\n        \n    except subprocess.TimeoutExpired:\n        # If the script exceeds the timeout, kill the process\n        process.kill()\n        process.wait()\n        \n        # Get the CPU and memory usage of the bash process after timeout\n        cpu_usage = bash_process.cpu_percent(interval=1)\n        memory_usage = bash_process.memory_info().rss\n        \n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n    \n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}", "raw_solution": "import subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} does not exist.\")\n    \n    # Start the script as a subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # Initialize CPU and memory usage\n    cpu_usage = 0.0\n    memory_usage = 0.0\n    \n    try:\n        # Wait for the script to complete or timeout\n        process.wait(timeout=timeout)\n        \n        # Get the process object by name\n        for proc in psutil.process_iter(['pid', 'name']):\n            if proc.info['name'] == 'bash':\n                bash_process = proc\n                break\n        else:\n            raise RuntimeError(\"Bash process not found.\")\n        \n        # Get the CPU and memory usage of the bash process\n        cpu_usage = bash_process.cpu_percent(interval=1)\n        memory_usage = bash_process.memory_info().rss\n        \n    except subprocess.TimeoutExpired:\n        # If the script exceeds the timeout, kill the process\n        process.kill()\n        process.wait()\n        \n        # Get the CPU and memory usage of the bash process after timeout\n        cpu_usage = bash_process.cpu_percent(interval=1)\n        memory_usage = bash_process.memory_info().rss\n        \n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n    \n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}\n```\n"}
{"task_id": "BigCodeBench/462", "solution": "import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be at least 1\")\n\n    random.seed(random_seed)\n\n    data = {\n        \"Category\": [random.choice(categories) for _ in range(num_rows)],\n        \"Value\": [random.randint(1, 100) for _ in range(num_rows)]\n    }\n\n    df = pd.DataFrame(data)\n\n    category_counts = df['Category'].value_counts()\n    ax = category_counts.plot(kind='bar', title='Category Counts')\n    plt.xlabel('Category')\n    plt.ylabel('Counts')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be at least 1\")\n\n    random.seed(random_seed)\n\n    data = {\n        \"Category\": [random.choice(categories) for _ in range(num_rows)],\n        \"Value\": [random.randint(1, 100) for _ in range(num_rows)]\n    }\n\n    df = pd.DataFrame(data)\n\n    category_counts = df['Category'].value_counts()\n    ax = category_counts.plot(kind='bar', title='Category Counts')\n    plt.xlabel('Category')\n    plt.ylabel('Counts')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/463", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_str, separator=\",\", bins=20):\n    if not data_str:\n        raise ValueError(\"Data string cannot be empty.\")\n\n    try:\n        data = pd.Series(data_str.split(separator)).astype(np.int64)\n    except ValueError:\n        raise ValueError(\"Data string contains non-integer values.\")\n\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, grid=True, rwidth=0.9, color='#607c8e')\n\n    return data, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_str, separator=\",\", bins=20):\n    if not data_str:\n        raise ValueError(\"Data string cannot be empty.\")\n\n    try:\n        data = pd.Series(data_str.split(separator)).astype(np.int64)\n    except ValueError:\n        raise ValueError(\"Data string contains non-integer values.\")\n\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, grid=True, rwidth=0.9, color='#607c8e')\n\n    return data, ax\n```\n"}
{"task_id": "BigCodeBench/464", "solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\ndef task_func(my_obj):\n    \"\"\"\n    This function takes an object, serializes it into a JSON string, and handles datetime and decimal types.\n    It uses the json module's default encoder, but it does not handle complex data types natively.\n    The 'my_class' parameter is reserved for future use and is not currently used in the current implementation.\n    It serializes a simple dictionary.\n\n    Parameters:\n    - my_obj (object): The object to be serialized, which may contain datetime and decimal types.\n\n    Returns:\n        str: A JSON-formatted string of the serialized object, properly handling datetime and decimal objects.\n\n    Requirements:\n    - json\n    - datetime.datetime\n    - decimal.Decimal\n\n    Example:\n    >>> result = task_func({'time': datetime(2023, 4, 1, 12, 0), 'amount': Decimal('10.99')})\n    >>> '2023-04-01T12:00:00' in result and '10.99' in result\n    True\n    \"\"\"\n    class DateTimeEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, datetime):\n                return obj.isoformat()\n            elif isinstance(obj, Decimal):\n                return str(obj)\n            return super().default(obj)\n\n    return json.dumps(my_obj, cls=DateTimeEncoder)", "raw_solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\n\ndef task_func(my_obj):\n    \"\"\"\n    This function takes an object, serializes it into a JSON string, and handles datetime and decimal types.\n    It uses the json module's default encoder, but it does not handle complex data types natively.\n    The 'my_class' parameter is reserved for future use and is not currently used in the current implementation.\n    It serializes a simple dictionary.\n\n    Parameters:\n    - my_obj (object): The object to be serialized, which may contain datetime and decimal types.\n\n    Returns:\n        str: A JSON-formatted string of the serialized object, properly handling datetime and decimal objects.\n\n    Requirements:\n    - json\n    - datetime.datetime\n    - decimal.Decimal\n\n    Example:\n    >>> result = task_func({'time': datetime(2023, 4, 1, 12, 0), 'amount': Decimal('10.99')})\n    >>> '2023-04-01T12:00:00' in result and '10.99' in result\n    True\n    \"\"\"\n    class DateTimeEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, datetime):\n                return obj.isoformat()\n            elif isinstance(obj, Decimal):\n                return str(obj)\n            return super().default(obj)\n\n    return json.dumps(my_obj, cls=DateTimeEncoder)\n\n# Example usage:\nresult = task_func({'time': datetime(2023, 4, 1, 12, 0), 'amount': Decimal('10.99')})"}
{"task_id": "BigCodeBench/465", "solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        else:\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)", "raw_solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\n\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        else:\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)\n\n# Test cases"}
{"task_id": "BigCodeBench/466", "solution": "import json\nfrom enum import Enum\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Enum):\n            return obj.name\n        return super().default(obj)\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)", "raw_solution": "import json\nfrom enum import Enum\n\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n    BLUE = 3\n\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Enum):\n            return obj.name\n        return super().default(obj)\n\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)\n\n# Test cases"}
{"task_id": "BigCodeBench/467", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    x = np.random.rand(n)\n    y = np.random.rand(n)\n    \n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_title(\"Random Points Scatter Plot\")\n    \n    points = list(zip(x, y))\n    \n    return fig, points", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    x = np.random.rand(n)\n    y = np.random.rand(n)\n    \n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_title(\"Random Points Scatter Plot\")\n    \n    points = list(zip(x, y))\n    \n    return fig, points\n\n# Example usage:\n# fig, points = task_func(5)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/468", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n\n    # Convert numeric values to floats\n    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n\n    # Plot a line chart of the data\n    ax = df[columns].plot(kind='line')\n    plt.title('Line Chart of Data')\n    plt.xlabel('Index')\n    plt.ylabel('Values')\n    plt.show()\n\n    # Calculate the cube root of the first column\n    croot = np.cbrt(df[columns[0]])\n\n    return df, ax, croot", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n\n    # Convert numeric values to floats\n    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n\n    # Plot a line chart of the data\n    ax = df[columns].plot(kind='line')\n    plt.title('Line Chart of Data')\n    plt.xlabel('Index')\n    plt.ylabel('Values')\n    plt.show()\n\n    # Calculate the cube root of the first column\n    croot = np.cbrt(df[columns[0]])\n\n    return df, ax, croot\n```\n"}
{"task_id": "BigCodeBench/469", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Convert all grades to uppercase and remove whitespace\n    cleaned_grades = [grade.upper().strip() for grade in student_grades]\n\n    # Count the occurrences of each grade\n    grade_counts = Counter(cleaned_grades)\n\n    # Create a DataFrame from the counts\n    report_df = pd.DataFrame.from_dict(grade_counts, orient='index', columns=['Count'])\n    report_df.index.name = 'Grade'\n\n    # Filter the DataFrame to only include possible grades\n    report_df = report_df.reindex(possible_grades)\n\n    # Plot the bar chart\n    ax = report_df.plot(kind='bar', legend=False)\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n    ax.set_title('Grade Distribution')\n\n    return report_df, ax\nstudent_grades = ['A', 'B', 'B', 'C', 'A', 'D', 'F', 'B', 'A', 'C']", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Convert all grades to uppercase and remove whitespace\n    cleaned_grades = [grade.upper().strip() for grade in student_grades]\n\n    # Count the occurrences of each grade\n    grade_counts = Counter(cleaned_grades)\n\n    # Create a DataFrame from the counts\n    report_df = pd.DataFrame.from_dict(grade_counts, orient='index', columns=['Count'])\n    report_df.index.name = 'Grade'\n\n    # Filter the DataFrame to only include possible grades\n    report_df = report_df.reindex(possible_grades)\n\n    # Plot the bar chart\n    ax = report_df.plot(kind='bar', legend=False)\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n    ax.set_title('Grade Distribution')\n\n    return report_df, ax\n\n# Example usage:\nstudent_grades = ['A', 'B', 'B', 'C', 'A', 'D', 'F', 'B', 'A', 'C']\nreport_df, ax = task_func(student_grades)\n```\n"}
{"task_id": "BigCodeBench/470", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(myList):\n    # Convert list to numpy array for easier manipulation\n    data = np.array(myList)\n    \n    # Calculate bin edges\n    bin_edges = np.arange(data.min(), data.max() + 1, 1)\n    \n    # Create histogram\n    counts, bin_edges, patches = plt.hist(data, bins=bin_edges, edgecolor='black', alpha=0.7)\n    \n    # Set labels and title\n    plt.xlabel('value')\n    plt.ylabel('frequency')\n    plt.title('values ka histogram')\n    \n    # Return the axes object\n    return plt.gca()", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(myList):\n    # Convert list to numpy array for easier manipulation\n    data = np.array(myList)\n    \n    # Calculate bin edges\n    bin_edges = np.arange(data.min(), data.max() + 1, 1)\n    \n    # Create histogram\n    counts, bin_edges, patches = plt.hist(data, bins=bin_edges, edgecolor='black', alpha=0.7)\n    \n    # Set labels and title\n    plt.xlabel('value')\n    plt.ylabel('frequency')\n    plt.title('values ka histogram')\n    \n    # Return the axes object\n    return plt.gca()\n```\n"}
{"task_id": "BigCodeBench/471", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(myList):\n    # Convert all strings to lowercase and split into words\n    words = [word.lower() for item in myList for word in item.split()]\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Create a DataFrame from the word counts\n    df = pd.DataFrame(word_counts.items(), columns=['Word', 'Count'])\n    \n    # Sort the DataFrame by count in descending order\n    df = df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n    \n    return df\nmyList = ['apple', 'banana', 'apple', 'cherry', 'banana', 'banana']", "raw_solution": "from collections import Counter\nimport pandas as pd\n\ndef task_func(myList):\n    # Convert all strings to lowercase and split into words\n    words = [word.lower() for item in myList for word in item.split()]\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Create a DataFrame from the word counts\n    df = pd.DataFrame(word_counts.items(), columns=['Word', 'Count'])\n    \n    # Sort the DataFrame by count in descending order\n    df = df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n    \n    return df\n\n# Example usage:\nmyList = ['apple', 'banana', 'apple', 'cherry', 'banana', 'banana']"}
{"task_id": "BigCodeBench/472", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\ndef task_func(myList, n_clusters):\n    if not isinstance(myList, list) or not all(isinstance(sublist, list) and len(sublist) == 2 for sublist in myList):\n        raise ValueError(\"myList should be a list of 2D points.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters should be a positive integer.\")\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    kmeans.fit(myList)\n    \n    plt.scatter(myList, [0]*len(myList), color='blue', label='Data Points')\n    plt.scatter(kmeans.cluster_centers_, [0]*len(kmeans.cluster_centers_), color='red', marker='x', label='Cluster Centers')\n    plt.title('KMeans Clustering')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    \n    return plt.gca()", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\ndef task_func(myList, n_clusters):\n    if not isinstance(myList, list) or not all(isinstance(sublist, list) and len(sublist) == 2 for sublist in myList):\n        raise ValueError(\"myList should be a list of 2D points.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters should be a positive integer.\")\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    kmeans.fit(myList)\n    \n    plt.scatter(myList, [0]*len(myList), color='blue', label='Data Points')\n    plt.scatter(kmeans.cluster_centers_, [0]*len(kmeans.cluster_centers_), color='red', marker='x', label='Cluster Centers')\n    plt.title('KMeans Clustering')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    \n    return plt.gca()\n```\n"}
{"task_id": "BigCodeBench/473", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(n_walks, n_steps, seed=None):\n    if n_walks <= 0 or n_steps <= 0:\n        raise ValueError(\"n_walks and n_steps must be positive integers.\")\n    \n    np.random.seed(seed)\n    \n    # Define the possible colors\n    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n    \n    # Create random walks\n    walks = np.random.choice([-1, 1], size=(n_walks, n_steps))\n    walk_positions = np.cumsum(walks, axis=1)\n    \n    # Plot the random walks\n    fig, ax = plt.subplots()\n    for walk, color in zip(walk_positions, itertools.cycle(colors)):\n        ax.plot(walk, color=color, alpha=0.6)\n    \n    ax.set_title('Random Walks')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    ax.set_xticks(np.arange(0, n_steps, n_steps // 5))\n    ax.set_xticklabels([str(i * (n_steps // 5)) for i in range(5)])\n    \n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(n_walks, n_steps, seed=None):\n    if n_walks <= 0 or n_steps <= 0:\n        raise ValueError(\"n_walks and n_steps must be positive integers.\")\n    \n    np.random.seed(seed)\n    \n    # Define the possible colors\n    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n    \n    # Create random walks\n    walks = np.random.choice([-1, 1], size=(n_walks, n_steps))\n    walk_positions = np.cumsum(walks, axis=1)\n    \n    # Plot the random walks\n    fig, ax = plt.subplots()\n    for walk, color in zip(walk_positions, itertools.cycle(colors)):\n        ax.plot(walk, color=color, alpha=0.6)\n    \n    ax.set_title('Random Walks')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    ax.set_xticks(np.arange(0, n_steps, n_steps // 5))\n    ax.set_xticklabels([str(i * (n_steps // 5)) for i in range(5)])\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/474", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    np.random.seed(random_seed)\n    samples = np.random.normal(mu, sigma, n_samples)\n    \n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    \n    pdf = norm.pdf(bins, mu, sigma)\n    ax.plot(bins, pdf, linewidth=2, color='r')\n    \n    ax.set_title('Histogram and PDF of Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    return ax, samples", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    np.random.seed(random_seed)\n    samples = np.random.normal(mu, sigma, n_samples)\n    \n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    \n    pdf = norm.pdf(bins, mu, sigma)\n    ax.plot(bins, pdf, linewidth=2, color='r')\n    \n    ax.set_title('Histogram and PDF of Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    return ax, samples\n```\n"}
{"task_id": "BigCodeBench/475", "solution": "import pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\ndef task_func(data, date_format, country, country_codes=None):\n    if not isinstance(data, pd.DataFrame) or not isinstance(date_format, str) or not isinstance(country, str) or not isinstance(country_codes, dict):\n        raise ValueError(\"Invalid input types\")\n\n    if 'dates' not in data.columns:\n        raise ValueError(\"DataFrame must contain 'dates' column\")\n\n    try:\n        data['dates'] = pd.to_datetime(data['dates'], format=date_format)\n    except ValueError:\n        raise ValueError(\"Date format is incorrect\")\n\n    if country not in country_codes:\n        raise ValueError(\"Country not found in country codes\")\n\n    plt.figure(figsize=(10, 6))\n    ax = data['dates'].dt.to_period('M').value_counts().sort_index().plot(kind='bar')\n    ax.set_title('Date Distribution')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Frequency')\n\n    return ax\ndata = pd.DataFrame({'dates': ['01/01/2000', '01/02/2000', '02/03/2000', '04/05/2000', '06/07/2000']})\ncountry_codes = {\n    'Russia': 'ru_RU',\n    'Germany': 'de_de',\n    'France': 'fr_FR',\n    'Spain': 'es_ES',\n    'Italy': 'it_IT'\n}", "raw_solution": "import pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndef task_func(data, date_format, country, country_codes=None):\n    if not isinstance(data, pd.DataFrame) or not isinstance(date_format, str) or not isinstance(country, str) or not isinstance(country_codes, dict):\n        raise ValueError(\"Invalid input types\")\n\n    if 'dates' not in data.columns:\n        raise ValueError(\"DataFrame must contain 'dates' column\")\n\n    try:\n        data['dates'] = pd.to_datetime(data['dates'], format=date_format)\n    except ValueError:\n        raise ValueError(\"Date format is incorrect\")\n\n    if country not in country_codes:\n        raise ValueError(\"Country not found in country codes\")\n\n    plt.figure(figsize=(10, 6))\n    ax = data['dates'].dt.to_period('M').value_counts().sort_index().plot(kind='bar')\n    ax.set_title('Date Distribution')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Frequency')\n\n    return ax\n\n# Example usage:\ndata = pd.DataFrame({'dates': ['01/01/2000', '01/02/2000', '02/03/2000', '04/05/2000', '06/07/2000']})\ncountry_codes = {\n    'Russia': 'ru_RU',\n    'Germany': 'de_de',\n    'France': 'fr_FR',\n    'Spain': 'es_ES',\n    'Italy': 'it_IT'\n}\nax = task_func(data, '%d/%m/%Y', 'Russia', country_codes)"}
{"task_id": "BigCodeBench/476", "solution": "import matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nimport numpy as np\ndef task_func(X, Y):\n    # Define a quadratic function\n    def quadratic(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    # Fit the quadratic function to the data\n    params, _ = curve_fit(quadratic, X, Y)\n\n    # Generate data points for the fitted curve\n    X_fit = np.linspace(min(X), max(X), 100)\n    Y_fit = quadratic(X_fit, *params)\n\n    # Plot the data and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y, label='Data Points')\n    ax.plot(X_fit, Y_fit, color='red', label='Fitted Curve')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Quadratic Fit')\n    ax.legend()\n\n    return params, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nimport numpy as np\n\ndef task_func(X, Y):\n    # Define a quadratic function\n    def quadratic(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    # Fit the quadratic function to the data\n    params, _ = curve_fit(quadratic, X, Y)\n\n    # Generate data points for the fitted curve\n    X_fit = np.linspace(min(X), max(X), 100)\n    Y_fit = quadratic(X_fit, *params)\n\n    # Plot the data and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y, label='Data Points')\n    ax.plot(X_fit, Y_fit, color='red', label='Fitted Curve')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Quadratic Fit')\n    ax.legend()\n\n    return params, ax\n\n# Example usage"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random data\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Generate random categories\n    if N > len(CATEGORIES):\n        categories = np.random.choice(CATEGORIES, N, replace=True)\n    else:\n        categories = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        \"x\": x,\n        \"y\": y,\n        \"category\": categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        subset = df[df[\"category\"] == category]\n        ax.scatter(subset[\"x\"], subset[\"y\"], label=category)\n    \n    ax.set_title(\"Scatter Plot\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.legend()\n    \n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random data\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Generate random categories\n    if N > len(CATEGORIES):\n        categories = np.random.choice(CATEGORIES, N, replace=True)\n    else:\n        categories = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        \"x\": x,\n        \"y\": y,\n        \"category\": categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        subset = df[df[\"category\"] == category]\n        ax.scatter(subset[\"x\"], subset[\"y\"], label=category)\n    \n    ax.set_title(\"Scatter Plot\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.legend()\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/478", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    modified_strings = []\n    for string in data_list:\n        # Remove leading and trailing whitespaces\n        string = string.strip()\n        # Split the string into substrings\n        substrings = string.split(', ')\n        # Randomly select a substring to remove\n        if substrings:\n            random_substring = random.choice(substrings)\n            # Remove the selected substring\n            modified_string = ', '.join(sub for sub in substrings if sub != random_substring)\n            modified_strings.append(modified_string)\n        else:\n            modified_strings.append('')\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport random\n\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    modified_strings = []\n    for string in data_list:\n        # Remove leading and trailing whitespaces\n        string = string.strip()\n        # Split the string into substrings\n        substrings = string.split(', ')\n        # Randomly select a substring to remove\n        if substrings:\n            random_substring = random.choice(substrings)\n            # Remove the selected substring\n            modified_string = ', '.join(sub for sub in substrings if sub != random_substring)\n            modified_strings.append(modified_string)\n        else:\n            modified_strings.append('')\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n\n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair, bag, lamp'], seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/479", "solution": "import random\nimport string\nimport pandas as pd\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    modified_strings = []\n    \n    for original_string in data_list:\n        # Remove leading and trailing whitespaces\n        original_string = original_string.strip()\n        \n        # Split the string into substrings\n        substrings = original_string.split(', ')\n        \n        # Choose a random substring to replace\n        random_substring = random.choice(substrings)\n        \n        # Generate a random string of the same length\n        random_string = ''.join(random.choices(string.ascii_lowercase, k=len(random_substring)))\n        \n        # Replace the random substring with the random string\n        modified_string = original_string.replace(random_substring, random_string)\n        \n        modified_strings.append(modified_string)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n    \n    return df", "raw_solution": "import random\nimport string\nimport pandas as pd\n\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    modified_strings = []\n    \n    for original_string in data_list:\n        # Remove leading and trailing whitespaces\n        original_string = original_string.strip()\n        \n        # Split the string into substrings\n        substrings = original_string.split(', ')\n        \n        # Choose a random substring to replace\n        random_substring = random.choice(substrings)\n        \n        # Generate a random string of the same length\n        random_string = ''.join(random.choices(string.ascii_lowercase, k=len(random_substring)))\n        \n        # Replace the random substring with the random string\n        modified_string = original_string.replace(random_substring, random_string)\n        \n        modified_strings.append(modified_string)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n    \n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair, bag, lamp'])\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/480", "solution": "import re\nimport random\nimport pandas as pd\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    results = []\n    for data in data_list:\n        # Remove leading and trailing whitespaces and split by commas\n        substrings = [s.strip() for s in re.split(r'\\s*,\\s*', data)]\n\n        # Shuffle the substrings\n        random.shuffle(substrings)\n\n        # Join the substrings back into a single string\n        shuffled_string = ', '.join(substrings)\n\n        results.append({'Original String': data, 'Shuffled String': shuffled_string})\n\n    # Create a DataFrame from the results\n    df = pd.DataFrame(results)\n\n    return df", "raw_solution": "import re\nimport random\nimport pandas as pd\n\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    results = []\n    for data in data_list:\n        # Remove leading and trailing whitespaces and split by commas\n        substrings = [s.strip() for s in re.split(r'\\s*,\\s*', data)]\n\n        # Shuffle the substrings\n        random.shuffle(substrings)\n\n        # Join the substrings back into a single string\n        shuffled_string = ', '.join(substrings)\n\n        results.append({'Original String': data, 'Shuffled String': shuffled_string})\n\n    # Create a DataFrame from the results\n    df = pd.DataFrame(results)\n\n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair'], seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/481", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=42):\n    random.seed(seed)\n    original_strings = []\n    randomized_strings = []\n\n    for string in data_list:\n        # Normalize spaces\n        normalized_string = re.sub(r'\\s+', ' ', string)\n        # Split by comma\n        words = normalized_string.split(',')\n        # Shuffle the words\n        random.shuffle(words)\n        # Join the words back together with a comma\n        randomized_string = ', '.join(words)\n        # Append to lists\n        original_strings.append(normalized_string)\n        randomized_strings.append(randomized_string)\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Randomized String': randomized_strings\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(data_list, seed=42):\n    random.seed(seed)\n    original_strings = []\n    randomized_strings = []\n\n    for string in data_list:\n        # Normalize spaces\n        normalized_string = re.sub(r'\\s+', ' ', string)\n        # Split by comma\n        words = normalized_string.split(',')\n        # Shuffle the words\n        random.shuffle(words)\n        # Join the words back together with a comma\n        randomized_string = ', '.join(words)\n        # Append to lists\n        original_strings.append(normalized_string)\n        randomized_strings.append(randomized_string)\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Randomized String': randomized_strings\n    })\n\n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair, bag'], seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/482", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def modify_string(s):\n        items = s.split(', ')\n        if len(items) <= 1:\n            return s\n\n        operation = random.choice(['remove', 'replace', 'shuffle', 'random'])\n        if operation == 'remove':\n            items.pop(random.randint(0, len(items) - 1))\n        elif operation == 'replace':\n            index = random.randint(0, len(items) - 1)\n            items[index] = random_string()\n        elif operation == 'shuffle':\n            random.shuffle(items)\n        elif operation == 'random':\n            random.shuffle(items)\n            items = items[::-1]\n\n        return ', '.join(items)\n\n    def random_string():\n        return ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=5))\n\n    original_strings = data_list\n    modified_strings = [modify_string(s) for s in data_list]\n\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Modified String': modified_strings\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def modify_string(s):\n        items = s.split(', ')\n        if len(items) <= 1:\n            return s\n\n        operation = random.choice(['remove', 'replace', 'shuffle', 'random'])\n        if operation == 'remove':\n            items.pop(random.randint(0, len(items) - 1))\n        elif operation == 'replace':\n            index = random.randint(0, len(items) - 1)\n            items[index] = random_string()\n        elif operation == 'shuffle':\n            random.shuffle(items)\n        elif operation == 'random':\n            random.shuffle(items)\n            items = items[::-1]\n\n        return ', '.join(items)\n\n    def random_string():\n        return ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=5))\n\n    original_strings = data_list\n    modified_strings = [modify_string(s) for s in data_list]\n\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Modified String': modified_strings\n    })\n\n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair, bag, lamp'], seed=0)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/483", "solution": "import re\nimport pandas as pd\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    if pattern == \"\":\n        return df.copy()\n\n    def reverse_words_in_match(match):\n        words = match.group(0).split()\n        return ' '.join(reversed(words))\n\n    df[column_name] = df[column_name].apply(lambda x: re.sub(pattern, reverse_words_in_match, x))\n    return df", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    if pattern == \"\":\n        return df.copy()\n\n    def reverse_words_in_match(match):\n        words = match.group(0).split()\n        return ' '.join(reversed(words))\n\n    df[column_name] = df[column_name].apply(lambda x: re.sub(pattern, reverse_words_in_match, x))\n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/484", "solution": "import math\nimport numpy as np\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    np.random.seed(random_seed)\n    timestamps = np.arange(start_time, end_time, step)\n    data = {\n        \"Timestamp\": [\n            datetime.fromtimestamp(ts / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n            for ts in timestamps\n        ],\n        \"Sensor1\": [math.sin(ts / 1000) for ts in timestamps],\n        \"Sensor2\": [math.cos(ts / 1000) for ts in timestamps],\n        \"Sensor3\": [math.tan(ts / 1000) for ts in timestamps],\n        \"SensorStatus\": np.random.choice(sensor_statuses, len(timestamps)),\n    }\n\n    df = pd.DataFrame(data, columns=columns)\n    return df", "raw_solution": "import math\nimport numpy as np\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    np.random.seed(random_seed)\n    timestamps = np.arange(start_time, end_time, step)\n    data = {\n        \"Timestamp\": [\n            datetime.fromtimestamp(ts / 1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n            for ts in timestamps\n        ],\n        \"Sensor1\": [math.sin(ts / 1000) for ts in timestamps],\n        \"Sensor2\": [math.cos(ts / 1000) for ts in timestamps],\n        \"Sensor3\": [math.tan(ts / 1000) for ts in timestamps],\n        \"SensorStatus\": np.random.choice(sensor_statuses, len(timestamps)),\n    }\n\n    df = pd.DataFrame(data, columns=columns)\n    return df\n\n# Example usage:\n# df = task_func(0, 5000, 1000)\n# print(df.head(1))\n```\n"}
{"task_id": "BigCodeBench/485", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time):\n    # Convert string dates to datetime objects\n    start = datetime.strptime(start_time, \"%Y-%m-%d\")\n    end = datetime.strptime(end_time, \"%Y-%m-%d\")\n    \n    # Create a timezone object for UTC\n    utc = pytz.utc\n    \n    # Define the timezones to compare with UTC\n    timezones = [\"America/Los_Angeles\", \"Europe/Paris\", \"Asia/Kolkata\", \"Australia/Sydney\"]\n    \n    # Create a list to store the differences\n    differences = []\n    \n    # Iterate over the date range\n    current = start\n    while current <= end:\n        utc_time = utc.localize(datetime.combine(current, datetime.min.time()))\n        for tz in timezones:\n            tz_time = pytz.timezone(tz).localize(datetime.combine(current, datetime.min.time()))\n            difference = (utc_time - tz_time).total_seconds() / 3600\n            differences.append(difference)\n        \n        current += timedelta(days=1)\n    \n    # Create a numpy array for plotting\n    differences = np.array(differences)\n    \n    # Plot the differences\n    fig, ax = plt.subplots()\n    ax.plot(differences, label=\"Time Differences\")\n    ax.set_title(\"Time Differences Between UTC and Timezones\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Difference (hours)\")\n    ax.legend()\n    \n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time):\n    # Convert string dates to datetime objects\n    start = datetime.strptime(start_time, \"%Y-%m-%d\")\n    end = datetime.strptime(end_time, \"%Y-%m-%d\")\n    \n    # Create a timezone object for UTC\n    utc = pytz.utc\n    \n    # Define the timezones to compare with UTC\n    timezones = [\"America/Los_Angeles\", \"Europe/Paris\", \"Asia/Kolkata\", \"Australia/Sydney\"]\n    \n    # Create a list to store the differences\n    differences = []\n    \n    # Iterate over the date range\n    current = start\n    while current <= end:\n        utc_time = utc.localize(datetime.combine(current, datetime.min.time()))\n        for tz in timezones:\n            tz_time = pytz.timezone(tz).localize(datetime.combine(current, datetime.min.time()))\n            difference = (utc_time - tz_time).total_seconds() / 3600\n            differences.append(difference)\n        \n        current += timedelta(days=1)\n    \n    # Create a numpy array for plotting\n    differences = np.array(differences)\n    \n    # Plot the differences\n    fig, ax = plt.subplots()\n    ax.plot(differences, label=\"Time Differences\")\n    ax.set_title(\"Time Differences Between UTC and Timezones\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Difference (hours)\")\n    ax.legend()\n    \n    return ax\n\n# Example usage:\n# ax = task_func('2021-01-01', '2021-01-10')\n# print(type(ax))\n# print(ax.get_xticklabels())\n```\n"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    if start_time >= end_time:\n        raise ValueError(\"start_time must be less than end_time\")\n    if step <= 0:\n        raise ValueError(\"step must be greater than 0\")\n\n    np.random.seed(seed)\n    time_series = pd.Series(dtype='float64')\n\n    for i in range(start_time, end_time, step):\n        value = np.random.normal(0, 1)\n        value += trend * i\n        time_series.loc[i] = value\n\n    time_series.index = pd.to_datetime(time_series.index, unit='ms')\n\n    fig, ax = plt.subplots()\n    ax.plot(time_series.index, time_series.values)\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series Plot')\n\n    return ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    if start_time >= end_time:\n        raise ValueError(\"start_time must be less than end_time\")\n    if step <= 0:\n        raise ValueError(\"step must be greater than 0\")\n\n    np.random.seed(seed)\n    time_series = pd.Series(dtype='float64')\n\n    for i in range(start_time, end_time, step):\n        value = np.random.normal(0, 1)\n        value += trend * i\n        time_series.loc[i] = value\n\n    time_series.index = pd.to_datetime(time_series.index, unit='ms')\n\n    fig, ax = plt.subplots()\n    ax.plot(time_series.index, time_series.values)\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series Plot')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/487", "solution": "import os\nimport pandas as pd\nimport re\ndef task_func(file_path: str) -> pd.DataFrame:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    log_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{6}) - ([A-Z]+) - (.+)')\n    data = {'Timestamp': [], 'Level': [], 'Message': []}\n    \n    with open(file_path, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line)\n            if match:\n                data['Timestamp'].append(match.group(1))\n                data['Level'].append(match.group(2))\n                data['Message'].append(match.group(3))\n    \n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import os\nimport pandas as pd\nimport re\n\ndef task_func(file_path: str) -> pd.DataFrame:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    log_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{6}) - ([A-Z]+) - (.+)')\n    data = {'Timestamp': [], 'Level': [], 'Message': []}\n    \n    with open(file_path, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line)\n            if match:\n                data['Timestamp'].append(match.group(1))\n                data['Level'].append(match.group(2))\n                data['Message'].append(match.group(3))\n    \n    df = pd.DataFrame(data)\n    return df\n```\n"}
{"task_id": "BigCodeBench/488", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, amplitude, period, seed=0):\n    np.random.seed(seed)\n    \n    # Convert start_time and end_time from milliseconds to datetime\n    start_time = datetime.utcfromtimestamp(start_time / 1000.0)\n    end_time = datetime.utcfromtimestamp(end_time / 1000.0)\n    \n    # Generate time range\n    time_range = pd.date_range(start=start_time, end=end_time, freq=f\"{step}ms\")\n    \n    # Generate time series data with seasonality\n    time_series = amplitude * np.sin(2 * np.pi * time_range.to_julian_date() / (period / 1000.0))\n    \n    # Create DataFrame\n    df = pd.DataFrame({'Timestamp': time_range, 'value': time_series})\n    \n    # Plot the time series\n    fig, ax = plt.subplots()\n    ax.plot(df['Timestamp'], df['value'])\n    ax.set_xlabel('Timestamp')\n    ax.set_ylabel('value')\n    ax.set_title('Time Series with Seasonality')\n    \n    return ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, amplitude, period, seed=0):\n    np.random.seed(seed)\n    \n    # Convert start_time and end_time from milliseconds to datetime\n    start_time = datetime.utcfromtimestamp(start_time / 1000.0)\n    end_time = datetime.utcfromtimestamp(end_time / 1000.0)\n    \n    # Generate time range\n    time_range = pd.date_range(start=start_time, end=end_time, freq=f\"{step}ms\")\n    \n    # Generate time series data with seasonality\n    time_series = amplitude * np.sin(2 * np.pi * time_range.to_julian_date() / (period / 1000.0))\n    \n    # Create DataFrame\n    df = pd.DataFrame({'Timestamp': time_range, 'value': time_series})\n    \n    # Plot the time series\n    fig, ax = plt.subplots()\n    ax.plot(df['Timestamp'], df['value'])\n    ax.set_xlabel('Timestamp')\n    ax.set_ylabel('value')\n    ax.set_title('Time Series with Seasonality')\n    \n    return ax\n\n# Example usage:\n# ax = task_func(0, 10000, 100, 1, 1000)\n# print(type(ax))\n# print(ax.get_xticklabels())\n```\n"}
{"task_id": "BigCodeBench/489", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(epoch_milliseconds, seed=0):\n    random.seed(seed)\n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n    \n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    \n    if start_time > current_time:\n        raise ValueError(\"Start time cannot be later than the current system time.\")\n    \n    data = []\n    while start_time <= current_time:\n        user = random.choice(users)\n        activity = random.choice(activities)\n        data.append({'user': user, 'activity': activity, 'time': start_time})\n        start_time += timedelta(seconds=random.randint(1, 10))\n    \n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(epoch_milliseconds, seed=0):\n    random.seed(seed)\n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n    \n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    \n    if start_time > current_time:\n        raise ValueError(\"Start time cannot be later than the current system time.\")\n    \n    data = []\n    while start_time <= current_time:\n        user = random.choice(users)\n        activity = random.choice(activities)\n        data.append({'user': user, 'activity': activity, 'time': start_time})\n        start_time += timedelta(seconds=random.randint(1, 10))\n    \n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\n# log = task_func(1615168051807)\n# print(log.head())\n```\nThis script generates a DataFrame with user activity logs, where each log entry includes a user, an activity, and a time-stamp. The time-stamps are generated incrementally from the provided epoch time, with a random delay between each log entry. The script also includes error handling for cases where the start time is later than the current system time.\n"}
{"task_id": "BigCodeBench/490", "solution": "import xmltodict\nimport json\ndef task_func(s, file_path):\n    # Convert XML string to dictionary\n    data_dict = xmltodict.parse(s)\n    \n    # Convert dictionary to JSON and save to file\n    with open(file_path, 'w') as json_file:\n        json.dump(data_dict, json_file, indent=4)\n    \n    return data_dict", "raw_solution": "import xmltodict\nimport json\n\ndef task_func(s, file_path):\n    # Convert XML string to dictionary\n    data_dict = xmltodict.parse(s)\n    \n    # Convert dictionary to JSON and save to file\n    with open(file_path, 'w') as json_file:\n        json.dump(data_dict, json_file, indent=4)\n    \n    return data_dict\n\n# Example usage:\n# result = task_func('<person><name>John</name><age>30</age></person>', \"temp.json\")\n# print(result['person']['name'] + ', ' + result['person']['age'])\n```\n"}
{"task_id": "BigCodeBench/491", "solution": "import random\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\ndef task_func(epoch_milliseconds, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer.\")\n\n    start_date = datetime.utcfromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.utcnow()\n    if start_date > end_date:\n        raise ValueError(\"start_date must be before or equal to the current time.\")\n\n    categories = ['Electronics', 'Books', 'Home', 'Sports', 'Fiction']\n    sales_data = {category: [] for category in categories}\n\n    current_date = start_date\n    while current_date <= end_date:\n        for category in categories:\n            sales = random.randint(10, 50)\n            sales_data[category].append(sales)\n        current_date += timedelta(days=1)\n\n    days = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range((end_date - start_date).days + 1)]\n\n    fig, ax = plt.subplots()\n    for category in categories:\n        ax.plot(days, sales_data[category], label=category)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Trend Over Time')\n    ax.legend()\n\n    return sales_data, ax", "raw_solution": "import random\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\ndef task_func(epoch_milliseconds, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer.\")\n\n    start_date = datetime.utcfromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.utcnow()\n    if start_date > end_date:\n        raise ValueError(\"start_date must be before or equal to the current time.\")\n\n    categories = ['Electronics', 'Books', 'Home', 'Sports', 'Fiction']\n    sales_data = {category: [] for category in categories}\n\n    current_date = start_date\n    while current_date <= end_date:\n        for category in categories:\n            sales = random.randint(10, 50)\n            sales_data[category].append(sales)\n        current_date += timedelta(days=1)\n\n    days = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range((end_date - start_date).days + 1)]\n\n    fig, ax = plt.subplots()\n    for category in categories:\n        ax.plot(days, sales_data[category], label=category)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Trend Over Time')\n    ax.legend()\n\n    return sales_data, ax\n```\n"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    random.seed(random_seed)\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_time = datetime.now()\n    date_range = pd.date_range(start_time, end_time, freq='D')\n    \n    data = []\n    for date in date_range:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append([product, date, sales])\n    \n    df = pd.DataFrame(data, columns=[\"Product\", \"Date\", \"Sales\"])\n    return df", "raw_solution": "import pandas as pd\nfrom datetime import datetime\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    random.seed(random_seed)\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_time = datetime.now()\n    date_range = pd.date_range(start_time, end_time, freq='D')\n    \n    data = []\n    for date in date_range:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append([product, date, sales])\n    \n    df = pd.DataFrame(data, columns=[\"Product\", \"Date\", \"Sales\"])\n    return df\n\n# Example usage:\n# sales_data = task_func(1236472051807, random_seed=42)\n# print(sales_data.head())\n```\n"}
{"task_id": "BigCodeBench/493", "solution": "from datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    today = datetime.now()\n    days = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range((today - start_date).days + 1)]\n\n    performance_data = {team: [] for team in teams}\n    for day in days:\n        for team in teams:\n            performance_data[team].append(random.uniform(0.1, 1.0))\n\n    fig, ax = plt.subplots()\n    for team, data in performance_data.items():\n        ax.plot(days, data, label=team)\n\n    ax.set_xlabel('Days')\n    ax.set_ylabel('Performance')\n    ax.set_title('Performance Trend Over Time')\n    ax.legend()\n\n    return performance_data, fig", "raw_solution": "from datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    today = datetime.now()\n    days = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range((today - start_date).days + 1)]\n\n    performance_data = {team: [] for team in teams}\n    for day in days:\n        for team in teams:\n            performance_data[team].append(random.uniform(0.1, 1.0))\n\n    fig, ax = plt.subplots()\n    for team, data in performance_data.items():\n        ax.plot(days, data, label=team)\n\n    ax.set_xlabel('Days')\n    ax.set_ylabel('Performance')\n    ax.set_title('Performance Trend Over Time')\n    ax.legend()\n\n    return performance_data, fig\n```\n"}
{"task_id": "BigCodeBench/494", "solution": "from datetime import datetime, time\nimport pytz\nimport re\nfrom faker import Faker\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    fake = Faker()\n    if seed is not None:\n        fake.seed_instance(seed)\n\n    # Convert epoch milliseconds to datetime object\n    dt = datetime.fromtimestamp(epoch_milliseconds / 1000.0, pytz.utc)\n\n    # Generate fake event name\n    event_name = fake.first_name()\n\n    # Prepare the schedule\n    schedule = []\n    for tz in timezones:\n        try:\n            # Convert to specified timezone\n            tz_info = pytz.timezone(tz)\n            local_dt = dt.astimezone(tz_info)\n            schedule.append({\n                'date': local_dt.date(),\n                'time': local_dt.time(),\n                'timezone': tz\n            })\n        except pytz.UnknownTimeZoneError:\n            # Handle unknown timezones\n            continue\n\n    # If no valid timezones were found, use UTC\n    if not schedule:\n        schedule.append({\n            'date': dt.date(),\n            'time': dt.time(),\n            'timezone': 'UTC'\n        })\n\n    # Return the event name and schedule\n    return {event_name: schedule}", "raw_solution": "from datetime import datetime, time\nimport pytz\nimport re\nfrom faker import Faker\n\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    fake = Faker()\n    if seed is not None:\n        fake.seed_instance(seed)\n\n    # Convert epoch milliseconds to datetime object\n    dt = datetime.fromtimestamp(epoch_milliseconds / 1000.0, pytz.utc)\n\n    # Generate fake event name\n    event_name = fake.first_name()\n\n    # Prepare the schedule\n    schedule = []\n    for tz in timezones:\n        try:\n            # Convert to specified timezone\n            tz_info = pytz.timezone(tz)\n            local_dt = dt.astimezone(tz_info)\n            schedule.append({\n                'date': local_dt.date(),\n                'time': local_dt.time(),\n                'timezone': tz\n            })\n        except pytz.UnknownTimeZoneError:\n            # Handle unknown timezones\n            continue\n\n    # If no valid timezones were found, use UTC\n    if not schedule:\n        schedule.append({\n            'date': dt.date(),\n            'time': dt.time(),\n            'timezone': 'UTC'\n        })\n\n    # Return the event name and schedule\n    return {event_name: schedule}\n\n# Example usage:\n# print(task_func(1236472051807, seed=42))\n# print(task_func(1609459200000, seed=24, timezones=['UTC', 'UTC+01:00']))\n```\n"}
{"task_id": "BigCodeBench/495", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(days, random_seed=0):\n    np.random.seed(random_seed)\n    date_rng = pd.date_range(start=\"2023-01-01\", periods=days, freq=\"D\")\n    data = {\n        \"Groceries\": np.random.randint(0, 100, size=(days)),\n        \"Entertainment\": np.random.randint(0, 100, size=(days)),\n        \"Rent\": np.random.randint(0, 100, size=(days)),\n        \"Utilities\": np.random.randint(0, 100, size=(days)),\n        \"Miscellaneous\": np.random.randint(0, 100, size=(days)),\n    }\n    df = pd.DataFrame(data, index=date_rng)\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(days, random_seed=0):\n    np.random.seed(random_seed)\n    date_rng = pd.date_range(start=\"2023-01-01\", periods=days, freq=\"D\")\n    data = {\n        \"Groceries\": np.random.randint(0, 100, size=(days)),\n        \"Entertainment\": np.random.randint(0, 100, size=(days)),\n        \"Rent\": np.random.randint(0, 100, size=(days)),\n        \"Utilities\": np.random.randint(0, 100, size=(days)),\n        \"Miscellaneous\": np.random.randint(0, 100, size=(days)),\n    }\n    df = pd.DataFrame(data, index=date_rng)\n    return df\n\n# Example usage:\n# df = task_func(5, random_seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/496", "solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be at least 1\")\n\n    np.random.seed(random_seed)\n    today = datetime.now()\n    dates = [today - timedelta(days=i) for i in range(days_in_past)]\n    temperatures = np.random.randint(15, 36, size=days_in_past)\n\n    fig, ax = plt.subplots()\n    ax.plot(dates, temperatures, marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Temperature (degC)')\n    ax.set_title('Temperature Trend')\n    ax.set_xticks(dates)\n    ax.set_xticklabels([date.strftime('%Y-%m-%d') for date in dates], rotation=45)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be at least 1\")\n\n    np.random.seed(random_seed)\n    today = datetime.now()\n    dates = [today - timedelta(days=i) for i in range(days_in_past)]\n    temperatures = np.random.randint(15, 36, size=days_in_past)\n\n    fig, ax = plt.subplots()\n    ax.plot(dates, temperatures, marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Temperature (degC)')\n    ax.set_title('Temperature Trend')\n    ax.set_xticks(dates)\n    ax.set_xticklabels([date.strftime('%Y-%m-%d') for date in dates], rotation=45)\n    plt.tight_layout()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/497", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\ndef task_func(days_in_past=7):\n    if days_in_past < 0:\n        raise ValueError(\"days_in_past must be a non-negative integer\")\n\n    # Get the current UTC time\n    utc_now = datetime.now(pytz.utc)\n\n    # Calculate the date 'days_in_past' days ago\n    past_date = utc_now - timedelta(days=days_in_past)\n\n    # Get the weekday of the past date\n    weekday = past_date.weekday()\n\n    # Get the name of the weekday\n    weekday_name = calendar.day_name[weekday]\n\n    return weekday_name", "raw_solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\n\ndef task_func(days_in_past=7):\n    if days_in_past < 0:\n        raise ValueError(\"days_in_past must be a non-negative integer\")\n\n    # Get the current UTC time\n    utc_now = datetime.now(pytz.utc)\n\n    # Calculate the date 'days_in_past' days ago\n    past_date = utc_now - timedelta(days=days_in_past)\n\n    # Get the weekday of the past date\n    weekday = past_date.weekday()\n\n    # Get the name of the weekday\n    weekday_name = calendar.day_name[weekday]\n\n    return weekday_name\n\n# Example usage:\n# print(task_func())  # Output: 'Monday'\n# print(task_func(3))  # Output: 'Wednesday'\n```\n"}
{"task_id": "BigCodeBench/498", "solution": "import xmltodict\nimport json\ndef task_func(s, save_json=False, json_file_path=None):\n    if not s.strip():\n        raise ValueError(\"Input XML string is empty or only contains whitespace.\")\n    \n    # Convert XML to dictionary\n    result_dict = xmltodict.parse(s)\n    \n    # Save dictionary to JSON file if required\n    if save_json and json_file_path:\n        with open(json_file_path, 'w') as json_file:\n            json.dump(result_dict, json_file, indent=4)\n    \n    return result_dict", "raw_solution": "import xmltodict\nimport json\n\ndef task_func(s, save_json=False, json_file_path=None):\n    if not s.strip():\n        raise ValueError(\"Input XML string is empty or only contains whitespace.\")\n    \n    # Convert XML to dictionary\n    result_dict = xmltodict.parse(s)\n    \n    # Save dictionary to JSON file if required\n    if save_json and json_file_path:\n        with open(json_file_path, 'w') as json_file:\n            json.dump(result_dict, json_file, indent=4)\n    \n    return result_dict\n\n# Example usage:\n# xml_string = '<person><name>John</name><age>30</age></person>'\n# result = task_func(xml_string, save_json=True, json_file_path='person.json')\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/499", "solution": "import xlwt\nimport os\nimport io\nimport csv\ndef task_func(csv_content, filename):\n    # Create a workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Read CSV content\n    csv_reader = csv.reader(io.StringIO(csv_content))\n    for row_idx, row in enumerate(csv_reader):\n        for col_idx, value in enumerate(row):\n            sheet.write(row_idx, col_idx, value)\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the created file\n    return os.path.abspath(filename)\ncsv_content = 'ID,Name,Age\\n1,John Doe,30\\n2,Jane Doe,28'\nfilename = 'test_data.xls'", "raw_solution": "import xlwt\nimport os\nimport io\nimport csv\n\ndef task_func(csv_content, filename):\n    # Create a workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Read CSV content\n    csv_reader = csv.reader(io.StringIO(csv_content))\n    for row_idx, row in enumerate(csv_reader):\n        for col_idx, value in enumerate(row):\n            sheet.write(row_idx, col_idx, value)\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the created file\n    return os.path.abspath(filename)\n\n# Test cases\ncsv_content = 'ID,Name,Age\\n1,John Doe,30\\n2,Jane Doe,28'\nfilename = 'test_data.xls'\nfile_path = task_func(csv_content, filename)"}
{"task_id": "BigCodeBench/500", "solution": "import xlwt\nimport os\nfrom collections import OrderedDict\nFIELDS = ['ID', 'Name', 'Age']\ndef task_func(values, filename):\n    # Create a new workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Write the header\n    for col, field in enumerate(FIELDS):\n        sheet.write(0, col, field)\n\n    # Write the data\n    for row, value in enumerate(values, start=1):\n        for col, field in enumerate(FIELDS):\n            sheet.write(row, col, value[field])\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the file\n    return os.path.abspath(filename)\nempty_data = []", "raw_solution": "import xlwt\nimport os\nfrom collections import OrderedDict\n\n# Constants\nFIELDS = ['ID', 'Name', 'Age']\n\ndef task_func(values, filename):\n    # Create a new workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Write the header\n    for col, field in enumerate(FIELDS):\n        sheet.write(0, col, field)\n\n    # Write the data\n    for row, value in enumerate(values, start=1):\n        for col, field in enumerate(FIELDS):\n            sheet.write(row, col, value[field])\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the file\n    return os.path.abspath(filename)\n\n# Test cases\nempty_data = []\npath = task_func(empty_data, 'empty_data.xls')\nassert os.path.exists(path) and 'empty_data.xls' in path\n\ndata = [OrderedDict([('ID', 1), ('Name', 'John Doe'), ('Age', 30)]),\n        OrderedDict([('ID', 2), ('Name', 'Jane Doe'), ('Age', 28)])]\npath = task_func(data, 'test_data.xls')\nassert os.path.exists(path) and 'test_data.xls' in path\n```\nThis script creates an Excel file with the given data and filename. It uses the `xlwt` library to create the Excel file and the `os` library to get the absolute path of the file. The `OrderedDict` is used to ensure that the data is in a specific order.\n"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    try:\n        # Convert JSON string to a pandas DataFrame\n        data = pd.read_json(json_str)\n    except ValueError:\n        raise ValueError(\"Invalid JSON string provided.\")\n    \n    # Create a new Excel workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet(sheet_name)\n    \n    # Write the DataFrame to the sheet\n    for col_index, col in enumerate(data.columns):\n        sheet.write(0, col_index, col)\n    for row_index, row in data.iterrows():\n        for col_index, col in enumerate(data.columns):\n            sheet.write(row_index + 1, col_index, row[col])\n    \n    # Save the workbook\n    workbook.save(filename)\n    \n    return os.path.abspath(filename)", "raw_solution": "import xlwt\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    try:\n        # Convert JSON string to a pandas DataFrame\n        data = pd.read_json(json_str)\n    except ValueError:\n        raise ValueError(\"Invalid JSON string provided.\")\n    \n    # Create a new Excel workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet(sheet_name)\n    \n    # Write the DataFrame to the sheet\n    for col_index, col in enumerate(data.columns):\n        sheet.write(0, col_index, col)\n    for row_index, row in data.iterrows():\n        for col_index, col in enumerate(data.columns):\n            sheet.write(row_index + 1, col_index, row[col])\n    \n    # Save the workbook\n    workbook.save(filename)\n    \n    return os.path.abspath(filename)\n\n# Example usage:\n# json_str = '[{\"Name\": \"John\", \"Age\": 30}, {\"Name\": \"Jane\", \"Age\": 28}]'\n# print(task_func(json_str, 'data.xls'))  # Output: /absolute/path/to/data.xls\n# os.remove('data.xls')\n```\n"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate dates\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    dates = [start_date + timedelta(days=i) for i in range(days_in_past)]\n    \n    # Generate random data\n    data = []\n    for date in dates:\n        activity = random.choice(activities)\n        duration = random.randint(0, 120)\n        data.append([date.strftime(\"%Y-%m-%d\"), activity, duration])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    axes = sns.lineplot(data=df, x=\"Date\", y=\"Duration\", hue=\"Activity\")\n    plt.xticks(rotation=45)\n    plt.title(\"Daily Activity Durations\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Duration\")\n    plt.tight_layout()\n    \n    return axes, df", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate dates\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    dates = [start_date + timedelta(days=i) for i in range(days_in_past)]\n    \n    # Generate random data\n    data = []\n    for date in dates:\n        activity = random.choice(activities)\n        duration = random.randint(0, 120)\n        data.append([date.strftime(\"%Y-%m-%d\"), activity, duration])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    axes = sns.lineplot(data=df, x=\"Date\", y=\"Duration\", hue=\"Activity\")\n    plt.xticks(rotation=45)\n    plt.title(\"Daily Activity Durations\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Duration\")\n    plt.tight_layout()\n    \n    return axes, df\n```\n"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    np.random.seed(random_seed)\n    date_today = datetime.now()\n    dates = [date_today - timedelta(days=i) for i in range(days_in_past)]\n    data = {stock: np.random.rand(days_in_past) for stock in stock_names}\n    df = pd.DataFrame(data, index=dates)\n    df.index.name = 'Date'\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    np.random.seed(random_seed)\n    date_today = datetime.now()\n    dates = [date_today - timedelta(days=i) for i in range(days_in_past)]\n    data = {stock: np.random.rand(days_in_past) for stock in stock_names}\n    df = pd.DataFrame(data, index=dates)\n    df.index.name = 'Date'\n    return df\n\n# Example usage:\ndf = task_func(5, random_seed=42)"}
{"task_id": "BigCodeBench/504", "solution": "import hashlib\nimport rsa\nimport base64\ndef task_func(file_path):\n    # Read the file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # Generate SHA-256 hash of the file content\n    sha256_hash = hashlib.sha256(file_content).digest()\n\n    # Load the private key\n    with open('private.pem', 'rb') as key_file:\n        private_key = rsa.PrivateKey.load_pkcs1(key_file.read())\n\n    # Sign the hash with the private key\n    signed_hash = rsa.sign(sha256_hash, private_key, 'SHA-256')\n\n    # Encode the signed hash in base64\n    base64_encoded_signed_hash = base64.b64encode(signed_hash).decode('utf-8')\n\n    return base64_encoded_signed_hash", "raw_solution": "import hashlib\nimport rsa\nimport base64\n\ndef task_func(file_path):\n    # Read the file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # Generate SHA-256 hash of the file content\n    sha256_hash = hashlib.sha256(file_content).digest()\n\n    # Load the private key\n    with open('private.pem', 'rb') as key_file:\n        private_key = rsa.PrivateKey.load_pkcs1(key_file.read())\n\n    # Sign the hash with the private key\n    signed_hash = rsa.sign(sha256_hash, private_key, 'SHA-256')\n\n    # Encode the signed hash in base64\n    base64_encoded_signed_hash = base64.b64encode(signed_hash).decode('utf-8')\n\n    return base64_encoded_signed_hash\n```\nThis script reads the file content, generates a SHA-256 hash of the file content, signs the hash with a private RSA key, and then encodes the signed hash in base64. The function returns the base64 encoded signed hash. The script assumes that there is a valid 'private.pem' file present in the same directory as the script.\n"}
{"task_id": "BigCodeBench/505", "solution": "import hashlib\nimport hmac\ndef task_func(secret, message):\n    \"\"\"\n    Ek hmac (hash-based world authentication code) signature generate karta hai jo ek secret key ka upyog karke diye gaye world ke liye banaya jaata hai.\n    Function hmac signature banane ke liye hash function ke roop mein SHA-256 ka upyog karta hai.\n    Same key ke saath ek alag world ke liye ek hmac generate karein.\n    >>> len(task_func('mysecretkey', 'Goodbye, world!')) == 64\n    true\n\n    Parameters:\n    - secret (str): hmac generation ke liye upyog ki jaane wali secret key.\n    - message (str): Vah world jiske liye hmac signature banaya jaana hai.\n\n    Returns:\n        str: World ka hmac signature, ek hexadecimal string ke roop mein lautaya gaya.\n\n    Requirements:\n    - hashlib\n    - hmac\n\n    Example:\n    Examples:\n    World ke liye hmac signature generate karein.\n    >>> len(task_func('mysecretkey', 'Hello, world!')) == 64\n    true\n\n    \"\"\"\n    # Convert secret key to bytes\n    secret_bytes = secret.encode('utf-8')\n    \n    # Convert message to bytes\n    message_bytes = message.encode('utf-8')\n    \n    # Create a new hmac object using the secret key and SHA-256 hash function\n    hmac_obj = hmac.new(secret_bytes, message_bytes, hashlib.sha256)\n    \n    # Return the hexadecimal representation of the hmac signature\n    return hmac_obj.hexdigest()", "raw_solution": "import hashlib\nimport hmac\n\ndef task_func(secret, message):\n    \"\"\"\n    Ek hmac (hash-based world authentication code) signature generate karta hai jo ek secret key ka upyog karke diye gaye world ke liye banaya jaata hai.\n    Function hmac signature banane ke liye hash function ke roop mein SHA-256 ka upyog karta hai.\n    Same key ke saath ek alag world ke liye ek hmac generate karein.\n    >>> len(task_func('mysecretkey', 'Goodbye, world!')) == 64\n    true\n\n    Parameters:\n    - secret (str): hmac generation ke liye upyog ki jaane wali secret key.\n    - message (str): Vah world jiske liye hmac signature banaya jaana hai.\n\n    Returns:\n        str: World ka hmac signature, ek hexadecimal string ke roop mein lautaya gaya.\n\n    Requirements:\n    - hashlib\n    - hmac\n\n    Example:\n    Examples:\n    World ke liye hmac signature generate karein.\n    >>> len(task_func('mysecretkey', 'Hello, world!')) == 64\n    true\n\n    \"\"\"\n    # Convert secret key to bytes\n    secret_bytes = secret.encode('utf-8')\n    \n    # Convert message to bytes\n    message_bytes = message.encode('utf-8')\n    \n    # Create a new hmac object using the secret key and SHA-256 hash function\n    hmac_obj = hmac.new(secret_bytes, message_bytes, hashlib.sha256)\n    \n    # Return the hexadecimal representation of the hmac signature\n    return hmac_obj.hexdigest()\n```\n"}
{"task_id": "BigCodeBench/506", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=['Date', 'Temperature', 'Humidity', 'Wind Speed', 'Precipitation'])\n\n    # Calculate the sum, mean, min, and max of the specified column\n    sum_val = df[column].sum()\n    mean_val = df[column].mean()\n    min_val = df[column].min()\n    max_val = df[column].max()\n\n    # Plot the histogram of the specified column\n    plt.figure(figsize=(10, 6))\n    plot = df[column].plot(kind='hist', bins=10, color='skyblue', edgecolor='black')\n    plt.title(f'Histogram of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()\n\n    # Return the results as a dictionary\n    result = {\n        'sum': sum_val,\n        'mean': mean_val,\n        'min': min_val,\n        'max': max_val,\n        'plot': plot\n    }\n\n    return result\ndata = [\n    ['2022-01-01', -5, 80, 10, 0],\n    ['2022-01-03', -2, 83, 15, 0]\n]", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=['Date', 'Temperature', 'Humidity', 'Wind Speed', 'Precipitation'])\n\n    # Calculate the sum, mean, min, and max of the specified column\n    sum_val = df[column].sum()\n    mean_val = df[column].mean()\n    min_val = df[column].min()\n    max_val = df[column].max()\n\n    # Plot the histogram of the specified column\n    plt.figure(figsize=(10, 6))\n    plot = df[column].plot(kind='hist', bins=10, color='skyblue', edgecolor='black')\n    plt.title(f'Histogram of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()\n\n    # Return the results as a dictionary\n    result = {\n        'sum': sum_val,\n        'mean': mean_val,\n        'min': min_val,\n        'max': max_val,\n        'plot': plot\n    }\n\n    return result\n\n# Example usage:\ndata = [\n    ['2022-01-01', -5, 80, 10, 0],\n    ['2022-01-03', -2, 83, 15, 0]\n]\nresult = task_func('Temperature', data)"}
{"task_id": "BigCodeBench/507", "solution": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime\ndef task_func(column, data):\n    valid_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n    if column not in valid_columns:\n        raise ValueError(f\"Invalid column name. Valid options are {valid_columns}\")\n\n    df = pd.DataFrame(data, columns=valid_columns)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the data\")\n\n    if df.empty:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n\n    column_data = df[column]\n    result = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    return result\ndata = [[datetime(2022, 1, 1), 100, 105, 95, 102, 10000]]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef task_func(column, data):\n    valid_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n    if column not in valid_columns:\n        raise ValueError(f\"Invalid column name. Valid options are {valid_columns}\")\n\n    df = pd.DataFrame(data, columns=valid_columns)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the data\")\n\n    if df.empty:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n\n    column_data = df[column]\n    result = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    return result\n\n# Example usage:\ndata = [[datetime(2022, 1, 1), 100, 105, 95, 102, 10000]]\nresults = task_func('Open', data)"}
{"task_id": "BigCodeBench/508", "solution": "import hashlib\nimport io\nimport os\ndef task_func(file_path1, file_path2):\n    if not os.path.exists(file_path1):\n        raise FileNotFoundError(f\"The file {file_path1} does not exist.\")\n    if not os.path.exists(file_path2):\n        raise FileNotFoundError(f\"The file {file_path2} does not exist.\")\n\n    def calculate_md5(file_path):\n        hash_md5 = hashlib.md5()\n        with io.open(file_path, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n\n    return calculate_md5(file_path1) == calculate_md5(file_path2)", "raw_solution": "import hashlib\nimport io\nimport os\n\ndef task_func(file_path1, file_path2):\n    if not os.path.exists(file_path1):\n        raise FileNotFoundError(f\"The file {file_path1} does not exist.\")\n    if not os.path.exists(file_path2):\n        raise FileNotFoundError(f\"The file {file_path2} does not exist.\")\n\n    def calculate_md5(file_path):\n        hash_md5 = hashlib.md5()\n        with io.open(file_path, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n\n    return calculate_md5(file_path1) == calculate_md5(file_path2)\n```\n"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\nimport os\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    if not os.path.exists(file_path1):\n        raise FileNotFoundError(f\"File {file_path1} does not exist.\")\n    if not os.path.exists(file_path2):\n        raise FileNotFoundError(f\"File {file_path2} does not exist.\")\n    \n    try:\n        with open(file_path1, newline='', encoding='utf-8') as f1, open(file_path2, newline='', encoding='utf-8') as f2:\n            lines1 = f1.readlines()\n            lines2 = f2.readlines()\n    except Exception as e:\n        raise Exception(f\"Error reading files: {e}\")\n\n    diff = list(ndiff(lines1, lines2, lineterm=''))\n\n    diff_data = []\n    for line in diff:\n        if line.startswith('- ') or line.startswith('+ ') or line.startswith('  '):\n            line_number = int(line.split(',')[0].strip('-+ '))\n            status = ' ' if line.startswith('  ') else ('-' if line.startswith('- ') else '+')\n            content = line.split(',')[1].strip()\n            diff_data.append({'Line Number': line_number, 'Status': status, 'Content': content})\n\n    df = pd.DataFrame(diff_data)\n    return df", "raw_solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\nimport os\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    if not os.path.exists(file_path1):\n        raise FileNotFoundError(f\"File {file_path1} does not exist.\")\n    if not os.path.exists(file_path2):\n        raise FileNotFoundError(f\"File {file_path2} does not exist.\")\n    \n    try:\n        with open(file_path1, newline='', encoding='utf-8') as f1, open(file_path2, newline='', encoding='utf-8') as f2:\n            lines1 = f1.readlines()\n            lines2 = f2.readlines()\n    except Exception as e:\n        raise Exception(f\"Error reading files: {e}\")\n\n    diff = list(ndiff(lines1, lines2, lineterm=''))\n\n    diff_data = []\n    for line in diff:\n        if line.startswith('- ') or line.startswith('+ ') or line.startswith('  '):\n            line_number = int(line.split(',')[0].strip('-+ '))\n            status = ' ' if line.startswith('  ') else ('-' if line.startswith('- ') else '+')\n            content = line.split(',')[1].strip()\n            diff_data.append({'Line Number': line_number, 'Status': status, 'Content': content})\n\n    df = pd.DataFrame(diff_data)\n    return df\n\n# Example usage:\n# df = task_func('file1.csv', 'file2.csv')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/510", "solution": "import difflib\nimport gzip\ndef task_func(file_path1, file_path2):\n    def read_gzip_file(file_path):\n        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n            return f.readlines()\n\n    file1_lines = read_gzip_file(file_path1)\n    file2_lines = read_gzip_file(file_path2)\n\n    diff = difflib.unified_diff(file1_lines, file2_lines, lineterm='', fromfile=file_path1, tofile=file_path2)\n    diff_text = '\\n'.join(diff)\n\n    return diff_text", "raw_solution": "import difflib\nimport gzip\n\ndef task_func(file_path1, file_path2):\n    def read_gzip_file(file_path):\n        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n            return f.readlines()\n\n    file1_lines = read_gzip_file(file_path1)\n    file2_lines = read_gzip_file(file_path2)\n\n    diff = difflib.unified_diff(file1_lines, file2_lines, lineterm='', fromfile=file_path1, tofile=file_path2)\n    diff_text = '\\n'.join(diff)\n\n    return diff_text\n\n# Example usage:\n# Assuming 'file1.gz' and 'file2.gz' contain slightly different text,\n# result = task_func('file1.gz', 'file2.gz')\n# print(len(result) > 0)  # Should return True\n```\n"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None\n\n    df = pd.DataFrame(data, columns=['Age', 'Salary', 'Experience'])\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in the data.\")\n\n    column_data = df[column]\n\n    stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    ax = column_data.plot.pie(autopct='%1.1f%%', title=f'Distribution of {column}')\n    plt.show()\n\n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not data:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None\n\n    df = pd.DataFrame(data, columns=['Age', 'Salary', 'Experience'])\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in the data.\")\n\n    column_data = df[column]\n\n    stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    ax = column_data.plot.pie(autopct='%1.1f%%', title=f'Distribution of {column}')\n    plt.show()\n\n    return stats, ax\n```\n"}
{"task_id": "BigCodeBench/512", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if column not in ['Product', 'Quantity Sold', 'Total Sales']:\n        raise ValueError(\"Invalid column name. Choose from ['Product', 'Quantity Sold', 'Total Sales']\")\n    \n    df = pd.DataFrame(data, columns=['Product', 'Quantity Sold', 'Total Sales'])\n    \n    if column not in df.columns:\n        raise ValueError(\"Invalid column name. Choose from ['Product', 'Quantity Sold', 'Total Sales']\")\n    \n    if column == 'Total Sales' and df['Total Sales'].isnull().any():\n        raise ValueError(\"Total Sales contains null values\")\n    \n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    ax = df.plot.bar(x='Product', y=column, title=f'Bar Chart of {column}')\n    plt.xlabel('Product')\n    \n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if column not in ['Product', 'Quantity Sold', 'Total Sales']:\n        raise ValueError(\"Invalid column name. Choose from ['Product', 'Quantity Sold', 'Total Sales']\")\n    \n    df = pd.DataFrame(data, columns=['Product', 'Quantity Sold', 'Total Sales'])\n    \n    if column not in df.columns:\n        raise ValueError(\"Invalid column name. Choose from ['Product', 'Quantity Sold', 'Total Sales']\")\n    \n    if column == 'Total Sales' and df['Total Sales'].isnull().any():\n        raise ValueError(\"Total Sales contains null values\")\n    \n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    ax = df.plot.bar(x='Product', y=column, title=f'Bar Chart of {column}')\n    plt.xlabel('Product')\n    \n    return stats, ax\n```\n"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\ndef task_func(column, data):\n    # Validate column\n    valid_columns = ['date', 'kadam', 'burn ki gayi calories', 'chali gayi distance']\n    if column not in valid_columns:\n        raise KeyError(f\"Invalid column. Valid columns are: {valid_columns}\")\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['date', 'kadam', 'burn ki gayi calories', 'chali gayi distance'])\n\n    # Validate data\n    if df.empty:\n        raise ValueError(\"Data list is empty.\")\n    if not all(isinstance(row[1], (int, float)) and row[1] >= 0 for row in df.itertuples(index=False)):\n        raise ValueError(\"Steps must be a non-negative number.\")\n    if not all(isinstance(row[2], (int, float)) and row[2] >= 0 for row in df.itertuples(index=False)):\n        raise ValueError(\"Burn ki gayi calories must be a non-negative number.\")\n    if not all(isinstance(row[3], (int, float)) and row[3] >= 0 for row in df.itertuples(index=False)):\n        raise ValueError(\"Chali gayi distance must be a non-negative number.\")\n\n    # Calculate statistics\n    column_data = df[column]\n    stats = {\n        'sum': np.sum(column_data),\n        'mean': np.mean(column_data),\n        'min': np.min(column_data),\n        'max': np.max(column_data)\n    }\n\n    # Plot line chart\n    ax = df.plot(x='date', y=column, title=column)\n    plt.xlabel('Date')\n    plt.ylabel(column)\n\n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\ndef task_func(column, data):\n    # Validate column\n    valid_columns = ['date', 'kadam', 'burn ki gayi calories', 'chali gayi distance']\n    if column not in valid_columns:\n        raise KeyError(f\"Invalid column. Valid columns are: {valid_columns}\")\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['date', 'kadam', 'burn ki gayi calories', 'chali gayi distance'])\n\n    # Validate data\n    if df.empty:\n        raise ValueError(\"Data list is empty.\")\n    if not all(isinstance(row[1], (int, float)) and row[1] >= 0 for row in df.itertuples(index=False)):\n        raise ValueError(\"Steps must be a non-negative number.\")\n    if not all(isinstance(row[2], (int, float)) and row[2] >= 0 for row in df.itertuples(index=False)):\n        raise ValueError(\"Burn ki gayi calories must be a non-negative number.\")\n    if not all(isinstance(row[3], (int, float)) and row[3] >= 0 for row in df.itertuples(index=False)):\n        raise ValueError(\"Chali gayi distance must be a non-negative number.\")\n\n    # Calculate statistics\n    column_data = df[column]\n    stats = {\n        'sum': np.sum(column_data),\n        'mean': np.mean(column_data),\n        'min': np.min(column_data),\n        'max': np.max(column_data)\n    }\n\n    # Plot line chart\n    ax = df.plot(x='date', y=column, title=column)\n    plt.xlabel('Date')\n    plt.ylabel(column)\n\n    return stats, ax\n```\n"}
{"task_id": "BigCodeBench/514", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    columns = ['A', 'B', 'C', 'D', 'E']\n    df = pd.DataFrame(array, columns=columns)\n    \n    sums = df.sum()\n    ax = sums.plot(kind='bar', color='skyblue')\n    ax.set_title('Sum of Columns')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Sum')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(array):\n    columns = ['A', 'B', 'C', 'D', 'E']\n    df = pd.DataFrame(array, columns=columns)\n    \n    sums = df.sum()\n    ax = sums.plot(kind='bar', color='skyblue')\n    ax.set_title('Sum of Columns')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Sum')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/515", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    if not array or any(len(sublist) != 5 for sublist in array):\n        raise ValueError(\"Input array must be non-empty and all sublists must have a length of 5.\")\n\n    # Create DataFrame from 2D list\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n\n    # Calculate correlation matrix\n    corr_matrix = df.corr()\n\n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(array):\n    if not array or any(len(sublist) != 5 for sublist in array):\n        raise ValueError(\"Input array must be non-empty and all sublists must have a length of 5.\")\n\n    # Create DataFrame from 2D list\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n\n    # Calculate correlation matrix\n    corr_matrix = df.corr()\n\n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/516", "solution": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\ndef task_func(\n    array: list, random_seed: int = 0\n) -> (pd.DataFrame, sm.regression.linear_model.RegressionResultsWrapper):\n    np.random.seed(random_seed)\n    \n    # Convert the list of lists into a DataFrame\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'response'])\n    \n    # Define the independent and dependent variables\n    X = df[['A', 'B', 'C', 'D']]\n    y = df['response']\n    \n    # Add a constant to the independent variables\n    X = sm.add_constant(X)\n    \n    # Fit the linear regression model\n    model = sm.OLS(y, X).fit()\n    \n    return df, model", "raw_solution": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef task_func(\n    array: list, random_seed: int = 0\n) -> (pd.DataFrame, sm.regression.linear_model.RegressionResultsWrapper):\n    np.random.seed(random_seed)\n    \n    # Convert the list of lists into a DataFrame\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'response'])\n    \n    # Define the independent and dependent variables\n    X = df[['A', 'B', 'C', 'D']]\n    y = df['response']\n    \n    # Add a constant to the independent variables\n    X = sm.add_constant(X)\n    \n    # Fit the linear regression model\n    model = sm.OLS(y, X).fit()\n    \n    return df, model\n```\n"}
{"task_id": "BigCodeBench/517", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(array: list, random_seed: int = 42) -> (pd.DataFrame, np.ndarray):\n    # Convert the 2D list to a pandas DataFrame\n    df = pd.DataFrame(array)\n    \n    # Initialize PCA with the specified random seed\n    pca = PCA(n_components=2, random_state=random_seed)\n    \n    # Fit and transform the data using PCA\n    transformed_data = pca.fit_transform(df)\n    \n    return df, transformed_data", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\ndef task_func(array: list, random_seed: int = 42) -> (pd.DataFrame, np.ndarray):\n    # Convert the 2D list to a pandas DataFrame\n    df = pd.DataFrame(array)\n    \n    # Initialize PCA with the specified random seed\n    pca = PCA(n_components=2, random_state=random_seed)\n    \n    # Fit and transform the data using PCA\n    transformed_data = pca.fit_transform(df)\n    \n    return df, transformed_data\n\n# Example usage:\ndata = [[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]]\ndf, transformed = task_func(data)"}
{"task_id": "BigCodeBench/518", "solution": "import pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\ndef task_func(array):\n    if not all(isinstance(sublist, list) and all(isinstance(item, (int, float)) for item in sublist) for sublist in array):\n        raise TypeError(\"Input must be a list of lists with integers or floats.\")\n\n    # Convert the 2d list to a DataFrame\n    df = pd.DataFrame(array)\n    df.columns = [chr(65 + i) for i in range(df.shape[1])]  # Assign alphabetical column names\n\n    # Calculate the Euclidean distance matrix\n    distance_matrix = pdist(df.values, metric='euclidean')\n    distance_matrix = squareform(distance_matrix)\n\n    # Convert the distance matrix to a DataFrame\n    distance_df = pd.DataFrame(distance_matrix, index=df.index, columns=df.index)\n\n    return df, distance_df", "raw_solution": "import pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\n\ndef task_func(array):\n    if not all(isinstance(sublist, list) and all(isinstance(item, (int, float)) for item in sublist) for sublist in array):\n        raise TypeError(\"Input must be a list of lists with integers or floats.\")\n\n    # Convert the 2d list to a DataFrame\n    df = pd.DataFrame(array)\n    df.columns = [chr(65 + i) for i in range(df.shape[1])]  # Assign alphabetical column names\n\n    # Calculate the Euclidean distance matrix\n    distance_matrix = pdist(df.values, metric='euclidean')\n    distance_matrix = squareform(distance_matrix)\n\n    # Convert the distance matrix to a DataFrame\n    distance_df = pd.DataFrame(distance_matrix, index=df.index, columns=df.index)\n\n    return df, distance_df\n\n# Example usage:\n# df, distance_matrix = task_func([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n# print(df)\n# print(distance_matrix)\n```\n"}
{"task_id": "BigCodeBench/519", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not all(isinstance(item, dict) for item in data):\n        raise TypeError(\"Each item in the data list must be a dictionary.\")\n\n    # Convert the list of dictionaries into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Replace NaN values with 0\n    df.fillna(0, inplace=True)\n\n    # Plotting the data\n    ax = df.plot(kind='line', figsize=(10, 6))\n    ax.set_title('Fruit Sales over Time')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Sales Quantity')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not all(isinstance(item, dict) for item in data):\n        raise TypeError(\"Each item in the data list must be a dictionary.\")\n\n    # Convert the list of dictionaries into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Replace NaN values with 0\n    df.fillna(0, inplace=True)\n\n    # Plotting the data\n    ax = df.plot(kind='line', figsize=(10, 6))\n    ax.set_title('Fruit Sales over Time')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Sales Quantity')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/520", "solution": "import collections\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return {}, None\n\n    for entry in data:\n        for key, value in entry.items():\n            if value < 0:\n                raise ValueError(\"Sales quantity cannot be negative\")\n\n    # Aggregate sales data\n    total_sales = collections.Counter()\n    for entry in data:\n        total_sales.update(entry)\n\n    # Plotting the bar chart\n    fig, ax = plt.subplots()\n    fruits = list(total_sales.keys())\n    quantities = list(total_sales.values())\n    colors = ['red', 'peela', 'green', 'blue', 'purple']\n    ax.bar(fruits, quantities, color=colors[:len(fruits)])\n    ax.set_xlabel('Fruits')\n    ax.set_ylabel('Total Sales')\n    ax.set_title('Total Sales by Fruit')\n\n    return dict(total_sales), ax", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        return {}, None\n\n    for entry in data:\n        for key, value in entry.items():\n            if value < 0:\n                raise ValueError(\"Sales quantity cannot be negative\")\n\n    # Aggregate sales data\n    total_sales = collections.Counter()\n    for entry in data:\n        total_sales.update(entry)\n\n    # Plotting the bar chart\n    fig, ax = plt.subplots()\n    fruits = list(total_sales.keys())\n    quantities = list(total_sales.values())\n    colors = ['red', 'peela', 'green', 'blue', 'purple']\n    ax.bar(fruits, quantities, color=colors[:len(fruits)])\n    ax.set_xlabel('Fruits')\n    ax.set_ylabel('Total Sales')\n    ax.set_title('Total Sales by Fruit')\n\n    return dict(total_sales), ax\n```\n"}
{"task_id": "BigCodeBench/521", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_list):\n    # Convert the list of dictionaries into a DataFrame\n    df = pd.DataFrame(data_list)\n\n    # Plotting the data\n    ax = df.plot(kind='line', marker='o')\n    ax.set_title('Student Scores Over Tests')\n    ax.set_xlabel('Test Number')\n    ax.set_ylabel('Score')\n    ax.set_xticks(range(len(df.columns)))\n    ax.set_xticklabels(df.columns, rotation=45)\n\n    return ax\ndata = [{'John': 5, 'Jane': 10}, {'John': 6, 'Jane': 8}, {'John': 5, 'Jane': 9}]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_list):\n    # Convert the list of dictionaries into a DataFrame\n    df = pd.DataFrame(data_list)\n\n    # Plotting the data\n    ax = df.plot(kind='line', marker='o')\n    ax.set_title('Student Scores Over Tests')\n    ax.set_xlabel('Test Number')\n    ax.set_ylabel('Score')\n    ax.set_xticks(range(len(df.columns)))\n    ax.set_xticklabels(df.columns, rotation=45)\n\n    return ax\n\n# Example usage:\ndata = [{'John': 5, 'Jane': 10}, {'John': 6, 'Jane': 8}, {'John': 5, 'Jane': 9}]\nax = task_func(data)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/522", "solution": "import collections\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return None\n\n    # Combine all dictionaries into a single one\n    combined = collections.defaultdict(int)\n    for d in data:\n        for key, value in d.items():\n            if value < 0:\n                raise ValueError(\"Score cannot be negative\")\n            combined[key] += value\n\n    # Calculate average scores\n    averages = {key: value / len(data) for key, value in combined.items()}\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(averages.keys(), averages.values(), color='blue')\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Average Score')\n    ax.set_title('Average Score per Student')\n\n    return ax\ndata = [{'John': 5, 'Jane': 10}, {'Joe': 7}, {'John': 6, 'Jane': 8, 'Joe': 10}, {'John': 5, 'Jane': 9, 'Joe': 8}, {'John': 7, 'Jane': 10}, {'Joe': 9}]", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        return None\n\n    # Combine all dictionaries into a single one\n    combined = collections.defaultdict(int)\n    for d in data:\n        for key, value in d.items():\n            if value < 0:\n                raise ValueError(\"Score cannot be negative\")\n            combined[key] += value\n\n    # Calculate average scores\n    averages = {key: value / len(data) for key, value in combined.items()}\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(averages.keys(), averages.values(), color='blue')\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Average Score')\n    ax.set_title('Average Score per Student')\n\n    return ax\n\n# Example usage:\ndata = [{'John': 5, 'Jane': 10}, {'Joe': 7}, {'John': 6, 'Jane': 8, 'Joe': 10}, {'John': 5, 'Jane': 9, 'Joe': 8}, {'John': 7, 'Jane': 10}, {'Joe': 9}]\nax = task_func(data)\n```\n"}
{"task_id": "BigCodeBench/523", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return None\n\n    # Convert list of dictionaries to a single dictionary\n    combined_data = {key: [] for key in data[0].keys()}\n    for entry in data:\n        for key, value in entry.items():\n            combined_data[key].append(value)\n\n    # Create a DataFrame from the combined data\n    df = pd.DataFrame(combined_data)\n\n    # Plot the data\n    ax = df.plot(kind='line')\n    ax.set_title('Data over Time')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Data Points')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        return None\n\n    # Convert list of dictionaries to a single dictionary\n    combined_data = {key: [] for key in data[0].keys()}\n    for entry in data:\n        for key, value in entry.items():\n            combined_data[key].append(value)\n\n    # Create a DataFrame from the combined data\n    df = pd.DataFrame(combined_data)\n\n    # Plot the data\n    ax = df.plot(kind='line')\n    ax.set_title('Data over Time')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Data Points')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/524", "solution": "from collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data cannot be empty.\")\n    if not all(isinstance(d, dict) for d in data):\n        raise TypeError(\"Input must be a list of dictionaries.\")\n    if not all(all(isinstance(k, (int, float)) for k in d.values()) for d in data):\n        raise TypeError(\"All dictionary values must be numeric.\")\n\n    stats = defaultdict(lambda: {'mean': None, 'std': None})\n    axes = []\n\n    for d in data:\n        for key, values in d.items():\n            if stats[key]['mean'] is None:\n                stats[key]['mean'] = np.mean(values)\n                stats[key]['std'] = np.std(values)\n            else:\n                stats[key]['mean'] = np.mean([stats[key]['mean'], np.mean(values)])\n                stats[key]['std'] = np.std([stats[key]['std'], np.std(values)])\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Std'], [stats[key]['mean'], stats[key]['std']], color=['blue', 'green'])\n        ax.set_title(f\"Statistics of {key}\")\n        ax.set_ylabel(\"Value\")\n        axes.append(ax)\n\n    return stats, axes", "raw_solution": "from collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data cannot be empty.\")\n    if not all(isinstance(d, dict) for d in data):\n        raise TypeError(\"Input must be a list of dictionaries.\")\n    if not all(all(isinstance(k, (int, float)) for k in d.values()) for d in data):\n        raise TypeError(\"All dictionary values must be numeric.\")\n\n    stats = defaultdict(lambda: {'mean': None, 'std': None})\n    axes = []\n\n    for d in data:\n        for key, values in d.items():\n            if stats[key]['mean'] is None:\n                stats[key]['mean'] = np.mean(values)\n                stats[key]['std'] = np.std(values)\n            else:\n                stats[key]['mean'] = np.mean([stats[key]['mean'], np.mean(values)])\n                stats[key]['std'] = np.std([stats[key]['std'], np.std(values)])\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Std'], [stats[key]['mean'], stats[key]['std']], color=['blue', 'green'])\n        ax.set_title(f\"Statistics of {key}\")\n        ax.set_ylabel(\"Value\")\n        axes.append(ax)\n\n    return stats, axes\n```\n"}
{"task_id": "BigCodeBench/525", "solution": "import json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(input_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    result = defaultdict(lambda: {'mean': None, 'median': None})\n    plots = []\n\n    for key, values in data.items():\n        if values:\n            mean_value = np.mean(values)\n            median_value = np.median(values)\n            result[key]['mean'] = mean_value\n            result[key]['median'] = median_value\n\n            fig, ax = plt.subplots()\n            ax.bar(['Mean', 'Median'], [mean_value, median_value])\n            ax.set_title(f\"Mean and Median for {key}\")\n            ax.set_ylabel(\"Value\")\n            plots.append(ax)\n\n    return result, plots", "raw_solution": "import json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(input_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    result = defaultdict(lambda: {'mean': None, 'median': None})\n    plots = []\n\n    for key, values in data.items():\n        if values:\n            mean_value = np.mean(values)\n            median_value = np.median(values)\n            result[key]['mean'] = mean_value\n            result[key]['median'] = median_value\n\n            fig, ax = plt.subplots()\n            ax.bar(['Mean', 'Median'], [mean_value, median_value])\n            ax.set_title(f\"Mean and Median for {key}\")\n            ax.set_ylabel(\"Value\")\n            plots.append(ax)\n\n    return result, plots\n```\nThis script reads a JSON file, calculates the mean and median for each key in the dictionaries, and plots a bar chart for each key. The results and plots are returned as a dictionary and a list of matplotlib axes objects, respectively.\n"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    stats = defaultdict(lambda: {'values': []})\n\n    for record in data:\n        for key, value in record.items():\n            if isinstance(value, (int, float)):\n                stats[key]['values'].append(value)\n\n    stats_df = pd.DataFrame(stats).transpose()\n    stats_df['mean'] = stats_df['values'].apply(lambda x: np.mean(x) if x else np.nan)\n    stats_df['median'] = stats_df['values'].apply(lambda x: np.median(x) if x else np.nan)\n\n    stats_df = stats_df.sort_index()\n    stats_df = stats_df[['mean', 'median']]\n\n    return stats_df", "raw_solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    stats = defaultdict(lambda: {'values': []})\n\n    for record in data:\n        for key, value in record.items():\n            if isinstance(value, (int, float)):\n                stats[key]['values'].append(value)\n\n    stats_df = pd.DataFrame(stats).transpose()\n    stats_df['mean'] = stats_df['values'].apply(lambda x: np.mean(x) if x else np.nan)\n    stats_df['median'] = stats_df['values'].apply(lambda x: np.median(x) if x else np.nan)\n\n    stats_df = stats_df.sort_index()\n    stats_df = stats_df[['mean', 'median']]\n\n    return stats_df\n```\n"}
{"task_id": "BigCodeBench/527", "solution": "import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file: str) -> plt.Axes:\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Convert JSON data to DataFrame\n    df = pd.DataFrame(data)\n    df = df.explode('Values for Each Key')\n\n    # Calculate mean and median for each key\n    results = defaultdict(lambda: {'mean': None, 'median': None})\n    for key, group in df.groupby('X'):\n        values = group['Values for Each Key'].values\n        results[key] = {\n            'mean': np.mean(values),\n            'median': np.median(values)\n        }\n\n    # Create a box plot\n    ax = sns.boxplot(x='X', y='Values for Each Key', data=df)\n    plt.title('Box Plot of Values for Each Key')\n    plt.xlabel('X')\n    plt.ylabel('Values for Each Key')\n\n    return results, ax", "raw_solution": "import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file: str) -> plt.Axes:\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Convert JSON data to DataFrame\n    df = pd.DataFrame(data)\n    df = df.explode('Values for Each Key')\n\n    # Calculate mean and median for each key\n    results = defaultdict(lambda: {'mean': None, 'median': None})\n    for key, group in df.groupby('X'):\n        values = group['Values for Each Key'].values\n        results[key] = {\n            'mean': np.mean(values),\n            'median': np.median(values)\n        }\n\n    # Create a box plot\n    ax = sns.boxplot(x='X', y='Values for Each Key', data=df)\n    plt.title('Box Plot of Values for Each Key')\n    plt.xlabel('X')\n    plt.ylabel('Values for Each Key')\n\n    return results, ax\n```\n"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    try:\n        # Read the CSV file\n        with open(file_path, mode='r') as file:\n            reader = csv.reader(file)\n            data = list(reader)\n\n        # Create a DataFrame\n        df = pd.DataFrame(data)\n\n        # Count duplicates\n        duplicates = Counter(tuple(row) for row in df.values)\n\n        # Filter out non-duplicate rows\n        duplicate_rows = {k: v for k, v in duplicates.items() if v > 1}\n\n        # Plotting\n        fig, ax = plt.subplots()\n        ax.bar(duplicate_rows.keys(), duplicate_rows.values())\n        ax.set_title('Duplicate Rows')\n        ax.set_xlabel('Rows')\n        ax.set_ylabel('Count')\n\n        return duplicate_rows, ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The specified file was not found.\")\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")", "raw_solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    try:\n        # Read the CSV file\n        with open(file_path, mode='r') as file:\n            reader = csv.reader(file)\n            data = list(reader)\n\n        # Create a DataFrame\n        df = pd.DataFrame(data)\n\n        # Count duplicates\n        duplicates = Counter(tuple(row) for row in df.values)\n\n        # Filter out non-duplicate rows\n        duplicate_rows = {k: v for k, v in duplicates.items() if v > 1}\n\n        # Plotting\n        fig, ax = plt.subplots()\n        ax.bar(duplicate_rows.keys(), duplicate_rows.values())\n        ax.set_title('Duplicate Rows')\n        ax.set_xlabel('Rows')\n        ax.set_ylabel('Count')\n\n        return duplicate_rows, ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The specified file was not found.\")\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n```\n"}
{"task_id": "BigCodeBench/529", "solution": "from collections import Counter\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rolls, num_dice, plot_path=None, random_seed=0):\n    random.seed(random_seed)\n    roll_sums = []\n\n    for _ in range(num_rolls):\n        roll_sum = sum(random.randint(1, 6) for _ in range(num_dice))\n        roll_sums.append(roll_sum)\n\n    roll_counter = Counter(roll_sums)\n\n    fig, ax = plt.subplots()\n    ax.bar(roll_counter.keys(), roll_counter.values())\n    ax.set_xlabel('Sum of Rolls')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Sum of Dice Rolls')\n\n    if plot_path:\n        plt.savefig(plot_path)\n\n    return roll_counter, ax", "raw_solution": "from collections import Counter\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(num_rolls, num_dice, plot_path=None, random_seed=0):\n    random.seed(random_seed)\n    roll_sums = []\n\n    for _ in range(num_rolls):\n        roll_sum = sum(random.randint(1, 6) for _ in range(num_dice))\n        roll_sums.append(roll_sum)\n\n    roll_counter = Counter(roll_sums)\n\n    fig, ax = plt.subplots()\n    ax.bar(roll_counter.keys(), roll_counter.values())\n    ax.set_xlabel('Sum of Rolls')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Sum of Dice Rolls')\n\n    if plot_path:\n        plt.savefig(plot_path)\n\n    return roll_counter, ax\n```\n"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty or 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame should not be empty and should contain 'name' and 'age' columns.\")\n    \n    df['age'] = df['age'].apply(lambda x: int(round(x)) if isinstance(x, (int, float)) else x)\n    \n    if (df['age'] < 0).any():\n        raise ValueError(\"Age should not be negative.\")\n    \n    duplicates = df.duplicated(subset='name', keep=False)\n    duplicates_df = df[duplicates]\n    \n    if duplicates_df.empty:\n        return Counter(), None\n    \n    age_distribution = Counter(duplicates_df['age'])\n    \n    ax = sns.histplot(duplicates_df['age'], bins=np.arange(min(duplicates_df['age']), max(duplicates_df['age']) + 1.5, 1), kde=False)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    \n    return age_distribution, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty or 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame should not be empty and should contain 'name' and 'age' columns.\")\n    \n    df['age'] = df['age'].apply(lambda x: int(round(x)) if isinstance(x, (int, float)) else x)\n    \n    if (df['age'] < 0).any():\n        raise ValueError(\"Age should not be negative.\")\n    \n    duplicates = df.duplicated(subset='name', keep=False)\n    duplicates_df = df[duplicates]\n    \n    if duplicates_df.empty:\n        return Counter(), None\n    \n    age_distribution = Counter(duplicates_df['age'])\n    \n    ax = sns.histplot(duplicates_df['age'], bins=np.arange(min(duplicates_df['age']), max(duplicates_df['age']) + 1.5, 1), kde=False)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    \n    return age_distribution, ax\n```\n"}
{"task_id": "BigCodeBench/531", "solution": "import pandas as pd\nfrom collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    # Check if the dataframe contains the required columns\n    if 'x' not in df.columns or 'y' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'x' and 'y' columns.\")\n\n    # Count duplicates\n    duplicates = Counter(tuple(row) for row in df.itertuples(index=False))\n\n    # Remove duplicates\n    df_unique = df.drop_duplicates()\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    df_unique['cluster'] = kmeans.fit_predict(df_unique[['x', 'y']])\n\n    # Plot clusters\n    ax = df_unique.plot.scatter(x='x', y='y', c='cluster', colormap='viridis')\n\n    return duplicates, df_unique, ax", "raw_solution": "import pandas as pd\nfrom collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    # Check if the dataframe contains the required columns\n    if 'x' not in df.columns or 'y' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'x' and 'y' columns.\")\n\n    # Count duplicates\n    duplicates = Counter(tuple(row) for row in df.itertuples(index=False))\n\n    # Remove duplicates\n    df_unique = df.drop_duplicates()\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    df_unique['cluster'] = kmeans.fit_predict(df_unique[['x', 'y']])\n\n    # Plot clusters\n    ax = df_unique.plot.scatter(x='x', y='y', c='cluster', colormap='viridis')\n\n    return duplicates, df_unique, ax\n```\n"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    if 'value' not in df.columns or df.empty:\n        return Counter(), None\n\n    # Count the frequency of each value\n    counter = Counter(df['value'])\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, density=True, alpha=0.6, color='g')\n\n    # Overlay the normal distribution curve\n    mu, std = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return counter, ax", "raw_solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    if 'value' not in df.columns or df.empty:\n        return Counter(), None\n\n    # Count the frequency of each value\n    counter = Counter(df['value'])\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, density=True, alpha=0.6, color='g')\n\n    # Overlay the normal distribution curve\n    mu, std = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return counter, ax\n```\n"}
{"task_id": "BigCodeBench/533", "solution": "import numpy as np\nimport secrets\nimport hashlib\nimport base64\ndef task_func(num, from_base, to_base, alphabet):\n    if from_base < 2 or to_base < 2:\n        raise ValueError(\"Invalid base for conversion. Base must be at least 2.\")\n    \n    try:\n        # Convert the number from the specified base to decimal\n        decimal_num = int(num, from_base)\n    except ValueError:\n        raise ValueError(\"Invalid number format for conversion.\")\n    \n    # Generate a random salt\n    salt = secrets.token_hex(16)\n    \n    # Convert the decimal number to the target base\n    target_base_num = np.base_repr(decimal_num, to_base)\n    \n    # Create a SHA-256 hash of the converted number\n    hash_object = hashlib.sha256(target_base_num.encode())\n    hash_digest = hash_object.digest()\n    \n    # Encode the hash using the custom alphabet\n    encoded_hash = base64.b64encode(hash_digest, alphabet).decode()\n    \n    return encoded_hash, salt", "raw_solution": "import numpy as np\nimport secrets\nimport hashlib\nimport base64\n\ndef task_func(num, from_base, to_base, alphabet):\n    if from_base < 2 or to_base < 2:\n        raise ValueError(\"Invalid base for conversion. Base must be at least 2.\")\n    \n    try:\n        # Convert the number from the specified base to decimal\n        decimal_num = int(num, from_base)\n    except ValueError:\n        raise ValueError(\"Invalid number format for conversion.\")\n    \n    # Generate a random salt\n    salt = secrets.token_hex(16)\n    \n    # Convert the decimal number to the target base\n    target_base_num = np.base_repr(decimal_num, to_base)\n    \n    # Create a SHA-256 hash of the converted number\n    hash_object = hashlib.sha256(target_base_num.encode())\n    hash_digest = hash_object.digest()\n    \n    # Encode the hash using the custom alphabet\n    encoded_hash = base64.b64encode(hash_digest, alphabet).decode()\n    \n    return encoded_hash, salt\n```\n"}
{"task_id": "BigCodeBench/534", "solution": "import numpy as np\nimport base64\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.asymmetric import rsa\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Convert the number from the source base to base 10\n    num_decimal = int(num, from_base)\n    \n    # Convert the decimal number to the target base\n    num_target_base = np.base_repr(num_decimal, to_base)\n    \n    # Sign the converted number using the RSA private key\n    signature = private_key.sign(\n        num_target_base.encode(),\n        padding.PSS(\n            mgf=padding.MGF1(hashes.SHA256()),\n            salt_length=padding.PSS.MAX_LENGTH\n        ),\n        hashes.SHA256()\n    )\n    \n    # Encode the signature using the custom alphabet\n    encoded_signature = base64.b64encode(signature, alphabet).decode()\n    \n    return encoded_signature", "raw_solution": "import numpy as np\nimport base64\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.asymmetric import rsa\n\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Convert the number from the source base to base 10\n    num_decimal = int(num, from_base)\n    \n    # Convert the decimal number to the target base\n    num_target_base = np.base_repr(num_decimal, to_base)\n    \n    # Sign the converted number using the RSA private key\n    signature = private_key.sign(\n        num_target_base.encode(),\n        padding.PSS(\n            mgf=padding.MGF1(hashes.SHA256()),\n            salt_length=padding.PSS.MAX_LENGTH\n        ),\n        hashes.SHA256()\n    )\n    \n    # Encode the signature using the custom alphabet\n    encoded_signature = base64.b64encode(signature, alphabet).decode()\n    \n    return encoded_signature\n\n# Example usage:"}
{"task_id": "BigCodeBench/535", "solution": "import sqlite3\nimport numpy as np\nfrom random import choice, seed\ndef task_func(db_path, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 65)\n    HEIGHTS = range(150, 200)\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    cursor.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT,\n            age INTEGER,\n            height INTEGER\n        )\n    \"\"\")\n\n    # Insert random data into the table\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        cursor.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n\n    conn.commit()\n    inserted_rows = cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n    conn.close()\n\n    return inserted_rows", "raw_solution": "import sqlite3\nimport numpy as np\nfrom random import choice, seed\n\ndef task_func(db_path, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 65)\n    HEIGHTS = range(150, 200)\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    cursor.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT,\n            age INTEGER,\n            height INTEGER\n        )\n    \"\"\")\n\n    # Insert random data into the table\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        cursor.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n\n    conn.commit()\n    inserted_rows = cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n    conn.close()\n\n    return inserted_rows\n\n# Example usage:\n# print(task_func('path_to_test.db', 'people', 100, random_seed=42))\n```\n"}
{"task_id": "BigCodeBench/536", "solution": "import sqlite3\nimport pandas as pd\nimport os\ndef task_func(db_name, table_name, csv_path=\"data.csv\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load the data from the specified table into a pandas DataFrame\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n    \n    # Close the database connection\n    conn.close()\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_path, index=False)\n    \n    # Return the absolute path of the CSV file\n    return os.path.abspath(csv_path)", "raw_solution": "import sqlite3\nimport pandas as pd\nimport os\n\ndef task_func(db_name, table_name, csv_path=\"data.csv\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load the data from the specified table into a pandas DataFrame\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n    \n    # Close the database connection\n    conn.close()\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_path, index=False)\n    \n    # Return the absolute path of the CSV file\n    return os.path.abspath(csv_path)\n```\n"}
{"task_id": "BigCodeBench/537", "solution": "import sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load data from the specified table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Check for negative age values\n    if (df['age'] < 0).any():\n        raise ValueError(\"Data contains negative age values.\")\n    \n    # Close the database connection\n    conn.close()\n    \n    # Plot the age distribution using seaborn\n    ax = sns.histplot(df['age'], bins=30, kde=True)\n    \n    return ax", "raw_solution": "import sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load data from the specified table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Check for negative age values\n    if (df['age'] < 0).any():\n        raise ValueError(\"Data contains negative age values.\")\n    \n    # Close the database connection\n    conn.close()\n    \n    # Plot the age distribution using seaborn\n    ax = sns.histplot(df['age'], bins=30, kde=True)\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/538", "solution": "import sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(db_name, table_name):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Close the connection\n    conn.close()\n    \n    # Check if there are at least two numerical columns\n    numerical_cols = df.select_dtypes(include=['number']).columns\n    if len(numerical_cols) < 2:\n        raise ValueError(\"The table must have at least two numerical columns.\")\n    \n    # Plot the relationship between the first and second numerical columns\n    fig, ax = plt.subplots()\n    ax.scatter(df[numerical_cols[0]], df[numerical_cols[1]])\n    ax.set_xlabel(numerical_cols[0])\n    ax.set_ylabel(numerical_cols[1])\n    ax.set_title(f'Relationship between {numerical_cols[0]} and {numerical_cols[1]}')\n    \n    return ax", "raw_solution": "import sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(db_name, table_name):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Close the connection\n    conn.close()\n    \n    # Check if there are at least two numerical columns\n    numerical_cols = df.select_dtypes(include=['number']).columns\n    if len(numerical_cols) < 2:\n        raise ValueError(\"The table must have at least two numerical columns.\")\n    \n    # Plot the relationship between the first and second numerical columns\n    fig, ax = plt.subplots()\n    ax.scatter(df[numerical_cols[0]], df[numerical_cols[1]])\n    ax.set_xlabel(numerical_cols[0])\n    ax.set_ylabel(numerical_cols[1])\n    ax.set_title(f'Relationship between {numerical_cols[0]} and {numerical_cols[1]}')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/539", "solution": "import sqlite3\nfrom random import choice, seed\nimport os\ndef task_func(db_name, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries must be a non-negative integer\")\n\n    names = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    seed(random_seed)\n\n    # Create a new SQLite3 database or connect to an existing one\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n\n    # Create a new table\n    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (name TEXT, age INTEGER, height INTEGER)\")\n\n    for _ in range(num_entries):\n        name = choice(names)\n        age = random.randint(18, 65)\n        height = random.randint(150, 200)\n        cursor.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    conn.close()\n\n    return os.path.abspath(db_name)", "raw_solution": "import sqlite3\nfrom random import choice, seed\nimport os\n\ndef task_func(db_name, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries must be a non-negative integer\")\n\n    names = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    seed(random_seed)\n\n    # Create a new SQLite3 database or connect to an existing one\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n\n    # Create a new table\n    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (name TEXT, age INTEGER, height INTEGER)\")\n\n    for _ in range(num_entries):\n        name = choice(names)\n        age = random.randint(18, 65)\n        height = random.randint(150, 200)\n        cursor.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    conn.close()\n\n    return os.path.abspath(db_name)\n```\n"}
{"task_id": "BigCodeBench/540", "solution": "from collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(list_of_menuitems, title=\"Menu Distribution\", color=\"blue\", width=1.0):\n    # Flatten the list of menu items\n    flat_menuitems = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count occurrences of each menu item\n    counter = Counter(flat_menuitems)\n    \n    # Sort the items alphabetically\n    sorted_items = sorted(counter.keys())\n    \n    # Get the frequencies of the sorted items\n    frequencies = [counter[item] for item in sorted_items]\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.bar(sorted_items, frequencies, color=color, width=width)\n    ax.set_title(title)\n    ax.set_xlabel('Menu Items')\n    ax.set_ylabel('Frequency')\n    \n    return ax", "raw_solution": "from collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(list_of_menuitems, title=\"Menu Distribution\", color=\"blue\", width=1.0):\n    # Flatten the list of menu items\n    flat_menuitems = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count occurrences of each menu item\n    counter = Counter(flat_menuitems)\n    \n    # Sort the items alphabetically\n    sorted_items = sorted(counter.keys())\n    \n    # Get the frequencies of the sorted items\n    frequencies = [counter[item] for item in sorted_items]\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.bar(sorted_items, frequencies, color=color, width=width)\n    ax.set_title(title)\n    ax.set_xlabel('Menu Items')\n    ax.set_ylabel('Frequency')\n    \n    return ax\n\n# Example usage:\n# ax = task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/541", "solution": "import os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\ndef task_func(package_name):\n    # Check if the package is already in the system path\n    if package_name not in sys.modules:\n        try:\n            # Try to import the package\n            importlib.import_module(package_name)\n        except ModuleNotFoundError:\n            raise ImportError(f\"The package '{package_name}' is not installed. Please install it using 'pip install {package_name}'\")\n    \n    # Get all modules in the package\n    package = sys.modules[package_name]\n    modules = [module.name for module in iter_modules(package.__path__)]\n    \n    return modules", "raw_solution": "import os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\n\ndef task_func(package_name):\n    # Check if the package is already in the system path\n    if package_name not in sys.modules:\n        try:\n            # Try to import the package\n            importlib.import_module(package_name)\n        except ModuleNotFoundError:\n            raise ImportError(f\"The package '{package_name}' is not installed. Please install it using 'pip install {package_name}'\")\n    \n    # Get all modules in the package\n    package = sys.modules[package_name]\n    modules = [module.name for module in iter_modules(package.__path__)]\n    \n    return modules\n\n# Test cases"}
{"task_id": "BigCodeBench/542", "solution": "import hashlib\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS, seed=42):\n    random.seed(seed)\n    selected_key = random.choice(hex_keys)\n    \n    try:\n        # Convert hexadecimal string to a floating-point number\n        float_num = struct.unpack('!f', bytes.fromhex(selected_key))[0]\n    except struct.error:\n        raise ValueError(\"Invalid hexadecimal string provided.\")\n    \n    # Compute MD5 hash of the floating-point number\n    hash_object = hashlib.md5(struct.pack('!f', float_num))\n    return hash_object.hexdigest()", "raw_solution": "import hashlib\nimport random\nimport struct\n\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_keys=KEYS, seed=42):\n    random.seed(seed)\n    selected_key = random.choice(hex_keys)\n    \n    try:\n        # Convert hexadecimal string to a floating-point number\n        float_num = struct.unpack('!f', bytes.fromhex(selected_key))[0]\n    except struct.error:\n        raise ValueError(\"Invalid hexadecimal string provided.\")\n    \n    # Compute MD5 hash of the floating-point number\n    hash_object = hashlib.md5(struct.pack('!f', float_num))\n    return hash_object.hexdigest()\n\n# Example usage:\n# print(task_func())\n```\n"}
{"task_id": "BigCodeBench/543", "solution": "import base64\nimport os\nimport random\ndef task_func():\n    # Generate a random float number\n    random_float = random.random()\n    \n    # Convert the float to hexadecimal\n    hex_representation = hex(int(random_float * (2**32)))\n    \n    # Encode the hexadecimal representation in base64\n    base64_encoded = base64.b64encode(hex_representation.encode()).decode()\n    \n    return base64_encoded", "raw_solution": "import base64\nimport os\nimport random\n\ndef task_func():\n    # Generate a random float number\n    random_float = random.random()\n    \n    # Convert the float to hexadecimal\n    hex_representation = hex(int(random_float * (2**32)))\n    \n    # Encode the hexadecimal representation in base64\n    base64_encoded = base64.b64encode(hex_representation.encode()).decode()\n    \n    return base64_encoded\n\n# Example usage"}
{"task_id": "BigCodeBench/544", "solution": "import struct\nimport zlib\nKEY = '470FC614'\ndef task_func(hex_string=KEY):\n    \"\"\"\n    Ek diye gaye hex string ko ek float number mein defaults hai and then binary32 float number ko sampidit karta hai.\n\n    Parameters:\n    - hex_string (str, optional): The hex string to be converted. Defaults to 470FC614.\n\n    Returns:\n        bytes: The compressed float number.\n\n    Requirements:\n    - struct\n    - zlib\n\n    Example:\n    >>> task_func(\"470FC614\")\n    b'x\\\\\\\\x9c\\\\\\\\xf3\\\\\\\\xeb\\\\\\\\x93\\\\\\\\xef\\\\\\\\x01\\\\\\\\x00\\\\\\\\x03\\\\\\\\xb0\\\\\\\\x01\\\\\\\\x88'\n    >>> task_func(\"ABCD1234\")\n    b'x\\\\\\\\x9c\\\\\\\\xf3\\\\\\\\xd7>+\\\\\\\\x04\\\\\\\\x00\\\\\\\\x03m\\\\\\\\x01Z'\n\n    \"\"\"\n    # Convert hex string to float\n    float_num = struct.unpack('f', bytes.fromhex(hex_string))[0]\n    \n    # Pack the float number into bytes\n    packed_bytes = struct.pack('f', float_num)\n    \n    # Compress the packed bytes\n    compressed_bytes = zlib.compress(packed_bytes)\n    \n    return compressed_bytes", "raw_solution": "import struct\nimport zlib\n\n# Constants\nKEY = '470FC614'\n\ndef task_func(hex_string=KEY):\n    \"\"\"\n    Ek diye gaye hex string ko ek float number mein defaults hai and then binary32 float number ko sampidit karta hai.\n\n    Parameters:\n    - hex_string (str, optional): The hex string to be converted. Defaults to 470FC614.\n\n    Returns:\n        bytes: The compressed float number.\n\n    Requirements:\n    - struct\n    - zlib\n\n    Example:\n    >>> task_func(\"470FC614\")\n    b'x\\\\\\\\x9c\\\\\\\\xf3\\\\\\\\xeb\\\\\\\\x93\\\\\\\\xef\\\\\\\\x01\\\\\\\\x00\\\\\\\\x03\\\\\\\\xb0\\\\\\\\x01\\\\\\\\x88'\n    >>> task_func(\"ABCD1234\")\n    b'x\\\\\\\\x9c\\\\\\\\xf3\\\\\\\\xd7>+\\\\\\\\x04\\\\\\\\x00\\\\\\\\x03m\\\\\\\\x01Z'\n\n    \"\"\"\n    # Convert hex string to float\n    float_num = struct.unpack('f', bytes.fromhex(hex_string))[0]\n    \n    # Pack the float number into bytes\n    packed_bytes = struct.pack('f', float_num)\n    \n    # Compress the packed bytes\n    compressed_bytes = zlib.compress(packed_bytes)\n    \n    return compressed_bytes\n\n# Test cases"}
{"task_id": "BigCodeBench/545", "solution": "import codecs\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS):\n    \"\"\"\n    Ek list mein se hex strings se ek random float number utpann karein aur phir float number ko utf-8 mein encode karein.\n\n    Parameters:\n    - hex_keys (list of str): Ek hexadecimal strings ki list jisse chunna hai.\n\n    Returns:\n        bytes: utf-8 encoded float number.\n\n    Requirements:\n    - struct\n    - codecs\n    - random\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func()\n    b'36806.078125'\n\n    \"\"\"\n    # Convert hexadecimal strings to float\n    float_num = struct.unpack('!f', bytes.fromhex(random.choice(hex_keys)))[0]\n\n    # Encode the float number to utf-8\n    encoded_float = codecs.encode(str(float_num), 'utf-8')\n\n    return encoded_float", "raw_solution": "import codecs\nimport random\nimport struct\n\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_keys=KEYS):\n    \"\"\"\n    Ek list mein se hex strings se ek random float number utpann karein aur phir float number ko utf-8 mein encode karein.\n\n    Parameters:\n    - hex_keys (list of str): Ek hexadecimal strings ki list jisse chunna hai.\n\n    Returns:\n        bytes: utf-8 encoded float number.\n\n    Requirements:\n    - struct\n    - codecs\n    - random\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func()\n    b'36806.078125'\n\n    \"\"\"\n    # Convert hexadecimal strings to float\n    float_num = struct.unpack('!f', bytes.fromhex(random.choice(hex_keys)))[0]\n\n    # Encode the float number to utf-8\n    encoded_float = codecs.encode(str(float_num), 'utf-8')\n\n    return encoded_float\n```\n"}
{"task_id": "BigCodeBench/546", "solution": "from collections import OrderedDict\nfrom prettytable import PrettyTable\ndef task_func(my_dict):\n    # Sort the dictionary by keys\n    sorted_dict = OrderedDict(sorted(my_dict.items()))\n\n    # Create a PrettyTable object\n    table = PrettyTable()\n    table.field_names = [\"Key\", \"Value\"]\n\n    # Populate the table with sorted dictionary items\n    for key, value in sorted_dict.items():\n        table.add_row([key, value])\n\n    return table", "raw_solution": "from collections import OrderedDict\nfrom prettytable import PrettyTable\n\ndef task_func(my_dict):\n    # Sort the dictionary by keys\n    sorted_dict = OrderedDict(sorted(my_dict.items()))\n\n    # Create a PrettyTable object\n    table = PrettyTable()\n    table.field_names = [\"Key\", \"Value\"]\n\n    # Populate the table with sorted dictionary items\n    for key, value in sorted_dict.items():\n        table.add_row([key, value])\n\n    return table\n\n# Test cases"}
{"task_id": "BigCodeBench/547", "solution": "import hashlib\nimport os\nimport base64\ndef task_func(password: str, salt_length: int = 8) -> str:\n    \"\"\"\n    password ko salt aur SHA-256 ka upayog karke encrypt karein, phir result ko base64 mein encode karein.\n\n    Parameters:\n    - password (str): encrypt kiye jaane wala password.\n    - salt_length (int, optional): generate kiye gaye salt ki length. Default 8 hai.\n\n    Returns:\n        str: base64 format mein encrypted password.\n\n    Requirements:\n    - base64\n    - hashlib\n    - os\n\n    Example:\n    >>> isinstance(task_func('my_password'), str)\n    True\n\n    \"\"\"\n    # Generate a random salt\n    salt = os.urandom(salt_length)\n    \n    # Create a new SHA-256 hash object\n    hash_obj = hashlib.sha256()\n    \n    # Update the hash object with the password and salt\n    hash_obj.update(password.encode('utf-8'))\n    hash_obj.update(salt)\n    \n    # Get the hashed password\n    hashed_password = hash_obj.digest()\n    \n    # Encode the hashed password with base64\n    encoded_password = base64.b64encode(hashed_password + salt)\n    \n    return encoded_password.decode('utf-8')", "raw_solution": "import hashlib\nimport os\nimport base64\n\ndef task_func(password: str, salt_length: int = 8) -> str:\n    \"\"\"\n    password ko salt aur SHA-256 ka upayog karke encrypt karein, phir result ko base64 mein encode karein.\n\n    Parameters:\n    - password (str): encrypt kiye jaane wala password.\n    - salt_length (int, optional): generate kiye gaye salt ki length. Default 8 hai.\n\n    Returns:\n        str: base64 format mein encrypted password.\n\n    Requirements:\n    - base64\n    - hashlib\n    - os\n\n    Example:\n    >>> isinstance(task_func('my_password'), str)\n    True\n\n    \"\"\"\n    # Generate a random salt\n    salt = os.urandom(salt_length)\n    \n    # Create a new SHA-256 hash object\n    hash_obj = hashlib.sha256()\n    \n    # Update the hash object with the password and salt\n    hash_obj.update(password.encode('utf-8'))\n    hash_obj.update(salt)\n    \n    # Get the hashed password\n    hashed_password = hash_obj.digest()\n    \n    # Encode the hashed password with base64\n    encoded_password = base64.b64encode(hashed_password + salt)\n    \n    return encoded_password.decode('utf-8')\n```\n"}
{"task_id": "BigCodeBench/548", "solution": "import random\nimport string\nimport base64\nimport zlib\ndef task_func(string_length=100):\n    # Generate a random string of specified length\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=string_length))\n    \n    # Compress the random string using zlib\n    compressed_data = zlib.compress(random_string.encode('utf-8'))\n    \n    # Encode the compressed data using base64\n    encoded_data = base64.b64encode(compressed_data).decode('utf-8')\n    \n    return encoded_data", "raw_solution": "import random\nimport string\nimport base64\nimport zlib\n\ndef task_func(string_length=100):\n    # Generate a random string of specified length\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=string_length))\n    \n    # Compress the random string using zlib\n    compressed_data = zlib.compress(random_string.encode('utf-8'))\n    \n    # Encode the compressed data using base64\n    encoded_data = base64.b64encode(compressed_data).decode('utf-8')\n    \n    return encoded_data\n\n# Example usage"}
{"task_id": "BigCodeBench/549", "solution": "import base64\nimport pandas as pd\ndef task_func(df):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Convert the DataFrame to CSV format\n    csv_bytes = df.to_csv(index=False).encode('utf-8')\n    \n    # Encode the CSV bytes to a base64 string\n    base64_bytes = base64.b64encode(csv_bytes)\n    base64_string = base64_bytes.decode('utf-8')\n    \n    return base64_string\ndf = {'A': [1, 2, 3], 'B': [4, 5, 6]}", "raw_solution": "import base64\nimport pandas as pd\n\ndef task_func(df):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Convert the DataFrame to CSV format\n    csv_bytes = df.to_csv(index=False).encode('utf-8')\n    \n    # Encode the CSV bytes to a base64 string\n    base64_bytes = base64.b64encode(csv_bytes)\n    base64_string = base64_bytes.decode('utf-8')\n    \n    return base64_string\n\n# Example usage:\ndf = {'A': [1, 2, 3], 'B': [4, 5, 6]}\nencoded_df = task_func(df)"}
{"task_id": "BigCodeBench/550", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(list_of_menuitems):\n    # Flatten the list of menu items\n    flat_menuitems = [item for sublist in list_of_menuitems for item in sublist]\n    \n    # Count the occurrences of each menu item\n    menuitem_counts = Counter(flat_menuitems)\n    \n    # Create a DataFrame from the counts\n    df = pd.DataFrame.from_dict(menuitem_counts, orient='index', columns=['Count'])\n    \n    return df", "raw_solution": "from collections import Counter\nimport pandas as pd\n\ndef task_func(list_of_menuitems):\n    # Flatten the list of menu items\n    flat_menuitems = [item for sublist in list_of_menuitems for item in sublist]\n    \n    # Count the occurrences of each menu item\n    menuitem_counts = Counter(flat_menuitems)\n    \n    # Create a DataFrame from the counts\n    df = pd.DataFrame.from_dict(menuitem_counts, orient='index', columns=['Count'])\n    \n    return df\n\n# Example usage:\n# result = task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/551", "solution": "from collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\ndef task_func(list_of_menuitems):\n    # Flatten the list of menu items\n    flat_menuitems = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_menuitems)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(counter.items(), columns=['Menu Item', 'Frequency'])\n\n    # Plot the frequencies using seaborn\n    if not df.empty:\n        plt.figure(figsize=(10, 6))\n        ax = sns.barplot(x='Menu Item', y='Frequency', data=df)\n        ax.set_title('Menu Item Frequency')\n        ax.set_ylabel('Frequency')\n        ax.set_xlabel('Menu Item')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return ax\n    else:\n        return None", "raw_solution": "from collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(list_of_menuitems):\n    # Flatten the list of menu items\n    flat_menuitems = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_menuitems)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(counter.items(), columns=['Menu Item', 'Frequency'])\n\n    # Plot the frequencies using seaborn\n    if not df.empty:\n        plt.figure(figsize=(10, 6))\n        ax = sns.barplot(x='Menu Item', y='Frequency', data=df)\n        ax.set_title('Menu Item Frequency')\n        ax.set_ylabel('Frequency')\n        ax.set_xlabel('Menu Item')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return ax\n    else:\n        return None\n```\n"}
{"task_id": "BigCodeBench/552", "solution": "import collections\nimport itertools\nimport matplotlib.pyplot as plt\nITEMS = ['apple', 'banana']\ndef task_func(a, b, items=ITEMS):\n    # Combine the two lists\n    combined = a + b\n\n    # Count the frequency of each item in the combined list\n    counter = collections.Counter(combined)\n\n    # Filter the counter to only include items from the optional list\n    filtered_counter = {item: counter.get(item, 0) for item in items}\n\n    # Create a bar chart of the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(filtered_counter.keys(), filtered_counter.values())\n    ax.set_xlabel('Items')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Items in Combined List')\n\n    return ax", "raw_solution": "import collections\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS = ['apple', 'banana']\n\ndef task_func(a, b, items=ITEMS):\n    # Combine the two lists\n    combined = a + b\n\n    # Count the frequency of each item in the combined list\n    counter = collections.Counter(combined)\n\n    # Filter the counter to only include items from the optional list\n    filtered_counter = {item: counter.get(item, 0) for item in items}\n\n    # Create a bar chart of the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(filtered_counter.keys(), filtered_counter.values())\n    ax.set_xlabel('Items')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Items in Combined List')\n\n    return ax\n\n# Example usage:\n# ax = task_func(['apple', 'banana', 'cherry'], ['date', 'elderberry', 'apple', 'banana', 'cherry'])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(a, b):\n    # Create a DataFrame with random values\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), index=a, columns=b)\n\n    # Plot the DataFrame\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Bar Chart')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Values')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Create a DataFrame with random values\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), index=a, columns=b)\n\n    # Plot the DataFrame\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Bar Chart')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Values')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:\n# ax = task_func([1, 2, 3], ['A', 'B', 'C', 'D', 'E'])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/554", "solution": "import numpy as np\nimport random\ndef task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL):\n    \"\"\"\n    Ek palindrome sentence utpann karta hai, jo specified pool se random words ka upayog karta hai. sentence ki length minimum (MIN_WORDS) and maximum (MAX_WORDS) words ki number ke beech random roop se\n    chuni jati hai. function yah sunishchit karta hai ki\n    sentence aage and peechhe same hi padha jaye.\n    Jaanch karein ki utpann sentence ki length specified range ke bheetar hai ya nahi.\n    >>> sentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)\n    >>> MIN_WORDS <= len(sentence.split()) <= MAX_WORDS\n    True\n\n    Parameters:\n    - MIN_WORDS (int): palindrome sentence mein words ki minimum number.\n    - MAX_WORDS (int): palindrome sentence mein words ki maximum number.\n    - WORDS_POOL (list): palindrome utpann karne ke liye words ki list.\n\n    Returns:\n        str: utpann palindrome sentence.\n\n    Requirements:\n    - numpy\n    - random\n\n    Example:\n    Examples:\n    Ek palindrome sentence utpann karein and jaanchen ki yah vaastav mein ek palindrome hai.\n    >>> MIN_WORDS, MAX_WORDS, WORDS_POOL = 3, 10, ['apple', 'banana', 'racecar', 'world', 'level', 'madam', 'radar', 'rotor']\n    >>> sentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)\n    >>> re_sentence = \" \".join(sentence.split()[::-1])\n    >>> sentence == re_sentence\n    True\n\n    \"\"\"\n    # Generate a random number of words between MIN_WORDS and MAX_WORDS\n    num_words = random.randint(MIN_WORDS, MAX_WORDS)\n    \n    # Select random words from the WORDS_POOL\n    selected_words = random.choices(WORDS_POOL, k=num_words)\n    \n    # Create a palindrome sentence by concatenating the selected words in reverse order\n    palindrome_sentence = \" \".join(selected_words[::-1])\n    \n    return palindrome_sentence", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL):\n    \"\"\"\n    Ek palindrome sentence utpann karta hai, jo specified pool se random words ka upayog karta hai. sentence ki length minimum (MIN_WORDS) and maximum (MAX_WORDS) words ki number ke beech random roop se\n    chuni jati hai. function yah sunishchit karta hai ki\n    sentence aage and peechhe same hi padha jaye.\n    Jaanch karein ki utpann sentence ki length specified range ke bheetar hai ya nahi.\n    >>> sentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)\n    >>> MIN_WORDS <= len(sentence.split()) <= MAX_WORDS\n    True\n\n    Parameters:\n    - MIN_WORDS (int): palindrome sentence mein words ki minimum number.\n    - MAX_WORDS (int): palindrome sentence mein words ki maximum number.\n    - WORDS_POOL (list): palindrome utpann karne ke liye words ki list.\n\n    Returns:\n        str: utpann palindrome sentence.\n\n    Requirements:\n    - numpy\n    - random\n\n    Example:\n    Examples:\n    Ek palindrome sentence utpann karein and jaanchen ki yah vaastav mein ek palindrome hai.\n    >>> MIN_WORDS, MAX_WORDS, WORDS_POOL = 3, 10, ['apple', 'banana', 'racecar', 'world', 'level', 'madam', 'radar', 'rotor']\n    >>> sentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)\n    >>> re_sentence = \" \".join(sentence.split()[::-1])\n    >>> sentence == re_sentence\n    True\n\n    \"\"\"\n    # Generate a random number of words between MIN_WORDS and MAX_WORDS\n    num_words = random.randint(MIN_WORDS, MAX_WORDS)\n    \n    # Select random words from the WORDS_POOL\n    selected_words = random.choices(WORDS_POOL, k=num_words)\n    \n    # Create a palindrome sentence by concatenating the selected words in reverse order\n    palindrome_sentence = \" \".join(selected_words[::-1])\n    \n    return palindrome_sentence\n```\n"}
{"task_id": "BigCodeBench/555", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef task_func(a, b):\n    # Create a Pandas DataFrame from the two lists\n    df = pd.DataFrame({'a': a, 'b': b})\n\n    # Calculate the Pearson correlation coefficient\n    correlation, _ = stats.pearsonr(df['a'], df['b'])\n\n    # Create a scatter plot with a regression line\n    fig, ax = plt.subplots()\n    ax.scatter(df['a'], df['b'], label='Data points')\n    ax.plot(df['a'], df['a'], color='red', label='Regression line')  # Simple linear regression line\n    ax.set_xlabel('a')\n    ax.set_ylabel('b')\n    ax.set_title('Scatter plot with regression line')\n    ax.legend()\n\n    return correlation, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef task_func(a, b):\n    # Create a Pandas DataFrame from the two lists\n    df = pd.DataFrame({'a': a, 'b': b})\n\n    # Calculate the Pearson correlation coefficient\n    correlation, _ = stats.pearsonr(df['a'], df['b'])\n\n    # Create a scatter plot with a regression line\n    fig, ax = plt.subplots()\n    ax.scatter(df['a'], df['b'], label='Data points')\n    ax.plot(df['a'], df['a'], color='red', label='Regression line')  # Simple linear regression line\n    ax.set_xlabel('a')\n    ax.set_ylabel('b')\n    ax.set_title('Scatter plot with regression line')\n    ax.legend()\n\n    return correlation, ax\n```\n"}
{"task_id": "BigCodeBench/556", "solution": "import numpy as np\nimport random\nfrom difflib import SequenceMatcher\ndef task_func(s, min_length, max_length, letters):\n    # Generate a random string of length between min_length and max_length\n    length = random.randint(min_length, max_length)\n    generated_s = ''.join(random.choice(letters) for _ in range(length))\n\n    # Calculate similarity between the generated string and the input string\n    similarity = SequenceMatcher(None, s, generated_s).ratio()\n\n    # Check if the similarity is above 0.5\n    is_similar = similarity >= 0.5\n\n    return generated_s, is_similar\ns = 'apple'\nmin_length = 5\nmax_length = 10\nletters = 'abcdefghijklmnopqrstuvwxyz'", "raw_solution": "import numpy as np\nimport random\nfrom difflib import SequenceMatcher\n\ndef task_func(s, min_length, max_length, letters):\n    # Generate a random string of length between min_length and max_length\n    length = random.randint(min_length, max_length)\n    generated_s = ''.join(random.choice(letters) for _ in range(length))\n\n    # Calculate similarity between the generated string and the input string\n    similarity = SequenceMatcher(None, s, generated_s).ratio()\n\n    # Check if the similarity is above 0.5\n    is_similar = similarity >= 0.5\n\n    return generated_s, is_similar\n\n# Example usage:\ns = 'apple'\nmin_length = 5\nmax_length = 10\nletters = 'abcdefghijklmnopqrstuvwxyz'\ngenerated_s, is_similar = task_func(s, min_length, max_length, letters)"}
{"task_id": "BigCodeBench/557", "solution": "import numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\ndef task_func(s_list, plot_path=None):\n    if not s_list:\n        raise ValueError(\"The list of strings cannot be empty.\")\n    \n    avg_scores = []\n    for i, s1 in enumerate(s_list):\n        total_similarity = 0\n        for s2 in s_list:\n            if i != s_list.index(s2):\n                total_similarity += SequenceMatcher(None, s1, s2).ratio()\n        avg_scores.append(total_similarity / (len(s_list) - 1))\n    \n    if plot_path:\n        plt.figure(figsize=(10, 5))\n        plt.plot(s_list, avg_scores, marker='o')\n        plt.title('Average Similarity Scores')\n        plt.xlabel('Strings')\n        plt.ylabel('Average Similarity Score')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.savefig(plot_path)\n        plt.close()\n\n    return avg_scores\ns_list = ['apple', 'apples', 'ape', 'app', 'april']", "raw_solution": "import numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\n\ndef task_func(s_list, plot_path=None):\n    if not s_list:\n        raise ValueError(\"The list of strings cannot be empty.\")\n    \n    avg_scores = []\n    for i, s1 in enumerate(s_list):\n        total_similarity = 0\n        for s2 in s_list:\n            if i != s_list.index(s2):\n                total_similarity += SequenceMatcher(None, s1, s2).ratio()\n        avg_scores.append(total_similarity / (len(s_list) - 1))\n    \n    if plot_path:\n        plt.figure(figsize=(10, 5))\n        plt.plot(s_list, avg_scores, marker='o')\n        plt.title('Average Similarity Scores')\n        plt.xlabel('Strings')\n        plt.ylabel('Average Similarity Score')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.savefig(plot_path)\n        plt.close()\n\n    return avg_scores\n\n# Example usage:\ns_list = ['apple', 'apples', 'ape', 'app', 'april']\navg_scores = task_func(s_list, 'similarity_plot.png')"}
{"task_id": "BigCodeBench/558", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(a, b, columns=['A', 'B']):\n    # Create a DataFrame from the input lists\n    data = {columns[0]: a, columns[1]: b}\n    df = pd.DataFrame(data)\n\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(df)\n    standardized_df = pd.DataFrame(standardized_data, columns=columns)\n\n    # Plot the standardized data\n    ax = standardized_df.plot(kind='bar')\n    ax.set_title('Standardized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n\n    return standardized_df, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(a, b, columns=['A', 'B']):\n    # Create a DataFrame from the input lists\n    data = {columns[0]: a, columns[1]: b}\n    df = pd.DataFrame(data)\n\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(df)\n    standardized_df = pd.DataFrame(standardized_data, columns=columns)\n\n    # Plot the standardized data\n    ax = standardized_df.plot(kind='bar')\n    ax.set_title('Standardized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n\n    return standardized_df, ax\n```\n"}
{"task_id": "BigCodeBench/559", "solution": "import pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\ndef task_func(a, b):\n    # Create a pandas DataFrame\n    df = pd.DataFrame({'A': a, 'B': b})\n\n    # Calculate Euclidean distance\n    euclidean_distance = distance.euclidean(a, b)\n\n    # Plot the values\n    ax = df.plot(kind='line')\n    ax.set_title('Line Plot of Lists')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    return euclidean_distance, df, ax", "raw_solution": "import pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\n\ndef task_func(a, b):\n    # Create a pandas DataFrame\n    df = pd.DataFrame({'A': a, 'B': b})\n\n    # Calculate Euclidean distance\n    euclidean_distance = distance.euclidean(a, b)\n\n    # Plot the values\n    ax = df.plot(kind='line')\n    ax.set_title('Line Plot of Lists')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    return euclidean_distance, df, ax\n```\n"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    # Convert the data string into a pandas DataFrame\n    data_list = data.split(',')\n    data_dict = {'Date': [], 'Value': []}\n    for item in data_list:\n        year, month, value = item.split('-')\n        data_dict['Date'].append(datetime.strptime(f'{year}-{month}', '%Y-%m'))\n        data_dict['Value'].append(int(value))\n    \n    df = pd.DataFrame(data_dict)\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Value'], marker='o')\n    ax.set_title(f\"{df['Date'].iloc[0].year} ke liye monthly data\")\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Values')\n    ax.set_xticks(df['Date'])\n    ax.set_xticklabels([d.strftime('%b') for d in df['Date']], rotation=45)\n    \n    return ax\ndata = '2022-01-100,2022-02-200,2022-03-150,2022-04-300,2022-05-250,2022-06-350,2022-07-400,2022-08-450,2022-09-500,2022-10-550,2022-11-600,2022-12-650'", "raw_solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Convert the data string into a pandas DataFrame\n    data_list = data.split(',')\n    data_dict = {'Date': [], 'Value': []}\n    for item in data_list:\n        year, month, value = item.split('-')\n        data_dict['Date'].append(datetime.strptime(f'{year}-{month}', '%Y-%m'))\n        data_dict['Value'].append(int(value))\n    \n    df = pd.DataFrame(data_dict)\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Value'], marker='o')\n    ax.set_title(f\"{df['Date'].iloc[0].year} ke liye monthly data\")\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Values')\n    ax.set_xticks(df['Date'])\n    ax.set_xticklabels([d.strftime('%b') for d in df['Date']], rotation=45)\n    \n    return ax\n\n# Example usage:\ndata = '2022-01-100,2022-02-200,2022-03-150,2022-04-300,2022-05-250,2022-06-350,2022-07-400,2022-08-450,2022-09-500,2022-10-550,2022-11-600,2022-12-650'\nax = task_func(data)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/561", "solution": "import pytz\nfrom dateutil import parser\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Convert time from one timezone to another.\n\n    Parameters:\n    - date_str (str): \"yyyy-mm-dd hh:mm:ss\" format date string.\n    - from_tz (str): The timezone of the given date string.\n    - to_tz (str): The timezone to convert the date to.\n\n    Returns:\n        str: \"yyyy-mm-dd hh:mm:ss\" format converted datetime string.\n\n    Requirements:\n    - pytz\n    - dateutil.parser\n\n    Example:\n    >>> task_func('2022-03-01 12:00:00', 'UTC', 'America/New_York')\n    '2022-03-01 07:00:00'\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    naive_datetime = parser.parse(date_str)\n    \n    # Get the source and target timezones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_datetime = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the target timezone\n    converted_datetime = aware_datetime.astimezone(to_timezone)\n    \n    # Return the converted datetime in string format\n    return converted_datetime.strftime('%Y-%m-%d %H:%M:%S')", "raw_solution": "import pytz\nfrom dateutil import parser\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Convert time from one timezone to another.\n\n    Parameters:\n    - date_str (str): \"yyyy-mm-dd hh:mm:ss\" format date string.\n    - from_tz (str): The timezone of the given date string.\n    - to_tz (str): The timezone to convert the date to.\n\n    Returns:\n        str: \"yyyy-mm-dd hh:mm:ss\" format converted datetime string.\n\n    Requirements:\n    - pytz\n    - dateutil.parser\n\n    Example:\n    >>> task_func('2022-03-01 12:00:00', 'UTC', 'America/New_York')\n    '2022-03-01 07:00:00'\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    naive_datetime = parser.parse(date_str)\n    \n    # Get the source and target timezones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_datetime = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the target timezone\n    converted_datetime = aware_datetime.astimezone(to_timezone)\n    \n    # Return the converted datetime in string format\n    return converted_datetime.strftime('%Y-%m-%d %H:%M:%S')\n```\n"}
{"task_id": "BigCodeBench/562", "solution": "import os\nimport ctypes\nimport sys\nimport subprocess\ndef task_func(filepath):\n    if not isinstance(filepath, str):\n        raise TypeError(\"The filepath must be a string.\")\n    if not filepath:\n        raise OSError(\"The filepath cannot be empty.\")\n\n    # Load the DLL file\n    try:\n        loaded_dll = ctypes.CDLL(filepath)\n    except OSError as e:\n        raise OSError(f\"Failed to load the DLL file: {e}\")\n\n    # Get system information\n    system_info = {\n        \"system\": os.uname().sysname,\n        \"node name\": os.uname().nodename,\n        \"release\": os.uname().release,\n        \"version\": os.uname().version,\n        \"machine\": os.uname().machine,\n        \"python version\": sys.version,\n        \"pip version\": subprocess.check_output(['pip', '--version']).decode('utf-8').strip()\n    }\n\n    # Print system information\n    for key, value in system_info.items():\n        print(f\"{key}: {value}\")\n\n    return loaded_dll._name", "raw_solution": "import os\nimport ctypes\nimport sys\nimport subprocess\n\ndef task_func(filepath):\n    if not isinstance(filepath, str):\n        raise TypeError(\"The filepath must be a string.\")\n    if not filepath:\n        raise OSError(\"The filepath cannot be empty.\")\n\n    # Load the DLL file\n    try:\n        loaded_dll = ctypes.CDLL(filepath)\n    except OSError as e:\n        raise OSError(f\"Failed to load the DLL file: {e}\")\n\n    # Get system information\n    system_info = {\n        \"system\": os.uname().sysname,\n        \"node name\": os.uname().nodename,\n        \"release\": os.uname().release,\n        \"version\": os.uname().version,\n        \"machine\": os.uname().machine,\n        \"python version\": sys.version,\n        \"pip version\": subprocess.check_output(['pip', '--version']).decode('utf-8').strip()\n    }\n\n    # Print system information\n    for key, value in system_info.items():\n        print(f\"{key}: {value}\")\n\n    return loaded_dll._name\n\n# Example usage:\n# task_func('libc.so.6')\n```\n"}
{"task_id": "BigCodeBench/563", "solution": "import ctypes\nimport os\nimport shutil\nimport glob\ndef task_func(filepath, destination_dir):\n    # Load the DLL file\n    lib = ctypes.CDLL(filepath)\n\n    # Move all DLL files from the source directory to the destination directory\n    for file in glob.glob(os.path.join(os.getcwd(), '*.dll')):\n        shutil.move(file, destination_dir)\n\n    # Return the name of the loaded DLL file\n    return os.path.basename(filepath)", "raw_solution": "import ctypes\nimport os\nimport shutil\nimport glob\n\ndef task_func(filepath, destination_dir):\n    # Load the DLL file\n    lib = ctypes.CDLL(filepath)\n\n    # Move all DLL files from the source directory to the destination directory\n    for file in glob.glob(os.path.join(os.getcwd(), '*.dll')):\n        shutil.move(file, destination_dir)\n\n    # Return the name of the loaded DLL file\n    return os.path.basename(filepath)\n\n# Example usage:\n# destination = 'destination_dir'\n# print(task_func('example.dll', destination))\n# assert isinstance(task_func('example.dll', destination), str)\n```\nPlease note that the above script assumes that the DLL files are located in the same directory as the script. If the DLL files are located elsewhere, you will need to adjust the `glob.glob` function accordingly.\n"}
{"task_id": "BigCodeBench/564", "solution": "import os\nimport ctypes\nfrom datetime import datetime\nimport pytz\ndef task_func(filepath):\n    # Load the DLL file\n    try:\n        lib = ctypes.CDLL(filepath)\n    except OSError as e:\n        return f\"Error loading the DLL: {e}\"\n\n    # Get file metadata\n    try:\n        file_stats = os.stat(filepath)\n    except FileNotFoundError as e:\n        return f\"Error accessing the file: {e}\"\n\n    # Convert times to UTC\n    creation_time = datetime.fromtimestamp(file_stats.st_ctime, pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n    modification_time = datetime.fromtimestamp(file_stats.st_mtime, pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n\n    # Prepare metadata dictionary\n    metadata = {\n        'name': lib._name,\n        'creation_time': creation_time,\n        'modification_time': modification_time,\n        'size': file_stats.st_size\n    }\n\n    return metadata", "raw_solution": "import os\nimport ctypes\nfrom datetime import datetime\nimport pytz\n\ndef task_func(filepath):\n    # Load the DLL file\n    try:\n        lib = ctypes.CDLL(filepath)\n    except OSError as e:\n        return f\"Error loading the DLL: {e}\"\n\n    # Get file metadata\n    try:\n        file_stats = os.stat(filepath)\n    except FileNotFoundError as e:\n        return f\"Error accessing the file: {e}\"\n\n    # Convert times to UTC\n    creation_time = datetime.fromtimestamp(file_stats.st_ctime, pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n    modification_time = datetime.fromtimestamp(file_stats.st_mtime, pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n\n    # Prepare metadata dictionary\n    metadata = {\n        'name': lib._name,\n        'creation_time': creation_time,\n        'modification_time': modification_time,\n        'size': file_stats.st_size\n    }\n\n    return metadata\n\n# Example usage:\n# print(task_func('path_to_dll_file'))\n```\n"}
{"task_id": "BigCodeBench/565", "solution": "import ctypes\nimport hashlib\nimport binascii\ndef task_func(filepath):\n    # Load the DLL file\n    lib = ctypes.CDLL(filepath)\n    \n    # Convert the library name to hexadecimal\n    lib_name_hex = binascii.hexlify(lib._name.encode()).decode()\n    \n    # Calculate MD5 hash of the library name\n    md5_hash = hashlib.md5(lib._name.encode()).hexdigest()\n    \n    # Calculate SHA256 hash of the library name\n    sha256_hash = hashlib.sha256(lib._name.encode()).hexdigest()\n    \n    # Print the hashes\n    print(f\"MD5 Hash: {md5_hash}\")\n    print(f\"SHA256 Hash: {sha256_hash}\")\n    \n    # Return the library name in hexadecimal format\n    return lib_name_hex", "raw_solution": "import ctypes\nimport hashlib\nimport binascii\n\ndef task_func(filepath):\n    # Load the DLL file\n    lib = ctypes.CDLL(filepath)\n    \n    # Convert the library name to hexadecimal\n    lib_name_hex = binascii.hexlify(lib._name.encode()).decode()\n    \n    # Calculate MD5 hash of the library name\n    md5_hash = hashlib.md5(lib._name.encode()).hexdigest()\n    \n    # Calculate SHA256 hash of the library name\n    sha256_hash = hashlib.sha256(lib._name.encode()).hexdigest()\n    \n    # Print the hashes\n    print(f\"MD5 Hash: {md5_hash}\")\n    print(f\"SHA256 Hash: {sha256_hash}\")\n    \n    # Return the library name in hexadecimal format\n    return lib_name_hex\n```\n"}
{"task_id": "BigCodeBench/566", "solution": "import inspect\nimport types\ndef task_func(f):\n    \"\"\"\n    Ek dee gaee function 'f' ka inspect karta hai aur iski specifications return, jismein function ka naam,\n    kya yah ek lambda function hai, iske arguments, defaults, aur annotations shaamil hain. Yah method\n    inspect aur types modules ka upayog function properties ka inspect karne ke liye karta hai.\n\n    Parameters:\n    - f (function): inspect karne ke liye function.\n\n    Returns:\n        dict: function ke baare mein details vaala ek dictionary, jaise ki iska naam, agar yah ek lambda function hai,\n        arguments, default values, aur annotations.\n\n    Requirements:\n    - inspect\n    - types\n\n    Example:\n    >>> def sample_function(x, y=5): return x + y\n    >>> result = task_func(sample_function)\n    >>> 'sample_function' == result['function_name'] and len(result['args']) == 2\n    true\n    >>> lambda_func = lambda x: x * 2\n    >>> task_func(lambda_func)['is_lambda']\n    true\n\n    \"\"\"\n    result = {\n        'function_name': f.__name__,\n        'is_lambda': isinstance(f, types.LambdaType),\n        'args': [param.name for param in inspect.signature(f).parameters.values()],\n        'defaults': list(inspect.signature(f).parameters.values())[-len(inspect.signature(f).defaults or ()) :],\n        'annotations': inspect.signature(f).annotations\n    }\n    return result", "raw_solution": "import inspect\nimport types\n\ndef task_func(f):\n    \"\"\"\n    Ek dee gaee function 'f' ka inspect karta hai aur iski specifications return, jismein function ka naam,\n    kya yah ek lambda function hai, iske arguments, defaults, aur annotations shaamil hain. Yah method\n    inspect aur types modules ka upayog function properties ka inspect karne ke liye karta hai.\n\n    Parameters:\n    - f (function): inspect karne ke liye function.\n\n    Returns:\n        dict: function ke baare mein details vaala ek dictionary, jaise ki iska naam, agar yah ek lambda function hai,\n        arguments, default values, aur annotations.\n\n    Requirements:\n    - inspect\n    - types\n\n    Example:\n    >>> def sample_function(x, y=5): return x + y\n    >>> result = task_func(sample_function)\n    >>> 'sample_function' == result['function_name'] and len(result['args']) == 2\n    true\n    >>> lambda_func = lambda x: x * 2\n    >>> task_func(lambda_func)['is_lambda']\n    true\n\n    \"\"\"\n    result = {\n        'function_name': f.__name__,\n        'is_lambda': isinstance(f, types.LambdaType),\n        'args': [param.name for param in inspect.signature(f).parameters.values()],\n        'defaults': list(inspect.signature(f).parameters.values())[-len(inspect.signature(f).defaults or ()) :],\n        'annotations': inspect.signature(f).annotations\n    }\n    return result\n```\n"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the string data into a pandas Series\n    data_series = pd.Series(data.split('-')).astype(int)\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(data_series, bins=np.arange(data_series.min(), data_series.max()+2) - 0.5, edgecolor='black')\n\n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n\n    return ax\ndata = '1-2-3-4-5-6-7-8-9-10'", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the string data into a pandas Series\n    data_series = pd.Series(data.split('-')).astype(int)\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(data_series, bins=np.arange(data_series.min(), data_series.max()+2) - 0.5, edgecolor='black')\n\n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n\n    return ax\n\n# Example usage:\ndata = '1-2-3-4-5-6-7-8-9-10'\nax = task_func(data)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/568", "solution": "import inspect\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(f_list):\n    if any(inspect.islambda(f) for f in f_list):\n        raise ValueError(\"The function should not be a lambda function.\")\n    \n    data = []\n    for f in f_list:\n        args = inspect.signature(f).parameters\n        data.append({'Function Name': f.__name__, 'Number of Arguments': len(args)})\n    \n    df = pd.DataFrame(data)\n    df.plot(kind='bar', x='Function Name', y='Number of Arguments', legend=False)\n    plt.xlabel('Function Name')\n    plt.ylabel('Number of Arguments')\n    plt.title('Number of Arguments for Each Function')\n    plt.show()\n    \n    return df\ndef f(x): return x * x", "raw_solution": "import inspect\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(f_list):\n    if any(inspect.islambda(f) for f in f_list):\n        raise ValueError(\"The function should not be a lambda function.\")\n    \n    data = []\n    for f in f_list:\n        args = inspect.signature(f).parameters\n        data.append({'Function Name': f.__name__, 'Number of Arguments': len(args)})\n    \n    df = pd.DataFrame(data)\n    df.plot(kind='bar', x='Function Name', y='Number of Arguments', legend=False)\n    plt.xlabel('Function Name')\n    plt.ylabel('Number of Arguments')\n    plt.title('Number of Arguments for Each Function')\n    plt.show()\n    \n    return df\n\n# Example usage:\ndef f(x): return x * x\ndef g(x, y=2): return x * y\n\n# This will raise a ValueError\n# lambda_func = lambda x: x * 2\n\n# Uncomment the line below to test the function\n# df = task_func([f, g])\n```\n"}
{"task_id": "BigCodeBench/569", "solution": "import inspect\nimport types\nimport math\ndef task_func(f):\n    \"\"\"\n    Ek diye gaye 'f' function ka analysis karta hai and iske naam, arguments ki number ke square, and default values mein present lambda functions ki count wali ek dictionary return hai. \n    Yeh function Python functions ke introspection and introspected data par mathematical operations ke use ko darshata hai.\n\n    Parameters:\n    - f (function): function jiska nirikshan karna hai.\n\n    Returns:\n        dict: Ek dictionary jismein function ka naam, arguments ki number ka square, and default values mein lambda functions ki count shaamil hai.\n\n    Requirements:\n    - inspect\n    - types\n    - math\n\n    Example:\n    >>> def sample_function(x, y=2): return x + y\n    >>> result = task_func(sample_function)\n    >>> 'sample_function' == result['function_name'] and result['sqrt_args'] == math.sqrt(2)\n    True\n    >>> lambda_func = lambda x: x * 2\n    >>> task_func(lambda_func)['lambda_in_defaults'] == 0\n    True\n\n    \"\"\"\n    # Get function details\n    func_details = {\n        'function_name': f.__name__,\n        'args_count': len(inspect.signature(f).parameters),\n        'lambda_in_defaults': 0\n    }\n\n    # Calculate square root of arguments count\n    func_details['sqrt_args'] = math.sqrt(func_details['args_count'])\n\n    # Check for lambda functions in default values\n    for name, param in inspect.signature(f).parameters.items():\n        if param.default is inspect.Parameter.empty:\n            continue\n        if isinstance(param.default, types.LambdaType):\n            func_details['lambda_in_defaults'] += 1\n\n    return func_details", "raw_solution": "import inspect\nimport types\nimport math\n\ndef task_func(f):\n    \"\"\"\n    Ek diye gaye 'f' function ka analysis karta hai and iske naam, arguments ki number ke square, and default values mein present lambda functions ki count wali ek dictionary return hai. \n    Yeh function Python functions ke introspection and introspected data par mathematical operations ke use ko darshata hai.\n\n    Parameters:\n    - f (function): function jiska nirikshan karna hai.\n\n    Returns:\n        dict: Ek dictionary jismein function ka naam, arguments ki number ka square, and default values mein lambda functions ki count shaamil hai.\n\n    Requirements:\n    - inspect\n    - types\n    - math\n\n    Example:\n    >>> def sample_function(x, y=2): return x + y\n    >>> result = task_func(sample_function)\n    >>> 'sample_function' == result['function_name'] and result['sqrt_args'] == math.sqrt(2)\n    True\n    >>> lambda_func = lambda x: x * 2\n    >>> task_func(lambda_func)['lambda_in_defaults'] == 0\n    True\n\n    \"\"\"\n    # Get function details\n    func_details = {\n        'function_name': f.__name__,\n        'args_count': len(inspect.signature(f).parameters),\n        'lambda_in_defaults': 0\n    }\n\n    # Calculate square root of arguments count\n    func_details['sqrt_args'] = math.sqrt(func_details['args_count'])\n\n    # Check for lambda functions in default values\n    for name, param in inspect.signature(f).parameters.items():\n        if param.default is inspect.Parameter.empty:\n            continue\n        if isinstance(param.default, types.LambdaType):\n            func_details['lambda_in_defaults'] += 1\n\n    return func_details\n```\n"}
{"task_id": "BigCodeBench/570", "solution": "import inspect\nimport types\nimport json\ndef task_func(f):\n    \"\"\"\n    Dee gaee function 'f' ka nirikshan karta hai and iske specifications ko ek JSON string ke roop mein return hai. Ismein\n    function ka naam, arguments, default values, ek string format mein annotations, and yah darshane wala ek boolean\n    shaamil hai ki kya yah ek lambda function hai.\n\n    Parameters:\n    - f (function): function jiska nirikshan karna hai.\n\n    Returns:\n        str: ek JSON string jismein function ke visheshataen shaamil hain.\n\n    Requirements:\n    - inspect\n    - types\n    - json\n\n    Example:\n    udaaharan:\n    >>> def sample_function(x, y=2): return x + y\n    >>> 'sample_function' in task_func(sample_function)\n    True\n    >>> def sample_function2(x, y=2): return x * y\n    >>> 'sample_function2' in task_func(sample_function2)\n    True\n\n    \"\"\"\n    if not isinstance(f, types.FunctionType):\n        raise TypeError(\"Provided input is not a function\")\n\n    # Get function details\n    func_name = f.__name__\n    func_args = inspect.signature(f).parameters\n    func_defaults = f.__defaults__ or []\n    func_annotations = f.__annotations__\n    is_lambda = isinstance(f, types.LambdaType)\n\n    # Prepare the function details\n    func_details = {\n        \"name\": func_name,\n        \"args\": {k: v.default for k, v in func_args.items()},\n        \"defaults\": func_defaults,\n        \"annotations\": func_annotations,\n        \"is_lambda\": is_lambda\n    }\n\n    # Convert the function details to a JSON string\n    func_details_json = json.dumps(func_details, default=str)\n\n    return func_details_json", "raw_solution": "import inspect\nimport types\nimport json\n\ndef task_func(f):\n    \"\"\"\n    Dee gaee function 'f' ka nirikshan karta hai and iske specifications ko ek JSON string ke roop mein return hai. Ismein\n    function ka naam, arguments, default values, ek string format mein annotations, and yah darshane wala ek boolean\n    shaamil hai ki kya yah ek lambda function hai.\n\n    Parameters:\n    - f (function): function jiska nirikshan karna hai.\n\n    Returns:\n        str: ek JSON string jismein function ke visheshataen shaamil hain.\n\n    Requirements:\n    - inspect\n    - types\n    - json\n\n    Example:\n    udaaharan:\n    >>> def sample_function(x, y=2): return x + y\n    >>> 'sample_function' in task_func(sample_function)\n    True\n    >>> def sample_function2(x, y=2): return x * y\n    >>> 'sample_function2' in task_func(sample_function2)\n    True\n\n    \"\"\"\n    if not isinstance(f, types.FunctionType):\n        raise TypeError(\"Provided input is not a function\")\n\n    # Get function details\n    func_name = f.__name__\n    func_args = inspect.signature(f).parameters\n    func_defaults = f.__defaults__ or []\n    func_annotations = f.__annotations__\n    is_lambda = isinstance(f, types.LambdaType)\n\n    # Prepare the function details\n    func_details = {\n        \"name\": func_name,\n        \"args\": {k: v.default for k, v in func_args.items()},\n        \"defaults\": func_defaults,\n        \"annotations\": func_annotations,\n        \"is_lambda\": is_lambda\n    }\n\n    # Convert the function details to a JSON string\n    func_details_json = json.dumps(func_details, default=str)\n\n    return func_details_json\n```\n"}
{"task_id": "BigCodeBench/571", "solution": "import inspect\nimport pandas as pd\ndef task_func(f_list, file_path):\n    if not isinstance(f_list, list) or not all(callable(f) for f in f_list):\n        raise ValueError(\"f_list should be a list of callable objects.\")\n    if not isinstance(file_path, str):\n        raise ValueError(\"file_path should be a string.\")\n    \n    try:\n        with open(file_path, 'w') as f:\n            pass\n    except IOError as e:\n        raise IOError(f\"Error accessing file path: {e}\")\n\n    data = []\n    for f in f_list:\n        func_name = f.__name__\n        num_args = len(inspect.signature(f).parameters)\n        defaults = list(inspect.signature(f).parameters.values())[-num_args:]\n        defaults = [default.default if default.default is not inspect.Parameter.empty else None for default in defaults]\n        annotations = inspect.getfullargspec(f).annotations\n        is_lambda = inspect.islambda(f)\n        data.append([func_name, num_args, defaults, annotations, is_lambda])\n\n    df = pd.DataFrame(data, columns=['function name', 'number of arguments', 'defaults', 'annotations', 'is lambda'])\n    df.to_csv(file_path, index=False)", "raw_solution": "import inspect\nimport pandas as pd\n\ndef task_func(f_list, file_path):\n    if not isinstance(f_list, list) or not all(callable(f) for f in f_list):\n        raise ValueError(\"f_list should be a list of callable objects.\")\n    if not isinstance(file_path, str):\n        raise ValueError(\"file_path should be a string.\")\n    \n    try:\n        with open(file_path, 'w') as f:\n            pass\n    except IOError as e:\n        raise IOError(f\"Error accessing file path: {e}\")\n\n    data = []\n    for f in f_list:\n        func_name = f.__name__\n        num_args = len(inspect.signature(f).parameters)\n        defaults = list(inspect.signature(f).parameters.values())[-num_args:]\n        defaults = [default.default if default.default is not inspect.Parameter.empty else None for default in defaults]\n        annotations = inspect.getfullargspec(f).annotations\n        is_lambda = inspect.islambda(f)\n        data.append([func_name, num_args, defaults, annotations, is_lambda])\n\n    df = pd.DataFrame(data, columns=['function name', 'number of arguments', 'defaults', 'annotations', 'is lambda'])\n    df.to_csv(file_path, index=False)\n\n# Example usage:\n# def f(x): return 2 * x\n# def g(x, y=2): return x * y\n# task_func([f, g], './function_info.csv')\n# import os\n# os.remove('./function_info.csv')\n```\n"}
{"task_id": "BigCodeBench/572", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100):\n    # Generate two random arrays of integers\n    array1 = np.array([randint(0, 100) for _ in range(array_length)])\n    array2 = np.array([randint(0, 100) for _ in range(array_length)])\n    \n    # Find the maximum values of the respective elements\n    max_values = np.maximum(array1, array2)\n    \n    # Plot the maximum values\n    fig, ax = plt.subplots()\n    ax.plot(max_values, label='Maximum Values')\n    ax.set_ylabel('Maximum Values')\n    ax.set_title('Line Diagram of Maximum Values')\n    ax.legend()\n    \n    return ax", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100):\n    # Generate two random arrays of integers\n    array1 = np.array([randint(0, 100) for _ in range(array_length)])\n    array2 = np.array([randint(0, 100) for _ in range(array_length)])\n    \n    # Find the maximum values of the respective elements\n    max_values = np.maximum(array1, array2)\n    \n    # Plot the maximum values\n    fig, ax = plt.subplots()\n    ax.plot(max_values, label='Maximum Values')\n    ax.set_ylabel('Maximum Values')\n    ax.set_title('Line Diagram of Maximum Values')\n    ax.legend()\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/573", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array_length=100):\n    # Generate two arrays of random numbers\n    array1 = np.random.rand(array_length)\n    array2 = np.random.rand(array_length)\n\n    # Calculate statistics\n    mean1 = np.mean(array1)\n    mean2 = np.mean(array2)\n    median1 = np.median(array1)\n    median2 = np.median(array2)\n    std1 = np.std(array1)\n    std2 = np.std(array2)\n\n    # Create a DataFrame to store the statistics\n    statistics = pd.DataFrame({\n        'Array1': [mean1, median1, std1],\n        'Array2': [mean2, median2, std2]\n    }, index=['Mean', 'Median', 'Standard Deviation'])\n\n    # Plot the statistics\n    ax = statistics.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Comparison of Statistics between Array1 and Array2')\n    ax.set_ylabel('Value')\n    ax.set_xlabel('Statistic')\n    plt.xticks(rotation=0)\n    plt.tight_layout()\n\n    return statistics, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(array_length=100):\n    # Generate two arrays of random numbers\n    array1 = np.random.rand(array_length)\n    array2 = np.random.rand(array_length)\n\n    # Calculate statistics\n    mean1 = np.mean(array1)\n    mean2 = np.mean(array2)\n    median1 = np.median(array1)\n    median2 = np.median(array2)\n    std1 = np.std(array1)\n    std2 = np.std(array2)\n\n    # Create a DataFrame to store the statistics\n    statistics = pd.DataFrame({\n        'Array1': [mean1, median1, std1],\n        'Array2': [mean2, median2, std2]\n    }, index=['Mean', 'Median', 'Standard Deviation'])\n\n    # Plot the statistics\n    ax = statistics.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Comparison of Statistics between Array1 and Array2')\n    ax.set_ylabel('Value')\n    ax.set_xlabel('Statistic')\n    plt.xticks(rotation=0)\n    plt.tight_layout()\n\n    return statistics, ax\n```\n"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate a sine wave with noise\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sin(x) + noise_level * np.random.normal(size=array_length)\n\n    # Define a model function to fit\n    def model(x, a, b):\n        return a * np.sin(b * x)\n\n    # Fit the model to the data\n    popt, _ = curve_fit(model, x, y)\n\n    # Plot the noisy data and the fitted curve\n    plt.figure()\n    plt.plot(x, y, 'b-', label='Noisy data')\n    plt.plot(x, model(x, *popt), 'r--', label='Fitted curve')\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Noisy sine wave and fitted curve')\n    plt.show()\n\n    return plt.gca()", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate a sine wave with noise\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sin(x) + noise_level * np.random.normal(size=array_length)\n\n    # Define a model function to fit\n    def model(x, a, b):\n        return a * np.sin(b * x)\n\n    # Fit the model to the data\n    popt, _ = curve_fit(model, x, y)\n\n    # Plot the noisy data and the fitted curve\n    plt.figure()\n    plt.plot(x, y, 'b-', label='Noisy data')\n    plt.plot(x, model(x, *popt), 'r--', label='Fitted curve')\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Noisy sine wave and fitted curve')\n    plt.show()\n\n    return plt.gca()\n```\n"}
{"task_id": "BigCodeBench/575", "solution": "from random import shuffle\nimport pandas as pd\nimport numpy as np\nN_GROUPS = 5\ndef task_func(l, n_groups=N_GROUPS):\n    # Shuffle the list\n    shuffle(l)\n    \n    # Create a DataFrame from the list\n    df = pd.DataFrame(l)\n    \n    # Split the DataFrame into groups\n    groups = np.array_split(df, n_groups)\n    \n    # Concatenate the groups along the columns\n    result_df = pd.concat(groups, axis=1)\n    \n    return result_df", "raw_solution": "from random import shuffle\nimport pandas as pd\nimport numpy as np\n\n# Constants\nN_GROUPS = 5\n\ndef task_func(l, n_groups=N_GROUPS):\n    # Shuffle the list\n    shuffle(l)\n    \n    # Create a DataFrame from the list\n    df = pd.DataFrame(l)\n    \n    # Split the DataFrame into groups\n    groups = np.array_split(df, n_groups)\n    \n    # Concatenate the groups along the columns\n    result_df = pd.concat(groups, axis=1)\n    \n    return result_df\n\n# Example usage"}
{"task_id": "BigCodeBench/576", "solution": "from random import shuffle, randint\nimport pandas as pd\ndef task_func(l, n_groups=5):\n    # Convert list to pandas Series\n    series = pd.Series(l)\n    \n    # Shuffle the series\n    series = series.sample(frac=1).reset_index(drop=True)\n    \n    # Create a longer series by cycling through the shuffled series\n    longer_series = pd.concat([series]*n_groups).reset_index(drop=True)\n    \n    # Modify each element by taking a random number of characters from the start and appending to the end\n    modified_series = longer_series.apply(lambda x: x[:randint(1, len(x))] + x[randint(1, len(x)):])\n    \n    return modified_series", "raw_solution": "from random import shuffle, randint\nimport pandas as pd\n\ndef task_func(l, n_groups=5):\n    # Convert list to pandas Series\n    series = pd.Series(l)\n    \n    # Shuffle the series\n    series = series.sample(frac=1).reset_index(drop=True)\n    \n    # Create a longer series by cycling through the shuffled series\n    longer_series = pd.concat([series]*n_groups).reset_index(drop=True)\n    \n    # Modify each element by taking a random number of characters from the start and appending to the end\n    modified_series = longer_series.apply(lambda x: x[:randint(1, len(x))] + x[randint(1, len(x)):])\n    \n    return modified_series\n\n# Example usage:\n# result = task_func(['ABC', 'DEF', 'GHI'])\n# print(isinstance(result, pd.Series))  # Should print True\n# print(len(result))  # Should print 15\n```\n"}
{"task_id": "BigCodeBench/577", "solution": "import os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\ndef task_func(directory):\n    result = {}\n    for file in pathlib.Path(directory).iterdir():\n        if file.is_file():\n            # Normalize filename to ASCII\n            ascii_name = unicodedata.normalize('NFKD', file.name).encode('ascii', 'ignore').decode('ascii')\n            # Calculate MD5 hash\n            with open(file, 'rb') as f:\n                file_hash = md5(f.read()).hexdigest()\n            # Get file size\n            file_size = file.stat().st_size\n            # Store the information in the result dictionary\n            result[ascii_name] = {'size': file_size, 'MD5 hash': file_hash}\n    return result", "raw_solution": "import os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\n\ndef task_func(directory):\n    result = {}\n    for file in pathlib.Path(directory).iterdir():\n        if file.is_file():\n            # Normalize filename to ASCII\n            ascii_name = unicodedata.normalize('NFKD', file.name).encode('ascii', 'ignore').decode('ascii')\n            # Calculate MD5 hash\n            with open(file, 'rb') as f:\n                file_hash = md5(f.read()).hexdigest()\n            # Get file size\n            file_size = file.stat().st_size\n            # Store the information in the result dictionary\n            result[ascii_name] = {'size': file_size, 'MD5 hash': file_hash}\n    return result\n\n# Example usage:\n# info = task_func('test')\n# print(info)\n```\n"}
{"task_id": "BigCodeBench/578", "solution": "import unicodedata\nimport requests\nURL = 'https://api.github.com/users/'\ndef task_func(username):\n    try:\n        response = requests.get(URL + username)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n\n        user_data = response.json()\n\n        # Normalize all string data to ASCII\n        normalized_data = {}\n        for key, value in user_data.items():\n            if isinstance(value, str):\n                normalized_data[key] = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n            else:\n                normalized_data[key] = value\n\n        return normalized_data\n\n    except requests.exceptions.HTTPError as e:\n        raise requests.exceptions.HTTPError(f\"HTTP error occurred: {e}\")", "raw_solution": "import unicodedata\nimport requests\n\nURL = 'https://api.github.com/users/'\n\ndef task_func(username):\n    try:\n        response = requests.get(URL + username)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n\n        user_data = response.json()\n\n        # Normalize all string data to ASCII\n        normalized_data = {}\n        for key, value in user_data.items():\n            if isinstance(value, str):\n                normalized_data[key] = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n            else:\n                normalized_data[key] = value\n\n        return normalized_data\n\n    except requests.exceptions.HTTPError as e:\n        raise requests.exceptions.HTTPError(f\"HTTP error occurred: {e}\")\n\n# Example usage:\n# result = task_func('torvalds')\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        with open(csv_file, newline='', encoding='utf-8') as csvfile:\n            reader = csv.reader(csvfile)\n            text = ''\n            for row in reader:\n                text += ' '.join(row)\n            \n            # Convert text to ASCII\n            text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n            \n            # Tokenize text into words\n            words = text.split()\n            \n            # Count word frequencies\n            word_counts = Counter(words)\n            \n            # Get the 10 most common words\n            most_common_words = word_counts.most_common(10)\n            \n            # Plot the frequencies\n            words, counts = zip(*most_common_words)\n            fig, ax = plt.subplots()\n            ax.bar(words, counts)\n            ax.set_xlabel('Words')\n            ax.set_ylabel('Frequencies')\n            ax.set_title('Top 10 Words')\n            \n            return ax, most_common_words\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    except IOError:\n        raise IOError(f\"There was an error reading the file {csv_file}.\")", "raw_solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        with open(csv_file, newline='', encoding='utf-8') as csvfile:\n            reader = csv.reader(csvfile)\n            text = ''\n            for row in reader:\n                text += ' '.join(row)\n            \n            # Convert text to ASCII\n            text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n            \n            # Tokenize text into words\n            words = text.split()\n            \n            # Count word frequencies\n            word_counts = Counter(words)\n            \n            # Get the 10 most common words\n            most_common_words = word_counts.most_common(10)\n            \n            # Plot the frequencies\n            words, counts = zip(*most_common_words)\n            fig, ax = plt.subplots()\n            ax.bar(words, counts)\n            ax.set_xlabel('Words')\n            ax.set_ylabel('Frequencies')\n            ax.set_title('Top 10 Words')\n            \n            return ax, most_common_words\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    except IOError:\n        raise IOError(f\"There was an error reading the file {csv_file}.\")\n```\n"}
{"task_id": "BigCodeBench/580", "solution": "import pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\nRANGE = 10000\nSIZE = 1000\nBIN_WIDTH = 100\ndef task_func():\n    # Generate random numbers\n    random_numbers = [random.randint(0, RANGE) for _ in range(SIZE)]\n    \n    # Calculate moving average\n    moving_average = [\n        statistics.mean(random_numbers[i-5:i+1]) if i >= 5 else np.nan\n        for i in range(len(random_numbers))\n    ]\n    \n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        'Random Numbers': random_numbers,\n        'Moving Average': moving_average\n    })\n    \n    # Plot histogram of random numbers\n    plt.hist(df['Random Numbers'], bins=np.arange(0, RANGE+BIN_WIDTH, BIN_WIDTH), alpha=0.7, color='blue')\n    plt.title('Histogram of Random Numbers')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return df", "raw_solution": "import pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nRANGE = 10000  # The range within which random numbers are generated\nSIZE = 1000  # The number of random numbers to generate\nBIN_WIDTH = 100  # The width of bins for the histogram\n\ndef task_func():\n    # Generate random numbers\n    random_numbers = [random.randint(0, RANGE) for _ in range(SIZE)]\n    \n    # Calculate moving average\n    moving_average = [\n        statistics.mean(random_numbers[i-5:i+1]) if i >= 5 else np.nan\n        for i in range(len(random_numbers))\n    ]\n    \n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        'Random Numbers': random_numbers,\n        'Moving Average': moving_average\n    })\n    \n    # Plot histogram of random numbers\n    plt.hist(df['Random Numbers'], bins=np.arange(0, RANGE+BIN_WIDTH, BIN_WIDTH), alpha=0.7, color='blue')\n    plt.title('Histogram of Random Numbers')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return df\n\n# Example usage\ndf = task_func()"}
{"task_id": "BigCodeBench/581", "solution": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nSIZE = 1000\nPI = np.pi\ndef task_func(size=SIZE, frequency=1):\n    '''\n    Create a list of random sinusoidal values and plot them in a graph.\n    \n    Parameters:\n    - size (int): The number of points for the sinusoidal wave. Default is 1000.\n    - frequency (float): The frequency of the sinusoidal wave. Default is 1.\n    \n    Returns:\n    - Axes object: The plot of the sinusoidal wave.\n    \n    Requirements:\n    - random\n    - math\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> import matplotlib\n    >>> ax = task_func(size=1000, frequency=1)\n    >>> isinstance(ax, matplotlib.axes.Axes)\n    True\n    >>> len(ax.lines[0].get_ydata()) == 1000  # Verify the number of data points in the sinusoidal wave\n    True\n    >>> isinstance(ax.lines[0].get_ydata()[0], float)  # Check if y-values are floating-point numbers\n    True\n    '''\n    # Generate random values\n    random.seed(42)\n    x_values = np.linspace(0, 2 * PI, size)\n    y_values = [math.sin(frequency * x) + random.uniform(-0.1, 0.1) for x in x_values]\n\n    # Plot the values\n    fig, ax = plt.subplots()\n    ax.plot(x_values, y_values, label='Sinusoidal Wave')\n    ax.set_title('Random Sinusoidal Wave')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.legend()\n\n    return ax", "raw_solution": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n# Constants\nRANGE = 10000\nSIZE = 1000\nPI = np.pi\n\n\ndef task_func(size=SIZE, frequency=1):\n    '''\n    Create a list of random sinusoidal values and plot them in a graph.\n    \n    Parameters:\n    - size (int): The number of points for the sinusoidal wave. Default is 1000.\n    - frequency (float): The frequency of the sinusoidal wave. Default is 1.\n    \n    Returns:\n    - Axes object: The plot of the sinusoidal wave.\n    \n    Requirements:\n    - random\n    - math\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> import matplotlib\n    >>> ax = task_func(size=1000, frequency=1)\n    >>> isinstance(ax, matplotlib.axes.Axes)\n    True\n    >>> len(ax.lines[0].get_ydata()) == 1000  # Verify the number of data points in the sinusoidal wave\n    True\n    >>> isinstance(ax.lines[0].get_ydata()[0], float)  # Check if y-values are floating-point numbers\n    True\n    '''\n    # Generate random values\n    random.seed(42)\n    x_values = np.linspace(0, 2 * PI, size)\n    y_values = [math.sin(frequency * x) + random.uniform(-0.1, 0.1) for x in x_values]\n\n    # Plot the values\n    fig, ax = plt.subplots()\n    ax.plot(x_values, y_values, label='Sinusoidal Wave')\n    ax.set_title('Random Sinusoidal Wave')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a histogram of the data\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the PDF\n    pdf = stats.norm.pdf(bins, loc=0, scale=1)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n    \n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a histogram of the data\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the PDF\n    pdf = stats.norm.pdf(bins, loc=0, scale=1)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n    \n    return fig\n```\n"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Cipher import AES\nfrom Crypto.Random import get_random_bytes\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Generate random password and nonce for AES encryption\n    password = get_random_bytes(16)\n    nonce = get_random_bytes(16)\n\n    # Encrypt the private key with AES\n    cipher = AES.new(password, AES.MODE_EAX, nonce)\n    encrypted_priv_key = cipher.encrypt(priv_key.export_key())\n\n    # Encode the encrypted private key and nonce for storage\n    encoded_priv_key = b64encode(encrypted_priv_key)\n    encoded_nonce = b64encode(nonce)\n\n    # Generate a random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Save the encoded private key and nonce to a file\n    with open(filename, 'wb') as f:\n        f.write(encoded_priv_key)\n        f.write(b\"\\n\")\n        f.write(encoded_nonce)\n\n    return pub_key, filename, password, nonce", "raw_solution": "import rsa\nimport os\nfrom Crypto.Cipher import AES\nfrom Crypto.Random import get_random_bytes\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Generate random password and nonce for AES encryption\n    password = get_random_bytes(16)\n    nonce = get_random_bytes(16)\n\n    # Encrypt the private key with AES\n    cipher = AES.new(password, AES.MODE_EAX, nonce)\n    encrypted_priv_key = cipher.encrypt(priv_key.export_key())\n\n    # Encode the encrypted private key and nonce for storage\n    encoded_priv_key = b64encode(encrypted_priv_key)\n    encoded_nonce = b64encode(nonce)\n\n    # Generate a random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Save the encoded private key and nonce to a file\n    with open(filename, 'wb') as f:\n        f.write(encoded_priv_key)\n        f.write(b\"\\n\")\n        f.write(encoded_nonce)\n\n    return pub_key, filename, password, nonce\n\n# Example usage:\n# pub_key, filename, password, nonce = task_func()\n# print(isinstance(pub_key, rsa.PublicKey))  # Should print True\n# print(isinstance(filename, str))  # Should print True\n# print(isinstance(password, bytes))  # Should print True\n# print(isinstance(nonce, bytes))  # Should print True\n```\n"}
{"task_id": "BigCodeBench/584", "solution": "import rsa\nimport urllib.request\nfrom hashlib import sha256\ndef task_func(url):\n    try:\n        # Fetch content from the URL\n        with urllib.request.urlopen(url) as response:\n            content = response.read()\n\n        # Generate SHA256 hash of the content\n        hash_value = sha256(content).hexdigest()\n\n        # Generate RSA keys\n        public_key, private_key = rsa.newkeys(512)\n\n        # Sign the hash using the private key\n        signed_hash = rsa.sign(hash_value.encode(), private_key, 'SHA-256')\n\n        # Verify the signature using the public key\n        rsa.verify(hash_value.encode(), signed_hash, public_key)\n\n        # Convert signed_hash to hexadecimal string\n        signed_hash_hex = signed_hash.hex()\n\n        return public_key, signed_hash_hex, content\n\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Failed to fetch data from {url}. URL error: {e}\")\n    except rsa.pkcs1.VerificationError:\n        raise ValueError(\"Failed to verify the signature.\")", "raw_solution": "import rsa\nimport urllib.request\nfrom hashlib import sha256\n\ndef task_func(url):\n    try:\n        # Fetch content from the URL\n        with urllib.request.urlopen(url) as response:\n            content = response.read()\n\n        # Generate SHA256 hash of the content\n        hash_value = sha256(content).hexdigest()\n\n        # Generate RSA keys\n        public_key, private_key = rsa.newkeys(512)\n\n        # Sign the hash using the private key\n        signed_hash = rsa.sign(hash_value.encode(), private_key, 'SHA-256')\n\n        # Verify the signature using the public key\n        rsa.verify(hash_value.encode(), signed_hash, public_key)\n\n        # Convert signed_hash to hexadecimal string\n        signed_hash_hex = signed_hash.hex()\n\n        return public_key, signed_hash_hex, content\n\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Failed to fetch data from {url}. URL error: {e}\")\n    except rsa.pkcs1.VerificationError:\n        raise ValueError(\"Failed to verify the signature.\")\n```\n"}
{"task_id": "BigCodeBench/585", "solution": "import rsa\nimport os\nimport zipfile\nfrom base64 import b64encode\ndef task_func(directory):\n    # Generate RSA keys\n    public_key, private_key = rsa.newkeys(512)\n\n    # List all files in the directory\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\n    # Create a zip file\n    zip_filename = 'encrypted_files.zip'\n    with zipfile.ZipFile(zip_filename, 'w') as zip_file:\n        for file in files:\n            file_path = os.path.join(directory, file)\n            with open(file_path, 'rb') as f:\n                file_data = f.read()\n                encrypted_data = rsa.encrypt(file_data, public_key)\n                encrypted_data_b64 = b64encode(encrypted_data)\n                zip_file.writestr(file, encrypted_data_b64)\n\n    return public_key, zip_filename", "raw_solution": "import rsa\nimport os\nimport zipfile\nfrom base64 import b64encode\n\ndef task_func(directory):\n    # Generate RSA keys\n    public_key, private_key = rsa.newkeys(512)\n\n    # List all files in the directory\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\n    # Create a zip file\n    zip_filename = 'encrypted_files.zip'\n    with zipfile.ZipFile(zip_filename, 'w') as zip_file:\n        for file in files:\n            file_path = os.path.join(directory, file)\n            with open(file_path, 'rb') as f:\n                file_data = f.read()\n                encrypted_data = rsa.encrypt(file_data, public_key)\n                encrypted_data_b64 = b64encode(encrypted_data)\n                zip_file.writestr(file, encrypted_data_b64)\n\n    return public_key, zip_filename\n```\n"}
{"task_id": "BigCodeBench/586", "solution": "import rsa\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Read the file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # Generate a Fernet key\n    fernet_key = Fernet.generate_key()\n    cipher = Fernet(fernet_key)\n\n    # Encrypt the file content\n    encrypted_content = cipher.encrypt(file_content)\n\n    # Encrypt the Fernet key with the RSA public key\n    encrypted_key = rsa.encrypt(fernet_key, pub_key)\n\n    # Save the encrypted content to a new file\n    encrypted_file_path = file_path + '.encrypted'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_content)\n\n    # Save the encrypted Fernet key to a new file\n    encrypted_key_file_path = 'fernet_key.encrypted'\n    with open(encrypted_key_file_path, 'wb') as f:\n        f.write(b64encode(encrypted_key))\n\n    return pub_key, encrypted_file_path, encrypted_key_file_path", "raw_solution": "import rsa\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Read the file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # Generate a Fernet key\n    fernet_key = Fernet.generate_key()\n    cipher = Fernet(fernet_key)\n\n    # Encrypt the file content\n    encrypted_content = cipher.encrypt(file_content)\n\n    # Encrypt the Fernet key with the RSA public key\n    encrypted_key = rsa.encrypt(fernet_key, pub_key)\n\n    # Save the encrypted content to a new file\n    encrypted_file_path = file_path + '.encrypted'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_content)\n\n    # Save the encrypted Fernet key to a new file\n    encrypted_key_file_path = 'fernet_key.encrypted'\n    with open(encrypted_key_file_path, 'wb') as f:\n        f.write(b64encode(encrypted_key))\n\n    return pub_key, encrypted_file_path, encrypted_key_file_path\n```\n"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Read the file to encrypt\n    with open(file_path, 'rb') as f:\n        data = f.read()\n\n    # Generate AES key and IV\n    aes_key = os.urandom(32)\n    iv = os.urandom(16)\n\n    # Create AES cipher\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Pad and encrypt the data\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pub_key)\n\n    # Save the encrypted data and encrypted AES key\n    encrypted_file_path = file_path + '.encrypted'\n    encrypted_key_file_path = file_path + '_key.encrypted'\n    \n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_data)\n\n    with open(encrypted_key_file_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pub_key, encrypted_file_path, encrypted_key_file_path", "raw_solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Read the file to encrypt\n    with open(file_path, 'rb') as f:\n        data = f.read()\n\n    # Generate AES key and IV\n    aes_key = os.urandom(32)\n    iv = os.urandom(16)\n\n    # Create AES cipher\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Pad and encrypt the data\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pub_key)\n\n    # Save the encrypted data and encrypted AES key\n    encrypted_file_path = file_path + '.encrypted'\n    encrypted_key_file_path = file_path + '_key.encrypted'\n    \n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_data)\n\n    with open(encrypted_key_file_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pub_key, encrypted_file_path, encrypted_key_file_path\n```\n"}
{"task_id": "BigCodeBench/588", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nRANGE = 100\nSIZE = 1000\ndef task_func():\n    # Generating random integers for 'X' and 'Y' columns\n    x = np.random.randint(0, RANGE, SIZE)\n    y = np.random.randint(0, RANGE, SIZE)\n\n    # Creating a DataFrame\n    df = pd.DataFrame({'X': x, 'Y': y})\n\n    # Plotting the data\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='X', y='Y', palette='viridis')\n    plt.title('Scatter Plot of Random Points')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants defining the range of random integers and the size of the DataFrame\nRANGE = 100\nSIZE = 1000\n\ndef task_func():\n    # Generating random integers for 'X' and 'Y' columns\n    x = np.random.randint(0, RANGE, SIZE)\n    y = np.random.randint(0, RANGE, SIZE)\n\n    # Creating a DataFrame\n    df = pd.DataFrame({'X': x, 'Y': y})\n\n    # Plotting the data\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='X', y='Y', palette='viridis')\n    plt.title('Scatter Plot of Random Points')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/589", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\ndef task_func():\n    # Generate random 2D points\n    data = np.random.rand(SIZE, 2) * RANGE\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=CLUSTERS, random_state=0)\n    kmeans.fit(data)\n\n    # Plot the clusters and centroids\n    plt.figure(figsize=(8, 6))\n    for i, color in zip(range(CLUSTERS), ['r', 'g', 'b', 'c', 'm']):\n        cluster_data = data[kmeans.labels_ == i]\n        plt.scatter(cluster_data[:, 0], cluster_data[:, 1], c=color, label=f'Cluster {i+1}')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', marker='X', label='Centroids')\n    plt.title('KMeans Clustering')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return data, kmeans", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\n# Constants for configuration\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\n\n\ndef task_func():\n    # Generate random 2D points\n    data = np.random.rand(SIZE, 2) * RANGE\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=CLUSTERS, random_state=0)\n    kmeans.fit(data)\n\n    # Plot the clusters and centroids\n    plt.figure(figsize=(8, 6))\n    for i, color in zip(range(CLUSTERS), ['r', 'g', 'b', 'c', 'm']):\n        cluster_data = data[kmeans.labels_ == i]\n        plt.scatter(cluster_data[:, 0], cluster_data[:, 1], c=color, label=f'Cluster {i+1}')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', marker='X', label='Centroids')\n    plt.title('KMeans Clustering')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return data, kmeans\n```\n"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    try:\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"Failed to retrieve the URL: {e}\")\n\n    document = pq(html_content)\n    anchors = document('a')\n\n    data = []\n    for anchor in anchors:\n        text = pq(anchor).text()\n        href = pq(anchor).attr('href')\n        fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        data.append({'text': text, 'href': href, 'fetch_time': fetch_time})\n\n    df = pd.DataFrame(data)\n\n    return df", "raw_solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"Failed to retrieve the URL: {e}\")\n\n    document = pq(html_content)\n    anchors = document('a')\n\n    data = []\n    for anchor in anchors:\n        text = pq(anchor).text()\n        href = pq(anchor).attr('href')\n        fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        data.append({'text': text, 'href': href, 'fetch_time': fetch_time})\n\n    df = pd.DataFrame(data)\n\n    return df\n\n# Example usage:\n# df = task_func('https://en.wikibooks.org/wiki/Main_Page')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/591", "solution": "from datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\nFILE_PATH = 'custom_data.csv'\ndef task_func(hours, file_path=FILE_PATH):\n    # Generate temperature data\n    data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = start_time + timedelta(hours=i)\n        temperature = randint(15, 35)  # Random temperature between 15 and 35\n        category = TEMP_CATEGORIES[randint(0, 2)]  # Random category\n        data.append({'time': time, 'temperature': temperature, 'category': category})\n\n    # Create a DataFrame\n    df = pd.DataFrame(data)\n\n    # Save to CSV\n    df.to_csv(file_path, index=False)\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['time'], df['temperature'], label='Temperature')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Temperature')\n    ax.set_title('Temperature over Time')\n    ax.legend()\n\n    return file_path, ax", "raw_solution": "from datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\nFILE_PATH = 'custom_data.csv'\n\ndef task_func(hours, file_path=FILE_PATH):\n    # Generate temperature data\n    data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = start_time + timedelta(hours=i)\n        temperature = randint(15, 35)  # Random temperature between 15 and 35\n        category = TEMP_CATEGORIES[randint(0, 2)]  # Random category\n        data.append({'time': time, 'temperature': temperature, 'category': category})\n\n    # Create a DataFrame\n    df = pd.DataFrame(data)\n\n    # Save to CSV\n    df.to_csv(file_path, index=False)\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['time'], df['temperature'], label='Temperature')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Temperature')\n    ax.set_title('Temperature over Time')\n    ax.legend()\n\n    return file_path, ax\n```\n"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nSENSORS = ['Time', 'Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(SENSORS)\n\n        for _ in range(hours * 60):  # Assuming each minute has data\n            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            temperature = randint(20, 100)\n            humidity = randint(30, 90)\n            pressure = randint(1000, 1100)\n            writer.writerow([time, temperature, humidity, pressure])\n\n    return filename", "raw_solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nSENSORS = ['Time', 'Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(SENSORS)\n\n        for _ in range(hours * 60):  # Assuming each minute has data\n            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            temperature = randint(20, 100)\n            humidity = randint(30, 90)\n            pressure = randint(1000, 1100)\n            writer.writerow([time, temperature, humidity, pressure])\n\n    return filename\n\n# Example usage:\n# file_path = task_func(1)  # Generate data for 1 hour\n# print(os.path.exists(file_path))  # Check if the file was created\n# print(isinstance(file_path, str))  # Validate that the return type is a string\n# print('sensor_data.csv' in file_path)  # Ensure the filename is correct\n```\n"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    data = {'Time': [], 'Car': [], 'Bus': [], 'Truck': [], 'Bike': []}\n    for hour in range(hours):\n        time = datetime.now() + timedelta(hours=hour)\n        data['Time'].append(time.strftime('%Y-%m-%d %H:%M:%S'))\n        for vehicle in VEHICLE_TYPES:\n            data[vehicle].append(randint(0, 100))\n\n    file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=data.keys())\n        writer.writeheader()\n        writer.writerows(data)\n\n    df = pd.DataFrame(data)\n    ax = df.plot(x='Time', y=VEHICLE_TYPES, kind='line', marker='o')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return file_path, ax", "raw_solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    data = {'Time': [], 'Car': [], 'Bus': [], 'Truck': [], 'Bike': []}\n    for hour in range(hours):\n        time = datetime.now() + timedelta(hours=hour)\n        data['Time'].append(time.strftime('%Y-%m-%d %H:%M:%S'))\n        for vehicle in VEHICLE_TYPES:\n            data[vehicle].append(randint(0, 100))\n\n    file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=data.keys())\n        writer.writeheader()\n        writer.writerows(data)\n\n    df = pd.DataFrame(data)\n    ax = df.plot(x='Time', y=VEHICLE_TYPES, kind='line', marker='o')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return file_path, ax\n```\n"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = os.path.join(output_dir, 'weather_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n\n        for _ in range(hours):\n            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n            writer.writerow([time, condition])\n\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    shutil.copy(filename, backup_dir)\n\n    return filename", "raw_solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = os.path.join(output_dir, 'weather_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n\n        for _ in range(hours):\n            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n            writer.writerow([time, condition])\n\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    shutil.copy(filename, backup_dir)\n\n    return filename\n```\n"}
{"task_id": "BigCodeBench/595", "solution": "import random\nimport bisect\nfrom array import array\ndef task_func(n=10, total=100):\n    sorted_nums = array('i', sorted(random.sample(range(total), n)))\n    new_num = random.randint(0, total)\n    pos = bisect.bisect_left(sorted_nums, new_num)\n    return sorted_nums, pos", "raw_solution": "import random\nimport bisect\nfrom array import array\n\ndef task_func(n=10, total=100):\n    sorted_nums = array('i', sorted(random.sample(range(total), n)))\n    new_num = random.randint(0, total)\n    pos = bisect.bisect_left(sorted_nums, new_num)\n    return sorted_nums, pos\n\n# Example usage:\nsorted_nums, pos = task_func(5, 50)"}
{"task_id": "BigCodeBench/596", "solution": "import time\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\ndef task_func(duration):\n    timestamps = []\n    random_values = []\n\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=duration)\n\n    while datetime.now() < end_time:\n        current_time = datetime.now()\n        timestamps.append(current_time.strftime('%H:%M:%S.%f'))\n        random_values.append(randint(0, 100))\n        time.sleep(0.1)  # Sleep for a short duration to reduce CPU usage\n\n    plt.plot(timestamps, random_values, marker='o')\n    plt.xlabel('Time')\n    plt.ylabel('Random Value')\n    plt.title('Random Value over Time')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return timestamps, random_values", "raw_solution": "import time\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef task_func(duration):\n    timestamps = []\n    random_values = []\n\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=duration)\n\n    while datetime.now() < end_time:\n        current_time = datetime.now()\n        timestamps.append(current_time.strftime('%H:%M:%S.%f'))\n        random_values.append(randint(0, 100))\n        time.sleep(0.1)  # Sleep for a short duration to reduce CPU usage\n\n    plt.plot(timestamps, random_values, marker='o')\n    plt.xlabel('Time')\n    plt.ylabel('Random Value')\n    plt.title('Random Value over Time')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return timestamps, random_values\n```\n"}
{"task_id": "BigCodeBench/597", "solution": "import pandas as pd\nimport time\ndef task_func(data, letter):\n    \"\"\"\n    Ek dictionary mein rows ko filters karta hai jahan 'name' column ke maan ek nirdisht letter se shuru hote hain.\n    first, dict ko ek DataFrame mein badlen and phir is DataFrame mein rows ko filters karen.\n\n    Parameters:\n    - df (dic of list): input dict. Ismein ek 'name' key honi chahiye.\n    - letter (str): 'name' column ko filters karne ke liye letter.\n\n    Returns:\n        pd.series: filters kiye gaye 'name' column ki ek series.\n\n    Requirements:\n    - pandas\n    - time\n\n    Example:\n    >>> data = {'name': ['Alice', 'Bob', 'Charlie', 'David', 'eva', 'Fiona']}\n    >>> filtered_names = task_func(data, 'a')\n    >>> filtered_names.index[0].startswith('A')\n    true\n    >>> len(filtered_names)\n    1\n\n    \"\"\"\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Filter the DataFrame based on the starting letter of the 'name' column\n    filtered_df = df[df['name'].str.startswith(letter)]\n    \n    # Return the 'name' column as a pandas Series\n    return filtered_df['name']", "raw_solution": "import pandas as pd\nimport time\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\n\ndef task_func(data, letter):\n    \"\"\"\n    Ek dictionary mein rows ko filters karta hai jahan 'name' column ke maan ek nirdisht letter se shuru hote hain.\n    first, dict ko ek DataFrame mein badlen and phir is DataFrame mein rows ko filters karen.\n\n    Parameters:\n    - df (dic of list): input dict. Ismein ek 'name' key honi chahiye.\n    - letter (str): 'name' column ko filters karne ke liye letter.\n\n    Returns:\n        pd.series: filters kiye gaye 'name' column ki ek series.\n\n    Requirements:\n    - pandas\n    - time\n\n    Example:\n    >>> data = {'name': ['Alice', 'Bob', 'Charlie', 'David', 'eva', 'Fiona']}\n    >>> filtered_names = task_func(data, 'a')\n    >>> filtered_names.index[0].startswith('A')\n    true\n    >>> len(filtered_names)\n    1\n\n    \"\"\"\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Filter the DataFrame based on the starting letter of the 'name' column\n    filtered_df = df[df['name'].str.startswith(letter)]\n    \n    # Return the 'name' column as a pandas Series\n    return filtered_df['name']\n```\n"}
{"task_id": "BigCodeBench/598", "solution": "import pandas as pd\nimport time\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Start timing the process\n    start_time = time.time()\n    \n    # Filter the DataFrame to only include words that start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Initialize an empty dictionary to store word lengths and their counts\n    word_lengths = {}\n    \n    # Iterate over the filtered DataFrame and count the lengths of the words\n    for index, row in filtered_df.iterrows():\n        word_length = len(row['Word'])\n        if word_length in word_lengths:\n            word_lengths[word_length] += 1\n        else:\n            word_lengths[word_length] = 1\n    \n    # Print the time taken to process the data\n    print(f\"Time taken: {time.time() - start_time} seconds\")\n    \n    return word_lengths\ndf = {'Word': ['apple', 'banana', 'cherry', 'date', 'fig', 'grape', 'kiwi']}", "raw_solution": "import pandas as pd\nimport time\n\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Start timing the process\n    start_time = time.time()\n    \n    # Filter the DataFrame to only include words that start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Initialize an empty dictionary to store word lengths and their counts\n    word_lengths = {}\n    \n    # Iterate over the filtered DataFrame and count the lengths of the words\n    for index, row in filtered_df.iterrows():\n        word_length = len(row['Word'])\n        if word_length in word_lengths:\n            word_lengths[word_length] += 1\n        else:\n            word_lengths[word_length] = 1\n    \n    # Print the time taken to process the data\n    print(f\"Time taken: {time.time() - start_time} seconds\")\n    \n    return word_lengths\n\n# Example usage\ndf = {'Word': ['apple', 'banana', 'cherry', 'date', 'fig', 'grape', 'kiwi']}"}
{"task_id": "BigCodeBench/599", "solution": "import pandas as pd\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Filter the DataFrame to only include words that start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the length of each word in the filtered DataFrame\n    filtered_df['Length'] = filtered_df['Word'].str.len()\n    \n    # Plot a histogram of the word lengths\n    ax = filtered_df['Length'].plot(kind='hist', bins=10, title=f'Word Lengths Starting with \"{letter}\"')\n    ax.set_xlabel('Length')\n    ax.set_ylabel('Frequency')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\ndf = {'Word': ['apple', 'banana', 'cherry', 'date', 'fig', 'grape', 'avocado']}", "raw_solution": "import pandas as pd\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Filter the DataFrame to only include words that start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the length of each word in the filtered DataFrame\n    filtered_df['Length'] = filtered_df['Word'].str.len()\n    \n    # Plot a histogram of the word lengths\n    ax = filtered_df['Length'].plot(kind='hist', bins=10, title=f'Word Lengths Starting with \"{letter}\"')\n    ax.set_xlabel('Length')\n    ax.set_ylabel('Frequency')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\n\n# Example usage\ndf = {'Word': ['apple', 'banana', 'cherry', 'date', 'fig', 'grape', 'avocado']}\nax = task_func(df, 'a')\n```\n"}
{"task_id": "BigCodeBench/600", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Filter the DataFrame to only include rows where the 'Word' starts with the given letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the length of each word in the filtered DataFrame\n    word_lengths = filtered_df['Word'].apply(len)\n    \n    # Calculate the mean, median, and mode of the word lengths\n    mean_length = np.mean(word_lengths)\n    median_length = np.median(word_lengths)\n    mode_length = word_lengths.mode()[0]\n    \n    # Return the calculated statistics in a dictionary\n    return {\n        'mean': mean_length,\n        'median': median_length,\n        'mode': mode_length\n    }\ndf = {'Word': ['apple', 'banana', 'apricot', 'blueberry', 'cherry', 'avocado']}", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Filter the DataFrame to only include rows where the 'Word' starts with the given letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the length of each word in the filtered DataFrame\n    word_lengths = filtered_df['Word'].apply(len)\n    \n    # Calculate the mean, median, and mode of the word lengths\n    mean_length = np.mean(word_lengths)\n    median_length = np.median(word_lengths)\n    mode_length = word_lengths.mode()[0]\n    \n    # Return the calculated statistics in a dictionary\n    return {\n        'mean': mean_length,\n        'median': median_length,\n        'mode': mode_length\n    }\n\n# Example usage:\ndf = {'Word': ['apple', 'banana', 'apricot', 'blueberry', 'cherry', 'avocado']}\nstats = task_func(df, 'a')"}
{"task_id": "BigCodeBench/601", "solution": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\ndef task_func(df, letter):\n    \"\"\"\n    DataFrame mein panktiyon ko 'Word' column mein maujood values ke starting letter ke aadhaar par filters. \n    Yeh phir in words ki lengths gnnnaa karta hai and distribution ko darshata hai.\n    In lambaiyon ka.\n\n    Parameters:\n    - df (pd.DataFrame): input DataFrame jismein 'Word' column mein string maan hain.\n    - letter (str): 'Word' column mein words ko filter karne ke liye ek chhota letter.\n\n    Returns:\n        Axes: specified letter se shuru hone waale words ki lambai ke distribution ko darshane waala ek box plot. Yadi DataFrame khaali hai or 'Word' column gum hai,\n        none lautaata hai.\n\n    Requirements:\n    - seaborn\n    - time\n\n    Example:\n    >>> import pandas as pd\n    >>> words = ['apple', 'banana', 'cherry', 'date', 'apricot', 'blueberry', 'avocado']\n    >>> df = pd.DataFrame({'Word': words})\n    >>> _ = task_func(df, 'a')\n\n    \"\"\"\n    if not isinstance(df, pd.DataFrame) or 'Word' not in df.columns:\n        return None\n\n    start_time = time.time()\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    if filtered_df.empty:\n        return None\n\n    filtered_df['Length'] = filtered_df['Word'].str.len()\n    ax = sns.boxplot(x='Length', data=filtered_df)\n    plt.title(f'Distribution of word lengths starting with \"{letter}\"')\n    plt.xlabel('Word Length')\n    plt.show()\n\n    end_time = time.time()\n    print(f\"Time taken: {end_time - start_time} seconds\")\n\n    return ax", "raw_solution": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\ndef task_func(df, letter):\n    \"\"\"\n    DataFrame mein panktiyon ko 'Word' column mein maujood values ke starting letter ke aadhaar par filters. \n    Yeh phir in words ki lengths gnnnaa karta hai and distribution ko darshata hai.\n    In lambaiyon ka.\n\n    Parameters:\n    - df (pd.DataFrame): input DataFrame jismein 'Word' column mein string maan hain.\n    - letter (str): 'Word' column mein words ko filter karne ke liye ek chhota letter.\n\n    Returns:\n        Axes: specified letter se shuru hone waale words ki lambai ke distribution ko darshane waala ek box plot. Yadi DataFrame khaali hai or 'Word' column gum hai,\n        none lautaata hai.\n\n    Requirements:\n    - seaborn\n    - time\n\n    Example:\n    >>> import pandas as pd\n    >>> words = ['apple', 'banana', 'cherry', 'date', 'apricot', 'blueberry', 'avocado']\n    >>> df = pd.DataFrame({'Word': words})\n    >>> _ = task_func(df, 'a')\n\n    \"\"\"\n    if not isinstance(df, pd.DataFrame) or 'Word' not in df.columns:\n        return None\n\n    start_time = time.time()\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    if filtered_df.empty:\n        return None\n\n    filtered_df['Length'] = filtered_df['Word'].str.len()\n    ax = sns.boxplot(x='Length', data=filtered_df)\n    plt.title(f'Distribution of word lengths starting with \"{letter}\"')\n    plt.xlabel('Word Length')\n    plt.show()\n\n    end_time = time.time()\n    print(f\"Time taken: {end_time - start_time} seconds\")\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/602", "solution": "import numpy as np\nimport pandas as pd\nimport os\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nOUTPUT_DIR = './output'\ndef task_func(file_path, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Ek CSV file banaen jismein exclusively random lowercase letters se bhara 2D matrix ho.\n\n    Parameters:\n    - file_path (str): CSV file ka path jise banaya jaana hai.\n    - output_dir (str, optional): CSV file ka dir jise banaya jaana hai.\n\n    Returns:\n        none: Nirdisht path par ek CSV file likhta hai.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    >>> task_func(os.path.join(OUTPUT_DIR, 'random_matrix.csv'))\n\n    \"\"\"\n    # Ensure output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate a random 5x5 matrix of lowercase letters\n    matrix = np.random.choice(LETTERS, size=(5, 5))\n\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(file_path, index=False)\n\n    return None", "raw_solution": "import numpy as np\nimport pandas as pd\nimport os\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nOUTPUT_DIR = './output'\n\ndef task_func(file_path, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Ek CSV file banaen jismein exclusively random lowercase letters se bhara 2D matrix ho.\n\n    Parameters:\n    - file_path (str): CSV file ka path jise banaya jaana hai.\n    - output_dir (str, optional): CSV file ka dir jise banaya jaana hai.\n\n    Returns:\n        none: Nirdisht path par ek CSV file likhta hai.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    >>> task_func(os.path.join(OUTPUT_DIR, 'random_matrix.csv'))\n\n    \"\"\"\n    # Ensure output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate a random 5x5 matrix of lowercase letters\n    matrix = np.random.choice(LETTERS, size=(5, 5))\n\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(file_path, index=False)\n\n    return None\n\n# Example usage"}
{"task_id": "BigCodeBench/603", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(matrix1, matrix2):\n    \"\"\"\n    Do 2D sankhyatmak arrays (matrices) ko second axis (kalam) ke sath jodata hai,\n    Unhen Pandas DataFrame mein badalta hai, aur DataFrame ka ek string representation lautaata hai.\n\n    Parameters:\n    - matrix1 (np.ndarray): Pahli 2D numpy array.\n    - matrix2 (np.ndarray): second 2D numpy array.\n\n    Returns:\n        str: DataFrame ka string representation bina index aur header ke.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    >>> matrix1 = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> matrix2 = np.array([[7, 8, 9], [10, 11, 12]])\n    >>> result = task_func(matrix1, matrix2)\n    >>> all(x in result.replace(' ', '') for x in ['123789', '456101112'])\n    True\n\n    \"\"\"\n    # Convert numpy arrays to pandas DataFrame\n    df = pd.DataFrame(np.column_stack((matrix1, matrix2)))\n\n    # Convert DataFrame to string representation without index and header\n    result = df.to_string(index=False, header=False)\n\n    return result\nmatrix1 = np.array([[1, 2, 3], [4, 5, 6]])\nmatrix2 = np.array([[7, 8, 9], [10, 11, 12]])", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(matrix1, matrix2):\n    \"\"\"\n    Do 2D sankhyatmak arrays (matrices) ko second axis (kalam) ke sath jodata hai,\n    Unhen Pandas DataFrame mein badalta hai, aur DataFrame ka ek string representation lautaata hai.\n\n    Parameters:\n    - matrix1 (np.ndarray): Pahli 2D numpy array.\n    - matrix2 (np.ndarray): second 2D numpy array.\n\n    Returns:\n        str: DataFrame ka string representation bina index aur header ke.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    >>> matrix1 = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> matrix2 = np.array([[7, 8, 9], [10, 11, 12]])\n    >>> result = task_func(matrix1, matrix2)\n    >>> all(x in result.replace(' ', '') for x in ['123789', '456101112'])\n    True\n\n    \"\"\"\n    # Convert numpy arrays to pandas DataFrame\n    df = pd.DataFrame(np.column_stack((matrix1, matrix2)))\n\n    # Convert DataFrame to string representation without index and header\n    result = df.to_string(index=False, header=False)\n\n    return result\n\n# Test cases\nmatrix1 = np.array([[1, 2, 3], [4, 5, 6]])\nmatrix2 = np.array([[7, 8, 9], [10, 11, 12]])\nresult = task_func(matrix1, matrix2)"}
{"task_id": "BigCodeBench/604", "solution": "import subprocess\nimport logging\ndef task_func(filepath):\n    # Set up logging\n    logging.basicConfig(filename='compilation.log', level=logging.DEBUG,\n                        format='%(asctime)s - %(levelname)s - %(message)s')\n\n    try:\n        # Compile the C++ file\n        result = subprocess.run(['g++', filepath, '-o', filepath.split('.')[0]], check=True)\n        logging.info(f\"Compilation successful for file: {filepath}\")\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"Compilation failed for file: {filepath}. Error: {e}\")\n        raise\n    except FileNotFoundError as e:\n        logging.error(f\"File not found: {filepath}. Error: {e}\")\n        raise\n\n    # Check if the executable file was created\n    if not os.path.exists(filepath.split('.')[0]):\n        logging.error(f\"Executable file not found for file: {filepath}\")\n        raise FileNotFoundError(f\"Executable file not found for file: {filepath}\")", "raw_solution": "import subprocess\nimport logging\n\ndef task_func(filepath):\n    # Set up logging\n    logging.basicConfig(filename='compilation.log', level=logging.DEBUG,\n                        format='%(asctime)s - %(levelname)s - %(message)s')\n\n    try:\n        # Compile the C++ file\n        result = subprocess.run(['g++', filepath, '-o', filepath.split('.')[0]], check=True)\n        logging.info(f\"Compilation successful for file: {filepath}\")\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"Compilation failed for file: {filepath}. Error: {e}\")\n        raise\n    except FileNotFoundError as e:\n        logging.error(f\"File not found: {filepath}. Error: {e}\")\n        raise\n\n    # Check if the executable file was created\n    if not os.path.exists(filepath.split('.')[0]):\n        logging.error(f\"Executable file not found for file: {filepath}\")\n        raise FileNotFoundError(f\"Executable file not found for file: {filepath}\")\n\n# Example usage"}
{"task_id": "BigCodeBench/605", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(matrix):\n    \"\"\"\n    matplotlib ka upayog karke 2D numeric array (matrix) ko heatmap ke roop mein kalpana karein, rang mapping ke liye ek cmap specified karna\n    and pixel rendering ko control karne ke liye interpolation.\n\n    Parameters:\n    - matrix (array): 2D numpy array.\n\n    Returns:\n        ax (matplotlib.axes._axes.Axes): 'hot' colormap ke heatmap ke saath axes object.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> import numpy as np\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> ax = task_func(matrix)\n\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    ax = df.plot(kind='heatmap', cmap='hot', interpolation='nearest')\n    plt.show()\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    \"\"\"\n    matplotlib ka upayog karke 2D numeric array (matrix) ko heatmap ke roop mein kalpana karein, rang mapping ke liye ek cmap specified karna\n    and pixel rendering ko control karne ke liye interpolation.\n\n    Parameters:\n    - matrix (array): 2D numpy array.\n\n    Returns:\n        ax (matplotlib.axes._axes.Axes): 'hot' colormap ke heatmap ke saath axes object.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> import numpy as np\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> ax = task_func(matrix)\n\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    ax = df.plot(kind='heatmap', cmap='hot', interpolation='nearest')\n    plt.show()\n    return ax\n```\n"}
{"task_id": "BigCodeBench/606", "solution": "import pandas as pd\nfrom scipy import stats\nimport numpy as np\ndef task_func(matrix):\n    \"\"\"\n    Z score ka upayog karake ek 2D numeric [array] (matrix) ko samanya karta hai.\n\n    Parameters:\n    - matrix (array): 2D numpy [array].\n\n    Returns:\n        DataFrame: samanyikrit DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy\n\n    Example:\n    >>> import numpy as np\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> normalized_df = task_func(matrix)\n    >>> isinstance(normalized_df, pd.DataFrame)\n    true\n    >>> np.allclose(normalized_df.mean(), 0)\n    true\n    >>> np.allclose(normalized_df.std(ddof=0), 1)\n    true\n\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    df_normalized = (df - df.mean()) / df.std(ddof=0)\n    return df_normalized", "raw_solution": "import pandas as pd\nfrom scipy import stats\nimport numpy as np\n\ndef task_func(matrix):\n    \"\"\"\n    Z score ka upayog karake ek 2D numeric [array] (matrix) ko samanya karta hai.\n\n    Parameters:\n    - matrix (array): 2D numpy [array].\n\n    Returns:\n        DataFrame: samanyikrit DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy\n\n    Example:\n    >>> import numpy as np\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> normalized_df = task_func(matrix)\n    >>> isinstance(normalized_df, pd.DataFrame)\n    true\n    >>> np.allclose(normalized_df.mean(), 0)\n    true\n    >>> np.allclose(normalized_df.std(ddof=0), 1)\n    true\n\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    df_normalized = (df - df.mean()) / df.std(ddof=0)\n    return df_normalized\n\n# Test cases"}
{"task_id": "BigCodeBench/607", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    # Remove rows from the dataframe based on column values\n    for tup in tuples:\n        df = df[~df.eq(tup).any(axis=1)]\n\n    # Generate random scatter plots\n    plots = []\n    for _ in range(n_plots):\n        # Randomly select two columns\n        columns = sample(COLUMNS, 2)\n        ax = df.plot.scatter(x=columns[0], y=columns[1])\n        plots.append(ax)\n        plt.close()\n\n    return df, plots", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants for column names to use in plots\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    # Remove rows from the dataframe based on column values\n    for tup in tuples:\n        df = df[~df.eq(tup).any(axis=1)]\n\n    # Generate random scatter plots\n    plots = []\n    for _ in range(n_plots):\n        # Randomly select two columns\n        columns = sample(COLUMNS, 2)\n        ax = df.plot.scatter(x=columns[0], y=columns[1])\n        plots.append(ax)\n        plt.close()\n\n    return df, plots\n```\n"}
{"task_id": "BigCodeBench/608", "solution": "import seaborn as sns\nfrom random import sample\nimport pandas as pd\nimport numpy as np\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on tuples\n    for t in tuples:\n        df = df[~df.isin(t).any(axis=1)]\n    \n    # Select n random columns\n    selected_columns = sample(COLUMNS, n_plots)\n    \n    # Create pairplots\n    plots = []\n    for col in selected_columns:\n        plot = sns.pairplot(df, hue=col)\n        plots.append(plot)\n    \n    return df, plots", "raw_solution": "import seaborn as sns\nfrom random import sample\nimport pandas as pd\nimport numpy as np\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on tuples\n    for t in tuples:\n        df = df[~df.isin(t).any(axis=1)]\n    \n    # Select n random columns\n    selected_columns = sample(COLUMNS, n_plots)\n    \n    # Create pairplots\n    plots = []\n    for col in selected_columns:\n        plot = sns.pairplot(df, hue=col)\n        plots.append(plot)\n    \n    return df, plots\n\n# Example usage"}
{"task_id": "BigCodeBench/609", "solution": "import pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom random import sample\ndef task_func(df, tuples, n_plots):\n    # Remove rows that match any tuple in the list\n    for t in tuples:\n        df = df[~df.isin(t).any(axis=1)]\n\n    # Generate random combinations of columns\n    columns = df.columns\n    plots = []\n    for _ in range(min(n_plots, len(columns))):\n        col1, col2 = sample(columns, 2)\n        plot = df.plot.scatter(x=col1, y=col2)\n        plots.append(((col1, col2), plot))\n\n    return df, plots\ndf = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])\ntuples = [(0.1, 0.2, 0.3, 0.4, 0.5)]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom random import sample\n\ndef task_func(df, tuples, n_plots):\n    # Remove rows that match any tuple in the list\n    for t in tuples:\n        df = df[~df.isin(t).any(axis=1)]\n\n    # Generate random combinations of columns\n    columns = df.columns\n    plots = []\n    for _ in range(min(n_plots, len(columns))):\n        col1, col2 = sample(columns, 2)\n        plot = df.plot.scatter(x=col1, y=col2)\n        plots.append(((col1, col2), plot))\n\n    return df, plots\n\n# Example usage:\nnp.random.seed(0)  # For reproducibility\ndf = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])\ntuples = [(0.1, 0.2, 0.3, 0.4, 0.5)]\nmodified_df, plots = task_func(df, tuples, 3)\n\n# Test cases\nassert isinstance(modified_df, pd.DataFrame), \"The function should return a DataFrame.\"\nassert all(col in modified_df.columns for col in ['A', 'B', 'C', 'D', 'E']), \"All columns should be present in the modified DataFrame.\"\nassert len(plots) <= 5, \"The number of plots should not exceed the number of columns.\"\n```\n"}
{"task_id": "BigCodeBench/610", "solution": "from random import sample\nimport seaborn as sns\nimport pandas as pd\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    '''\n    Remove rows from a dataframe based on values of multiple columns, \n    and then create n random joint plots of two columns against each other if the DataFrame is not empty.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    tuples (list): A list of tuples, where each tuple represents a row to be removed.\n    n_plots (int): The number of jointplots to be generated.\n    \n    Returns:\n    tuple: A tuple containing:\n        - DataFrame: The modified DataFrame.\n        - list: A list of generated joint plots (sns.JointGrid objects) if the DataFrame is not empty, otherwise an empty list.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - random\n    \n    Example:\n    >>> import numpy as np\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> modified_df, plots = task_func(df, tuples, 3)\n    '''\n    # Remove rows based on the provided tuples\n    for t in tuples:\n        df = df[~df.eq(t).any(axis=1)]\n    \n    plots = []\n    if not df.empty:\n        # Generate n random joint plots\n        for _ in range(n_plots):\n            # Randomly select two columns\n            columns = sample(COLUMNS, 2)\n            plot = sns.jointplot(x=columns[0], y=columns[1], data=df)\n            plots.append(plot)\n    \n    return df, plots", "raw_solution": "from random import sample\nimport seaborn as sns\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    '''\n    Remove rows from a dataframe based on values of multiple columns, \n    and then create n random joint plots of two columns against each other if the DataFrame is not empty.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    tuples (list): A list of tuples, where each tuple represents a row to be removed.\n    n_plots (int): The number of jointplots to be generated.\n    \n    Returns:\n    tuple: A tuple containing:\n        - DataFrame: The modified DataFrame.\n        - list: A list of generated joint plots (sns.JointGrid objects) if the DataFrame is not empty, otherwise an empty list.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - random\n    \n    Example:\n    >>> import numpy as np\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> modified_df, plots = task_func(df, tuples, 3)\n    '''\n    # Remove rows based on the provided tuples\n    for t in tuples:\n        df = df[~df.eq(t).any(axis=1)]\n    \n    plots = []\n    if not df.empty:\n        # Generate n random joint plots\n        for _ in range(n_plots):\n            # Randomly select two columns\n            columns = sample(COLUMNS, 2)\n            plot = sns.jointplot(x=columns[0], y=columns[1], data=df)\n            plots.append(plot)\n    \n    return df, plots\n```\n"}
{"task_id": "BigCodeBench/611", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nimport numpy as np\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on tuples\n    df = df[~df.isin(tuples).any(axis=1)]\n\n    # Select two columns randomly\n    selected_columns = sample(COLUMNS, 2)\n\n    # Generate line plots\n    plot_details = []\n    for _ in range(n_plots):\n        plt.figure()\n        plt.plot(df[selected_columns[0]], label=selected_columns[0])\n        plt.plot(df[selected_columns[1]], label=selected_columns[1])\n        plt.xlabel('Index')\n        plt.ylabel('Value')\n        plt.title(f'Line Plot of {selected_columns[0]} and {selected_columns[1]}')\n        plt.legend()\n        plt.show()\n        plot_details.append((selected_columns[0], selected_columns[1]))\n\n    return df, plot_details\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\ntuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on tuples\n    df = df[~df.isin(tuples).any(axis=1)]\n\n    # Select two columns randomly\n    selected_columns = sample(COLUMNS, 2)\n\n    # Generate line plots\n    plot_details = []\n    for _ in range(n_plots):\n        plt.figure()\n        plt.plot(df[selected_columns[0]], label=selected_columns[0])\n        plt.plot(df[selected_columns[1]], label=selected_columns[1])\n        plt.xlabel('Index')\n        plt.ylabel('Value')\n        plt.title(f'Line Plot of {selected_columns[0]} and {selected_columns[1]}')\n        plt.legend()\n        plt.show()\n        plot_details.append((selected_columns[0], selected_columns[1]))\n\n    return df, plot_details\n\n# Example usage\nimport numpy as np\n\n# Create a sample DataFrame\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\ntuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\nmodified_df, plot_details = task_func(df, tuples, 3)\n```\n"}
{"task_id": "BigCodeBench/612", "solution": "from random import choice\nimport numpy as np\nimport pandas as pd\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COSTS = [100, 200, 300, 400, 500]\ndef task_func(goals, penalties, teams=TEAMS, penalties_costs=PENALTIES_COSTS):\n    report_data = []\n\n    for team in teams:\n        team_goals = goals.get(team, 0)\n        team_penalties = penalties.get(team, 0)\n        penalty_cost = choice(penalties_costs)\n        performance_score = team_goals - team_penalties\n\n        report_data.append({\n            'team': team,\n            'goals': team_goals,\n            'penalties': team_penalties,\n            'penalties cost': penalty_cost,\n            'performance score': performance_score\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df\ngoals = {'Team A': 3, 'Team B': 2}\npenalties = {'Team A': 1, 'Team B': 0}", "raw_solution": "from random import choice\nimport numpy as np\nimport pandas as pd\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COSTS = [100, 200, 300, 400, 500]\n\ndef task_func(goals, penalties, teams=TEAMS, penalties_costs=PENALTIES_COSTS):\n    report_data = []\n\n    for team in teams:\n        team_goals = goals.get(team, 0)\n        team_penalties = penalties.get(team, 0)\n        penalty_cost = choice(penalties_costs)\n        performance_score = team_goals - team_penalties\n\n        report_data.append({\n            'team': team,\n            'goals': team_goals,\n            'penalties': team_penalties,\n            'penalties cost': penalty_cost,\n            'performance score': performance_score\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df\n\n# Example usage:\ngoals = {'Team A': 3, 'Team B': 2}\npenalties = {'Team A': 1, 'Team B': 0}\nreport = task_func(goals, penalties)"}
{"task_id": "BigCodeBench/613", "solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\ndef task_func(goals, penalties):\n    # Calculate the net score for each team\n    scores = {team: max(GOALS_RANGE[0], min(GOALS_RANGE[1], goals[team] - penalties[team])) for team in TEAMS}\n\n    # Create a DataFrame from the scores\n    df = pd.DataFrame(list(scores.items()), columns=['Team', 'Score'])\n\n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Team'], df['Score'], color='skyblue')\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Net Score for Each Team')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return df\ngoals = {'Team A': 5, 'Team B': 3, 'Team C': 1, 'Team D': 0, 'Team E': 4}\npenalties = {'Team A': 1, 'Team B': 1, 'Team C': 1, 'Team D': 0, 'Team E': 2}", "raw_solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\n\n\ndef task_func(goals, penalties):\n    # Calculate the net score for each team\n    scores = {team: max(GOALS_RANGE[0], min(GOALS_RANGE[1], goals[team] - penalties[team])) for team in TEAMS}\n\n    # Create a DataFrame from the scores\n    df = pd.DataFrame(list(scores.items()), columns=['Team', 'Score'])\n\n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Team'], df['Score'], color='skyblue')\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Net Score for Each Team')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return df\n\n# Example usage\ngoals = {'Team A': 5, 'Team B': 3, 'Team C': 1, 'Team D': 0, 'Team E': 4}\npenalties = {'Team A': 1, 'Team B': 1, 'Team C': 1, 'Team D': 0, 'Team E': 2}\ndf = task_func(goals, penalties)"}
{"task_id": "BigCodeBench/614", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(goals, penalties):\n    # Create a DataFrame from the input dictionaries\n    df = pd.DataFrame(list(goals.items()), columns=['Team', 'Goals'])\n    df['Penalties'] = df['Team'].map(penalties)\n\n    # Create a pairplot using seaborn\n    sns.set(style=\"whitegrid\")\n    pairplot = sns.pairplot(df, hue='Goals', palette='Set2', diag_kind='kde')\n\n    # Return the DataFrame and the pairplot\n    return df, pairplot\ngoals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\npenalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(goals, penalties):\n    # Create a DataFrame from the input dictionaries\n    df = pd.DataFrame(list(goals.items()), columns=['Team', 'Goals'])\n    df['Penalties'] = df['Team'].map(penalties)\n\n    # Create a pairplot using seaborn\n    sns.set(style=\"whitegrid\")\n    pairplot = sns.pairplot(df, hue='Goals', palette='Set2', diag_kind='kde')\n\n    # Return the DataFrame and the pairplot\n    return df, pairplot\n\n# Example usage:\ngoals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\npenalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\ndf, plot = task_func(goals, penalties)"}
{"task_id": "BigCodeBench/615", "solution": "from random import randint, seed\nimport pandas as pd\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for i in range(5):  # Assuming we have 5 teams for this example\n        team_goals = randint(0, goals)\n        penalty_fines = randint(0, penalties) * 1000\n        data.append({'team': f'Team {i+1}', 'match result': (team_goals, penalty_fines)})\n\n    df = pd.DataFrame(data, columns=['team', 'match result'])\n    return df", "raw_solution": "from random import randint, seed\nimport pandas as pd\n\n# Method\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for i in range(5):  # Assuming we have 5 teams for this example\n        team_goals = randint(0, goals)\n        penalty_fines = randint(0, penalties) * 1000\n        data.append({'team': f'Team {i+1}', 'match result': (team_goals, penalty_fines)})\n\n    df = pd.DataFrame(data, columns=['team', 'match result'])\n    return df\n\n# Example usage:\nseed(42)  # Setting the seed for reproducibility\nresults = task_func(5, 3, 42)"}
{"task_id": "BigCodeBench/616", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalty_cost = team_goals * penalty_cost\n        data.append([team, team_goals, team_penalty_cost])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    ax = df.plot(kind='bar', x='Team', y=['Goals', 'Penalty Cost'], figsize=(10, 6))\n    plt.title('Football Match Results')\n    plt.xlabel('Teams')\n    plt.ylabel('Goals and Penalty Costs')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return df, ax", "raw_solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants (they can be overridden with default parameters)\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalty_cost = team_goals * penalty_cost\n        data.append([team, team_goals, team_penalty_cost])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    ax = df.plot(kind='bar', x='Team', y=['Goals', 'Penalty Cost'], figsize=(10, 6))\n    plt.title('Football Match Results')\n    plt.xlabel('Teams')\n    plt.ylabel('Goals and Penalty Costs')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/617", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, rng_seed=None, teams=TEAMS):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalty_cost = randint(0, penalties) * PENALTY_COST\n        data.append([team, team_goals, team_penalty_cost])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Team'], df['Goals'], label='Goals', color='blue')\n    plt.bar(df['Team'], df['Penalty Cost'], label='Penalty Cost', color='red', bottom=df['Goals'])\n    plt.xlabel('Teams')\n    plt.ylabel('Goals and Penalty Cost')\n    plt.title('Team Performance in the Match')\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    return df", "raw_solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, rng_seed=None, teams=TEAMS):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalty_cost = randint(0, penalties) * PENALTY_COST\n        data.append([team, team_goals, team_penalty_cost])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Team'], df['Goals'], label='Goals', color='blue')\n    plt.bar(df['Team'], df['Penalty Cost'], label='Penalty Cost', color='red', bottom=df['Goals'])\n    plt.xlabel('Teams')\n    plt.ylabel('Goals and Penalty Cost')\n    plt.title('Team Performance in the Match')\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    return df\n\n# Example usage:\n# analyzed_data = task_func(5, 3, rng_seed=42)\n# print(analyzed_data[['Team', 'Goals', 'Penalty Cost']])\n```\nThis script generates a DataFrame with random goals and penalty costs for each team, then plots a bar chart showing the total goals and penalty costs for each team. The `task_func` function is self-contained and includes all necessary imports and constants.\n"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    data = []\n    for team in TEAMS:\n        for _ in range(randint(1, goals)):\n            penalty = randint(0, penalties)\n            penalty_cost = penalty * PENALTY_COST\n            data.append({'Team': team, 'Goals': 1, 'Penalty': penalty, 'Penalty Cost': penalty_cost})\n\n    df = pd.DataFrame(data)\n\n    # Plotting\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n    sns.countplot(x='Team', data=df, ax=ax[0])\n    ax[0].set_title('Goals per Team')\n    ax[0].set_xlabel('Team')\n    ax[0].set_ylabel('Goals')\n\n    sns.boxplot(x='Team', y='Penalty Cost', data=df, ax=ax[1])\n    ax[1].set_title('Penalty Cost per Team')\n    ax[1].set_xlabel('Team')\n    ax[1].set_ylabel('Penalty Cost')\n\n    plt.tight_layout()\n\n    return df, [ax[0], ax[1]]", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    data = []\n    for team in TEAMS:\n        for _ in range(randint(1, goals)):\n            penalty = randint(0, penalties)\n            penalty_cost = penalty * PENALTY_COST\n            data.append({'Team': team, 'Goals': 1, 'Penalty': penalty, 'Penalty Cost': penalty_cost})\n\n    df = pd.DataFrame(data)\n\n    # Plotting\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n    sns.countplot(x='Team', data=df, ax=ax[0])\n    ax[0].set_title('Goals per Team')\n    ax[0].set_xlabel('Team')\n    ax[0].set_ylabel('Goals')\n\n    sns.boxplot(x='Team', y='Penalty Cost', data=df, ax=ax[1])\n    ax[1].set_title('Penalty Cost per Team')\n    ax[1].set_xlabel('Team')\n    ax[1].set_ylabel('Penalty Cost')\n\n    plt.tight_layout()\n\n    return df, [ax[0], ax[1]]\n```\n"}
{"task_id": "BigCodeBench/619", "solution": "from random import randint, seed\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in TEAMS:\n        number = randint(0, goals)\n        penalty = randint(0, penalties)\n        data.append([team, number, penalty])\n\n    df = pd.DataFrame(data, columns=['team', 'number', 'penalty laagat'])\n\n    X = df[['number']].values\n    y = df['penalty laagat'].values\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return df, model", "raw_solution": "from random import randint, seed\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in TEAMS:\n        number = randint(0, goals)\n        penalty = randint(0, penalties)\n        data.append([team, number, penalty])\n\n    df = pd.DataFrame(data, columns=['team', 'number', 'penalty laagat'])\n\n    X = df[['number']].values\n    y = df['penalty laagat'].values\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return df, model\n```\n"}
{"task_id": "BigCodeBench/620", "solution": "import numpy as np\nimport pandas as pd\nRANGE = (1, 100)\ndef task_func(L):\n    '''\n    Generates a DataFrame filled with random integers. The dimensions of the DataFrame (number of rows and columns)\n    are determined by multiplying pairs of integers from nested lists within the input list of lists 'L'.\n    \n    Requirements:\n    - numpy\n    - pandas\n\n    Parameters:\n    L (list of lists): A list of lists where each sublist contains two integers.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n    \n    Example:\n    >>> df = task_func([[2, 3], [5, 6]])\n    >>> type(df)\n    <class 'pandas.core.frame.DataFrame'>\n    '''\n    # Initialize an empty list to store data\n    data = []\n\n    # Iterate over each pair of integers in the input list\n    for pair in L:\n        # Generate a random 2D array with shape determined by the pair\n        array = np.random.randint(RANGE[0], RANGE[1], size=(pair[0], pair[1]))\n        # Append the array to the data list\n        data.append(array)\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\n# Constants\nRANGE = (1, 100)\n\ndef task_func(L):\n    '''\n    Generates a DataFrame filled with random integers. The dimensions of the DataFrame (number of rows and columns)\n    are determined by multiplying pairs of integers from nested lists within the input list of lists 'L'.\n    \n    Requirements:\n    - numpy\n    - pandas\n\n    Parameters:\n    L (list of lists): A list of lists where each sublist contains two integers.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n    \n    Example:\n    >>> df = task_func([[2, 3], [5, 6]])\n    >>> type(df)\n    <class 'pandas.core.frame.DataFrame'>\n    '''\n    # Initialize an empty list to store data\n    data = []\n\n    # Iterate over each pair of integers in the input list\n    for pair in L:\n        # Generate a random 2D array with shape determined by the pair\n        array = np.random.randint(RANGE[0], RANGE[1], size=(pair[0], pair[1]))\n        # Append the array to the data list\n        data.append(array)\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/621", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Flatten the list of lists into a single list of integers\n    flat_list = list(chain(*L))\n    \n    # Convert the list to a numpy array\n    array = np.array(flat_list)\n    \n    # Standardize the array\n    scaler = StandardScaler()\n    standardized_array = scaler.fit_transform(array.reshape(-1, 1)).flatten()\n    \n    # Plot the standardized values\n    fig, ax = plt.subplots()\n    ax.plot(standardized_array, marker='o')\n    ax.set_title('Standardized Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n    \n    return ax", "raw_solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    # Flatten the list of lists into a single list of integers\n    flat_list = list(chain(*L))\n    \n    # Convert the list to a numpy array\n    array = np.array(flat_list)\n    \n    # Standardize the array\n    scaler = StandardScaler()\n    standardized_array = scaler.fit_transform(array.reshape(-1, 1)).flatten()\n    \n    # Plot the standardized values\n    fig, ax = plt.subplots()\n    ax.plot(standardized_array, marker='o')\n    ax.set_title('Standardized Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n    \n    return ax\n\n# Example usage:\n# ax = task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/622", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import chain\ndef task_func(L):\n    # Flatten the list of lists into a single list of integers\n    flat_list = list(chain.from_iterable(L))\n    \n    # Convert the list to a numpy array\n    data = np.array(flat_list)\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=20, density=True, alpha=0.6, color='g')\n    \n    # Plot the normal distribution overlay\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import chain\n\ndef task_func(L):\n    # Flatten the list of lists into a single list of integers\n    flat_list = list(chain.from_iterable(L))\n    \n    # Convert the list to a numpy array\n    data = np.array(flat_list)\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=20, density=True, alpha=0.6, color='g')\n    \n    # Plot the normal distribution overlay\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    return ax\n\n# Example usage:\n# ax = task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/623", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Flatten the list of lists into a single list\n    flat_list = list(chain(*L))\n    \n    # Convert the list to a numpy array\n    data = np.array(flat_list).reshape(-1, 1)\n    \n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    kmeans.fit(data)\n    \n    # Get the cluster centers and labels\n    centers = kmeans.cluster_centers_\n    labels = kmeans.labels_\n    \n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data, np.zeros_like(data), c=labels, cmap='viridis')\n    \n    # Plot the cluster centers\n    ax.scatter(centers, np.zeros_like(centers), c='red', marker='x')\n    \n    # Add a color bar\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    \n    return ax", "raw_solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    # Flatten the list of lists into a single list\n    flat_list = list(chain(*L))\n    \n    # Convert the list to a numpy array\n    data = np.array(flat_list).reshape(-1, 1)\n    \n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    kmeans.fit(data)\n    \n    # Get the cluster centers and labels\n    centers = kmeans.cluster_centers_\n    labels = kmeans.labels_\n    \n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data, np.zeros_like(data), c=labels, cmap='viridis')\n    \n    # Plot the cluster centers\n    ax.scatter(centers, np.zeros_like(centers), c='red', marker='x')\n    \n    # Add a color bar\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    \n    return ax\n\n# Example usage:\n# ax = task_func([[1, 2, 3], [50, 60, 70], [100, 110, 120]])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/624", "solution": "from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\nN_COMPONENTS = 2\ndef task_func(L):\n    \"\"\"\n     'L' ki list ko 2D numeric array mein badlein, us par PCA lagu karein and PCA result and scatter plot lautaen.\n\n    Parameters:\n    - L (list of lists): ek list ki list jahan pratyek sublist mein integers hote hain.\n\n    Returns:\n        tuple: PCA result (numpy array) and scatter plot (matplotlib Axes object) yukt ek tuple.\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> pca_result, plot = task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> type(pca_result)\n    <class 'numpy.ndarray'>\n\n    \"\"\"\n    # Convert list to numpy array\n    data = np.array(L)\n\n    # Apply PCA\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(data)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_title('PCA Result')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return pca_result, ax", "raw_solution": "from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nN_COMPONENTS = 2\n\n\ndef task_func(L):\n    \"\"\"\n     'L' ki list ko 2D numeric array mein badlein, us par PCA lagu karein and PCA result and scatter plot lautaen.\n\n    Parameters:\n    - L (list of lists): ek list ki list jahan pratyek sublist mein integers hote hain.\n\n    Returns:\n        tuple: PCA result (numpy array) and scatter plot (matplotlib Axes object) yukt ek tuple.\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> pca_result, plot = task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> type(pca_result)\n    <class 'numpy.ndarray'>\n\n    \"\"\"\n    # Convert list to numpy array\n    data = np.array(L)\n\n    # Apply PCA\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(data)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_title('PCA Result')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return pca_result, ax\n```\n"}
{"task_id": "BigCodeBench/625", "solution": "import math\nfrom random import randint\nimport pandas as pd\ndef task_func(cities_list):\n    data = {'City': cities_list, 'Population': []}\n    for _ in range(len(cities_list)):\n        # Randomly generate population data\n        population = randint(1, 1000000)\n        data['Population'].append(population)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data)\n    \n    return df", "raw_solution": "import math\nfrom random import randint\nimport pandas as pd\n\ndef task_func(cities_list):\n    data = {'City': cities_list, 'Population': []}\n    for _ in range(len(cities_list)):\n        # Randomly generate population data\n        population = randint(1, 1000000)\n        data['Population'].append(population)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data)\n    \n    return df\n\n# Example usage:\ncities = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\npop_data = task_func(cities)"}
{"task_id": "BigCodeBench/626", "solution": "from random import choice\nimport pytz\nfrom dateutil.parser import parse\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\ndef task_func(date_str, from_tz):\n    # Parse the input date string\n    naive_dt = parse(date_str)\n    \n    # Get the source timezone\n    from_timezone = pytz.timezone(from_tz)\n    aware_dt_from = from_timezone.localize(naive_dt)\n    \n    # Choose a random timezone\n    to_tz = choice(TIMEZONES)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Convert the datetime to the new timezone\n    aware_dt_to = aware_dt_from.astimezone(to_timezone)\n    \n    # Return the converted datetime and the new timezone\n    return aware_dt_to.strftime('%Y-%m-%d %H:%M:%S'), to_tz", "raw_solution": "from random import choice\nimport pytz\nfrom dateutil.parser import parse\n\n# Constants\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef task_func(date_str, from_tz):\n    # Parse the input date string\n    naive_dt = parse(date_str)\n    \n    # Get the source timezone\n    from_timezone = pytz.timezone(from_tz)\n    aware_dt_from = from_timezone.localize(naive_dt)\n    \n    # Choose a random timezone\n    to_tz = choice(TIMEZONES)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Convert the datetime to the new timezone\n    aware_dt_to = aware_dt_from.astimezone(to_timezone)\n    \n    # Return the converted datetime and the new timezone\n    return aware_dt_to.strftime('%Y-%m-%d %H:%M:%S'), to_tz\n```\n"}
{"task_id": "BigCodeBench/627", "solution": "from random import randint\nfrom statistics import mean\nimport pandas as pd\ndef task_func(products_list):\n    sales_data = []\n\n    for product in products_list:\n        monthly_sales = [randint(50, 500) for _ in range(12)]\n        avg_sales = mean(monthly_sales)\n        sales_data.append([product] + monthly_sales + [avg_sales])\n\n    columns = ['product'] + [f'month {i+1}' for i in range(12)] + ['average sales']\n    df = pd.DataFrame(sales_data, columns=columns)\n\n    return df\nproducts = ['Apples', 'Bananas', 'Grapes', 'Oranges', 'Pineapples']", "raw_solution": "from random import randint\nfrom statistics import mean\nimport pandas as pd\n\ndef task_func(products_list):\n    sales_data = []\n\n    for product in products_list:\n        monthly_sales = [randint(50, 500) for _ in range(12)]\n        avg_sales = mean(monthly_sales)\n        sales_data.append([product] + monthly_sales + [avg_sales])\n\n    columns = ['product'] + [f'month {i+1}' for i in range(12)] + ['average sales']\n    df = pd.DataFrame(sales_data, columns=columns)\n\n    return df\n\n# Example usage:\nproducts = ['Apples', 'Bananas', 'Grapes', 'Oranges', 'Pineapples']\nsales_data = task_func(products)"}
{"task_id": "BigCodeBench/628", "solution": "import math\nfrom random import randint\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate random frequency, amplitude, and phase shift\n    frequency = randint(1, 5)\n    amplitude = randint(1, 5)\n    phase_shift = randint(0, 2 * math.pi)\n\n    # Generate time values\n    time = [i * 0.1 for i in range(100)]\n\n    # Generate sine wave\n    sine_wave = [amplitude * math.sin(2 * math.pi * frequency * t + phase_shift) for t in time]\n\n    # Plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(time, sine_wave)\n    ax.set_title('Random Sine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n\n    return ax", "raw_solution": "import math\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    # Generate random frequency, amplitude, and phase shift\n    frequency = randint(1, 5)\n    amplitude = randint(1, 5)\n    phase_shift = randint(0, 2 * math.pi)\n\n    # Generate time values\n    time = [i * 0.1 for i in range(100)]\n\n    # Generate sine wave\n    sine_wave = [amplitude * math.sin(2 * math.pi * frequency * t + phase_shift) for t in time]\n\n    # Plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(time, sine_wave)\n    ax.set_title('Random Sine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/629", "solution": "import os\nimport time\nimport pandas as pd\nOUTPUT_DIR = './output'\ndef task_func(dataset, filename, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filepath = os.path.join(output_dir, filename)\n    with open(filepath, 'w') as file:\n        for df in dataset:\n            file.write(df.to_csv(index=False))\n            file.write(\"\\n\" + \"-\" * 10 + \"\\n\")\n\n    print(f\"DataFrames have been written to {filepath}\")", "raw_solution": "import os\nimport time\nimport pandas as pd\n\nOUTPUT_DIR = './output'\n\ndef task_func(dataset, filename, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filepath = os.path.join(output_dir, filename)\n    with open(filepath, 'w') as file:\n        for df in dataset:\n            file.write(df.to_csv(index=False))\n            file.write(\"\\n\" + \"-\" * 10 + \"\\n\")\n\n    print(f\"DataFrames have been written to {filepath}\")\n\n# Example usage:"}
{"task_id": "BigCodeBench/630", "solution": "import pandas as pd\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Ek Pandas df ko ek specified directory mein ek JSON file mein sahejeyn.\n\n    Notes:\n        Yeh function ek Pandas df mein herpher karta hai and isey JSON file ke roop mein sahejta hai.\n\n    Parameters:\n    - df (df): Ek Pandas df jisey sahejja jaana hai.\n    - filename (str): JSON file ka filename jahaan df sahejja jaayega.\n    - output_dir (str, optional): Output directory.\n\n    Returns:\n        str: Vah full file path jahaan df sahejja gaya hai.\n\n    Requirements:\n    - os\n    - pandas\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> task_func(df, 'data.json')\n    './output/data.json'\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, filename)\n    df.to_json(file_path, orient='records', lines=True)\n\n    return file_path", "raw_solution": "import pandas as pd\nimport os\nOUTPUT_DIR = './output'\n\n\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Ek Pandas df ko ek specified directory mein ek JSON file mein sahejeyn.\n\n    Notes:\n        Yeh function ek Pandas df mein herpher karta hai and isey JSON file ke roop mein sahejta hai.\n\n    Parameters:\n    - df (df): Ek Pandas df jisey sahejja jaana hai.\n    - filename (str): JSON file ka filename jahaan df sahejja jaayega.\n    - output_dir (str, optional): Output directory.\n\n    Returns:\n        str: Vah full file path jahaan df sahejja gaya hai.\n\n    Requirements:\n    - os\n    - pandas\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> task_func(df, 'data.json')\n    './output/data.json'\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, filename)\n    df.to_json(file_path, orient='records', lines=True)\n\n    return file_path\n```\n"}
{"task_id": "BigCodeBench/631", "solution": "import csv\nimport os\nimport pandas as pd\nOUTPUT_DIR = './output'\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, filename)\n    \n    df.to_csv(file_path, index=False)\n    \n    return os.path.abspath(file_path)", "raw_solution": "import csv\nimport os\nimport pandas as pd\n\nOUTPUT_DIR = './output'\n\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, filename)\n    \n    df.to_csv(file_path, index=False)\n    \n    return os.path.abspath(file_path)\n\n# Example usage:"}
{"task_id": "BigCodeBench/632", "solution": "import pandas as pd\nimport time\nOUTPUT_DIR = './output'\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    \"\"\"\n    Pandas df ko JSON Lines file mein likhen aur ise ek *specified* *directory* mein save karen.\n\n    Parameters:\n    - df (pd.df): ek Pandas df jise save karna hai.\n    - filename (str): JSON Lines file ka filename jise save karna hai.\n\n    Returns:\n        str: vah *full* *path* jahan JSON Lines file ko save kiya gaya tha.\n\n    Requirements:\n    - pandas\n    - time\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> 'data.jsonl' in task_func(df, 'data.jsonl')\n    true\n\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # Convert DataFrame to JSON Lines format\n    jsonl_data = df.to_json(orient='records', lines=True)\n\n    # Define the full path to save the file\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Save the JSON Lines data to the specified file\n    with open(file_path, 'w') as file:\n        file.write(jsonl_data)\n\n    # Return the full path of the saved file\n    return file_path", "raw_solution": "import pandas as pd\nimport time\nOUTPUT_DIR = './output'\n\n\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    \"\"\"\n    Pandas df ko JSON Lines file mein likhen aur ise ek *specified* *directory* mein save karen.\n\n    Parameters:\n    - df (pd.df): ek Pandas df jise save karna hai.\n    - filename (str): JSON Lines file ka filename jise save karna hai.\n\n    Returns:\n        str: vah *full* *path* jahan JSON Lines file ko save kiya gaya tha.\n\n    Requirements:\n    - pandas\n    - time\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> 'data.jsonl' in task_func(df, 'data.jsonl')\n    true\n\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # Convert DataFrame to JSON Lines format\n    jsonl_data = df.to_json(orient='records', lines=True)\n\n    # Define the full path to save the file\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Save the JSON Lines data to the specified file\n    with open(file_path, 'w') as file:\n        file.write(jsonl_data)\n\n    # Return the full path of the saved file\n    return file_path\n```\n"}
{"task_id": "BigCodeBench/633", "solution": "import re\nfrom nltk.corpus import stopwords\ndef task_func(text: str) -> dict:\n    # Ensure stopwords are downloaded\n    nltk.download('stopwords')\n    \n    # Define a set of common English stopwords\n    stop_words = set(stopwords.words('english'))\n    \n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Filter out stopwords\n    filtered_words = [word for word in words if word not in stop_words]\n    \n    # Calculate frequency distribution\n    frequency = {}\n    for word in filtered_words:\n        if word in frequency:\n            frequency[word] += 1\n        else:\n            frequency[word] = 1\n    \n    return frequency", "raw_solution": "import re\nfrom nltk.corpus import stopwords\n\ndef task_func(text: str) -> dict:\n    # Ensure stopwords are downloaded\n    nltk.download('stopwords')\n    \n    # Define a set of common English stopwords\n    stop_words = set(stopwords.words('english'))\n    \n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Filter out stopwords\n    filtered_words = [word for word in words if word not in stop_words]\n    \n    # Calculate frequency distribution\n    frequency = {}\n    for word in filtered_words:\n        if word in frequency:\n            frequency[word] += 1\n        else:\n            frequency[word] = 1\n    \n    return frequency\n\n# Example usage:\n# print(task_func(\"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"))\n```\n"}
{"task_id": "BigCodeBench/634", "solution": "import itertools\nfrom typing import Any\nfrom scipy import stats\ndef task_func(input_list: list, repetitions: int) -> Any:\n    \"\"\"\n    Ek original list ke number-number repetitions ke saath elements ki ek list ke mode ki ganana karem.\n    Functionality:\n    - Ek list and repetitions count input ke rup mem leta hai.\n    - Multiple repetitions ke saath list ko samatal karta hai.\n    - Samatal list ke mode ki ganana karta hai.\n    >>> task_func([1, 2, 3], 5)\n    ModeResult(mode=array([1]), count=array([5]))\n\n    Parameters:\n    - input_list (list): Ek list jismem tatva samil haim (ki c bhi hashable type ka ho sakta hai).\n    - repetitions (int): Original list ko kitni number repetition jana chahie.\n\n    Returns:\n        scipy.stats.ModeResult: Ek object jismem samatal list mem sabse most number ane vale tatva/elements ka mode and count samil hai.\n\n    Requirements:\n    - typing\n    - itertools\n    - scipy\n\n    Example:\n    Examples:\n    >>> task_func(['A', 'B', 'C'], 10)\n    ModeResult(mode=array(['A'], dtype='<U1'), count=array([10]))\n\n    \"\"\"\n    # Create a list with repetitions\n    repeated_list = list(itertools.chain.from_iterable([input_list] * repetitions))\n    \n    # Calculate the mode of the repeated list\n    mode_result = stats.mode(repeated_list)\n    \n    return mode_result", "raw_solution": "import itertools\nfrom typing import Any\nfrom scipy import stats\n\ndef task_func(input_list: list, repetitions: int) -> Any:\n    \"\"\"\n    Ek original list ke number-number repetitions ke saath elements ki ek list ke mode ki ganana karem.\n    Functionality:\n    - Ek list and repetitions count input ke rup mem leta hai.\n    - Multiple repetitions ke saath list ko samatal karta hai.\n    - Samatal list ke mode ki ganana karta hai.\n    >>> task_func([1, 2, 3], 5)\n    ModeResult(mode=array([1]), count=array([5]))\n\n    Parameters:\n    - input_list (list): Ek list jismem tatva samil haim (ki c bhi hashable type ka ho sakta hai).\n    - repetitions (int): Original list ko kitni number repetition jana chahie.\n\n    Returns:\n        scipy.stats.ModeResult: Ek object jismem samatal list mem sabse most number ane vale tatva/elements ka mode and count samil hai.\n\n    Requirements:\n    - typing\n    - itertools\n    - scipy\n\n    Example:\n    Examples:\n    >>> task_func(['A', 'B', 'C'], 10)\n    ModeResult(mode=array(['A'], dtype='<U1'), count=array([10]))\n\n    \"\"\"\n    # Create a list with repetitions\n    repeated_list = list(itertools.chain.from_iterable([input_list] * repetitions))\n    \n    # Calculate the mode of the repeated list\n    mode_result = stats.mode(repeated_list)\n    \n    return mode_result\n```\n"}
{"task_id": "BigCodeBench/635", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\ndef task_func(text, n=2):\n    # Removing consecutive duplicates\n    text = re.sub(r'\\b(\\w+)\\s\\1\\b', r'\\1', text)\n\n    # Removing stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in text.split() if word.lower() not in stop_words]\n\n    # Creating a count vectorizer\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    matrix = vectorizer.fit_transform([' '.join(words)])\n\n    # Creating a DataFrame from the matrix\n    df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    # Plotting the DataFrame\n    ax = df.plot(kind='bar', figsize=(10, 5))\n    plt.title('Word Co-occurrence Matrix')\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n\n    return df, ax", "raw_solution": "# Importing the required libraries\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\n# Downloading the stopwords from nltk\nimport nltk\nnltk.download('stopwords')\n\ndef task_func(text, n=2):\n    # Removing consecutive duplicates\n    text = re.sub(r'\\b(\\w+)\\s\\1\\b', r'\\1', text)\n\n    # Removing stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in text.split() if word.lower() not in stop_words]\n\n    # Creating a count vectorizer\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    matrix = vectorizer.fit_transform([' '.join(words)])\n\n    # Creating a DataFrame from the matrix\n    df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    # Plotting the DataFrame\n    ax = df.plot(kind='bar', figsize=(10, 5))\n    plt.title('Word Co-occurrence Matrix')\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Generate random integer values between 0 and 9 for the specified number of rows\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.apply(lambda x: x[x != 0].count(), axis=0)\n\n    # Create a bar plot of non-zero value counts\n    ax = non_zero_counts.plot(kind='bar', color='skyblue')\n    ax.set_title('Non-Zero Value Counts')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count')\n\n    return df, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Generate random integer values between 0 and 9 for the specified number of rows\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.apply(lambda x: x[x != 0].count(), axis=0)\n\n    # Create a bar plot of non-zero value counts\n    ax = non_zero_counts.plot(kind='bar', color='skyblue')\n    ax.set_title('Non-Zero Value Counts')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n    # Constants\n    COURSES = ['Math', 'Science', 'English', 'History', 'Art']\n    GRADES = range(0, 101)\n\n    # Generate random student data\n    students = sample(range(1, num_students + 1), num_students)\n    data = {course: np.random.choice(GRADES, num_students) for course in COURSES}\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=students)\n\n    # Calculate average and count passing grades\n    averages = df.mean()\n    passing_counts = df.apply(lambda x: (x >= 60).sum(), axis=0)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    averages.plot(kind='bar', ax=ax, color='skyblue', label='Average Grade')\n    passing_counts.plot(kind='bar', ax=ax, color='lightgreen', label='Passing Grade Count')\n\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Grades')\n    ax.legend()\n\n    return df, ax", "raw_solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students):\n    # Constants\n    COURSES = ['Math', 'Science', 'English', 'History', 'Art']\n    GRADES = range(0, 101)\n\n    # Generate random student data\n    students = sample(range(1, num_students + 1), num_students)\n    data = {course: np.random.choice(GRADES, num_students) for course in COURSES}\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=students)\n\n    # Calculate average and count passing grades\n    averages = df.mean()\n    passing_counts = df.apply(lambda x: (x >= 60).sum(), axis=0)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    averages.plot(kind='bar', ax=ax, color='skyblue', label='Average Grade')\n    passing_counts.plot(kind='bar', ax=ax, color='lightgreen', label='Passing Grade Count')\n\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Grades')\n    ax.legend()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/638", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(num_teams=5, num_games=100):\n    # Generate random scores for each team in each game\n    scores = np.random.randint(0, 101, size=(num_teams, num_games))\n    \n    # Create a DataFrame with team names as columns and game numbers as rows\n    teams = [f'Team{i+1}' for i in range(num_teams)]\n    games = [f'Game{i+1}' for i in range(num_games)]\n    df = pd.DataFrame(scores, index=teams, columns=games)\n    \n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(num_teams=5, num_games=100):\n    # Generate random scores for each team in each game\n    scores = np.random.randint(0, 101, size=(num_teams, num_games))\n    \n    # Create a DataFrame with team names as columns and game numbers as rows\n    teams = [f'Team{i+1}' for i in range(num_teams)]\n    games = [f'Game{i+1}' for i in range(num_games)]\n    df = pd.DataFrame(scores, index=teams, columns=games)\n    \n    return df\n\n# Example usage:\n# df = task_func(num_teams=3, num_games=10)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/639", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(num_samples=100, num_features=5):\n    # Generate random data\n    data = np.random.rand(num_samples, num_features)\n    feature_names = [f'Feature_{i+1}' for i in range(num_features)]\n    df = pd.DataFrame(data, columns=feature_names)\n\n    # Calculate correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    ax.set_title('Correlation Matrix Heatmap')\n\n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(num_samples=100, num_features=5):\n    # Generate random data\n    data = np.random.rand(num_samples, num_features)\n    feature_names = [f'Feature_{i+1}' for i in range(num_features)]\n    df = pd.DataFrame(data, columns=feature_names)\n\n    # Calculate correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    ax.set_title('Correlation Matrix Heatmap')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/640", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\ndef task_func():\n    # Generate random sales data\n    sales_data = np.random.randint(100, 1000, size=(len(MONTHS), len(PRODUCTS)))\n    df = pd.DataFrame(sales_data, index=MONTHS, columns=PRODUCTS)\n\n    # Plot total sales for each product\n    plt.figure(figsize=(10, 6))\n    for product in PRODUCTS:\n        plt.plot(MONTHS, df[product], label=product)\n    plt.xlabel('Months')\n    plt.ylabel('Sales')\n    plt.title('Total Sales for Each Product')\n    plt.legend()\n    plt.show()\n\n    # Plot heatmap of sales data\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(df, annot=True, fmt='d', cmap='YlGnBu', cbar=True)\n    plt.title('Sales Data Heatmap')\n    plt.show()\n\n    return df", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\n\ndef task_func():\n    # Generate random sales data\n    sales_data = np.random.randint(100, 1000, size=(len(MONTHS), len(PRODUCTS)))\n    df = pd.DataFrame(sales_data, index=MONTHS, columns=PRODUCTS)\n\n    # Plot total sales for each product\n    plt.figure(figsize=(10, 6))\n    for product in PRODUCTS:\n        plt.plot(MONTHS, df[product], label=product)\n    plt.xlabel('Months')\n    plt.ylabel('Sales')\n    plt.title('Total Sales for Each Product')\n    plt.legend()\n    plt.show()\n\n    # Plot heatmap of sales data\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(df, annot=True, fmt='d', cmap='YlGnBu', cbar=True)\n    plt.title('Sales Data Heatmap')\n    plt.show()\n\n    return df\n```\nThis script generates a DataFrame with random sales data for each product and each month. It then plots a line graph of total sales for each product and a heatmap of the sales data. The DataFrame is returned at the end of the function.\n"}
{"task_id": "BigCodeBench/641", "solution": "import os\nimport re\nimport pandas as pd\ndef task_func(pattern: str, directory: str, output_csv: str) -> pd.DataFrame:\n    \"\"\"\n    Dee gayee directory mein files kee searches jo ek diye gaye regex pattern se mel khaatee hain. \n    Yaha function directory mein ghoomta hai, filenames ko pattern ke saath mel khaata hai, \n    and mel khaane vaale file paths ko ek csv file mein sahajaata hai. Yaha in paths ke ek DataFrame ko lautata hai\n    column 'File Path' ke saath.\n\n    Parameters:\n    - pattern (str): filenames se mel khaane ke lie Regex pattern. \n    - directory (str): Files kee talaash ke lie directory. \n    - output_csv (str): Milaan kiye gaye file paths ko sahajaane ke lie csv file path.\n\n    Returns:\n        pd.DataFrame: Mel khaane vaale paths ke 'File Path' ke ek column ke saath DataFrame.\n\n    Requirements:\n    - re\n    - pandas\n    - os\n\n    Example:\n    >>> df = task_func(\".*\\\\.txt$\", \"/path/to/search\", \"matched_files.csv\")\n\n    \"\"\"\n    matched_files = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if re.match(pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    df = pd.DataFrame(matched_files, columns=[\"File Path\"])\n\n    df.to_csv(output_csv, index=False)\n\n    return df", "raw_solution": "import os\nimport re\nimport pandas as pd\n\n\ndef task_func(pattern: str, directory: str, output_csv: str) -> pd.DataFrame:\n    \"\"\"\n    Dee gayee directory mein files kee searches jo ek diye gaye regex pattern se mel khaatee hain. \n    Yaha function directory mein ghoomta hai, filenames ko pattern ke saath mel khaata hai, \n    and mel khaane vaale file paths ko ek csv file mein sahajaata hai. Yaha in paths ke ek DataFrame ko lautata hai\n    column 'File Path' ke saath.\n\n    Parameters:\n    - pattern (str): filenames se mel khaane ke lie Regex pattern. \n    - directory (str): Files kee talaash ke lie directory. \n    - output_csv (str): Milaan kiye gaye file paths ko sahajaane ke lie csv file path.\n\n    Returns:\n        pd.DataFrame: Mel khaane vaale paths ke 'File Path' ke ek column ke saath DataFrame.\n\n    Requirements:\n    - re\n    - pandas\n    - os\n\n    Example:\n    >>> df = task_func(\".*\\\\.txt$\", \"/path/to/search\", \"matched_files.csv\")\n\n    \"\"\"\n    matched_files = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if re.match(pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    df = pd.DataFrame(matched_files, columns=[\"File Path\"])\n\n    df.to_csv(output_csv, index=False)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/642", "solution": "import binascii\nimport hashlib\nimport re\nimport os\ndef task_func(directory: str, pattern: str = r\"(?<!Distillr)\\\\AcroTray\\.exe\") -> dict:\n    \"\"\"\n    Dee gaee regex (regex) pattern se mel khaane vaalee files ke lie nirdisht directory ke andar khoj karta hai\n    and pratyek file kee content ka SHA256 hash kee ganana karta hai.\n\n    Parameters:\n    - directory (str): fileon kee khoj karane ke lie directory.\n    - pattern (str): regex (regex) pattern jisase filenames ka milaan hona chaahie. default pattern 'AcroTray.exe' se mel khaatee hai.\n\n    Returns:\n        dict: ek dict jisme file path keys ke roop mein hain and unake SHA256 hash maan ke roop mein hain.\n\n    Requirements:\n    - re\n    - hashlib\n    - binascii\n    - os\n\n    Example:\n    >>> task_func(OUTPUT_DIR)\n    {}\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    hash_dict = {}\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if re.search(pattern, file):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'rb') as f:\n                    file_content = f.read()\n                    sha256_hash = hashlib.sha256(file_content).hexdigest()\n                    hash_dict[file_path] = sha256_hash\n\n    return hash_dict", "raw_solution": "import binascii\nimport hashlib\nimport re\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(directory: str, pattern: str = r\"(?<!Distillr)\\\\AcroTray\\.exe\") -> dict:\n    \"\"\"\n    Dee gaee regex (regex) pattern se mel khaane vaalee files ke lie nirdisht directory ke andar khoj karta hai\n    and pratyek file kee content ka SHA256 hash kee ganana karta hai.\n\n    Parameters:\n    - directory (str): fileon kee khoj karane ke lie directory.\n    - pattern (str): regex (regex) pattern jisase filenames ka milaan hona chaahie. default pattern 'AcroTray.exe' se mel khaatee hai.\n\n    Returns:\n        dict: ek dict jisme file path keys ke roop mein hain and unake SHA256 hash maan ke roop mein hain.\n\n    Requirements:\n    - re\n    - hashlib\n    - binascii\n    - os\n\n    Example:\n    >>> task_func(OUTPUT_DIR)\n    {}\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    hash_dict = {}\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if re.search(pattern, file):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'rb') as f:\n                    file_content = f.read()\n                    sha256_hash = hashlib.sha256(file_content).hexdigest()\n                    hash_dict[file_path] = sha256_hash\n\n    return hash_dict\n```\n"}
{"task_id": "BigCodeBench/643", "solution": "import re\nimport pandas as pd\nimport numpy as np\nDATA_PATTERN = r'>\\d+\\.\\d+<'\ndef task_func(dataframe, data_pattern=DATA_PATTERN):\n    \"\"\"\n    Ek Pandas dataframe se ek specific pattern ke aadhar par numeric data extract. Function search ta hai\n    regex pattern '>number<number>' (udaaharan ke lie, '>1.23<') kee ghatnaon ke lie pratyek cell\n    aur cell kee saamagree ko nikaale gae numeric maan se badal deta hai. Yadi koee match nahin mila hai, to cell ko NaN se badal diya jaata hai.\n\n    Parameters:\n    - dataframe (pd.dataframe): Sansadhit kie jaane vaale data ko rakhate hue ek pandas dataframe. \n    - data_pattern (str, optional): Data search pattern. Default maan '>\\d+\\.\\d+<' hai.\n\n    Returns:\n        pd.dataframe: Nikaale gae numeric values ya NaN vaale cells ke saath ek sanshodhit dataframe.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n\n    Example:\n    >>> import pandas as pd\n    >>> df = pd.dataframe({'A': ['>1.23<', '>4.56<'], 'B': ['>7.89<', '>0.12<']})\n    >>> task_func(df)\n    A     B\n    0  1.23  7.89\n    1  4.56  0.12\n\n    \"\"\"\n    def extract_data(cell):\n        match = re.search(data_pattern, cell)\n        if match:\n            return float(match.group().strip('><'))\n        return np.nan\n\n    return dataframe.applymap(extract_data)", "raw_solution": "import re\nimport pandas as pd\nimport numpy as np\n# Constants\nDATA_PATTERN = r'>\\d+\\.\\d+<'\n\n\ndef task_func(dataframe, data_pattern=DATA_PATTERN):\n    \"\"\"\n    Ek Pandas dataframe se ek specific pattern ke aadhar par numeric data extract. Function search ta hai\n    regex pattern '>number<number>' (udaaharan ke lie, '>1.23<') kee ghatnaon ke lie pratyek cell\n    aur cell kee saamagree ko nikaale gae numeric maan se badal deta hai. Yadi koee match nahin mila hai, to cell ko NaN se badal diya jaata hai.\n\n    Parameters:\n    - dataframe (pd.dataframe): Sansadhit kie jaane vaale data ko rakhate hue ek pandas dataframe. \n    - data_pattern (str, optional): Data search pattern. Default maan '>\\d+\\.\\d+<' hai.\n\n    Returns:\n        pd.dataframe: Nikaale gae numeric values ya NaN vaale cells ke saath ek sanshodhit dataframe.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n\n    Example:\n    >>> import pandas as pd\n    >>> df = pd.dataframe({'A': ['>1.23<', '>4.56<'], 'B': ['>7.89<', '>0.12<']})\n    >>> task_func(df)\n    A     B\n    0  1.23  7.89\n    1  4.56  0.12\n\n    \"\"\"\n    def extract_data(cell):\n        match = re.search(data_pattern, cell)\n        if match:\n            return float(match.group().strip('><'))\n        return np.nan\n\n    return dataframe.applymap(extract_data)\n```\n"}
{"task_id": "BigCodeBench/644", "solution": "import hashlib\nimport base64\ndef task_func(filename, data, password):\n    # Hash the password using SHA256\n    hashed_password = hashlib.sha256(password.encode()).digest()\n\n    # Encrypt the data using AES (Advanced Encryption Standard)\n    cipher = AES.new(hashed_password, AES.MODE_EAX)\n    ciphertext, tag = cipher.encrypt_and_digest(data.encode())\n\n    # Encode the ciphertext and tag using base64\n    encrypted_data = base64.b64encode(ciphertext + tag).decode()\n\n    # Write the encrypted data to the file\n    with open(filename, 'w') as file:\n        file.write(encrypted_data)\n\n    return encrypted_data", "raw_solution": "import hashlib\nimport base64\n\ndef task_func(filename, data, password):\n    # Hash the password using SHA256\n    hashed_password = hashlib.sha256(password.encode()).digest()\n\n    # Encrypt the data using AES (Advanced Encryption Standard)\n    cipher = AES.new(hashed_password, AES.MODE_EAX)\n    ciphertext, tag = cipher.encrypt_and_digest(data.encode())\n\n    # Encode the ciphertext and tag using base64\n    encrypted_data = base64.b64encode(ciphertext + tag).decode()\n\n    # Write the encrypted data to the file\n    with open(filename, 'w') as file:\n        file.write(encrypted_data)\n\n    return encrypted_data\n\n# Example usage:\n# encrypted_data = task_func('test.txt', 'Hello, World!', 'password')\n# print(encrypted_data)\n```\nPlease note that the above code is a simplified example and does not include the actual encryption and decryption logic. In a real-world scenario, you would need to handle the encryption and decryption properly, including the use of random initialization vectors and secure key management. Also, the AES.MODE_EAX is used for demonstration purposes, but you might want to use a different mode depending on your specific requirements.\n"}
{"task_id": "BigCodeBench/645", "solution": "import os\nimport pandas as pd\ndef task_func(filename: str) -> pd.DataFrame:\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"No such file: '{filename}'\")\n\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(filename)\n\n    # Remove the original file\n    os.remove(filename)\n\n    return df", "raw_solution": "import os\nimport pandas as pd\n\ndef task_func(filename: str) -> pd.DataFrame:\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"No such file: '{filename}'\")\n\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(filename)\n\n    # Remove the original file\n    os.remove(filename)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/646", "solution": "import os\nimport pandas as pd\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\nOUTPUT_DIR = './output'\ndef task_func(csv_path=os.path.join(OUTPUT_DIR, 'data.csv'), date_column='date'):\n    if not os.path.exists(csv_path):\n        raise FileNotFoundError(f\"{csv_path} does not exist\")\n\n    df = pd.read_csv(csv_path)\n    if date_column not in df.columns:\n        raise ValueError(f\"Column {date_column} does not exist in the CSV file\")\n\n    df[date_column] = pd.to_datetime(df[date_column].apply(parse))\n\n    df['year'] = df[date_column].dt.year\n    year_counts = df['year'].value_counts().sort_index()\n\n    ax = year_counts.plot(kind='bar', figsize=(10, 6), color='skyblue')\n    ax.set_title('Year Distribution')\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Frequency')\n\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import os\nimport pandas as pd\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\n\nOUTPUT_DIR = './output'\n\ndef task_func(csv_path=os.path.join(OUTPUT_DIR, 'data.csv'), date_column='date'):\n    if not os.path.exists(csv_path):\n        raise FileNotFoundError(f\"{csv_path} does not exist\")\n\n    df = pd.read_csv(csv_path)\n    if date_column not in df.columns:\n        raise ValueError(f\"Column {date_column} does not exist in the CSV file\")\n\n    df[date_column] = pd.to_datetime(df[date_column].apply(parse))\n\n    df['year'] = df[date_column].dt.year\n    year_counts = df['year'].value_counts().sort_index()\n\n    ax = year_counts.plot(kind='bar', figsize=(10, 6), color='skyblue')\n    ax.set_title('Year Distribution')\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Frequency')\n\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/647", "solution": "import pytz\nfrom dateutil.parser import parse\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Ek date string ko ek time zone se dusre mein badlen and current samay se seconds mein time difference lautaen\n    destination time zone mein.\n\n    Parameters:\n    - date_str (str): date string \"yyyy-mm-dd hh:mm:ss\" format mein.\n    - from_tz (str): dee gaee date string ka timezone.\n    - to_tz (str): timezone jis mein date string ko convert kiya jaana chaahie.\n\n    Returns:\n        int: seconds mein time difference.\n\n    Requirements:\n    - pytz\n    - dateutil.parser\n\n    Example:\n    >>> type(task_func('2022-10-22 11:59:59', 'UTC', 'America/Chicago'))\n    <class 'int'>\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    naive_datetime = parse(date_str)\n    \n    # Get the source and destination time zones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_datetime = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the destination timezone\n    converted_datetime = aware_datetime.astimezone(to_timezone)\n    \n    # Calculate the time difference in seconds\n    time_difference = int((converted_datetime - aware_datetime).total_seconds())\n    \n    return time_difference", "raw_solution": "import pytz\nfrom dateutil.parser import parse\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Ek date string ko ek time zone se dusre mein badlen and current samay se seconds mein time difference lautaen\n    destination time zone mein.\n\n    Parameters:\n    - date_str (str): date string \"yyyy-mm-dd hh:mm:ss\" format mein.\n    - from_tz (str): dee gaee date string ka timezone.\n    - to_tz (str): timezone jis mein date string ko convert kiya jaana chaahie.\n\n    Returns:\n        int: seconds mein time difference.\n\n    Requirements:\n    - pytz\n    - dateutil.parser\n\n    Example:\n    >>> type(task_func('2022-10-22 11:59:59', 'UTC', 'America/Chicago'))\n    <class 'int'>\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    naive_datetime = parse(date_str)\n    \n    # Get the source and destination time zones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_datetime = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the destination timezone\n    converted_datetime = aware_datetime.astimezone(to_timezone)\n    \n    # Calculate the time difference in seconds\n    time_difference = int((converted_datetime - aware_datetime).total_seconds())\n    \n    return time_difference\n```\n"}
{"task_id": "BigCodeBench/648", "solution": "from dateutil.parser import parse\nfrom datetime import timedelta, datetime\ndef task_func(date_str):\n    \"\"\"\n    next business day (weekday) after a given date string. This function uses dateutil.parser and datetime.\n\n    Parameters:\n    - date_str (str): \"yyyy-mm-dd\" format date string.\n\n    Returns:\n        datetime: next business day's datetime object.\n\n    Requirements:\n    - datetime\n    - dateutil.parser\n\n    Example:\n    >>> task_func('2022-10-22')\n    datetime.datetime(2022, 10, 24, 0, 0)\n    >>> task_func('2022-10-28')\n    datetime.datetime(2022, 10, 31, 0, 0)\n\n    \"\"\"\n    # Parse the input date string into a datetime object\n    date = parse(date_str)\n\n    # Calculate the next business day\n    next_business_day = date + timedelta(days=1)\n    while next_business_day.weekday() >= 5:  # 5 for Saturday, 6 for Sunday\n        next_business_day += timedelta(days=1)\n\n    return next_business_day", "raw_solution": "from dateutil.parser import parse\nfrom datetime import timedelta, datetime\n\n\ndef task_func(date_str):\n    \"\"\"\n    next business day (weekday) after a given date string. This function uses dateutil.parser and datetime.\n\n    Parameters:\n    - date_str (str): \"yyyy-mm-dd\" format date string.\n\n    Returns:\n        datetime: next business day's datetime object.\n\n    Requirements:\n    - datetime\n    - dateutil.parser\n\n    Example:\n    >>> task_func('2022-10-22')\n    datetime.datetime(2022, 10, 24, 0, 0)\n    >>> task_func('2022-10-28')\n    datetime.datetime(2022, 10, 31, 0, 0)\n\n    \"\"\"\n    # Parse the input date string into a datetime object\n    date = parse(date_str)\n\n    # Calculate the next business day\n    next_business_day = date + timedelta(days=1)\n    while next_business_day.weekday() >= 5:  # 5 for Saturday, 6 for Sunday\n        next_business_day += timedelta(days=1)\n\n    return next_business_day\n\n\n# Test cases"}
{"task_id": "BigCodeBench/649", "solution": "import numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\ndef task_func(dates_str_list):\n    # Parse the dates using dateutil.parser\n    dates = [parse(date) for date in dates_str_list]\n    \n    # Create a pandas Series with weekdays as index\n    weekdays = pd.Series([date.strftime('%A') for date in dates])\n    \n    # Calculate the distribution of weekdays\n    weekday_counts = weekdays.value_counts().sort_index()\n    \n    # Create a DataFrame to hold the counts\n    df = pd.DataFrame(weekday_counts).T\n    \n    # Fill NaN values with 0\n    df.fillna(0, inplace=True)\n    \n    return df.iloc[0]", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\n\ndef task_func(dates_str_list):\n    # Parse the dates using dateutil.parser\n    dates = [parse(date) for date in dates_str_list]\n    \n    # Create a pandas Series with weekdays as index\n    weekdays = pd.Series([date.strftime('%A') for date in dates])\n    \n    # Calculate the distribution of weekdays\n    weekday_counts = weekdays.value_counts().sort_index()\n    \n    # Create a DataFrame to hold the counts\n    df = pd.DataFrame(weekday_counts).T\n    \n    # Fill NaN values with 0\n    df.fillna(0, inplace=True)\n    \n    return df.iloc[0]\n\n# Example usage:\n# print(task_func(['2022-10-22', '2022-10-23', '2022-10-24', '2022-10-25']))\n```\n"}
{"task_id": "BigCodeBench/650", "solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\ndef task_func(date_str, tz_str):\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n    \n    # Get the specified timezone\n    timezone = pytz.timezone(tz_str)\n    \n    # Localize the datetime object to the specified timezone\n    localized_date = timezone.localize(date)\n    \n    # Get the current year\n    current_year = localized_date.year\n    \n    # Calculate the next year\n    next_year = current_year + 1\n    \n    # Create a new datetime object for the next year at the same time\n    next_year_date = localized_date.replace(year=next_year)\n    \n    # Calculate the difference in seconds\n    time_difference = int((next_year_date - localized_date).total_seconds())\n    \n    return time_difference", "raw_solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\ndef task_func(date_str, tz_str):\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n    \n    # Get the specified timezone\n    timezone = pytz.timezone(tz_str)\n    \n    # Localize the datetime object to the specified timezone\n    localized_date = timezone.localize(date)\n    \n    # Get the current year\n    current_year = localized_date.year\n    \n    # Calculate the next year\n    next_year = current_year + 1\n    \n    # Create a new datetime object for the next year at the same time\n    next_year_date = localized_date.replace(year=next_year)\n    \n    # Calculate the difference in seconds\n    time_difference = int((next_year_date - localized_date).total_seconds())\n    \n    return time_difference\n\n# Example usage:\n# print(task_func('2022-10-22 11:59:59', 'America/Chicago'))\n```\n"}
{"task_id": "BigCodeBench/651", "solution": "import pandas as pd\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(df, target_value):\n    if not isinstance(df, dict) or 'Name' not in df:\n        raise ValueError(\"Input must be a dictionary with a 'Name' key.\")\n\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n\n    # Check if the DataFrame is empty\n    if df.empty:\n        return pd.Series(), None\n\n    # Start timer\n    start_time = time.time()\n\n    # Count occurrences of the target value in each column\n    counts = df.apply(lambda col: col[col == target_value].count())\n\n    # Plot the counts\n    ax = counts.plot(kind='bar', title='Count of Target Value per Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count')\n\n    # End timer\n    end_time = time.time()\n    print(f\"Time taken: {end_time - start_time} seconds\")\n\n    return counts, ax", "raw_solution": "import pandas as pd\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_value):\n    if not isinstance(df, dict) or 'Name' not in df:\n        raise ValueError(\"Input must be a dictionary with a 'Name' key.\")\n\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n\n    # Check if the DataFrame is empty\n    if df.empty:\n        return pd.Series(), None\n\n    # Start timer\n    start_time = time.time()\n\n    # Count occurrences of the target value in each column\n    counts = df.apply(lambda col: col[col == target_value].count())\n\n    # Plot the counts\n    ax = counts.plot(kind='bar', title='Count of Target Value per Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count')\n\n    # End timer\n    end_time = time.time()\n    print(f\"Time taken: {end_time - start_time} seconds\")\n\n    return counts, ax\n```\n"}
{"task_id": "BigCodeBench/652", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\ndef task_func(target_value=TARGET_VALUE, array=ARRAY):\n    \"\"\"\n    Ek numpy array mein row suchkaank khoje jahan first cell target_value \"332\" se mel khaati hai\n    In indices par statistical analysis karta hai aur unke distribution ko plot karta hai.\n    Yadi koi target value nahi milti hai to sabhi stats ke liye 'N/A' lautayein.\n\n    Parameters:\n    - target_value (str): The target value. Default value is '332'\n    - array (np.ndarray): The input array\n\n    Returns:\n        tuple: mean, variance, skewness aur indices ki kurtosis ke saath ek tuple, or\n        'N/A' yadi statistical analysis nahi kiya ja sakta hai.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> task_func()\n    (2.0, 'N/A', 'N/A', 'N/A')\n\n    \"\"\"\n    # Initialize variables\n    found_indices = []\n    values = []\n\n    # Find rows where the first cell matches the target value\n    for i, row in enumerate(array):\n        if row[0] == target_value:\n            found_indices.append(i)\n            values.append(int(row[1]))\n\n    # Check if any rows were found\n    if not found_indices:\n        return 'N/A', 'N/A', 'N/A', 'N/A'\n\n    # Calculate statistics\n    mean = np.mean(values)\n    variance = np.var(values)\n    skewness = stats.skew(values)\n    kurtosis = stats.kurtosis(values)\n\n    # Plot histogram\n    plt.hist(values, bins='auto', alpha=0.7, color='blue')\n    plt.title('Distribution of Values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return mean, variance, skewness, kurtosis", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Constants\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\n\n\ndef task_func(target_value=TARGET_VALUE, array=ARRAY):\n    \"\"\"\n    Ek numpy array mein row suchkaank khoje jahan first cell target_value \"332\" se mel khaati hai\n    In indices par statistical analysis karta hai aur unke distribution ko plot karta hai.\n    Yadi koi target value nahi milti hai to sabhi stats ke liye 'N/A' lautayein.\n\n    Parameters:\n    - target_value (str): The target value. Default value is '332'\n    - array (np.ndarray): The input array\n\n    Returns:\n        tuple: mean, variance, skewness aur indices ki kurtosis ke saath ek tuple, or\n        'N/A' yadi statistical analysis nahi kiya ja sakta hai.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> task_func()\n    (2.0, 'N/A', 'N/A', 'N/A')\n\n    \"\"\"\n    # Initialize variables\n    found_indices = []\n    values = []\n\n    # Find rows where the first cell matches the target value\n    for i, row in enumerate(array):\n        if row[0] == target_value:\n            found_indices.append(i)\n            values.append(int(row[1]))\n\n    # Check if any rows were found\n    if not found_indices:\n        return 'N/A', 'N/A', 'N/A', 'N/A'\n\n    # Calculate statistics\n    mean = np.mean(values)\n    variance = np.var(values)\n    skewness = stats.skew(values)\n    kurtosis = stats.kurtosis(values)\n\n    # Plot histogram\n    plt.hist(values, bins='auto', alpha=0.7, color='blue')\n    plt.title('Distribution of Values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return mean, variance, skewness, kurtosis\n```\n"}
{"task_id": "BigCodeBench/653", "solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ndef task_func(dataframe, target_value='332'):\n    # Create a boolean mask for the target value\n    mask = dataframe.applymap(lambda x: x == target_value).any(axis=1)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(dataframe[mask], annot=True, cmap='coolwarm', cbar=False)\n    plt.title(f\"Heatmap of rows containing '{target_value}'\")\n    plt.show()\n\n    return mask, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef task_func(dataframe, target_value='332'):\n    # Create a boolean mask for the target value\n    mask = dataframe.applymap(lambda x: x == target_value).any(axis=1)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(dataframe[mask], annot=True, cmap='coolwarm', cbar=False)\n    plt.title(f\"Heatmap of rows containing '{target_value}'\")\n    plt.show()\n\n    return mask, ax\n```\n"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Filter the array to only include rows where the first column matches the target value\n    filtered_array = array[array[:, 0] == target_value]\n\n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Extract the x and y values from the filtered array\n    x_data = filtered_array[:, 1]\n    y_data = filtered_array[:, 2]\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(exp_decay, x_data, y_data)\n\n    # Generate x values for plotting the fitted curve\n    x_fit = np.linspace(min(x_data), max(x_data), 100)\n    y_fit = exp_decay(x_fit, *popt)\n\n    # Plot the data and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(x_data, y_data, label='Data')\n    ax.plot(x_fit, y_fit, label='Fitted Curve', color='red')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Exponential Decay Fit')\n    ax.legend()\n\n    return popt, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Filter the array to only include rows where the first column matches the target value\n    filtered_array = array[array[:, 0] == target_value]\n\n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Extract the x and y values from the filtered array\n    x_data = filtered_array[:, 1]\n    y_data = filtered_array[:, 2]\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(exp_decay, x_data, y_data)\n\n    # Generate x values for plotting the fitted curve\n    x_fit = np.linspace(min(x_data), max(x_data), 100)\n    y_fit = exp_decay(x_fit, *popt)\n\n    # Plot the data and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(x_data, y_data, label='Data')\n    ax.plot(x_fit, y_fit, label='Fitted Curve', color='red')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Exponential Decay Fit')\n    ax.legend()\n\n    return popt, ax\n```\n"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    # Preprocess texts\n    processed_texts = [' '.join(ALPHANUMERIC.sub(' ', text).lower().split()) for text in texts]\n    processed_texts = [' '.join(word for word in text.split() if word not in STOPWORDS) for text in processed_texts]\n\n    # Vectorize texts\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(tfidf_matrix)\n\n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]]\n        topics.append(top_words)\n\n    return topics\ntexts = [\n    \"Data science involves the study of data.\",\n    \"Machine learning provides systems the ability to learn from data.\",\n    \"Python is a programming language used in data science.\"\n]", "raw_solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts, num_topics):\n    # Preprocess texts\n    processed_texts = [' '.join(ALPHANUMERIC.sub(' ', text).lower().split()) for text in texts]\n    processed_texts = [' '.join(word for word in text.split() if word not in STOPWORDS) for text in processed_texts]\n\n    # Vectorize texts\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(tfidf_matrix)\n\n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]]\n        topics.append(top_words)\n\n    return topics\n\n# Example usage\ntexts = [\n    \"Data science involves the study of data.\",\n    \"Machine learning provides systems the ability to learn from data.\",\n    \"Python is a programming language used in data science.\"\n]\ntopics = task_func(texts, 2)"}
{"task_id": "BigCodeBench/656", "solution": "import re\nimport string\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\ndef task_func(text: str, sia: SentimentIntensityAnalyzer) -> dict:\n    # Remove non-alphanumeric characters and convert to lowercase\n    text = ALPHANUMERIC.sub(' ', text.lower())\n    \n    # Remove punctuation\n    text = ''.join(ch for ch in text if ch not in PUNCTUATIONS)\n    \n    # Perform sentiment analysis\n    sentiment_scores = sia.polarity_scores(text)\n    \n    return sentiment_scores", "raw_solution": "import re\nimport string\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nnltk.download('vader_lexicon')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\n\ndef task_func(text: str, sia: SentimentIntensityAnalyzer) -> dict:\n    # Remove non-alphanumeric characters and convert to lowercase\n    text = ALPHANUMERIC.sub(' ', text.lower())\n    \n    # Remove punctuation\n    text = ''.join(ch for ch in text if ch not in PUNCTUATIONS)\n    \n    # Perform sentiment analysis\n    sentiment_scores = sia.polarity_scores(text)\n    \n    return sentiment_scores\n\n# Example usage"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Gensim Word2Vec model and nltk.corpus.stopwords ka upyog kar ke, text ki ek list se word vectors banaen.\n    Text ko first space ko chhodkar sabhi non-akshar-alphanumeric characters ko hatakar saaf kiya jaata hai,\n    lowercase mein badla jaata hai, and words hata diye jaate hain.\n\n    Parameters:\n    - texts (list): String ki ek list.\n    - stopwords (list, optional): Hataane ke liye words ki ek list. Yadi pradaan nahin kiya gaya hai, to nltk ke words ka upyog kiya jaayega.\n\n    Returns:\n        Word2Vec: Ek prashikshit Word2Vec model.\n\n    Requirements:\n    - re\n    - nltk\n    - gensim\n\n    Example:\n    >>> texts = [\"Hello, World!\", \"Machine Learning is great\", \"Python is my favorite programming language\"]\n    >>> model = task_func(texts)\n    >>> vector = model.wv['python']\n\n    \"\"\"\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Preprocess the texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Tokenize the text\n        words = text.split()\n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        processed_texts.append(words)\n\n    # Train the Word2Vec model\n    model = Word2Vec(processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n    return model", "raw_solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Gensim Word2Vec model and nltk.corpus.stopwords ka upyog kar ke, text ki ek list se word vectors banaen.\n    Text ko first space ko chhodkar sabhi non-akshar-alphanumeric characters ko hatakar saaf kiya jaata hai,\n    lowercase mein badla jaata hai, and words hata diye jaate hain.\n\n    Parameters:\n    - texts (list): String ki ek list.\n    - stopwords (list, optional): Hataane ke liye words ki ek list. Yadi pradaan nahin kiya gaya hai, to nltk ke words ka upyog kiya jaayega.\n\n    Returns:\n        Word2Vec: Ek prashikshit Word2Vec model.\n\n    Requirements:\n    - re\n    - nltk\n    - gensim\n\n    Example:\n    >>> texts = [\"Hello, World!\", \"Machine Learning is great\", \"Python is my favorite programming language\"]\n    >>> model = task_func(texts)\n    >>> vector = model.wv['python']\n\n    \"\"\"\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Preprocess the texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Tokenize the text\n        words = text.split()\n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        processed_texts.append(words)\n\n    # Train the Word2Vec model\n    model = Word2Vec(processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n    return model\n```\n"}
{"task_id": "BigCodeBench/658", "solution": "import re\nimport nltk\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts):\n    \"\"\"\n    Scikit-learn se CountVectorizer ka upayog kar ke text documents ki ek list se ek document-term matrix (DTM) banata hai.\n    text ko gair-akshar-ankiy varnon (space ko chhodkar) ko hatakar poorv-sansaadhit kiya jaata hai,\n    chote aksharon mein parivartit karna, aur NLTK mein paribhashit angrezi stop terms ko chhodkar.\n\n    Parameters:\n    - texts (list of str): DTM mein badalne ke liye text documents ki list.\n\n    Returns:\n        pd.DataFrame: ek datafreym jahan panktiyan documents ka pratinidhitva karti hain aur column unik termon ka pratinidhitva karte hain;\n        sel maan ek document mein ek term ki aavritti ko darshate hain.\n\n    Requirements:\n    - re\n    - nltk\n    - pandas\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> texts = [\"Namaste, duniya!\", \"Machine learning bahut achchha hai.\", \"Python mere favorite programming bhasha hai.\"]\n    >>> dtm = task_func(texts)\n\n    \"\"\"\n    # Preprocess the texts by removing non-alphanumeric characters and converting to lowercase\n    processed_texts = [ALPHANUMERIC.sub(' ', text.lower()) for text in texts]\n\n    # Initialize CountVectorizer with English stop words\n    vectorizer = CountVectorizer(stop_words=STOPWORDS)\n\n    # Fit and transform the processed texts to create the DTM\n    dtm = vectorizer.fit_transform(processed_texts)\n\n    # Convert the DTM to a pandas DataFrame\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df", "raw_solution": "import re\nimport nltk\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Make sure to download NLTK stopwords\nnltk.download('stopwords')\n\n# Define a regex pattern for matching all non-alphanumeric characters\nALPHANUMERIC = re.compile('[\\W_]+')\n\n# Load NLTK's list of English stop words\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts):\n    \"\"\"\n    Scikit-learn se CountVectorizer ka upayog kar ke text documents ki ek list se ek document-term matrix (DTM) banata hai.\n    text ko gair-akshar-ankiy varnon (space ko chhodkar) ko hatakar poorv-sansaadhit kiya jaata hai,\n    chote aksharon mein parivartit karna, aur NLTK mein paribhashit angrezi stop terms ko chhodkar.\n\n    Parameters:\n    - texts (list of str): DTM mein badalne ke liye text documents ki list.\n\n    Returns:\n        pd.DataFrame: ek datafreym jahan panktiyan documents ka pratinidhitva karti hain aur column unik termon ka pratinidhitva karte hain;\n        sel maan ek document mein ek term ki aavritti ko darshate hain.\n\n    Requirements:\n    - re\n    - nltk\n    - pandas\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> texts = [\"Namaste, duniya!\", \"Machine learning bahut achchha hai.\", \"Python mere favorite programming bhasha hai.\"]\n    >>> dtm = task_func(texts)\n\n    \"\"\"\n    # Preprocess the texts by removing non-alphanumeric characters and converting to lowercase\n    processed_texts = [ALPHANUMERIC.sub(' ', text.lower()) for text in texts]\n\n    # Initialize CountVectorizer with English stop words\n    vectorizer = CountVectorizer(stop_words=STOPWORDS)\n\n    # Fit and transform the processed texts to create the DTM\n    dtm = vectorizer.fit_transform(processed_texts)\n\n    # Convert the DTM to a pandas DataFrame\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/659", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\ndef task_func(x, y, labels):\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        # Fit a normal distribution to the data\n        mu, std = stats.norm.fit(x[i])\n        p = stats.norm.pdf(x[i], mu, std)\n\n        ax.plot(x[i], y[i], label=labels[i], linestyle='--', marker='o')\n        ax.plot(x[i], p, label=f'Fit: {labels[i]}', linestyle='-', alpha=0.5)\n\n    ax.set_xlabel('X values')\n    ax.set_ylabel('Y values')\n    ax.set_title('Normal Distribution Fits')\n    ax.legend()\n\n    return fig\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\ndef task_func(x, y, labels):\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        # Fit a normal distribution to the data\n        mu, std = stats.norm.fit(x[i])\n        p = stats.norm.pdf(x[i], mu, std)\n\n        ax.plot(x[i], y[i], label=labels[i], linestyle='--', marker='o')\n        ax.plot(x[i], p, label=f'Fit: {labels[i]}', linestyle='-', alpha=0.5)\n\n    ax.set_xlabel('X values')\n    ax.set_ylabel('Y values')\n    ax.set_title('Normal Distribution Fits')\n    ax.legend()\n\n    return fig\n\n# Example usage:\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']\nfig = task_func(x, y, labels)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/660", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(x, y, labels):\n    scaler = StandardScaler()\n    x_scaled = [scaler.fit_transform(i.reshape(-1, 1)).flatten() for i in x]\n    y_scaled = [scaler.fit_transform(i.reshape(-1, 1)).flatten() for i in y]\n\n    fig, ax = plt.subplots()\n\n    for i in range(len(x_scaled)):\n        ax.plot(x_scaled[i], y_scaled[i], label=labels[i])\n\n    ax.set_xlabel('Scaled X values')\n    ax.set_ylabel('Scaled Y values')\n    ax.set_title('Scaled Data Plot')\n    ax.legend()\n\n    return fig\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['A', 'B', 'C']", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(x, y, labels):\n    scaler = StandardScaler()\n    x_scaled = [scaler.fit_transform(i.reshape(-1, 1)).flatten() for i in x]\n    y_scaled = [scaler.fit_transform(i.reshape(-1, 1)).flatten() for i in y]\n\n    fig, ax = plt.subplots()\n\n    for i in range(len(x_scaled)):\n        ax.plot(x_scaled[i], y_scaled[i], label=labels[i])\n\n    ax.set_xlabel('Scaled X values')\n    ax.set_ylabel('Scaled Y values')\n    ax.set_title('Scaled Data Plot')\n    ax.legend()\n\n    return fig\n\n# Example usage:\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['A', 'B', 'C']\nfig = task_func(x, y, labels)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/661", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(x, y, labels):\n    # Create a DataFrame from the input data\n    data = []\n    for i in range(len(x)):\n        for j in range(len(y)):\n            data.append([x[i][j], y[j][i]])\n    \n    df = pd.DataFrame(data, columns=labels, index=labels)\n\n    # Create a heatmap using seaborn\n    ax = sns.heatmap(df, annot=True, fmt=\"d\", cmap=\"Blues\")\n\n    return ax, df\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H\\u2082O', 'O\\u2082', 'CO\\u2082']", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\n\ndef task_func(x, y, labels):\n    # Create a DataFrame from the input data\n    data = []\n    for i in range(len(x)):\n        for j in range(len(y)):\n            data.append([x[i][j], y[j][i]])\n    \n    df = pd.DataFrame(data, columns=labels, index=labels)\n\n    # Create a heatmap using seaborn\n    ax = sns.heatmap(df, annot=True, fmt=\"d\", cmap=\"Blues\")\n\n    return ax, df\n\n# Example usage:\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H\\u2082O', 'O\\u2082', 'CO\\u2082']\nax = task_func(x, y, labels)\n```\n"}
{"task_id": "BigCodeBench/662", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(x, y, labels):\n    # Concatenate x and y values into a single dataset\n    data = np.concatenate((np.array(x), np.array(y)), axis=1)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(data)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot each label with a different color\n    for i, label in enumerate(labels):\n        ax.scatter(principal_components[i, 0], principal_components[i, 1], label=label)\n\n    # Add legend\n    ax.legend()\n\n    return fig\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(x, y, labels):\n    # Concatenate x and y values into a single dataset\n    data = np.concatenate((np.array(x), np.array(y)), axis=1)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(data)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot each label with a different color\n    for i, label in enumerate(labels):\n        ax.scatter(principal_components[i, 0], principal_components[i, 1], label=label)\n\n    # Add legend\n    ax.legend()\n\n    return fig\n\n# Example usage\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']\nfig = task_func(x, y, labels)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/663", "solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\ndef task_func(x, y, labels):\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    fig, ax = plt.subplots()\n\n    for i, (x_data, y_data) in enumerate(zip(x, y)):\n        popt, _ = curve_fit(func, x_data, y_data)\n        ax.plot(x_data, func(x_data, *popt), label=labels[i])\n\n    ax.legend()\n    return fig\nx_data = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny_data = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']", "raw_solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef task_func(x, y, labels):\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    fig, ax = plt.subplots()\n\n    for i, (x_data, y_data) in enumerate(zip(x, y)):\n        popt, _ = curve_fit(func, x_data, y_data)\n        ax.plot(x_data, func(x_data, *popt), label=labels[i])\n\n    ax.legend()\n    return fig\n\n# Example usage:\nx_data = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny_data = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']\n\nfig = task_func(x_data, y_data, labels)\nplt.show()\n```\nThis script uses `curve_fit` from `scipy.optimize` to fit an exponential curve to the data points and `matplotlib.pyplot` to plot the curves with their corresponding labels. The function `task_func` takes in three parameters: a list of `x` data arrays, a list of `y` data arrays, and a list of labels. It returns a `matplotlib.figure.Figure` object that contains the plotted curves.\n"}
{"task_id": "BigCodeBench/664", "solution": "import statistics\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(sales_data):\n    \"\"\"\n    Ek year mein paanch producton ke liye sales trends ko plot karein, standard deviation shading ke saath variability ko ujagar karna\n    'month' ko x-axis par and 'sales' ko y-axis par rakhein.\n\n    Parameters:\n    - sales_data (pd.dataFrame): sales data ke saath dataFrame, apekshit columns: 'month', 'product a' se 'product e'.\n\n    Returns:\n        ax (matplotlib.axes.axes): sales trends ke plot ke saath axis object.\n\n    Requirements:\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> import pandas as pd, numpy as np\n    >>> sales_data = pd.DataFrame({\n    ...     'Month': range(1, 13),\n    ...     'Product a': np.random.randint(100, 200, size=12),\n    ...     'Product B': np.random.randint(150, 250, size=12),\n    ...     'Product c': np.random.randint(120, 220, size=12),\n    ...     'Product d': np.random.randint(130, 230, size=12),\n    ...     'Product e': np.random.randint(140, 240, size=12)\n    ... })\n    >>> ax = task_func(sales_data)\n    >>> plt.show()  # displays the plot\n\n    \"\"\"\n    # Extract months and sales data\n    months = sales_data['Month']\n    sales = sales_data[['Product a', 'Product B', 'Product c', 'Product d', 'Product e']]\n\n    # Calculate standard deviation for each product\n    std_devs = sales.apply(lambda x: statistics.stdev(x))\n\n    # Plot sales trends\n    ax = sales.plot(x='Month', kind='line', marker='o', title='Sales Trends Over Months')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n\n    # Shade areas of standard deviation\n    for i, product in enumerate(sales.columns):\n        ax.fill_between(months, sales[product] - std_devs[i], sales[product] + std_devs[i], alpha=0.2)\n\n    return ax", "raw_solution": "import statistics\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(sales_data):\n    \"\"\"\n    Ek year mein paanch producton ke liye sales trends ko plot karein, standard deviation shading ke saath variability ko ujagar karna\n    'month' ko x-axis par and 'sales' ko y-axis par rakhein.\n\n    Parameters:\n    - sales_data (pd.dataFrame): sales data ke saath dataFrame, apekshit columns: 'month', 'product a' se 'product e'.\n\n    Returns:\n        ax (matplotlib.axes.axes): sales trends ke plot ke saath axis object.\n\n    Requirements:\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> import pandas as pd, numpy as np\n    >>> sales_data = pd.DataFrame({\n    ...     'Month': range(1, 13),\n    ...     'Product a': np.random.randint(100, 200, size=12),\n    ...     'Product B': np.random.randint(150, 250, size=12),\n    ...     'Product c': np.random.randint(120, 220, size=12),\n    ...     'Product d': np.random.randint(130, 230, size=12),\n    ...     'Product e': np.random.randint(140, 240, size=12)\n    ... })\n    >>> ax = task_func(sales_data)\n    >>> plt.show()  # displays the plot\n\n    \"\"\"\n    # Extract months and sales data\n    months = sales_data['Month']\n    sales = sales_data[['Product a', 'Product B', 'Product c', 'Product d', 'Product e']]\n\n    # Calculate standard deviation for each product\n    std_devs = sales.apply(lambda x: statistics.stdev(x))\n\n    # Plot sales trends\n    ax = sales.plot(x='Month', kind='line', marker='o', title='Sales Trends Over Months')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n\n    # Shade areas of standard deviation\n    for i, product in enumerate(sales.columns):\n        ax.fill_between(months, sales[product] - std_devs[i], sales[product] + std_devs[i], alpha=0.2)\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/665", "solution": "import shutil\nimport os\nimport fnmatch\nimport itertools\ndef task_func(src_dir, dst_dir):\n    \"\"\"\n    Copy all files from src_dir to dst_dir that match any of the patterns in ['*.txt', '*.docx'].\n\n    Parameters:\n    - src_dir (str): The source directory.\n    - dst_dir (str): The destination directory.\n\n    Returns:\n        str: destination directory.\n\n    Requirements:\n    - shutil\n    - os\n    - fnmatch\n    - itertools\n\n    Example:\n    >>> task_func('./source', './destination')\n    >>> './destination'\n\n    \"\"\"\n    # Ensure the destination directory exists\n    os.makedirs(dst_dir, exist_ok=True)\n\n    # List of file patterns to match\n    patterns = ['*.txt', '*.docx']\n\n    # Iterate over all files in the source directory\n    for root, _, files in os.walk(src_dir):\n        for filename in files:\n            # Check if the file matches any of the patterns\n            if any(fnmatch.fnmatch(filename, pattern) for pattern in patterns):\n                # Construct full file paths\n                src_file = os.path.join(root, filename)\n                dst_file = os.path.join(dst_dir, filename)\n                # Copy the file to the destination directory\n                shutil.copy2(src_file, dst_file)\n\n    return dst_dir", "raw_solution": "import shutil\nimport os\nimport fnmatch\nimport itertools\n\ndef task_func(src_dir, dst_dir):\n    \"\"\"\n    Copy all files from src_dir to dst_dir that match any of the patterns in ['*.txt', '*.docx'].\n\n    Parameters:\n    - src_dir (str): The source directory.\n    - dst_dir (str): The destination directory.\n\n    Returns:\n        str: destination directory.\n\n    Requirements:\n    - shutil\n    - os\n    - fnmatch\n    - itertools\n\n    Example:\n    >>> task_func('./source', './destination')\n    >>> './destination'\n\n    \"\"\"\n    # Ensure the destination directory exists\n    os.makedirs(dst_dir, exist_ok=True)\n\n    # List of file patterns to match\n    patterns = ['*.txt', '*.docx']\n\n    # Iterate over all files in the source directory\n    for root, _, files in os.walk(src_dir):\n        for filename in files:\n            # Check if the file matches any of the patterns\n            if any(fnmatch.fnmatch(filename, pattern) for pattern in patterns):\n                # Construct full file paths\n                src_file = os.path.join(root, filename)\n                dst_file = os.path.join(dst_dir, filename)\n                # Copy the file to the destination directory\n                shutil.copy2(src_file, dst_file)\n\n    return dst_dir\n```\n"}
{"task_id": "BigCodeBench/666", "solution": "from itertools import combinations\nimport math\ndef task_func(seq, letter_weight_dict):\n    max_weight = float('-inf')\n    max_subsequence = \"\"\n\n    for r in range(1, len(seq) + 1):\n        for combination in combinations(seq, r):\n            weight_sum = sum(letter_weight_dict[char] for char in combination)\n            if weight_sum > max_weight:\n                max_weight = weight_sum\n                max_subsequence = combination\n\n    return \"\".join(max_subsequence)", "raw_solution": "from itertools import combinations\nimport math\n\ndef task_func(seq, letter_weight_dict):\n    max_weight = float('-inf')\n    max_subsequence = \"\"\n\n    for r in range(1, len(seq) + 1):\n        for combination in combinations(seq, r):\n            weight_sum = sum(letter_weight_dict[char] for char in combination)\n            if weight_sum > max_weight:\n                max_weight = weight_sum\n                max_subsequence = combination\n\n    return \"\".join(max_subsequence)\n\n# Test cases"}
{"task_id": "BigCodeBench/667", "solution": "import heapq\nimport collections\ndef task_func(x, n):\n    \"\"\"\n    dictionary x mein n sabse aam letter ka pata lagaen, jahan key akshar and maan unki frequencies hain.\n\n    Parameters:\n    - x (dict): letter ki aavrittiyon ka dictionary.\n    - n (int): lautaye jaane vaale sabse frequent letter ki number.\n\n    Returns:\n        list: n sabse frequent akshar.\n\n    Requirements:\n    - heapq\n    - collections\n\n    Example:\n    >>> task_func({'a': 1, 'b': 2, 'c': 3}, 2)\n    ['c', 'b']\n\n    \"\"\"\n    # Convert dictionary to a list of tuples\n    items = list(x.items())\n    \n    # Use heapq to create a max heap based on frequencies\n    max_heap = [(-freq, letter) for letter, freq in items]\n    heapq.heapify(max_heap)\n    \n    # Extract the top n letters with highest frequencies\n    top_n_letters = [heapq.heappop(max_heap)[1] for _ in range(min(n, len(items)))]\n    \n    return top_n_letters", "raw_solution": "import heapq\nimport collections\n\ndef task_func(x, n):\n    \"\"\"\n    dictionary x mein n sabse aam letter ka pata lagaen, jahan key akshar and maan unki frequencies hain.\n\n    Parameters:\n    - x (dict): letter ki aavrittiyon ka dictionary.\n    - n (int): lautaye jaane vaale sabse frequent letter ki number.\n\n    Returns:\n        list: n sabse frequent akshar.\n\n    Requirements:\n    - heapq\n    - collections\n\n    Example:\n    >>> task_func({'a': 1, 'b': 2, 'c': 3}, 2)\n    ['c', 'b']\n\n    \"\"\"\n    # Convert dictionary to a list of tuples\n    items = list(x.items())\n    \n    # Use heapq to create a max heap based on frequencies\n    max_heap = [(-freq, letter) for letter, freq in items]\n    heapq.heapify(max_heap)\n    \n    # Extract the top n letters with highest frequencies\n    top_n_letters = [heapq.heappop(max_heap)[1] for _ in range(min(n, len(items)))]\n    \n    return top_n_letters\n\n# Test cases"}
{"task_id": "BigCodeBench/668", "solution": "import itertools\nimport math\ndef task_func(x):\n    \"\"\"\n    Ek dictionary, x, ka sub gyaat karein jiski total length minimum ho, jahaan keys letters hain aur maan unki length hain.\n\n    Parameters:\n    - x (dict): letter ki length ka dictionary.\n\n    Returns:\n        list: minimum total length wala sub.\n\n    Requirements:\n    - itertools\n    - math\n\n    Example:\n    >>> task_func({'a': 1, 'b': 2, 'c': 3})\n    ['a']\n    >>> task_func({'a': 1, 'b': -2, 'c': -5, 'd': 4})\n    ['b', 'c']\n\n    \"\"\"\n    # Filter out negative values\n    positive_values = {k: v for k, v in x.items() if v >= 0}\n\n    # Sort the dictionary by values\n    sorted_values = sorted(positive_values.items(), key=lambda item: item[1])\n\n    # Initialize variables\n    min_length = math.inf\n    min_sub = []\n\n    # Iterate over all possible combinations of keys\n    for r in range(1, len(sorted_values) + 1):\n        for combination in itertools.combinations(sorted_values, r):\n            total_length = sum([item[1] for item in combination])\n            if total_length < min_length:\n                min_length = total_length\n                min_sub = [item[0] for item in combination]\n\n    return min_sub", "raw_solution": "import itertools\nimport math\n\ndef task_func(x):\n    \"\"\"\n    Ek dictionary, x, ka sub gyaat karein jiski total length minimum ho, jahaan keys letters hain aur maan unki length hain.\n\n    Parameters:\n    - x (dict): letter ki length ka dictionary.\n\n    Returns:\n        list: minimum total length wala sub.\n\n    Requirements:\n    - itertools\n    - math\n\n    Example:\n    >>> task_func({'a': 1, 'b': 2, 'c': 3})\n    ['a']\n    >>> task_func({'a': 1, 'b': -2, 'c': -5, 'd': 4})\n    ['b', 'c']\n\n    \"\"\"\n    # Filter out negative values\n    positive_values = {k: v for k, v in x.items() if v >= 0}\n\n    # Sort the dictionary by values\n    sorted_values = sorted(positive_values.items(), key=lambda item: item[1])\n\n    # Initialize variables\n    min_length = math.inf\n    min_sub = []\n\n    # Iterate over all possible combinations of keys\n    for r in range(1, len(sorted_values) + 1):\n        for combination in itertools.combinations(sorted_values, r):\n            total_length = sum([item[1] for item in combination])\n            if total_length < min_length:\n                min_length = total_length\n                min_sub = [item[0] for item in combination]\n\n    return min_sub\n```\n"}
{"task_id": "BigCodeBench/669", "solution": "import itertools\nimport math\ndef task_func(x):\n    \"\"\"\n    kisi dictionary, x, mein ki pair dhoondhe jiski values ke cosine ka highest adhik sum ho.\n\n    Parameters:\n    - x (dict): ki-value jode ki dictionary.\n\n    Returns:\n        tuple: keys ki pair jiske values ke cosine ka highest adhik sum ho.\n\n    Requirements:\n    - itertools\n    - math\n\n    Example:\n    >>> task_func({'a': 1, 'b': 2, 'c': 3})\n    ('a', 'b')\n    ('a', 'b')\n    >>> task_func({'a': 1, 'b': 2, 'c': 3, 'd': 4})\n    ('a', 'b')\n    ('a', 'b')\n\n    \"\"\"\n    pairs = list(itertools.combinations(x.keys(), 2))\n    max_sum = -math.inf\n    max_pair = None\n\n    for pair in pairs:\n        val1, val2 = x[pair[0]], x[pair[1]]\n        cosine_sum = math.cos(val1) + math.cos(val2)\n        if cosine_sum > max_sum:\n            max_sum = cosine_sum\n            max_pair = pair\n\n    return max_pair", "raw_solution": "import itertools\nimport math\n\ndef task_func(x):\n    \"\"\"\n    kisi dictionary, x, mein ki pair dhoondhe jiski values ke cosine ka highest adhik sum ho.\n\n    Parameters:\n    - x (dict): ki-value jode ki dictionary.\n\n    Returns:\n        tuple: keys ki pair jiske values ke cosine ka highest adhik sum ho.\n\n    Requirements:\n    - itertools\n    - math\n\n    Example:\n    >>> task_func({'a': 1, 'b': 2, 'c': 3})\n    ('a', 'b')\n    ('a', 'b')\n    >>> task_func({'a': 1, 'b': 2, 'c': 3, 'd': 4})\n    ('a', 'b')\n    ('a', 'b')\n\n    \"\"\"\n    pairs = list(itertools.combinations(x.keys(), 2))\n    max_sum = -math.inf\n    max_pair = None\n\n    for pair in pairs:\n        val1, val2 = x[pair[0]], x[pair[1]]\n        cosine_sum = math.cos(val1) + math.cos(val2)\n        if cosine_sum > max_sum:\n            max_sum = cosine_sum\n            max_pair = pair\n\n    return max_pair\n\n# Test cases"}
{"task_id": "BigCodeBench/670", "solution": "from itertools import combinations\nimport math\ndef task_func(x, w):\n    max_weight = float('-inf')\n    max_substr = \"\"\n\n    for i in range(1, len(x) + 1):\n        for combo in combinations(x, i):\n            weight_sum = sum(w[char] for char in combo)\n            if weight_sum > max_weight:\n                max_weight = weight_sum\n                max_substr = combo[0]\n\n    return max_substr", "raw_solution": "from itertools import combinations\nimport math\n\ndef task_func(x, w):\n    max_weight = float('-inf')\n    max_substr = \"\"\n\n    for i in range(1, len(x) + 1):\n        for combo in combinations(x, i):\n            weight_sum = sum(w[char] for char in combo)\n            if weight_sum > max_weight:\n                max_weight = weight_sum\n                max_substr = combo[0]\n\n    return max_substr\n\n# Test cases"}
{"task_id": "BigCodeBench/671", "solution": "import os\nimport random\nimport json\ndef task_func(directory, n):\n    \"\"\"\n    Ek directory mein 'number' key ke saath json content wali n random files banaen, jismein 1 aur 100 ke beech ek random integer maan ho, aur phir pratyek file ki beginning mein cursor ko reset karen.\n\n    Parameters:\n    - directory (str): Vah directory jismein files utpann ki jaani hain.\n    - n (int): Utpann ki jaane wali files ki sankhya.\n\n    Returns:\n        directory (str): Vah directory jahaan files utpann ki gayin.\n\n    Requirements:\n    - os\n    - random\n    - json\n\n    Example:\n    >>> task_func('/path/to/directory', 1)\n    '/path/to/directory'\n\n    \"\"\"\n    for _ in range(n):\n        filename = f\"file_{random.randint(1, 100)}.json\"\n        filepath = os.path.join(directory, filename)\n        with open(filepath, 'w') as f:\n            json.dump({\"number\": random.randint(1, 100)}, f)\n        with open(filepath, 'r+') as f:\n            f.seek(0, 0)\n    return directory", "raw_solution": "import os\nimport random\nimport json\n\ndef task_func(directory, n):\n    \"\"\"\n    Ek directory mein 'number' key ke saath json content wali n random files banaen, jismein 1 aur 100 ke beech ek random integer maan ho, aur phir pratyek file ki beginning mein cursor ko reset karen.\n\n    Parameters:\n    - directory (str): Vah directory jismein files utpann ki jaani hain.\n    - n (int): Utpann ki jaane wali files ki sankhya.\n\n    Returns:\n        directory (str): Vah directory jahaan files utpann ki gayin.\n\n    Requirements:\n    - os\n    - random\n    - json\n\n    Example:\n    >>> task_func('/path/to/directory', 1)\n    '/path/to/directory'\n\n    \"\"\"\n    for _ in range(n):\n        filename = f\"file_{random.randint(1, 100)}.json\"\n        filepath = os.path.join(directory, filename)\n        with open(filepath, 'w') as f:\n            json.dump({\"number\": random.randint(1, 100)}, f)\n        with open(filepath, 'r+') as f:\n            f.seek(0, 0)\n    return directory\n```\n"}
{"task_id": "BigCodeBench/672", "solution": "import csv\nimport sys\ndef task_func(filename):\n    \"\"\"\n    Ek CSV file padhen, lines ke order ko ulta karen and ulti hui lines ko back file mein likhen. then cursor ko file ki beginning mein reset karen.\n\n    Parameters:\n    - filename (str): CSV file ka name.\n\n    Returns:\n        filename (str): CSV file ka name.\n\n    Requirements:\n    - csv\n    - sys\n\n    Example:\n    >>> task_func('file.csv')\n    'file.csv'\n\n    \"\"\"\n    # Read the file\n    with open(filename, 'r') as file:\n        reader = csv.reader(file)\n        lines = list(reader)\n\n    # Reverse the lines\n    lines.reverse()\n\n    # Write the reversed lines back to the file\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(lines)\n\n    # Reset cursor to the beginning of the file\n    with open(filename, 'r') as file:\n        pass\n\n    return filename", "raw_solution": "import csv\nimport sys\n\ndef task_func(filename):\n    \"\"\"\n    Ek CSV file padhen, lines ke order ko ulta karen and ulti hui lines ko back file mein likhen. then cursor ko file ki beginning mein reset karen.\n\n    Parameters:\n    - filename (str): CSV file ka name.\n\n    Returns:\n        filename (str): CSV file ka name.\n\n    Requirements:\n    - csv\n    - sys\n\n    Example:\n    >>> task_func('file.csv')\n    'file.csv'\n\n    \"\"\"\n    # Read the file\n    with open(filename, 'r') as file:\n        reader = csv.reader(file)\n        lines = list(reader)\n\n    # Reverse the lines\n    lines.reverse()\n\n    # Write the reversed lines back to the file\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(lines)\n\n    # Reset cursor to the beginning of the file\n    with open(filename, 'r') as file:\n        pass\n\n    return filename\n```\n"}
{"task_id": "BigCodeBench/673", "solution": "import os\nimport random\ndef task_func(directory, n_files):\n    \"\"\"\n    single specific directory os n yaadrichchhik txt files banaen, pratyek file os only single digit ka yaadrichchhik integer likhen, and then pratyek file ki beginning os cursor ko reset karen.\n    file ke names 'file_1.txt' se shuru hote hain and pratyek file ke lie 1 se increment hain.\n\n    Parameters:\n    - directory (str): vah directory jis os files utpann karnee hain.\n    - n_files (int): utpann ki jaane vaalee fileon ki number.\n\n    Returns:\n        n_files (int): utpann fileon ki number.\n\n    Requirements:\n    - os\n    - random\n\n    Example:\n    >>> random.seed(2)\n    >>> task_func('/path/to/directory', 5)\n    5\n\n    \"\"\"\n    for i in range(n_files):\n        filename = f\"file_{i+1}.txt\"\n        filepath = os.path.join(directory, filename)\n        with open(filepath, 'w') as file:\n            random_number = random.randint(0, 9)\n            file.write(str(random_number))\n            file.seek(0)  # Reset cursor to the beginning of the file\n    return n_files", "raw_solution": "import os\nimport random\n\ndef task_func(directory, n_files):\n    \"\"\"\n    single specific directory os n yaadrichchhik txt files banaen, pratyek file os only single digit ka yaadrichchhik integer likhen, and then pratyek file ki beginning os cursor ko reset karen.\n    file ke names 'file_1.txt' se shuru hote hain and pratyek file ke lie 1 se increment hain.\n\n    Parameters:\n    - directory (str): vah directory jis os files utpann karnee hain.\n    - n_files (int): utpann ki jaane vaalee fileon ki number.\n\n    Returns:\n        n_files (int): utpann fileon ki number.\n\n    Requirements:\n    - os\n    - random\n\n    Example:\n    >>> random.seed(2)\n    >>> task_func('/path/to/directory', 5)\n    5\n\n    \"\"\"\n    for i in range(n_files):\n        filename = f\"file_{i+1}.txt\"\n        filepath = os.path.join(directory, filename)\n        with open(filepath, 'w') as file:\n            random_number = random.randint(0, 9)\n            file.write(str(random_number))\n            file.seek(0)  # Reset cursor to the beginning of the file\n    return n_files\n```\n"}
{"task_id": "BigCodeBench/674", "solution": "import pandas as pd\nimport os\ndef task_func(filename):\n    \"\"\"\n    Ek pandas ki CSV file padho, panktiyon ke order ko ulat do aur ulti hui panktiyon ko file mein vaapas likho. then file ke shuru mein cursor ko vaapas le jao.\n    header ko ulatna nahi chahiye aur file empty ho sakti hai.\n\n    Parameters:\n    - filename (str): CSV file ka name.\n\n    Returns:\n        filename (str): CSV file ka name.\n\n    Requirements:\n    - os\n    - pandas\n\n    Example:\n    >>> task_func('file.csv')\n    'file.csv'\n\n    \"\"\"\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n    \n    df = pd.read_csv(filename, header=None)\n    df = df.iloc[::-1]\n    df.to_csv(filename, header=False, index=False)\n    \n    with open(filename, 'r+') as f:\n        f.seek(0, 0)\n        f.truncate()\n    \n    return filename", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(filename):\n    \"\"\"\n    Ek pandas ki CSV file padho, panktiyon ke order ko ulat do aur ulti hui panktiyon ko file mein vaapas likho. then file ke shuru mein cursor ko vaapas le jao.\n    header ko ulatna nahi chahiye aur file empty ho sakti hai.\n\n    Parameters:\n    - filename (str): CSV file ka name.\n\n    Returns:\n        filename (str): CSV file ka name.\n\n    Requirements:\n    - os\n    - pandas\n\n    Example:\n    >>> task_func('file.csv')\n    'file.csv'\n\n    \"\"\"\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n    \n    df = pd.read_csv(filename, header=None)\n    df = df.iloc[::-1]\n    df.to_csv(filename, header=False, index=False)\n    \n    with open(filename, 'r+') as f:\n        f.seek(0, 0)\n        f.truncate()\n    \n    return filename\n```\n"}
{"task_id": "BigCodeBench/675", "solution": "import os\nimport random\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        filename = f\"file_{i+1}.txt\"\n        filepath = os.path.join(directory, filename)\n        \n        with open(filepath, 'w') as file:\n            random_string = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=10))\n            file.write(random_string)\n            \n            # Reset cursor to the beginning of the file\n            file.seek(0)\n\n    return directory", "raw_solution": "import os\nimport random\n\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        filename = f\"file_{i+1}.txt\"\n        filepath = os.path.join(directory, filename)\n        \n        with open(filepath, 'w') as file:\n            random_string = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=10))\n            file.write(random_string)\n            \n            # Reset cursor to the beginning of the file\n            file.seek(0)\n\n    return directory\n\n# Example usage:\n# print(task_func('/path/to/directory', 5))\n```\nThis script creates a specified number of text files in a given directory, each containing a random string of 10 lowercase letters. It also resets the cursor to the beginning of each file.\n"}
{"task_id": "BigCodeBench/676", "solution": "import pandas as pd\nimport random\nimport numpy as np\ndef task_func(df):\n    def determine_winner(row):\n        if row['score1'] > row['score2']:\n            return row['team1']\n        elif row['score1'] < row['score2']:\n            return row['team2']\n        else:\n            return random.choice([row['team1'], row['team2']])\n\n    df['winner'] = df.apply(determine_winner, axis=1)\n    return df\ndf = pd.DataFrame({\n    'team1': np.random.choice(['Team A', 'Team B', 'Team C', 'Team D', 'Team E'], 20),\n    'team2': np.random.choice(['Team A', 'Team B', 'Team C', 'Team D', 'Team E'], 20),\n    'score1': np.random.randint(0, 10, 20),\n    'score2': np.random.randint(0, 10, 20)\n})", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(df):\n    def determine_winner(row):\n        if row['score1'] > row['score2']:\n            return row['team1']\n        elif row['score1'] < row['score2']:\n            return row['team2']\n        else:\n            return random.choice([row['team1'], row['team2']])\n\n    df['winner'] = df.apply(determine_winner, axis=1)\n    return df\n\n# Example usage:\nimport numpy as np\n\n# Create a sample dataframe\nnp.random.seed(0)  # For reproducibility\ndf = pd.DataFrame({\n    'team1': np.random.choice(['Team A', 'Team B', 'Team C', 'Team D', 'Team E'], 20),\n    'team2': np.random.choice(['Team A', 'Team B', 'Team C', 'Team D', 'Team E'], 20),\n    'score1': np.random.randint(0, 10, 20),\n    'score2': np.random.randint(0, 10, 20)\n})\n\n# Apply the function\ndf = task_func(df)\n\n# Test cases\nassert 'winner' in df.columns\nassert df['winner'].dtype == object\nassert all(winner in ['Team A', 'Team B', 'Team C', 'Team D', 'Team E'] for winner in df['winner'])\n```\n"}
{"task_id": "BigCodeBench/677", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\ndef task_func(df):\n    \"\"\"\n    df mein do variables ke beech ke relationship ka analysis karen.\n    function do variables par ek linear regression karta hai and df mein ek 'predicted' column jorta hai.\n\n    Parameters:\n    - df (pandas.df): 'var1', 'var2' columns ke saath input df.\n\n    Returns:\n        df (pandas.df): 'predicted' column ke saath df.\n\n    Requirements:\n    - numpy\n    - pandas\n    - scipy\n\n    Example:\n    >>> df = pd.DataFrame({'var1': np.random.randn(10),\n    ...                    'var2': np.random.randn(10)})\n    >>> df = task_func(df)\n    >>> assert 'predicted' in df.columns\n    >>> assert len(df) == 10\n    >>> assert len(df.columns) == 3\n\n    \"\"\"\n    # Ensure the DataFrame contains the required columns\n    if 'var1' not in df.columns or 'var2' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'var1' and 'var2' columns.\")\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(df['var1'], df['var2'])\n\n    # Add the predicted values to the DataFrame\n    df['predicted'] = slope * df['var1'] + intercept\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\n\ndef task_func(df):\n    \"\"\"\n    df mein do variables ke beech ke relationship ka analysis karen.\n    function do variables par ek linear regression karta hai and df mein ek 'predicted' column jorta hai.\n\n    Parameters:\n    - df (pandas.df): 'var1', 'var2' columns ke saath input df.\n\n    Returns:\n        df (pandas.df): 'predicted' column ke saath df.\n\n    Requirements:\n    - numpy\n    - pandas\n    - scipy\n\n    Example:\n    >>> df = pd.DataFrame({'var1': np.random.randn(10),\n    ...                    'var2': np.random.randn(10)})\n    >>> df = task_func(df)\n    >>> assert 'predicted' in df.columns\n    >>> assert len(df) == 10\n    >>> assert len(df.columns) == 3\n\n    \"\"\"\n    # Ensure the DataFrame contains the required columns\n    if 'var1' not in df.columns or 'var2' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'var1' and 'var2' columns.\")\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(df['var1'], df['var2'])\n\n    # Add the predicted values to the DataFrame\n    df['predicted'] = slope * df['var1'] + intercept\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    all_data = []\n\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(path, filename)\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n                for item in data:\n                    item['source'] = filename\n                    all_data.append(item)\n\n    df = pd.DataFrame(all_data)\n\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(path, filename)\n            shutil.move(file_path, processed_path)\n\n    return df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    all_data = []\n\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(path, filename)\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n                for item in data:\n                    item['source'] = filename\n                    all_data.append(item)\n\n    df = pd.DataFrame(all_data)\n\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(path, filename)\n            shutil.move(file_path, processed_path)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/679", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(df):\n    # Create a new column 'combination' that contains tuples of all items\n    df['combination'] = list(zip(df['item1'], df['item2'], df['item3'], df['item4'], df['item5']))\n\n    # Count the frequency of each combination\n    combination_counts = Counter(df['combination'])\n\n    return dict(combination_counts)\ndf = pd.DataFrame({\n    'item1': ['a', 'b', 'a'],\n    'item2': ['b', 'c', 'b'],\n    'item3': ['c', 'd', 'c'],\n    'item4': ['d', 'e', 'd'],\n    'item5': ['e', 'f', 'e']\n})", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(df):\n    # Create a new column 'combination' that contains tuples of all items\n    df['combination'] = list(zip(df['item1'], df['item2'], df['item3'], df['item4'], df['item5']))\n\n    # Count the frequency of each combination\n    combination_counts = Counter(df['combination'])\n\n    return dict(combination_counts)\n\n# Example usage:\ndf = pd.DataFrame({\n    'item1': ['a', 'b', 'a'],\n    'item2': ['b', 'c', 'b'],\n    'item3': ['c', 'd', 'c'],\n    'item4': ['d', 'e', 'd'],\n    'item5': ['e', 'f', 'e']\n})\n"}
{"task_id": "BigCodeBench/680", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, features):\n    \"\"\"\n    df mein functions ko standard banaen.\n    Yeh function features par standard scaling lagu karta hai.\n\n    Parameters:\n    - df (pandas.DataFrame): input df.\n    - features (list): standardized kiye jaane wali features ki list. Khaali ho sakta hai.\n\n    Returns:\n        df (pandas.DataFrame): standardized features wala df.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scikit-learn\n\n    Example:\n    >>> np.random.seed(42)\n    >>> df = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])\n    >>> df = task_func(df, ['a', 'b'])\n    >>> df.head(2)\n    a         b         c\n    0  0.608932  0.127900  0.647689\n    1  2.025355  0.031682 -0.234137\n\n    \"\"\"\n    scaler = StandardScaler()\n    df[features] = scaler.fit_transform(df[features])\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, features):\n    \"\"\"\n    df mein functions ko standard banaen.\n    Yeh function features par standard scaling lagu karta hai.\n\n    Parameters:\n    - df (pandas.DataFrame): input df.\n    - features (list): standardized kiye jaane wali features ki list. Khaali ho sakta hai.\n\n    Returns:\n        df (pandas.DataFrame): standardized features wala df.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scikit-learn\n\n    Example:\n    >>> np.random.seed(42)\n    >>> df = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])\n    >>> df = task_func(df, ['a', 'b'])\n    >>> df.head(2)\n    a         b         c\n    0  0.608932  0.127900  0.647689\n    1  2.025355  0.031682 -0.234137\n\n    \"\"\"\n    scaler = StandardScaler()\n    df[features] = scaler.fit_transform(df[features])\n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/681", "solution": "import pandas as pd\nimport json\ndef task_func(file_path, key):\n    \"\"\"\n    Ek JSON file ko ek Pandas df mein load karein, pratyek ele se ek specific key hata dein and sansadhit df ko records dwara unmukhi JSON file mein back likhein.\n\n    Parameters:\n    - file_path (str): JSON file ka path.\n    - key (str): Pratyek ele se hatane ke liye key.\n\n    Returns:\n        df (df): Sansadhit JSON data ka ek pandas df representation.\n\n    Requirements:\n    - pandas\n    - json\n\n    Example:\n    >>> df = task_func('data.json', 'ele')\n\n    \"\"\"\n    # Load JSON data from file\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    # Convert JSON data to Pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Drop the specified key from each record\n    df = df.drop(columns=[key])\n\n    # Convert the DataFrame back to JSON format and write to file\n    df.to_json(file_path, orient='records', lines=True)\n\n    return df", "raw_solution": "import pandas as pd\nimport json\n\n\ndef task_func(file_path, key):\n    \"\"\"\n    Ek JSON file ko ek Pandas df mein load karein, pratyek ele se ek specific key hata dein and sansadhit df ko records dwara unmukhi JSON file mein back likhein.\n\n    Parameters:\n    - file_path (str): JSON file ka path.\n    - key (str): Pratyek ele se hatane ke liye key.\n\n    Returns:\n        df (df): Sansadhit JSON data ka ek pandas df representation.\n\n    Requirements:\n    - pandas\n    - json\n\n    Example:\n    >>> df = task_func('data.json', 'ele')\n\n    \"\"\"\n    # Load JSON data from file\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    # Convert JSON data to Pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Drop the specified key from each record\n    df = df.drop(columns=[key])\n\n    # Convert the DataFrame back to JSON format and write to file\n    df.to_json(file_path, orient='records', lines=True)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/682", "solution": "from collections import Counter\nimport math\ndef task_func(nested_dict):\n    aggregated_values = Counter()\n\n    for key, values in nested_dict.items():\n        if 'ele' in values:\n            del values['ele']\n        aggregated_values.update(values)\n\n    result = {}\n    for key, count in aggregated_values.items():\n        result[key] = math.sin(count)\n\n    return result\nnested_dict = {\n    'dict1': {'ale': 1, 'ele': 2, 'ile': 3},\n    'dict2': {'ele': 4, 'ole': 5, 'ule': 6},\n    'dict3': {'ile': 7, 'ale': 8, 'ele': 9}\n}", "raw_solution": "from collections import Counter\nimport math\n\ndef task_func(nested_dict):\n    aggregated_values = Counter()\n\n    for key, values in nested_dict.items():\n        if 'ele' in values:\n            del values['ele']\n        aggregated_values.update(values)\n\n    result = {}\n    for key, count in aggregated_values.items():\n        result[key] = math.sin(count)\n\n    return result\n\n# Example usage:\nnested_dict = {\n    'dict1': {'ale': 1, 'ele': 2, 'ile': 3},\n    'dict2': {'ele': 4, 'ole': 5, 'ule': 6},\n    'dict3': {'ile': 7, 'ale': 8, 'ele': 9}\n}\n"}
{"task_id": "BigCodeBench/683", "solution": "import math\nimport yaml\ndef task_func(yaml_path, key):\n    \"\"\"\n    Ek YAML file padho, data mein se ek specific key par cosine lagu karo, and phir sanshodhit data ko YAML file mein back likho.\n\n    Parameters:\n    - yaml_path (str): YAML file ka path.\n    - key (str): cosine lene ke liye key.\n\n    Returns:\n        data (dict): sanshodhit YAML data ka ek dictionary representation.\n\n    Requirements:\n    - math\n    - yaml\n\n    Example:\n    >>> yaml_data = task_func('data.yaml', 'ele')\n\n    \"\"\"\n    # Read the YAML file\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n\n    # Calculate cosine of the value associated with the key\n    if key in data:\n        value = data[key]\n        data[key] = math.cos(value)\n    else:\n        raise KeyError(f\"Key '{key}' not found in the YAML file.\")\n\n    # Write the updated data back to the YAML file\n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n\n    return data", "raw_solution": "import math\nimport yaml\n\ndef task_func(yaml_path, key):\n    \"\"\"\n    Ek YAML file padho, data mein se ek specific key par cosine lagu karo, and phir sanshodhit data ko YAML file mein back likho.\n\n    Parameters:\n    - yaml_path (str): YAML file ka path.\n    - key (str): cosine lene ke liye key.\n\n    Returns:\n        data (dict): sanshodhit YAML data ka ek dictionary representation.\n\n    Requirements:\n    - math\n    - yaml\n\n    Example:\n    >>> yaml_data = task_func('data.yaml', 'ele')\n\n    \"\"\"\n    # Read the YAML file\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n\n    # Calculate cosine of the value associated with the key\n    if key in data:\n        value = data[key]\n        data[key] = math.cos(value)\n    else:\n        raise KeyError(f\"Key '{key}' not found in the YAML file.\")\n\n    # Write the updated data back to the YAML file\n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n\n    return data\n```\n"}
{"task_id": "BigCodeBench/684", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(df, col):\n    \"\"\"\n    Process a Pandas DataFrame by removing a specific column and adding an 'isevenindex' column.\n    'isevenindex' column is a boolean flag that indicates whether the index of each row is even or not.\n\n    Parameters:\n    - df (pd.DataFrame): pandas DataFrame to process.\n    - col (str): column to remove.\n\n    Returns:\n        df (pd.DataFrame): processed pandas DataFrame with the specific column removed and a new 'isevenindex' column added.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    >>> np.random.seed(42)\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(5, 4)), columns=list('AbcD'))\n    >>> df = task_func(df, 'c')\n    >>> print(df)\n    A   b   D  isevenindex\n    0  51  92  71         true\n    1  60  20  86        false\n    2  74  74  99         true\n    3  23   2  52        false\n    4   1  87  37         true\n\n    \"\"\"\n    # Remove the specified column\n    df = df.drop(columns=[col])\n\n    # Add a new column 'isevenindex' indicating if the index is even\n    df['isevenindex'] = df.index % 2 == 0\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(df, col):\n    \"\"\"\n    Process a Pandas DataFrame by removing a specific column and adding an 'isevenindex' column.\n    'isevenindex' column is a boolean flag that indicates whether the index of each row is even or not.\n\n    Parameters:\n    - df (pd.DataFrame): pandas DataFrame to process.\n    - col (str): column to remove.\n\n    Returns:\n        df (pd.DataFrame): processed pandas DataFrame with the specific column removed and a new 'isevenindex' column added.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    >>> np.random.seed(42)\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(5, 4)), columns=list('AbcD'))\n    >>> df = task_func(df, 'c')\n    >>> print(df)\n    A   b   D  isevenindex\n    0  51  92  71         true\n    1  60  20  86        false\n    2  74  74  99         true\n    3  23   2  52        false\n    4   1  87  37         true\n\n    \"\"\"\n    # Remove the specified column\n    df = df.drop(columns=[col])\n\n    # Add a new column 'isevenindex' indicating if the index is even\n    df['isevenindex'] = df.index % 2 == 0\n\n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/685", "solution": "from collections import Counter\nfrom itertools import chain\ndef task_func(list_of_lists):\n    \"\"\"\n    Ek list mein se sublists ko ek list mein merged and elements ki count lautaen.\n\n    Parameters:\n    - list_of_lists (list): Sansadhit ki jaane wali list.\n\n    Returns:\n        collections.Counter: Merged list mein elements ki count ke object Counter object.\n\n    Requirements:\n    - itertools\n    - collections\n\n    Example:\n    >>> task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    Counter({1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1})\n\n    \"\"\"\n    merged_list = list(chain(*list_of_lists))\n    element_count = Counter(merged_list)\n    return element_count", "raw_solution": "from collections import Counter\nfrom itertools import chain\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Ek list mein se sublists ko ek list mein merged and elements ki count lautaen.\n\n    Parameters:\n    - list_of_lists (list): Sansadhit ki jaane wali list.\n\n    Returns:\n        collections.Counter: Merged list mein elements ki count ke object Counter object.\n\n    Requirements:\n    - itertools\n    - collections\n\n    Example:\n    >>> task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    Counter({1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1})\n\n    \"\"\"\n    merged_list = list(chain(*list_of_lists))\n    element_count = Counter(merged_list)\n    return element_count\n```\n"}
{"task_id": "BigCodeBench/686", "solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\ndef task_func(list_of_lists):\n    \"\"\"\n    Poorv-nirdhaarit lists ke ek set ko ek list mein milaaya jaata hai and list ke elements ko hot encodes kiya jaata hai.\n\n    Parameters:\n    - list_of_lists (list): Vah list jise sansaadhan kiya jaana hai.\n\n    Returns:\n        one_hot (numpy.array): Mili hui list ka hot encoding.\n\n    Requirements:\n    - numpy\n    - scikit-learn\n\n    Example:\n    >>> task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n    [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n    [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n    [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n    [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n    [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n    [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n    [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n    [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n\n    \"\"\"\n    # Convert the list of lists into a numpy array\n    array = np.array(list_of_lists)\n    \n    # Initialize the OneHotEncoder\n    encoder = OneHotEncoder(sparse=False)\n    \n    # Fit and transform the array\n    one_hot = encoder.fit_transform(array)\n    \n    return one_hot", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Poorv-nirdhaarit lists ke ek set ko ek list mein milaaya jaata hai and list ke elements ko hot encodes kiya jaata hai.\n\n    Parameters:\n    - list_of_lists (list): Vah list jise sansaadhan kiya jaana hai.\n\n    Returns:\n        one_hot (numpy.array): Mili hui list ka hot encoding.\n\n    Requirements:\n    - numpy\n    - scikit-learn\n\n    Example:\n    >>> task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n    [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n    [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n    [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n    [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n    [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n    [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n    [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n    [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n\n    \"\"\"\n    # Convert the list of lists into a numpy array\n    array = np.array(list_of_lists)\n    \n    # Initialize the OneHotEncoder\n    encoder = OneHotEncoder(sparse=False)\n    \n    # Fit and transform the array\n    one_hot = encoder.fit_transform(array)\n    \n    return one_hot\n```\n"}
{"task_id": "BigCodeBench/687", "solution": "import numpy as np\nfrom scipy.stats import mode\ndef task_func(list_of_lists):\n    # Merge all lists into one\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Calculate mode\n    mode_value, mode_count = mode(merged_list)\n    \n    return mode_value, mode_count", "raw_solution": "import numpy as np\nfrom scipy.stats import mode\n\ndef task_func(list_of_lists):\n    # Merge all lists into one\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Calculate mode\n    mode_value, mode_count = mode(merged_list)\n    \n    return mode_value, mode_count\n\n# Example usage:\n# mode_value, mode_count = task_func([[1, 1, 3], [4, 5, 6], [7, 8, 9]])\n# print(f\"Mode value: {mode_value}, Mode count: {mode_count}\")\n```\n"}
{"task_id": "BigCodeBench/688", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    \"\"\"\n    Ek Pandas df, jismein random numeric maan hain, ko sklearn ke standard scaler ke saath standard karein.\n\n    Parameters:\n    - df (df): standardized hone wala df.\n\n    Returns:\n        df_standardized (df): standardized df.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    >>> task_func(df)\n    a         b\n    0 -1.224745 -1.224745\n    1  0.000000  0.000000\n    2  1.224745  1.224745\n\n    \"\"\"\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    \"\"\"\n    Ek Pandas df, jismein random numeric maan hain, ko sklearn ke standard scaler ke saath standard karein.\n\n    Parameters:\n    - df (df): standardized hone wala df.\n\n    Returns:\n        df_standardized (df): standardized df.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    >>> task_func(df)\n    a         b\n    0 -1.224745 -1.224745\n    1  0.000000  0.000000\n    2  1.224745  1.224745\n\n    \"\"\"\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized\n```\n"}
{"task_id": "BigCodeBench/689", "solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\ndef task_func(df):\n    p_values = {}\n    for column in df.columns:\n        p_value = stats.shapiro(df[column])[1]\n        p_values[column] = p_value\n    return p_values", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\n\ndef task_func(df):\n    p_values = {}\n    for column in df.columns:\n        p_value = stats.shapiro(df[column])[1]\n        p_values[column] = p_value\n    return p_values\n\n# Example usage:"}
{"task_id": "BigCodeBench/690", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    \"\"\"\n    Ek Pandas df, jis mein random numeric values hain and columns X and y hain, ka upyog karke sklearn ke linear regression ko data ko ek linear model se milane ke liye karein.\n\n    Parameters:\n    - df (df): Upyog karne ke liye df.\n\n    Returns:\n        model (LinearRegression): Fit kiya gaya linear model.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> import numpy as np\n    >>> np.random.seed(42)\n    >>> df = pd.DataFrame(np.random.normal(size=(100, 2)), columns=['X', 'Y'])\n    >>> model = task_func(df)\n    >>> print(model)\n    LinearRegression()\n\n    \"\"\"\n    # Extracting the features and target from the DataFrame\n    X = df[['X']].values\n    y = df['Y'].values\n\n    # Creating and training the Linear Regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nROWS = 100\nCOLUMNS = ['X', 'Y']\n\ndef task_func(df):\n    \"\"\"\n    Ek Pandas df, jis mein random numeric values hain and columns X and y hain, ka upyog karke sklearn ke linear regression ko data ko ek linear model se milane ke liye karein.\n\n    Parameters:\n    - df (df): Upyog karne ke liye df.\n\n    Returns:\n        model (LinearRegression): Fit kiya gaya linear model.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> import numpy as np\n    >>> np.random.seed(42)\n    >>> df = pd.DataFrame(np.random.normal(size=(100, 2)), columns=['X', 'Y'])\n    >>> model = task_func(df)\n    >>> print(model)\n    LinearRegression()\n\n    \"\"\"\n    # Extracting the features and target from the DataFrame\n    X = df[['X']].values\n    y = df['Y'].values\n\n    # Creating and training the Linear Regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\n```\n"}
{"task_id": "BigCodeBench/691", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    # Standardizing the data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n\n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    kmeans.fit(scaled_data)\n\n    # Returning the cluster labels\n    return kmeans.labels_", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    # Standardizing the data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n\n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    kmeans.fit(scaled_data)\n\n    # Returning the cluster labels\n    return kmeans.labels_\n```\n"}
{"task_id": "BigCodeBench/692", "solution": "import math\nimport pandas as pd\ndef task_func(tuples_list):\n    \"\"\"\n    Given a list of tuples turn them into a Pandas DataFrame with math.sin applied to each number.\n\n    Parameters:\n    - tuples_list (list): The list of tuples.\n\n    Returns:\n        df (DataFrame): A pandas DataFrame. Each row of df corresponds to a tuple from tuples_list, with the values being the sine of the original values in the tuple.\n\n    Requirements:\n    - math\n    - pandas\n\n    Example:\n    >>> df = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)])\n    >>> print(df)\n    0         1         2         3\n    0  0.841471  0.909297  0.141120 -0.756802\n    1 -0.958924 -0.279415  0.656987  0.989358\n    2  0.412118 -0.544021 -0.999990 -0.536573\n\n    \"\"\"\n    # Convert the list of tuples into a list of lists\n    data = [[math.sin(x) for x in tup] for tup in tuples_list]\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=[f'col_{i}' for i in range(len(tuples_list[0]))])\n    \n    return df", "raw_solution": "import math\nimport pandas as pd\n\ndef task_func(tuples_list):\n    \"\"\"\n    Given a list of tuples turn them into a Pandas DataFrame with math.sin applied to each number.\n\n    Parameters:\n    - tuples_list (list): The list of tuples.\n\n    Returns:\n        df (DataFrame): A pandas DataFrame. Each row of df corresponds to a tuple from tuples_list, with the values being the sine of the original values in the tuple.\n\n    Requirements:\n    - math\n    - pandas\n\n    Example:\n    >>> df = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)])\n    >>> print(df)\n    0         1         2         3\n    0  0.841471  0.909297  0.141120 -0.756802\n    1 -0.958924 -0.279415  0.656987  0.989358\n    2  0.412118 -0.544021 -0.999990 -0.536573\n\n    \"\"\"\n    # Convert the list of tuples into a list of lists\n    data = [[math.sin(x) for x in tup] for tup in tuples_list]\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=[f'col_{i}' for i in range(len(tuples_list[0]))])\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/693", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(tuples_list, columns):\n    # Create a DataFrame from the list of tuples\n    df = pd.DataFrame(tuples_list, columns=columns)\n    \n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n    \n    # Apply the scaler to the DataFrame\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n    \n    return df_scaled", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(tuples_list, columns):\n    # Create a DataFrame from the list of tuples\n    df = pd.DataFrame(tuples_list, columns=columns)\n    \n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n    \n    # Apply the scaler to the DataFrame\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n    \n    return df_scaled\n\n# Example usage:\n# df = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)], ['A', 'B', 'C', 'D'])\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/694", "solution": "import itertools\nimport random\ndef task_func(t, n):\n    \"\"\"\n    Ek tuple se length `n` ke sabhi combinations banaen and length `n` ka ek random combination lautaen.\n\n    Parameters:\n    - t (tuple): The tuple.\n    - n (int): The length of the combinations.\n\n    Returns:\n        tuple: input tuple ka ek combinations.\n\n    Requirements:\n    - itertools\n    - random\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func((1, 2, 3, 4), 2)\n    (3, 4)\n\n    \"\"\"\n    # Generate all combinations of length n\n    combinations = list(itertools.combinations(t, n))\n    \n    # Select a random combination\n    random_combination = random.choice(combinations)\n    \n    return random_combination", "raw_solution": "import itertools\nimport random\n\ndef task_func(t, n):\n    \"\"\"\n    Ek tuple se length `n` ke sabhi combinations banaen and length `n` ka ek random combination lautaen.\n\n    Parameters:\n    - t (tuple): The tuple.\n    - n (int): The length of the combinations.\n\n    Returns:\n        tuple: input tuple ka ek combinations.\n\n    Requirements:\n    - itertools\n    - random\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func((1, 2, 3, 4), 2)\n    (3, 4)\n\n    \"\"\"\n    # Generate all combinations of length n\n    combinations = list(itertools.combinations(t, n))\n    \n    # Select a random combination\n    random_combination = random.choice(combinations)\n    \n    return random_combination\n```\n"}
{"task_id": "BigCodeBench/695", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(tuples_list, n_components):\n    \"\"\"\n    Perform principal component analysis (PCA) on a list of tuples.\n\n    Parameters:\n    - tuples_list (list): A list of tuples where each tuple is a sequence of numeric data.\n\n    Returns:\n    - transformed_data (ndarray): The transformed data after PCA.\n\n    Requirements:\n    - numpy\n    - sklearn\n\n    Example:\n    >>> data = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)], 2)\n    >>> print(data)\n    [[ 8.00000000e+00  3.84592537e-16]\n    [ 0.00000000e+00  0.00000000e+00]\n    [-8.00000000e+00  3.84592537e-16]]\n    \"\"\"\n    # Convert list of tuples to numpy array\n    data = np.array(tuples_list)\n\n    # Initialize PCA model\n    pca = PCA(n_components=n_components)\n\n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n\n    return transformed_data", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\n\ndef task_func(tuples_list, n_components):\n    \"\"\"\n    Perform principal component analysis (PCA) on a list of tuples.\n\n    Parameters:\n    - tuples_list (list): A list of tuples where each tuple is a sequence of numeric data.\n\n    Returns:\n    - transformed_data (ndarray): The transformed data after PCA.\n\n    Requirements:\n    - numpy\n    - sklearn\n\n    Example:\n    >>> data = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)], 2)\n    >>> print(data)\n    [[ 8.00000000e+00  3.84592537e-16]\n    [ 0.00000000e+00  0.00000000e+00]\n    [-8.00000000e+00  3.84592537e-16]]\n    \"\"\"\n    # Convert list of tuples to numpy array\n    data = np.array(tuples_list)\n\n    # Initialize PCA model\n    pca = PCA(n_components=n_components)\n\n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n\n    return transformed_data\n```\n"}
{"task_id": "BigCodeBench/696", "solution": "import numpy as np\nimport math\nimport random\nfrom random import uniform\ndef task_func(radius, num_points):\n    points = []\n    for _ in range(num_points):\n        angle = uniform(0, 2 * math.pi)\n        r = radius * math.sqrt(uniform(0, 1))\n        x = r * math.cos(angle)\n        y = r * math.sin(angle)\n        points.append((x, y))\n    return points", "raw_solution": "import numpy as np\nimport math\nimport random\nfrom random import uniform\n\ndef task_func(radius, num_points):\n    points = []\n    for _ in range(num_points):\n        angle = uniform(0, 2 * math.pi)\n        r = radius * math.sqrt(uniform(0, 1))\n        x = r * math.cos(angle)\n        y = r * math.sin(angle)\n        points.append((x, y))\n    return points\n\n# Example usage:\nrandom.seed(42)"}
{"task_id": "BigCodeBench/697", "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Extracting the feature and value columns\n    X = df[['feature']].values\n    y = df['value'].values\n\n    # Creating and fitting the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Getting the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n\n    # Returning the results in a dictionary\n    result = {\n        'coefficients': coefficients.tolist(),\n        'intercept': [intercept]\n    }\n\n    return result", "raw_solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Extracting the feature and value columns\n    X = df[['feature']].values\n    y = df['value'].values\n\n    # Creating and fitting the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Getting the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n\n    # Returning the results in a dictionary\n    result = {\n        'coefficients': coefficients.tolist(),\n        'intercept': [intercept]\n    }\n\n    return result\n\n# Example usage"}
{"task_id": "BigCodeBench/698", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df):\n    # Splitting the dataframe into features and target\n    X = df.drop(columns=['target'])\n    y = df['target']\n    \n    # Splitting the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    return X_train, X_test, y_train, y_test", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df):\n    # Splitting the dataframe into features and target\n    X = df.drop(columns=['target'])\n    y = df['target']\n    \n    # Splitting the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    return X_train, X_test, y_train, y_test\n\n# Example usage:"}
{"task_id": "BigCodeBench/699", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    # Create a DataFrame from the input lists\n    df = pd.DataFrame({'x': x_list, 'y': y_list})\n\n    # Initialize and fit the KMeans model\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(df)\n\n    # Return the labels and centroids\n    return kmeans.labels_, kmeans.cluster_centers_", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    # Create a DataFrame from the input lists\n    df = pd.DataFrame({'x': x_list, 'y': y_list})\n\n    # Initialize and fit the KMeans model\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(df)\n\n    # Return the labels and centroids\n    return kmeans.labels_, kmeans.cluster_centers_\n\n# Example usage:\n# df = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6], 'y': [2, 3, 4, 5, 6, 7]})\n# labels, centroids = task_func([1, 2, 3, 4, 5, 6], [2, 3, 4, 5, 6, 7], 2, 0)\n```\n"}
{"task_id": "BigCodeBench/700", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, cols):\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=cols)\n    \n    # Calculate the correlation matrix\n    correlation_matrix = df.corr()\n    \n    # Print the correlation matrix\n    print(correlation_matrix)\n    \n    return correlation_matrix", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data, cols):\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=cols)\n    \n    # Calculate the correlation matrix\n    correlation_matrix = df.corr()\n    \n    # Print the correlation matrix\n    print(correlation_matrix)\n    \n    return correlation_matrix\n\n# Example usage:\n# correlation_matrix = task_func([[5.1, 3.5, 1.4], [4.9, 3.0, 1.4], [4.7, 3.2, 1.3]], ['x', 'y', 'z'])\n# print(correlation_matrix)\n```\n"}
{"task_id": "BigCodeBench/701", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, target):\n    \"\"\"\n    Perform a linear regression analysis on the given pandas DataFrame.\n\n    Parameters:\n    - df (pd.DataFrame): The pandas DataFrame.\n    - target (str): The target variable.\n\n    Returns:\n        score (float): The R-squared score of the model.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> import numpy as np\n    >>> np.random.seed(42)\n    >>> df = pd.DataFrame({'feature': np.random.rand(100), 'target': np.random.rand(100)})  # explicitly using pd\n    >>> r_squared = task_func(df, 'target')\n    >>> print(r_squared)\n    0.0011582111228732872\n\n    \"\"\"\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model.score(X, y)", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, target):\n    \"\"\"\n    Perform a linear regression analysis on the given pandas DataFrame.\n\n    Parameters:\n    - df (pd.DataFrame): The pandas DataFrame.\n    - target (str): The target variable.\n\n    Returns:\n        score (float): The R-squared score of the model.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> import numpy as np\n    >>> np.random.seed(42)\n    >>> df = pd.DataFrame({'feature': np.random.rand(100), 'target': np.random.rand(100)})  # explicitly using pd\n    >>> r_squared = task_func(df, 'target')\n    >>> print(r_squared)\n    0.0011582111228732872\n\n    \"\"\"\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model.score(X, y)\n```\n"}
{"task_id": "BigCodeBench/702", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(df):\n    \"\"\"\n    df par Principal Component Analysis (PCA) karein and first do main components ko record karein.\n\n    Parameters:\n    - df (df): The pandas df.\n\n    Returns:\n        df_pca (df): df jismein first do principal components 'PC1' and 'PC2' ko columns ke tand par naam diya gaya hai.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> df = pd.DataFrame([[5.1, 3.5, 1.4], [4.9, 3.0, 1.4], [4.7, 3.2, 1.3]], columns = ['x', 'y', 'z'])\n    >>> df_pca = task_func(df)\n    >>> print(df_pca)\n    PC1       PC2\n    0  0.334781 -0.011992\n    1 -0.187649 -0.142630\n    2 -0.147132  0.154622\n\n    \"\"\"\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df)\n    df_pca = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])\n    return df_pca", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\n\ndef task_func(df):\n    \"\"\"\n    df par Principal Component Analysis (PCA) karein and first do main components ko record karein.\n\n    Parameters:\n    - df (df): The pandas df.\n\n    Returns:\n        df_pca (df): df jismein first do principal components 'PC1' and 'PC2' ko columns ke tand par naam diya gaya hai.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> df = pd.DataFrame([[5.1, 3.5, 1.4], [4.9, 3.0, 1.4], [4.7, 3.2, 1.3]], columns = ['x', 'y', 'z'])\n    >>> df_pca = task_func(df)\n    >>> print(df_pca)\n    PC1       PC2\n    0  0.334781 -0.011992\n    1 -0.187649 -0.142630\n    2 -0.147132  0.154622\n\n    \"\"\"\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df)\n    df_pca = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])\n    return df_pca\n```\n"}
{"task_id": "BigCodeBench/703", "solution": "import pandas as pd\nfrom sklearn.cluster import DBSCAN\ndef task_func(data, cols):\n    df = pd.DataFrame(data, columns=cols)\n    dbscan = DBSCAN(eps=3, min_samples=2)\n    df['Cluster'] = dbscan.fit_predict(df)\n    return df\ndata = [[5.1, 3.5], [4.9, 3.0], [4.7, 3.2]]\ncols = ['x', 'y']", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import DBSCAN\n\ndef task_func(data, cols):\n    df = pd.DataFrame(data, columns=cols)\n    dbscan = DBSCAN(eps=3, min_samples=2)\n    df['Cluster'] = dbscan.fit_predict(df)\n    return df\n\n# Example usage:\ndata = [[5.1, 3.5], [4.9, 3.0], [4.7, 3.2]]\ncols = ['x', 'y']\ndf = task_func(data, cols)"}
{"task_id": "BigCodeBench/704", "solution": "import pandas as pd\nfrom itertools import combinations\ndef task_func(data, cols, percentage):\n    df = pd.DataFrame(data, columns=cols)\n    corr_combinations = []\n\n    for col1, col2 in combinations(cols, 2):\n        corr = df[col1].corr(df[col2])\n        if abs(corr) > percentage:\n            corr_combinations.append((col1, col2))\n\n    return corr_combinations", "raw_solution": "import pandas as pd\nfrom itertools import combinations\n\n# Constants\nMIN_PERCENTAGE = 0.75\n\ndef task_func(data, cols, percentage):\n    df = pd.DataFrame(data, columns=cols)\n    corr_combinations = []\n\n    for col1, col2 in combinations(cols, 2):\n        corr = df[col1].corr(df[col2])\n        if abs(corr) > percentage:\n            corr_combinations.append((col1, col2))\n\n    return corr_combinations\n\n# Example usage:\nresult = task_func([[5.1, 5.0, 1.4], [4.9, 4.8, 1.4], [4.7, 4.6, 2.0]], ['x', 'y', 'z'], 0.9)"}
{"task_id": "BigCodeBench/705", "solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\ndef task_func(df, column, alpha):\n    \"\"\"\n    df ke ek particular numeric column ki normality ka test Shapiro-Wilk test se kare,\n    jismein np ka explicitly upayog karne ke liye ek artificial step bhi shamil hai.\n\n    Parameters:\n    - df (pd.df): input df.\n    - column (str): column ka naam.\n    - alpha (float): alpha level.\n\n    Returns:\n        bool: true yadi column normality test pass karta hai, otherwise false.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> import pandas as pd\n    >>> np.random.seed(0)\n    >>> df = pd.DataFrame({'Value': np.random.normal(0, 1, 1000)})\n    >>> print(task_func(df, 'Value', 0.05))\n    true\n\n    \"\"\"\n    # Extract the column data\n    data = df[column].dropna().values\n\n    # Perform the Shapiro-Wilk test\n    stat, p_value = stats.shapiro(data)\n\n    # Check if the p-value is greater than the alpha level\n    return p_value > alpha", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\n\ndef task_func(df, column, alpha):\n    \"\"\"\n    df ke ek particular numeric column ki normality ka test Shapiro-Wilk test se kare,\n    jismein np ka explicitly upayog karne ke liye ek artificial step bhi shamil hai.\n\n    Parameters:\n    - df (pd.df): input df.\n    - column (str): column ka naam.\n    - alpha (float): alpha level.\n\n    Returns:\n        bool: true yadi column normality test pass karta hai, otherwise false.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> import pandas as pd\n    >>> np.random.seed(0)\n    >>> df = pd.DataFrame({'Value': np.random.normal(0, 1, 1000)})\n    >>> print(task_func(df, 'Value', 0.05))\n    true\n\n    \"\"\"\n    # Extract the column data\n    data = df[column].dropna().values\n\n    # Perform the Shapiro-Wilk test\n    stat, p_value = stats.shapiro(data)\n\n    # Check if the p-value is greater than the alpha level\n    return p_value > alpha\n\n# Example usage"}
{"task_id": "BigCodeBench/706", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\ndef task_func(data, columns, target_column):\n    # Convert the numpy array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=columns)\n    \n    # Split the data into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Initialize and train the Logistic Regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Predict on the test set\n    y_pred = model.predict(X_test)\n    \n    # Calculate the accuracy of the model\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef task_func(data, columns, target_column):\n    # Convert the numpy array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=columns)\n    \n    # Split the data into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Initialize and train the Logistic Regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Predict on the test set\n    y_pred = model.predict(X_test)\n    \n    # Calculate the accuracy of the model\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy\n\n# Example usage:\n# import numpy as np\n# np.random.seed(42)\n# data = np.random.randint(0, 100, size=(100, 4))  # Using np to generate random data\n# columns = ['A', 'B', 'C', 'target']\n# print(task_func(data, columns, 'target'))\n```\n"}
{"task_id": "BigCodeBench/707", "solution": "import json\nimport numpy as np\nimport pandas as pd\nimport os\ndef task_func(df):\n    # Check if 'intcol' column exists in the dataframe\n    if 'intcol' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'intcol' column\")\n\n    # Apply logarithm transformation on 'intcol' column\n    df['intcol'] = np.log10(df['intcol'])\n\n    # Save the transformed data to a JSON file\n    with open('intcol.json', 'w') as f:\n        json.dump(df['intcol'].tolist(), f)\n\n    # Return the transformed dataframe\n    return df", "raw_solution": "import json\nimport numpy as np\nimport pandas as pd\nimport os\n\ndef task_func(df):\n    # Check if 'intcol' column exists in the dataframe\n    if 'intcol' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'intcol' column\")\n\n    # Apply logarithm transformation on 'intcol' column\n    df['intcol'] = np.log10(df['intcol'])\n\n    # Save the transformed data to a JSON file\n    with open('intcol.json', 'w') as f:\n        json.dump(df['intcol'].tolist(), f)\n\n    # Return the transformed dataframe\n    return df\n\n# Example usage:\n# df = pd.DataFrame({'intcol': [10, 100, 1000, 10000, 100000]})\n# df_transformed = task_func(df)\n# print(df_transformed)\n```\nThis script first checks if the 'intcol' column exists in the dataframe. If it does, it applies a logarithm transformation to the column using numpy's log10 function. The transformed data is then saved to a JSON file named 'intcol.json'. Finally, the transformed dataframe is returned. The example usage demonstrates how to use the function with a sample dataframe.\n"}
{"task_id": "BigCodeBench/708", "solution": "import json\nimport csv\nimport os\nimport base64\ndef task_func(raw_string, filename, output_dir):\n    # Decode the base64 encoded string\n    decoded_bytes = base64.b64decode(raw_string)\n    decoded_str = decoded_bytes.decode('utf-8')\n    \n    # Load the decoded string as a JSON object\n    data = json.loads(decoded_str)\n    \n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Define the file path\n    file_path = os.path.join(output_dir, f\"{filename}.csv\")\n    \n    # Write the data to a CSV file\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(data.keys())\n        writer.writerow(data.values())\n    \n    return file_path", "raw_solution": "import json\nimport csv\nimport os\nimport base64\n\ndef task_func(raw_string, filename, output_dir):\n    # Decode the base64 encoded string\n    decoded_bytes = base64.b64decode(raw_string)\n    decoded_str = decoded_bytes.decode('utf-8')\n    \n    # Load the decoded string as a JSON object\n    data = json.loads(decoded_str)\n    \n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Define the file path\n    file_path = os.path.join(output_dir, f\"{filename}.csv\")\n    \n    # Write the data to a CSV file\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(data.keys())\n        writer.writerow(data.values())\n    \n    return file_path\n\n# Example usage:\n# print(task_func('eyJrZXkiOiAiVmFsdWUifQ==', 'data', './output'))\n```\n"}
{"task_id": "BigCodeBench/709", "solution": "import base64\nimport re\nfrom html import unescape\nimport textwrap\ndef task_func(raw_string, line_length):\n    # Decode the base64 encoded string\n    decoded_bytes = base64.b64decode(raw_string)\n    decoded_str = decoded_bytes.decode('utf-8')\n\n    # Unescape HTML entities\n    unescaped_str = unescape(decoded_str)\n\n    # Replace multiple spaces with a single space\n    cleaned_str = re.sub(r'\\s+', ' ', unescaped_str)\n\n    # Remove leading and trailing spaces\n    trimmed_str = cleaned_str.strip()\n\n    # Wrap the text to the specified line length\n    wrapped_text = textwrap.fill(trimmed_str, width=line_length)\n\n    return wrapped_text", "raw_solution": "import base64\nimport re\nfrom html import unescape\nimport textwrap\n\ndef task_func(raw_string, line_length):\n    # Decode the base64 encoded string\n    decoded_bytes = base64.b64decode(raw_string)\n    decoded_str = decoded_bytes.decode('utf-8')\n\n    # Unescape HTML entities\n    unescaped_str = unescape(decoded_str)\n\n    # Replace multiple spaces with a single space\n    cleaned_str = re.sub(r'\\s+', ' ', unescaped_str)\n\n    # Remove leading and trailing spaces\n    trimmed_str = cleaned_str.strip()\n\n    # Wrap the text to the specified line length\n    wrapped_text = textwrap.fill(trimmed_str, width=line_length)\n\n    return wrapped_text\n\n# Example usage:\n# print(task_func('SGVsbG8sICBXb3JsZCEgICAg', 5))\n```\n"}
{"task_id": "BigCodeBench/710", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data_path):\n    \"\"\"\n    Ek .csv file se ek dataset ko samanyikrit karta hai.\n\n    Parameters:\n    - data_path (str): .csv data file ka path.\n\n    Returns:\n        df (DataFrame): samanyikrit dataset.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> df = task_func('path_to_data_file.csv')\n\n    \"\"\"\n    # Load the dataset\n    df = pd.read_csv(data_path)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the scaler to the dataframe\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_scaled", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data_path):\n    \"\"\"\n    Ek .csv file se ek dataset ko samanyikrit karta hai.\n\n    Parameters:\n    - data_path (str): .csv data file ka path.\n\n    Returns:\n        df (DataFrame): samanyikrit dataset.\n\n    Requirements:\n    - pandas\n    - sklearn\n\n    Example:\n    >>> df = task_func('path_to_data_file.csv')\n\n    \"\"\"\n    # Load the dataset\n    df = pd.read_csv(data_path)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the scaler to the dataframe\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_scaled\n```\n"}
{"task_id": "BigCodeBench/711", "solution": "import json\nimport csv\ndef task_func(json_file, csv_file):\n    \"\"\"\n    Ek JSON **json_file** ko CSV mein badlen.\n\n    Parameters:\n    - json_file (str): JSON **json_file** ka **path**.\n    - csv_file (str): CSV **json_file** ka **path**.\n\n    Returns:\n        csv_file: **Function** us CSV **json_file** ke **path** ko lautaata hai jise likha gaya tha.\n\n    Requirements:\n    - json\n    - csv\n\n    Example:\n    >>> task_func('path_to_json_file.json', 'path_to_csv_file.csv')\n    'path_to_csv_file.csv'\n\n    \"\"\"\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data[0].keys())\n        for row in data:\n            writer.writerow(row.values())\n\n    return csv_file", "raw_solution": "import json\nimport csv\n\ndef task_func(json_file, csv_file):\n    \"\"\"\n    Ek JSON **json_file** ko CSV mein badlen.\n\n    Parameters:\n    - json_file (str): JSON **json_file** ka **path**.\n    - csv_file (str): CSV **json_file** ka **path**.\n\n    Returns:\n        csv_file: **Function** us CSV **json_file** ke **path** ko lautaata hai jise likha gaya tha.\n\n    Requirements:\n    - json\n    - csv\n\n    Example:\n    >>> task_func('path_to_json_file.json', 'path_to_csv_file.csv')\n    'path_to_csv_file.csv'\n\n    \"\"\"\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data[0].keys())\n        for row in data:\n            writer.writerow(row.values())\n\n    return csv_file\n```\n"}
{"task_id": "BigCodeBench/712", "solution": "import os\nimport shutil\nimport glob\ndef task_func(source_dir, dest_dir, extension):\n    \"\"\"\n    Ek particular extension wali sabhi files ko ek **directory** se dusri **directory** mein le jaayein.\n\n    Parameters:\n    - source_dir (str): The source directory.\n    - dest_dir (str): The destination directory.\n    - extension (str): The file extension.\n\n    Returns:\n        result (int): The count of files that were moved.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> task_func('path_to_source_dir', 'path_to_dest_dir', '.txt')\n    10\n\n    \"\"\"\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    files = glob.glob(os.path.join(source_dir, f'*{extension}'))\n    moved_files_count = 0\n\n    for file in files:\n        shutil.move(file, dest_dir)\n        moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(source_dir, dest_dir, extension):\n    \"\"\"\n    Ek particular extension wali sabhi files ko ek **directory** se dusri **directory** mein le jaayein.\n\n    Parameters:\n    - source_dir (str): The source directory.\n    - dest_dir (str): The destination directory.\n    - extension (str): The file extension.\n\n    Returns:\n        result (int): The count of files that were moved.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> task_func('path_to_source_dir', 'path_to_dest_dir', '.txt')\n    10\n\n    \"\"\"\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    files = glob.glob(os.path.join(source_dir, f'*{extension}'))\n    moved_files_count = 0\n\n    for file in files:\n        shutil.move(file, dest_dir)\n        moved_files_count += 1\n\n    return moved_files_count\n```\n"}
{"task_id": "BigCodeBench/713", "solution": "import os\nimport re\ndef task_func(log_file_path: str, keywords: list):\n    formatted_lines = []\n    pattern = '|'.join(map(re.escape, keywords))\n    \n    with open(log_file_path, 'r') as file:\n        for line in file:\n            match = re.search(pattern, line)\n            if match:\n                keyword = match.group(0)\n                timestamp = re.search(r'\\d{2}:\\d{2}:\\d{2}', line).group(0)\n                message = re.sub(r'\\s+', ' ', line.split(keyword)[-1].strip())\n                formatted_line = f\"    {keyword} :    {timestamp} : {message}\"\n                formatted_lines.append(formatted_line)\n    \n    return formatted_lines", "raw_solution": "import os\nimport re\n\ndef task_func(log_file_path: str, keywords: list):\n    formatted_lines = []\n    pattern = '|'.join(map(re.escape, keywords))\n    \n    with open(log_file_path, 'r') as file:\n        for line in file:\n            match = re.search(pattern, line)\n            if match:\n                keyword = match.group(0)\n                timestamp = re.search(r'\\d{2}:\\d{2}:\\d{2}', line).group(0)\n                message = re.sub(r'\\s+', ' ', line.split(keyword)[-1].strip())\n                formatted_line = f\"    {keyword} :    {timestamp} : {message}\"\n                formatted_lines.append(formatted_line)\n    \n    return formatted_lines\n```\n"}
{"task_id": "BigCodeBench/714", "solution": "import sys\nfrom pathlib import Path\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    sys.path mein ek specific path jodein and yadi vah maujood nahi hai to us path mein ek directory banaen. \n    >>> task_func()\n    \"/path/to/whatever\"\n\n    Notes:\n        function ek constant PATH_TO_APPEND ka upyog karta hai jo default ruup se '/path/to/whatever' hai.\n\n    Parameters:\n    - path_to_append (str): sys.path mein jodne and ek directory banane ka path. Default '/path/to/whatever' hai.\n\n    Returns:\n        path_to_append (str): Vah path jise joda gaya tha and jahan directory banai gayi thi.\n\n    Requirements:\n    - sys\n    - pathlib\n\n    Example:\n    Examples:\n    >>> task_func(\"/new/path/to/append\")\n    \"/new/path/to/append\"\n\n    \"\"\"\n    path = Path(path_to_append)\n    if not path.exists():\n        path.mkdir(parents=True)\n    sys.path.append(str(path))\n    return str(path)", "raw_solution": "import sys\nfrom pathlib import Path\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef task_func(path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    sys.path mein ek specific path jodein and yadi vah maujood nahi hai to us path mein ek directory banaen. \n    >>> task_func()\n    \"/path/to/whatever\"\n\n    Notes:\n        function ek constant PATH_TO_APPEND ka upyog karta hai jo default ruup se '/path/to/whatever' hai.\n\n    Parameters:\n    - path_to_append (str): sys.path mein jodne and ek directory banane ka path. Default '/path/to/whatever' hai.\n\n    Returns:\n        path_to_append (str): Vah path jise joda gaya tha and jahan directory banai gayi thi.\n\n    Requirements:\n    - sys\n    - pathlib\n\n    Example:\n    Examples:\n    >>> task_func(\"/new/path/to/append\")\n    \"/new/path/to/append\"\n\n    \"\"\"\n    path = Path(path_to_append)\n    if not path.exists():\n        path.mkdir(parents=True)\n    sys.path.append(str(path))\n    return str(path)\n\n# Test cases"}
{"task_id": "BigCodeBench/715", "solution": "import sys\nimport subprocess\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Python ke ek specific version par jaaen and sys.path mein ek specific path joden.\n\n    Notes:\n        Yah function global python_version ko badalta hai and carefully upayog kiya jaana chaahie.\n\n    Parameters:\n    - python_version (str): Vah python_version jis par switch karna hai. Default '3.8' hai.\n    - path_to_append (str): sys.path mein jodne ka path. Default '/path/to/whatever' hai.\n\n    Returns:\n        python_version (str): Vah python_version jis par switch kiya gaya tha.\n\n    Requirements:\n    - sys\n    - subprocess\n\n    Example:\n    >>> task_func('3.7', '/path/to/new_directory')\n    '3.7'\n\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Change the Python version using subprocess\n    subprocess.run(['sudo', 'update-alternatives', '--set', 'python', f'/usr/bin/python{python_version}'])\n\n    return python_version", "raw_solution": "import sys\nimport subprocess\n\n# Constants\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Python ke ek specific version par jaaen and sys.path mein ek specific path joden.\n\n    Notes:\n        Yah function global python_version ko badalta hai and carefully upayog kiya jaana chaahie.\n\n    Parameters:\n    - python_version (str): Vah python_version jis par switch karna hai. Default '3.8' hai.\n    - path_to_append (str): sys.path mein jodne ka path. Default '/path/to/whatever' hai.\n\n    Returns:\n        python_version (str): Vah python_version jis par switch kiya gaya tha.\n\n    Requirements:\n    - sys\n    - subprocess\n\n    Example:\n    >>> task_func('3.7', '/path/to/new_directory')\n    '3.7'\n\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Change the Python version using subprocess\n    subprocess.run(['sudo', 'update-alternatives', '--set', 'python', f'/usr/bin/python{python_version}'])\n\n    return python_version\n```\n"}
{"task_id": "BigCodeBench/716", "solution": "import sys\nimport json\nfrom datetime import datetime\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\ndef task_func(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Load the JSON file\n    with open(json_file, 'r') as file:\n        json_data = json.load(file)\n\n    # Update the 'last_updated' key with the current datetime\n    json_data['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    # Save the updated JSON data\n    with open(json_file, 'w') as file:\n        json.dump(json_data, file, indent=4)\n\n    return json_data", "raw_solution": "import sys\nimport json\nfrom datetime import datetime\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\n\ndef task_func(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Load the JSON file\n    with open(json_file, 'r') as file:\n        json_data = json.load(file)\n\n    # Update the 'last_updated' key with the current datetime\n    json_data['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    # Save the updated JSON data\n    with open(json_file, 'w') as file:\n        json.dump(json_data, file, indent=4)\n\n    return json_data\n\n# Example usage:\n# print(task_func('/path/to/new_directory', '/path/to/new_json_file.json'))\n```\nThis script appends a given path to `sys.path`, loads a JSON file, updates the 'last_updated' key with the current date and time, and then saves the updated JSON data back to the file. The function returns the updated JSON data.\n"}
{"task_id": "BigCodeBench/717", "solution": "import sys\nfrom configparser import ConfigParser\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\ndef task_func(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    \"\"\"\n    sys.path mein ek vishesh path joden aur is path ke saath ek configuration file ko update karen.\n\n    Parameters:\n    - path_to_append (str): sys.path mein jodne ke liye path. default '/path/to/whatever' hai.\n    - config_file (str): update karne ke liye config file ka path. default '/path/to/config.ini' hai.\n\n    Returns:\n        config (object): vah object jismein update kiya gaya configuration hai.\n        config_file (str): configuration file ka path jise abhi badla gaya.\n\n    Requirements:\n    - sys\n    - configparser.ConfigParser\n\n    Example:\n    >>> config = task_func('/path/to/new_directory', '/path/to/new_config.ini')\n    >>> 'path_to_append' in config['DEFAULT']\n    True\n\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Read and update the configuration file\n    config = ConfigParser()\n    config.read(config_file)\n\n    # Update the configuration file\n    config['DEFAULT']['path_to_append'] = path_to_append\n\n    # Write the updated configuration back to the file\n    with open(config_file, 'w') as configfile:\n        config.write(configfile)\n\n    return config, config_file", "raw_solution": "import sys\nfrom configparser import ConfigParser\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\n\ndef task_func(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    \"\"\"\n    sys.path mein ek vishesh path joden aur is path ke saath ek configuration file ko update karen.\n\n    Parameters:\n    - path_to_append (str): sys.path mein jodne ke liye path. default '/path/to/whatever' hai.\n    - config_file (str): update karne ke liye config file ka path. default '/path/to/config.ini' hai.\n\n    Returns:\n        config (object): vah object jismein update kiya gaya configuration hai.\n        config_file (str): configuration file ka path jise abhi badla gaya.\n\n    Requirements:\n    - sys\n    - configparser.ConfigParser\n\n    Example:\n    >>> config = task_func('/path/to/new_directory', '/path/to/new_config.ini')\n    >>> 'path_to_append' in config['DEFAULT']\n    True\n\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Read and update the configuration file\n    config = ConfigParser()\n    config.read(config_file)\n\n    # Update the configuration file\n    config['DEFAULT']['path_to_append'] = path_to_append\n\n    # Write the updated configuration back to the file\n    with open(config_file, 'w') as configfile:\n        config.write(configfile)\n\n    return config, config_file\n```\n"}
{"task_id": "BigCodeBench/718", "solution": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\ndef task_func(text1, text2):\n    \"\"\"\n    Do a paired t-test for the number of words in two strings, only if the strings produce words in the same number.\n\n    Parameters:\n    - text1 (str), text2 (str): Do text strings.\n\n    Returns:\n        t_statistic (float): t-statistic, or NaN agar tests unequal lengths ke karan nahi kiye ja sakte hain.\n        p_value (float): p_value, or NaN agar tests unequal lengths ke karan nahi kiye ja sakte hain.\n\n    Requirements:\n    - re\n    - numpy\n    - scipy\n\n    Example:\n    >>> task_func('Words, words, words.', 'And more words!')\n    (1.7320508075688774, 0.22540333075851657)\n\n    \"\"\"\n    # Extract words using regular expressions\n    words1 = re.findall(r'\\b\\w+\\b', text1)\n    words2 = re.findall(r'\\b\\w+\\b', text2)\n\n    # Check if both texts have the same number of words\n    if len(words1) != len(words2):\n        return np.nan, np.nan\n\n    # Perform a paired t-test\n    t_statistic, p_value = ttest_rel(words1, words2)\n\n    return t_statistic, p_value", "raw_solution": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\n\ndef task_func(text1, text2):\n    \"\"\"\n    Do a paired t-test for the number of words in two strings, only if the strings produce words in the same number.\n\n    Parameters:\n    - text1 (str), text2 (str): Do text strings.\n\n    Returns:\n        t_statistic (float): t-statistic, or NaN agar tests unequal lengths ke karan nahi kiye ja sakte hain.\n        p_value (float): p_value, or NaN agar tests unequal lengths ke karan nahi kiye ja sakte hain.\n\n    Requirements:\n    - re\n    - numpy\n    - scipy\n\n    Example:\n    >>> task_func('Words, words, words.', 'And more words!')\n    (1.7320508075688774, 0.22540333075851657)\n\n    \"\"\"\n    # Extract words using regular expressions\n    words1 = re.findall(r'\\b\\w+\\b', text1)\n    words2 = re.findall(r'\\b\\w+\\b', text2)\n\n    # Check if both texts have the same number of words\n    if len(words1) != len(words2):\n        return np.nan, np.nan\n\n    # Perform a paired t-test\n    t_statistic, p_value = ttest_rel(words1, words2)\n\n    return t_statistic, p_value\n```\n"}
{"task_id": "BigCodeBench/719", "solution": "import re\nimport os\nimport glob\ndef task_func(directory, word):\n    \"\"\"\n    Ek nirdeshika mein un files ki **number** ginen jinmein ek **specific** **shabd** hai.\n\n    Parameters:\n    - directory (str): The directory path.\n    - word (str): The word to search for.\n\n    Returns:\n        count (int): Un **files** ki **number** jinmein diya gaya **shabd** hai.\n\n    Requirements:\n    - re\n    - os\n    - glob\n\n    Example:\n    >>> task_func('./documents', 'word')\n    2\n    >>> task_func('./documents', 'apple')\n    3\n\n    \"\"\"\n    # Get list of all files in the directory\n    files = glob.glob(os.path.join(directory, '*'))\n\n    # Initialize count\n    count = 0\n\n    # Loop through each file\n    for file in files:\n        # Open file and read content\n        with open(file, 'r') as f:\n            content = f.read()\n\n            # Search for the word using regex\n            if re.search(r'\\b' + re.escape(word) + r'\\b', content):\n                count += 1\n\n    return count", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(directory, word):\n    \"\"\"\n    Ek nirdeshika mein un files ki **number** ginen jinmein ek **specific** **shabd** hai.\n\n    Parameters:\n    - directory (str): The directory path.\n    - word (str): The word to search for.\n\n    Returns:\n        count (int): Un **files** ki **number** jinmein diya gaya **shabd** hai.\n\n    Requirements:\n    - re\n    - os\n    - glob\n\n    Example:\n    >>> task_func('./documents', 'word')\n    2\n    >>> task_func('./documents', 'apple')\n    3\n\n    \"\"\"\n    # Get list of all files in the directory\n    files = glob.glob(os.path.join(directory, '*'))\n\n    # Initialize count\n    count = 0\n\n    # Loop through each file\n    for file in files:\n        # Open file and read content\n        with open(file, 'r') as f:\n            content = f.read()\n\n            # Search for the word using regex\n            if re.search(r'\\b' + re.escape(word) + r'\\b', content):\n                count += 1\n\n    return count\n```\n"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the path for the output file\n    output_file_path = \"task_func_data/Output.txt\"\n    \n    # Create a directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n    \n    # Generate random temperature and humidity data\n    temperature = random.uniform(20.0, 30.0)\n    humidity = random.uniform(40.0, 60.0)\n    \n    # Get the current date and time\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # Open the file in append mode\n    with open(output_file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the data to the CSV file\n        writer.writerow([current_time, temperature, humidity])\n    \n    # Return the path of the output file\n    return output_file_path", "raw_solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Define the path for the output file\n    output_file_path = \"task_func_data/Output.txt\"\n    \n    # Create a directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n    \n    # Generate random temperature and humidity data\n    temperature = random.uniform(20.0, 30.0)\n    humidity = random.uniform(40.0, 60.0)\n    \n    # Get the current date and time\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # Open the file in append mode\n    with open(output_file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the data to the CSV file\n        writer.writerow([current_time, temperature, humidity])\n    \n    # Return the path of the output file\n    return output_file_path\n```\n"}
{"task_id": "BigCodeBench/721", "solution": "import os\nimport csv\nfrom collections import Counter\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return None\n\n    with open(file_path, newline='', encoding='utf-8') as csvfile:\n        reader = csv.reader(csvfile)\n        words = []\n\n        for row in reader:\n            for word in row:\n                words.append(word)\n\n        word_counts = Counter(words)\n        most_common_word, frequency = word_counts.most_common(1)[0]\n\n        return most_common_word, frequency", "raw_solution": "import os\nimport csv\nfrom collections import Counter\n\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return None\n\n    with open(file_path, newline='', encoding='utf-8') as csvfile:\n        reader = csv.reader(csvfile)\n        words = []\n\n        for row in reader:\n            for word in row:\n                words.append(word)\n\n        word_counts = Counter(words)\n        most_common_word, frequency = word_counts.most_common(1)[0]\n\n        return most_common_word, frequency\n\n# Example usage:\n# Assuming 'example.csv' contains the following data:\n# word1,word2,word1\n# word3,word1,word2\n# word1,word4,word3\n# The function call should return ('word1', 3) as 'word1' is the most common word with a frequency of 3.\n```\n"}
{"task_id": "BigCodeBench/722", "solution": "import urllib.request\nimport os\nimport re\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\ndef task_func(url):\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    # Read the file\n    with open(TARGET_FILE, 'r') as file:\n        content = file.read()\n\n    # Find occurrences of 'ERROR'\n    occurrences = len(re.findall(SEARCH_PATTERN, content, re.IGNORECASE))\n\n    # Clean up by removing the downloaded file\n    os.remove(TARGET_FILE)\n\n    return occurrences", "raw_solution": "import urllib.request\nimport os\nimport re\n\n# Constants\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\n\ndef task_func(url):\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    # Read the file\n    with open(TARGET_FILE, 'r') as file:\n        content = file.read()\n\n    # Find occurrences of 'ERROR'\n    occurrences = len(re.findall(SEARCH_PATTERN, content, re.IGNORECASE))\n\n    # Clean up by removing the downloaded file\n    os.remove(TARGET_FILE)\n\n    return occurrences\n\n# Example usage:\n# print(task_func('http://example.com/log.txt'))\n```\nThis script downloads a file from the provided URL, reads its content, counts the occurrences of the word 'ERROR' (case-insensitive), and then cleans up by removing the downloaded file. The function returns the count of occurrences.\n"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrape data from the given URL and save it to a CSV file.\n\n    Parameters:\n    - url (str): The URL to scrape data from.\n\n    Returns:\n        CSV_FILE_PATH (str): The path of the CSV file where the scraped data is saved.\n\n    Requirements:\n    - urllib\n    - bs4\n    - csv\n    - os\n\n    Example:\n    >>> task_func('http://www.example.com/')\n    'scraped_data.csv'\n    \"\"\"\n    # Fetch the HTML content from the URL\n    response = urllib.request.urlopen(url)\n    html_content = response.read()\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # Extract data from the HTML (for example, all paragraph texts)\n    data = []\n    for paragraph in soup.find_all('p'):\n        data.append(paragraph.get_text())\n\n    # Write the data to a CSV file\n    with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        for row in data:\n            writer.writerow([row])\n\n    return CSV_FILE_PATH", "raw_solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Scrape data from the given URL and save it to a CSV file.\n\n    Parameters:\n    - url (str): The URL to scrape data from.\n\n    Returns:\n        CSV_FILE_PATH (str): The path of the CSV file where the scraped data is saved.\n\n    Requirements:\n    - urllib\n    - bs4\n    - csv\n    - os\n\n    Example:\n    >>> task_func('http://www.example.com/')\n    'scraped_data.csv'\n    \"\"\"\n    # Fetch the HTML content from the URL\n    response = urllib.request.urlopen(url)\n    html_content = response.read()\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # Extract data from the HTML (for example, all paragraph texts)\n    data = []\n    for paragraph in soup.find_all('p'):\n        data.append(paragraph.get_text())\n\n    # Write the data to a CSV file\n    with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        for row in data:\n            writer.writerow([row])\n\n    return CSV_FILE_PATH\n```\n"}
{"task_id": "BigCodeBench/724", "solution": "import os\nimport json\ndef task_func(config_path: str) -> dict:\n    \"\"\"\n    Ek json configuration file load karein and configuration dictionary lautayein.\n\n    Parameters:\n    - config_path (str): configuration file ka Path.\n\n    Returns:\n        config (dict): file se load hua configuration dictionary.\n\n    Requirements:\n    - os\n    - json\n\n    Raises:\n    - FileNotFoundError: Agar di gayi configuration file maujood nahi hai.\n\n    Example:\n    >>> task_func(\"config.json\")\n    {'key': 'value', 'setting': true}\n\n    \"\"\"\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file at {config_path} does not exist.\")\n    \n    with open(config_path, 'r') as file:\n        config = json.load(file)\n    \n    return config", "raw_solution": "import os\nimport json\n\ndef task_func(config_path: str) -> dict:\n    \"\"\"\n    Ek json configuration file load karein and configuration dictionary lautayein.\n\n    Parameters:\n    - config_path (str): configuration file ka Path.\n\n    Returns:\n        config (dict): file se load hua configuration dictionary.\n\n    Requirements:\n    - os\n    - json\n\n    Raises:\n    - FileNotFoundError: Agar di gayi configuration file maujood nahi hai.\n\n    Example:\n    >>> task_func(\"config.json\")\n    {'key': 'value', 'setting': true}\n\n    \"\"\"\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file at {config_path} does not exist.\")\n    \n    with open(config_path, 'r') as file:\n        config = json.load(file)\n    \n    return config\n```\n"}
{"task_id": "BigCodeBench/725", "solution": "import codecs\nimport os\nimport glob\nDIRECTORY_PATH = './files/'\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Ek nirdisht directory mein sabhi text files ki encoding ko ek encoding se doosre mein badlen.\n    function files ko in-place sanshodhit karta hai.\n\n    Parameters:\n    - directory (str): vah directory jahaan text files sthit hain. Default './files/' hai.\n    - from_encoding (str): text files ki original encoding. Default 'cp1251' hai.\n    - to_encoding (str): vah encoding jismein text files ko badla jaana chahiye. Default 'utf8' hai.\n\n    Returns:\n        none\n\n    Requirements:\n    - codecs\n    - os\n    - glob\n\n    Example:\n    >>> task_func('./files/', 'cp1251', 'utf8')  # './files/' mein sabhi .txt files ko 'cp1251' se 'utf8' mein badalta hai\n    >>> task_func('./other_files/', 'utf8', 'ascii')  # './other_files/' mein sabhi .txt files ko 'utf8' se 'ascii' mein badalta hai\n\n    \"\"\"\n    # Ensure the directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Get all text files in the directory\n    text_files = glob.glob(os.path.join(directory, '*.txt'))\n\n    for file_path in text_files:\n        # Read the file with the original encoding\n        with codecs.open(file_path, 'r', encoding=from_encoding) as file:\n            content = file.read()\n\n        # Write the content to the file with the new encoding\n        with codecs.open(file_path, 'w', encoding=to_encoding) as file:\n            file.write(content)", "raw_solution": "import codecs\nimport os\nimport glob\n\n# Constants\nDIRECTORY_PATH = './files/'\n\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Ek nirdisht directory mein sabhi text files ki encoding ko ek encoding se doosre mein badlen.\n    function files ko in-place sanshodhit karta hai.\n\n    Parameters:\n    - directory (str): vah directory jahaan text files sthit hain. Default './files/' hai.\n    - from_encoding (str): text files ki original encoding. Default 'cp1251' hai.\n    - to_encoding (str): vah encoding jismein text files ko badla jaana chahiye. Default 'utf8' hai.\n\n    Returns:\n        none\n\n    Requirements:\n    - codecs\n    - os\n    - glob\n\n    Example:\n    >>> task_func('./files/', 'cp1251', 'utf8')  # './files/' mein sabhi .txt files ko 'cp1251' se 'utf8' mein badalta hai\n    >>> task_func('./other_files/', 'utf8', 'ascii')  # './other_files/' mein sabhi .txt files ko 'utf8' se 'ascii' mein badalta hai\n\n    \"\"\"\n    # Ensure the directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Get all text files in the directory\n    text_files = glob.glob(os.path.join(directory, '*.txt'))\n\n    for file_path in text_files:\n        # Read the file with the original encoding\n        with codecs.open(file_path, 'r', encoding=from_encoding) as file:\n            content = file.read()\n\n        # Write the content to the file with the new encoding\n        with codecs.open(file_path, 'w', encoding=to_encoding) as file:\n            file.write(content)\n```\n"}
{"task_id": "BigCodeBench/726", "solution": "import re\nimport random\nfrom nltk.corpus import words\nfrom random import sample\nimport nltk\nSAMPLE_ENGLISH_WORDS = set(words.words())\ndef task_func(s, n):\n    # Extract words from the string\n    extracted_words = re.findall(r'\\b\\w+\\b', s.lower())\n\n    # Filter out only English words\n    english_words = [word for word in extracted_words if word in SAMPLE_ENGLISH_WORDS]\n\n    # Randomly sample n words from the list of English words\n    sampled_words = sample(english_words, min(n, len(english_words)))\n\n    return sampled_words\ns = 'Yah kuchh yaadrichchhik words ke saath ek example string hai: Apple, banana, Test, hello, world'", "raw_solution": "import re\nimport random\nfrom nltk.corpus import words\nfrom random import sample\n\n# Ensure the words corpus is downloaded\nimport nltk\nnltk.download('words')\n\n# Constants\nSAMPLE_ENGLISH_WORDS = set(words.words())  # Correct initialization\n\ndef task_func(s, n):\n    # Extract words from the string\n    extracted_words = re.findall(r'\\b\\w+\\b', s.lower())\n\n    # Filter out only English words\n    english_words = [word for word in extracted_words if word in SAMPLE_ENGLISH_WORDS]\n\n    # Randomly sample n words from the list of English words\n    sampled_words = sample(english_words, min(n, len(english_words)))\n\n    return sampled_words\n\n# Example usage\ns = 'Yah kuchh yaadrichchhik words ke saath ek example string hai: Apple, banana, Test, hello, world'"}
{"task_id": "BigCodeBench/727", "solution": "import re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nSENTENCES = ['This is a sentence', 'Another sentence here', 'More sentences']\ndef task_func(s: str) -> np.ndarray:\n    \"\"\"\n    Bag-of-words model ka upyog kar ke ek 'string' ka vectorize karein. 'string' ko 'words' mein vibhajit kiya gaya hai and pratyek 'word' ko ek 'attribute' ke roop mein mana jata hai. Pratyek 'attribute' ka 'value' 'string' mein 'word' ki 'occurrences' ki 'number' hai. 'Function' 'vectorization' ke liye kuchh poorvnirdharit 'sentences' ('SENTENCES constant') ka 'also' upyog karti hai.\n\n    Parameters:\n    - s (str): The string to vectorize.\n\n    Returns:\n        np.ndarray: A numpy array with the vectorized string.\n\n    Requirements:\n    - re\n    - sklearn.feature_extraction.text.CountVectorizer\n    - numpy\n\n    Example:\n    >>> s = 'This is a test string.'\n    >>> vec = task_func(s)\n    >>> print(vec)\n    [0 0 1 0 0 0 1 1 1]\n\n    \"\"\"\n    # Preprocess the string to remove punctuation and make lowercase\n    processed_s = re.sub(r'[^\\w\\s]', '', s).lower()\n\n    # Initialize the CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    sentence_vectors = vectorizer.fit_transform(SENTENCES)\n\n    # Transform the input string\n    input_vector = vectorizer.transform([processed_s])\n\n    # Return the vectorized string as a numpy array\n    return input_vector.toarray()[0]", "raw_solution": "import re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Constants\nSENTENCES = ['This is a sentence', 'Another sentence here', 'More sentences']\n\ndef task_func(s: str) -> np.ndarray:\n    \"\"\"\n    Bag-of-words model ka upyog kar ke ek 'string' ka vectorize karein. 'string' ko 'words' mein vibhajit kiya gaya hai and pratyek 'word' ko ek 'attribute' ke roop mein mana jata hai. Pratyek 'attribute' ka 'value' 'string' mein 'word' ki 'occurrences' ki 'number' hai. 'Function' 'vectorization' ke liye kuchh poorvnirdharit 'sentences' ('SENTENCES constant') ka 'also' upyog karti hai.\n\n    Parameters:\n    - s (str): The string to vectorize.\n\n    Returns:\n        np.ndarray: A numpy array with the vectorized string.\n\n    Requirements:\n    - re\n    - sklearn.feature_extraction.text.CountVectorizer\n    - numpy\n\n    Example:\n    >>> s = 'This is a test string.'\n    >>> vec = task_func(s)\n    >>> print(vec)\n    [0 0 1 0 0 0 1 1 1]\n\n    \"\"\"\n    # Preprocess the string to remove punctuation and make lowercase\n    processed_s = re.sub(r'[^\\w\\s]', '', s).lower()\n\n    # Initialize the CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    sentence_vectors = vectorizer.fit_transform(SENTENCES)\n\n    # Transform the input string\n    input_vector = vectorizer.transform([processed_s])\n\n    # Return the vectorized string as a numpy array\n    return input_vector.toarray()[0]\n```\n"}
{"task_id": "BigCodeBench/728", "solution": "import csv\nimport io\ndef task_func(filename, from_encoding='cp1251', to_encoding='utf8', delimiter=','):\n    # Read the CSV file with the specified encoding\n    with open(filename, 'r', encoding=from_encoding) as f:\n        reader = csv.DictReader(f, delimiter=delimiter)\n        data = [row for row in reader]\n\n    # Convert the data to a string with the specified encoding\n    output = io.StringIO()\n    writer = csv.DictWriter(output, fieldnames=reader.fieldnames, delimiter=delimiter)\n    writer.writeheader()\n    writer.writerows(data)\n    converted_csv = output.getvalue().encode(to_encoding).decode(to_encoding)\n\n    return data, converted_csv", "raw_solution": "import csv\nimport io\n\ndef task_func(filename, from_encoding='cp1251', to_encoding='utf8', delimiter=','):\n    # Read the CSV file with the specified encoding\n    with open(filename, 'r', encoding=from_encoding) as f:\n        reader = csv.DictReader(f, delimiter=delimiter)\n        data = [row for row in reader]\n\n    # Convert the data to a string with the specified encoding\n    output = io.StringIO()\n    writer = csv.DictWriter(output, fieldnames=reader.fieldnames, delimiter=delimiter)\n    writer.writeheader()\n    writer.writerows(data)\n    converted_csv = output.getvalue().encode(to_encoding).decode(to_encoding)\n\n    return data, converted_csv\n\n# Example usage:\n# data, converted_csv = task_func('sample.csv', 'cp1251', 'utf8')\n# print(data)\n# print(converted_csv)\n```\n"}
{"task_id": "BigCodeBench/729", "solution": "import pickle\nimport os\nimport random\nimport string\ndef task_func(strings, filename=None):\n    \n    if filename is None:\n        filename = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10)) + '.pkl'\n    \n    # Save the list of strings to a pickle file\n    with open(filename, 'wb') as file:\n        pickle.dump(strings, file)\n    \n    # Load the list of strings from the pickle file\n    with open(filename, 'rb') as file:\n        loaded_strings = pickle.load(file)\n    \n    # Remove the pickle file after loading\n    os.remove(filename)\n    \n    return loaded_strings", "raw_solution": "import pickle\nimport os\nimport random\nimport string\n\ndef task_func(strings, filename=None):\n    \n    if filename is None:\n        filename = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10)) + '.pkl'\n    \n    # Save the list of strings to a pickle file\n    with open(filename, 'wb') as file:\n        pickle.dump(strings, file)\n    \n    # Load the list of strings from the pickle file\n    with open(filename, 'rb') as file:\n        loaded_strings = pickle.load(file)\n    \n    # Remove the pickle file after loading\n    os.remove(filename)\n    \n    return loaded_strings\n\n# Example usage:"}
{"task_id": "BigCodeBench/730", "solution": "import pickle\nimport os\nfrom datetime import datetime\nimport pytz\nFILE_NAME = 'save.pkl'\ndef task_func(dt):\n    \"\"\"\n    \"save.pkl\" naamak pickle file mein date time object \"dt\" ko save karen and phir validation ke liye use back padhen.\n\n    Parameters:\n    - dt (datetime): save kiya jaane wala datetime object.\n\n    Returns:\n        loaded_dt (datetime): 'save.pkl' se load kiya gaya datetime object.\n\n    Requirements:\n    - pickle\n    - os\n\n    Example:\n    >>> dt = datetime.now(pytz.UTC)\n    >>> loaded_dt = task_func(dt)\n    >>> assert dt == loaded_dt\n\n    \"\"\"\n    # Save the datetime object to a pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump(dt, file)\n\n    # Load the datetime object from the pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_dt = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_dt", "raw_solution": "import pickle\nimport os\nfrom datetime import datetime\nimport pytz\n\n# Constants\nFILE_NAME = 'save.pkl'\n\ndef task_func(dt):\n    \"\"\"\n    \"save.pkl\" naamak pickle file mein date time object \"dt\" ko save karen and phir validation ke liye use back padhen.\n\n    Parameters:\n    - dt (datetime): save kiya jaane wala datetime object.\n\n    Returns:\n        loaded_dt (datetime): 'save.pkl' se load kiya gaya datetime object.\n\n    Requirements:\n    - pickle\n    - os\n\n    Example:\n    >>> dt = datetime.now(pytz.UTC)\n    >>> loaded_dt = task_func(dt)\n    >>> assert dt == loaded_dt\n\n    \"\"\"\n    # Save the datetime object to a pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump(dt, file)\n\n    # Load the datetime object from the pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_dt = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_dt\n\n# Example usage"}
{"task_id": "BigCodeBench/731", "solution": "import pickle\nimport os\nfrom sklearn.datasets import make_classification\nimport numpy as np\nFILE_NAME = 'save.pkl'\ndef task_func(data, target):\n    \"\"\"\n    Sklearn dataset (\"data\" and \"destination\") ko pickle file \"save.pkl\" mein sahejen and then validation ke liye ise back padhen.\n\n    Parameters:\n    - data (numpy array): sklearn dataset ka data part.\n    - target (numpy array): sklearn dataset ka target part.\n\n    Returns:\n        tuple: 'save.pkl' se load kiya gaya tuple (data, target).\n\n    Requirements:\n    - pickle\n    - os\n    - sklearn.datasets\n\n    Example:\n    >>> data, target = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n    >>> loaded_data, loaded_target = task_func(data, target)\n    >>> assert np.array_equal(data, loaded_data) and np.array_equal(target, loaded_target)\n\n    \"\"\"\n    # Save the data and target to a pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n\n    # Load the data and target from the pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_data, loaded_target", "raw_solution": "import pickle\nimport os\nfrom sklearn.datasets import make_classification\nimport numpy as np\n\n# Constants\nFILE_NAME = 'save.pkl'\nDATA, TARGET = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n\ndef task_func(data, target):\n    \"\"\"\n    Sklearn dataset (\"data\" and \"destination\") ko pickle file \"save.pkl\" mein sahejen and then validation ke liye ise back padhen.\n\n    Parameters:\n    - data (numpy array): sklearn dataset ka data part.\n    - target (numpy array): sklearn dataset ka target part.\n\n    Returns:\n        tuple: 'save.pkl' se load kiya gaya tuple (data, target).\n\n    Requirements:\n    - pickle\n    - os\n    - sklearn.datasets\n\n    Example:\n    >>> data, target = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n    >>> loaded_data, loaded_target = task_func(data, target)\n    >>> assert np.array_equal(data, loaded_data) and np.array_equal(target, loaded_target)\n\n    \"\"\"\n    # Save the data and target to a pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n\n    # Load the data and target from the pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_data, loaded_target\n\n# Example usage"}
{"task_id": "BigCodeBench/732", "solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nSTEMMER = PorterStemmer()\ndef task_func(content):\n    # Remove punctuation and convert to lowercase\n    content = content.lower()\n    content = re.sub(f'[{re.escape(string.punctuation)}]', '', content)\n\n    # Tokenize the content\n    words = content.split()\n\n    # Stem the words\n    stemmed_words = [STEMMER.stem(word) for word in words]\n\n    # Count the frequency of each word\n    word_count = Counter(stemmed_words)\n\n    return dict(word_count)", "raw_solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\n\nSTEMMER = PorterStemmer()\n\ndef task_func(content):\n    # Remove punctuation and convert to lowercase\n    content = content.lower()\n    content = re.sub(f'[{re.escape(string.punctuation)}]', '', content)\n\n    # Tokenize the content\n    words = content.split()\n\n    # Stem the words\n    stemmed_words = [STEMMER.stem(word) for word in words]\n\n    # Count the frequency of each word\n    word_count = Counter(stemmed_words)\n\n    return dict(word_count)\n\n# Example usage"}
{"task_id": "BigCodeBench/733", "solution": "import re\nimport string\ndef task_func(content):\n    \"\"\"\n    Antim shabd ke bina, ek content mein bina ruke words ki count karein.\n\n    Parameters:\n    - content (str): Jis content se stop words ki count karni hai.\n\n    Returns:\n        count (int): Stop words ki count.\n\n    Requirements:\n    - re\n    - string\n\n    Example:\n    >>> task_func('this is an example content')\n    1\n\n    \"\"\"\n    # List of stop words", "raw_solution": "import re\nimport string\n\ndef task_func(content):\n    \"\"\"\n    Antim shabd ke bina, ek content mein bina ruke words ki count karein.\n\n    Parameters:\n    - content (str): Jis content se stop words ki count karni hai.\n\n    Returns:\n        count (int): Stop words ki count.\n\n    Requirements:\n    - re\n    - string\n\n    Example:\n    >>> task_func('this is an example content')\n    1\n\n    \"\"\"\n    # List of stop words\n    stop_words = set([\n        'a', 'an', 'the', 'and', 'or', 'but', 'so', 'if', 'then', 'else', 'when', 'where', 'why', 'how', 'what', 'which', 'whose', 'whom', 'whither', 'however', 'although', 'though', 'whereas', 'wherever', 'whenever', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whereas', '"}
{"task_id": "BigCodeBench/734", "solution": "import nltk\nfrom collections import Counter\ndef task_func(content):\n    \"\"\"\n    Antim word ko chhodkar, ek sentence mein Part-of-Speech (POS) tags ki count karein.\n\n    Parameters:\n    - content (str): POS tags ki count karne ke liye sentence.\n\n    Returns:\n        dict: Ek dict jismein POS tags, keys ke taur par and unki sankhya, values ke taur par hain.\n\n    Requirements:\n    - nltk\n    - collections.Counter\n\n    Example:\n    >>> task_func('Yeh ek example content hai')\n    {'DT': 2, 'VBZ': 1, 'NN': 1}\n\n    \"\"\"\n    # Tokenize the sentence into words\n    words = nltk.word_tokenize(content)\n\n    # Get the POS tags for each word\n    pos_tags = nltk.pos_tag(words)\n\n    # Count the occurrences of each POS tag\n    pos_counts = Counter(tag for _, tag in pos_tags)\n\n    return dict(pos_counts)", "raw_solution": "import nltk\n\n# Download necessary NLTK data (if not already present)\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\nfrom collections import Counter\n\ndef task_func(content):\n    \"\"\"\n    Antim word ko chhodkar, ek sentence mein Part-of-Speech (POS) tags ki count karein.\n\n    Parameters:\n    - content (str): POS tags ki count karne ke liye sentence.\n\n    Returns:\n        dict: Ek dict jismein POS tags, keys ke taur par and unki sankhya, values ke taur par hain.\n\n    Requirements:\n    - nltk\n    - collections.Counter\n\n    Example:\n    >>> task_func('Yeh ek example content hai')\n    {'DT': 2, 'VBZ': 1, 'NN': 1}\n\n    \"\"\"\n    # Tokenize the sentence into words\n    words = nltk.word_tokenize(content)\n\n    # Get the POS tags for each word\n    pos_tags = nltk.pos_tag(words)\n\n    # Count the occurrences of each POS tag\n    pos_counts = Counter(tag for _, tag in pos_tags)\n\n    return dict(pos_counts)\n```\n"}
{"task_id": "BigCodeBench/735", "solution": "import numpy as np\nfrom itertools import chain\ndef task_func(L):\n    \"\"\"\n    Calculate the mean and variance of all the elements in the nested list 'L'.\n\n    Parameters:\n    - L (list): The nested list.\n\n    Returns:\n        dict: A dictionary containing mean and variance.\n\n    Requirements:\n    - numpy\n    - itertools.chain\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    {'mean': 3.5, 'variance': 2.9166666666666665}\n\n    \"\"\"\n    # Flatten the list using itertools.chain\n    flat_list = list(chain(*L))\n    \n    # Convert the list to a numpy array\n    array = np.array(flat_list)\n    \n    # Calculate mean and variance\n    mean = np.mean(array)\n    variance = np.var(array)\n    \n    return {'mean': mean, 'variance': variance}", "raw_solution": "import numpy as np\nfrom itertools import chain\n\ndef task_func(L):\n    \"\"\"\n    Calculate the mean and variance of all the elements in the nested list 'L'.\n\n    Parameters:\n    - L (list): The nested list.\n\n    Returns:\n        dict: A dictionary containing mean and variance.\n\n    Requirements:\n    - numpy\n    - itertools.chain\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    {'mean': 3.5, 'variance': 2.9166666666666665}\n\n    \"\"\"\n    # Flatten the list using itertools.chain\n    flat_list = list(chain(*L))\n    \n    # Convert the list to a numpy array\n    array = np.array(flat_list)\n    \n    # Calculate mean and variance\n    mean = np.mean(array)\n    variance = np.var(array)\n    \n    return {'mean': mean, 'variance': variance}\n```\n"}
{"task_id": "BigCodeBench/736", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(L):\n    '''\n    Calculate the mode of all elements in a nested list 'L'.\n    \n    Parameters:\n    L (list): The nested list.\n    \n    Returns:\n    - mode (int): The mode.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    1\n    '''\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the mode\n    mode_result = stats.mode(flat_list)\n    \n    return mode_result.mode[0]", "raw_solution": "import numpy as np\nfrom scipy import stats\n\ndef task_func(L):\n    '''\n    Calculate the mode of all elements in a nested list 'L'.\n    \n    Parameters:\n    L (list): The nested list.\n    \n    Returns:\n    - mode (int): The mode.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    1\n    '''\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the mode\n    mode_result = stats.mode(flat_list)\n    \n    return mode_result.mode[0]\n\n# Test cases"}
{"task_id": "BigCodeBench/737", "solution": "import numpy as np\nimport math\ndef task_func(L):\n    \"\"\"\n    'L' is a nested list, and this function calculates the median of all elements in the list.\n\n    Parameters:\n    - L (list): nested list.\n\n    Returns:\n        median (float): median.\n\n    Requirements:\n    - numpy\n    - math\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    3.5\n\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the median\n    median = np.median(flat_list)\n    \n    return median", "raw_solution": "import numpy as np\nimport math\n\ndef task_func(L):\n    \"\"\"\n    'L' is a nested list, and this function calculates the median of all elements in the list.\n\n    Parameters:\n    - L (list): nested list.\n\n    Returns:\n        median (float): median.\n\n    Requirements:\n    - numpy\n    - math\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    3.5\n\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the median\n    median = np.median(flat_list)\n    \n    return median\n\n# Test cases"}
{"task_id": "BigCodeBench/738", "solution": "import numpy as np\nfrom scipy.stats import iqr\ndef task_func(L):\n    \"\"\"\n    'L' mein maujood sabhi elements ka interquartile range calculate karen.\n\n    Parameters:\n    - L (list): nested list.\n\n    Returns:\n        iqr_value (float): interquartile range.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    2.5\n\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the interquartile range\n    iqr_value = iqr(flat_list)\n    \n    return iqr_value", "raw_solution": "import numpy as np\nfrom scipy.stats import iqr\n\ndef task_func(L):\n    \"\"\"\n    'L' mein maujood sabhi elements ka interquartile range calculate karen.\n\n    Parameters:\n    - L (list): nested list.\n\n    Returns:\n        iqr_value (float): interquartile range.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    2.5\n\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the interquartile range\n    iqr_value = iqr(flat_list)\n    \n    return iqr_value\n```\n"}
{"task_id": "BigCodeBench/739", "solution": "import struct\nimport random\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_key=None):\n    \"\"\"\n    hexadecimal strings ki ek list se ek random float number utpann karein aur phir float number ko 2 decimal places tak rounded.\n\n    Parameters:\n    - None\n\n    Returns:\n        rounded_float (float): rounded_float number.\n\n    Requirements:\n    - struct\n    - random\n\n    Example:\n    >>> random.seed(42)\n    >>> print(repr(f\"{task_func():.1f}\"))\n    '36806.1'\n\n    \"\"\"\n    # Select a random key from KEYS\n    if hex_key is None:\n        hex_key = random.choice(KEYS)\n\n    # Convert the hexadecimal key to a float\n    float_value = struct.unpack('!f', bytes.fromhex(hex_key))[0]\n\n    # Round the float to 2 decimal places\n    rounded_float = round(float_value, 2)\n\n    return rounded_float", "raw_solution": "import struct\nimport random\n\n# Constants\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_key=None):\n    \"\"\"\n    hexadecimal strings ki ek list se ek random float number utpann karein aur phir float number ko 2 decimal places tak rounded.\n\n    Parameters:\n    - None\n\n    Returns:\n        rounded_float (float): rounded_float number.\n\n    Requirements:\n    - struct\n    - random\n\n    Example:\n    >>> random.seed(42)\n    >>> print(repr(f\"{task_func():.1f}\"))\n    '36806.1'\n\n    \"\"\"\n    # Select a random key from KEYS\n    if hex_key is None:\n        hex_key = random.choice(KEYS)\n\n    # Convert the hexadecimal key to a float\n    float_value = struct.unpack('!f', bytes.fromhex(hex_key))[0]\n\n    # Round the float to 2 decimal places\n    rounded_float = round(float_value, 2)\n\n    return rounded_float\n```\n"}
{"task_id": "BigCodeBench/740", "solution": "from collections import Counter\nimport heapq\ndef task_func(my_dict):\n    \"\"\"\n    Ek dictionary banao jisme keys letters hain aur maan random integers hain.\n    dictionary mein most common 3 letters khojen.\n\n    Parameters:\n    - my_dict (dict): Sansadhit karne ke liye dictionary.\n\n    Returns:\n        most_common_letters (list): most common 3 letters.\n\n    Requirements:\n    - collections\n    - heapq\n\n    Example:\n    >>> random.seed(43)\n    >>> my_dict = {letter: random.randint(1, 100) for letter in LETTERS}\n    >>> most_common_letters = task_func(my_dict)\n    >>> print(most_common_letters)\n    ['d', 'v', 'c']\n\n    \"\"\"\n    # Count the frequency of each letter in the dictionary\n    letter_counts = Counter(my_dict.keys())\n\n    # Use heapq to find the 3 most common letters\n    most_common_letters = heapq.nlargest(3, letter_counts, key=letter_counts.get)\n\n    return most_common_letters", "raw_solution": "from collections import Counter\nimport heapq\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef task_func(my_dict):\n    \"\"\"\n    Ek dictionary banao jisme keys letters hain aur maan random integers hain.\n    dictionary mein most common 3 letters khojen.\n\n    Parameters:\n    - my_dict (dict): Sansadhit karne ke liye dictionary.\n\n    Returns:\n        most_common_letters (list): most common 3 letters.\n\n    Requirements:\n    - collections\n    - heapq\n\n    Example:\n    >>> random.seed(43)\n    >>> my_dict = {letter: random.randint(1, 100) for letter in LETTERS}\n    >>> most_common_letters = task_func(my_dict)\n    >>> print(most_common_letters)\n    ['d', 'v', 'c']\n\n    \"\"\"\n    # Count the frequency of each letter in the dictionary\n    letter_counts = Counter(my_dict.keys())\n\n    # Use heapq to find the 3 most common letters\n    most_common_letters = heapq.nlargest(3, letter_counts, key=letter_counts.get)\n\n    return most_common_letters\n```\n"}
{"task_id": "BigCodeBench/741", "solution": "from itertools import groupby\nfrom operator import itemgetter\nKEY_FUNC = itemgetter(0)\ndef task_func(my_dict):\n    \"\"\"\n    dictionary entries ko key ke first character ke baad groupit karein and pratyek group ke liye maan jodein.\n\n    Parameters:\n    - my_dict (dict): sansadhit karne ke liye dictionary.\n\n    Returns:\n        aggregated_dict (dict): aggregated dictionary.\n\n    Requirements:\n    - itertools\n    - operator\n\n    Example:\n    >>> my_dict = {'apple': 1, 'banana': 2, 'avocado': 3, 'blueberry': 4, 'blackberry': 5}\n    >>> aggregated_dict = task_func(my_dict)\n    >>> print(aggregated_dict)\n    {'a': 4, 'b': 11}\n\n    \"\"\"\n    # Sort the dictionary by the first character of the keys\n    sorted_dict = dict(sorted(my_dict.items(), key=KEY_FUNC))\n\n    # Group the dictionary by the first character of the keys\n    grouped_dict = {key: list(group) for key, group in groupby(sorted_dict.items(), key=KEY_FUNC)}\n\n    # Aggregate the values for each group\n    aggregated_dict = {key: sum(itemgetter(1)(item) for item in group) for key, group in grouped_dict.items()}\n\n    return aggregated_dict", "raw_solution": "from itertools import groupby\nfrom operator import itemgetter\n\n# Constants\nKEY_FUNC = itemgetter(0)\n\ndef task_func(my_dict):\n    \"\"\"\n    dictionary entries ko key ke first character ke baad groupit karein and pratyek group ke liye maan jodein.\n\n    Parameters:\n    - my_dict (dict): sansadhit karne ke liye dictionary.\n\n    Returns:\n        aggregated_dict (dict): aggregated dictionary.\n\n    Requirements:\n    - itertools\n    - operator\n\n    Example:\n    >>> my_dict = {'apple': 1, 'banana': 2, 'avocado': 3, 'blueberry': 4, 'blackberry': 5}\n    >>> aggregated_dict = task_func(my_dict)\n    >>> print(aggregated_dict)\n    {'a': 4, 'b': 11}\n\n    \"\"\"\n    # Sort the dictionary by the first character of the keys\n    sorted_dict = dict(sorted(my_dict.items(), key=KEY_FUNC))\n\n    # Group the dictionary by the first character of the keys\n    grouped_dict = {key: list(group) for key, group in groupby(sorted_dict.items(), key=KEY_FUNC)}\n\n    # Aggregate the values for each group\n    aggregated_dict = {key: sum(itemgetter(1)(item) for item in group) for key, group in grouped_dict.items()}\n\n    return aggregated_dict\n```\n"}
{"task_id": "BigCodeBench/742", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input list is empty.\")\n    \n    try:\n        df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    except ValueError as e:\n        raise ValueError(\"Values in the list must be numeric.\") from e\n\n    scaler = MinMaxScaler()\n    df['Value'] = scaler.fit_transform(df[['Value']])\n\n    return df\nlist_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input list is empty.\")\n    \n    try:\n        df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    except ValueError as e:\n        raise ValueError(\"Values in the list must be numeric.\") from e\n\n    scaler = MinMaxScaler()\n    df['Value'] = scaler.fit_transform(df[['Value']])\n\n    return df\n\n# Example usage:\nlist_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\ndf = task_func(list_of_pairs)"}
{"task_id": "BigCodeBench/743", "solution": "import json\nimport os\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\ndef task_func(directory):\n    prefix_counts = {prefix: 0 for prefix in PREFIXES}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                data = json.load(file)\n                for key in data.keys():\n                    for prefix in PREFIXES:\n                        if key.startswith(prefix):\n                            prefix_counts[prefix] += 1\n\n    return prefix_counts", "raw_solution": "import json\nimport os\n\n# Constants\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\n\ndef task_func(directory):\n    prefix_counts = {prefix: 0 for prefix in PREFIXES}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                data = json.load(file)\n                for key in data.keys():\n                    for prefix in PREFIXES:\n                        if key.startswith(prefix):\n                            prefix_counts[prefix] += 1\n\n    return prefix_counts\n\n# Example usage:\n# print(task_func('/path/to/json/files'))\n```\nThis script will count the occurrences of keys in JSON files that start with the specified prefixes. It assumes that the JSON files are in the specified directory and that the directory path is given as a string to the `task_func` function.\n"}
{"task_id": "BigCodeBench/744", "solution": "import nltk\nfrom string import punctuation\nimport pandas as pd\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input text must be a string.\")\n\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Filter out punctuation and words that do not start with '$'\n    words = [word for word in words if word.startswith('$') and word not in punctuation]\n\n    # Create a DataFrame\n    df = pd.DataFrame(words, columns=['Word'])\n    df['Frequency'] = df['Word'].apply(lambda x: words.count(x))\n\n    return df\ntext = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"", "raw_solution": "import nltk\nfrom string import punctuation\nimport pandas as pd\n\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input text must be a string.\")\n\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Filter out punctuation and words that do not start with '$'\n    words = [word for word in words if word.startswith('$') and word not in punctuation]\n\n    # Create a DataFrame\n    df = pd.DataFrame(words, columns=['Word'])\n    df['Frequency'] = df['Word'].apply(lambda x: words.count(x))\n\n    return df\n\n# Example usage:\ntext = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\""}
{"task_id": "BigCodeBench/745", "solution": "import subprocess\nimport random\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'\ndef task_func():\n    \"\"\"\n    Randomly selects a bash script from the list and runs it.\n\n    Parameters:\n    - None\n\n    Returns:\n        script (str): Full path of the executed script.\n\n    Requirements:\n    - subprocess\n    - random\n\n    Example:\n    >>> task_func()\n\n    \"\"\"\n    # Select a random script from the list\n    script = random.choice(SCRIPTS)\n    script_path = f\"{SCRIPTS_DIR}/{script}\"\n\n    # Run the script using subprocess\n    subprocess.run(['bash', script_path], check=True)\n\n    return script_path", "raw_solution": "import subprocess\nimport random\n\n# Constants\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'  \n\ndef task_func():\n    \"\"\"\n    Randomly selects a bash script from the list and runs it.\n\n    Parameters:\n    - None\n\n    Returns:\n        script (str): Full path of the executed script.\n\n    Requirements:\n    - subprocess\n    - random\n\n    Example:\n    >>> task_func()\n\n    \"\"\"\n    # Select a random script from the list\n    script = random.choice(SCRIPTS)\n    script_path = f\"{SCRIPTS_DIR}/{script}\"\n\n    # Run the script using subprocess\n    subprocess.run(['bash', script_path], check=True)\n\n    return script_path\n```\n"}
{"task_id": "BigCodeBench/746", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, target_column, target_values=None):\n    if not isinstance(df, pd.DataFrame) or not isinstance(target_column, str) or not isinstance(target_values, (list, np.ndarray, type(None))):\n        raise ValueError(\"Invalid input types\")\n\n    # Replace values in DataFrame that are not in target_values with 0\n    if target_values is not None:\n        for col in df.columns:\n            df[col] = np.where(df[col].isin(target_values), df[col], 0)\n\n    # Prepare data for linear regression\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\nrng = np.random.default_rng(seed=0)\ndf = pd.DataFrame(rng.integers(0, 100, size=(1000, 2)), columns=['A', 'predict'])", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, target_column, target_values=None):\n    if not isinstance(df, pd.DataFrame) or not isinstance(target_column, str) or not isinstance(target_values, (list, np.ndarray, type(None))):\n        raise ValueError(\"Invalid input types\")\n\n    # Replace values in DataFrame that are not in target_values with 0\n    if target_values is not None:\n        for col in df.columns:\n            df[col] = np.where(df[col].isin(target_values), df[col], 0)\n\n    # Prepare data for linear regression\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\n\n# Example usage:\nrng = np.random.default_rng(seed=0)\ndf = pd.DataFrame(rng.integers(0, 100, size=(1000, 2)), columns=['A', 'predict'])\nmodel = task_func(df, 'predict')"}
{"task_id": "BigCodeBench/747", "solution": "import re\nimport math\ndef task_func(s):\n    # Regular expression to find numbers\n    numbers = re.findall(r'\\b\\d+(\\.\\d+)?\\b', s)\n    \n    count = len(numbers)\n    sqrt_sum = sum(math.sqrt(float(num)) for num in numbers)\n    \n    return count, sqrt_sum", "raw_solution": "import re\nimport math\n\ndef task_func(s):\n    # Regular expression to find numbers\n    numbers = re.findall(r'\\b\\d+(\\.\\d+)?\\b', s)\n    \n    count = len(numbers)\n    sqrt_sum = sum(math.sqrt(float(num)) for num in numbers)\n    \n    return count, sqrt_sum\n\n# Test cases"}
{"task_id": "BigCodeBench/748", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, age, weight):\n    \"\"\"\n    Filters the DataFrame based on 'Age' and 'Weight' criteria and standardizes the numerical values.\n\n    Parameters:\n    - df (pd.DataFrame): input DataFrame where 'Age' and 'Weight' columns exist.\n    - age (numeric): age threshold for filtering rows. Rows with 'Age' less than this value are filtered out.\n    - weight (numeric): weight threshold for filtering rows. Rows with 'Weight' greater than this value are filtered out.\n\n    Returns:\n    - pd.DataFrame: DataFrame after filtering and standardizing numerical values.\n\n    Requirements:\n    - sklearn.preprocessing.StandardScaler\n    - pandas\n\n    Raises:\n    - KeyError: if input DataFrame does not contain 'Age' and 'Weight' columns.\n    \"\"\"\n    if 'Age' not in df.columns or 'Weight' not in df.columns:\n        raise KeyError(\"Input DataFrame must contain 'Age' and 'Weight' columns.\")\n\n    # Filter the DataFrame\n    filtered_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    # Standardize the numerical values\n    scaler = StandardScaler()\n    numerical_cols = filtered_df.select_dtypes(include=['float64', 'int64']).columns\n    filtered_df[numerical_cols] = scaler.fit_transform(filtered_df[numerical_cols])\n\n    return filtered_df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    \"\"\"\n    Filters the DataFrame based on 'Age' and 'Weight' criteria and standardizes the numerical values.\n\n    Parameters:\n    - df (pd.DataFrame): input DataFrame where 'Age' and 'Weight' columns exist.\n    - age (numeric): age threshold for filtering rows. Rows with 'Age' less than this value are filtered out.\n    - weight (numeric): weight threshold for filtering rows. Rows with 'Weight' greater than this value are filtered out.\n\n    Returns:\n    - pd.DataFrame: DataFrame after filtering and standardizing numerical values.\n\n    Requirements:\n    - sklearn.preprocessing.StandardScaler\n    - pandas\n\n    Raises:\n    - KeyError: if input DataFrame does not contain 'Age' and 'Weight' columns.\n    \"\"\"\n    if 'Age' not in df.columns or 'Weight' not in df.columns:\n        raise KeyError(\"Input DataFrame must contain 'Age' and 'Weight' columns.\")\n\n    # Filter the DataFrame\n    filtered_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    # Standardize the numerical values\n    scaler = StandardScaler()\n    numerical_cols = filtered_df.select_dtypes(include=['float64', 'int64']).columns\n    filtered_df[numerical_cols] = scaler.fit_transform(filtered_df[numerical_cols])\n\n    return filtered_df\n\n# Test cases\ndata = pd.DataFrame({\n    \"Age\": [32, 51, 11, 5, 88, 434],\n    \"Weight\": [62, 76, 72, 859, 69, 102],\n    \"shoe_size\": [12, 6, 7, 8, 9, 6]\n})\n"}
{"task_id": "BigCodeBench/749", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\ndef task_func(myList):\n    \"\"\"\n    min-max scaling ka upyog karke [0, 1] range mein numeric values ki ek list ko samaanya karein.\n\n    Parameters:\n    - myList (list): samaanya karne ke liye numeric values ki list.\n\n    Returns:\n        ndarray: samaanyikrit values ki ek array.\n\n    Requirements:\n    - sklearn.preprocessing.MinMaxScaler\n    - numpy\n\n    Example:\n    >>> myList = [10, 20, 30, 40, 50]\n    >>> task_func(myList)\n    array([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n    \"\"\"\n    scaler = MinMaxScaler()\n    array = np.array(myList).reshape(-1, 1)\n    scaled_array = scaler.fit_transform(array)\n    return scaled_array.flatten()", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\ndef task_func(myList):\n    \"\"\"\n    min-max scaling ka upyog karke [0, 1] range mein numeric values ki ek list ko samaanya karein.\n\n    Parameters:\n    - myList (list): samaanya karne ke liye numeric values ki list.\n\n    Returns:\n        ndarray: samaanyikrit values ki ek array.\n\n    Requirements:\n    - sklearn.preprocessing.MinMaxScaler\n    - numpy\n\n    Example:\n    >>> myList = [10, 20, 30, 40, 50]\n    >>> task_func(myList)\n    array([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n    \"\"\"\n    scaler = MinMaxScaler()\n    array = np.array(myList).reshape(-1, 1)\n    scaled_array = scaler.fit_transform(array)\n    return scaled_array.flatten()\n\n# Test cases"}
{"task_id": "BigCodeBench/750", "solution": "import pandas as pd\nimport statsmodels.api as sm\ndef task_func(df: pd.DataFrame, height: int, weight: int, columns: list) -> sm.regression.linear_model.RegressionResultsWrapper:\n    if df.empty or not all(column in df.columns for column in columns):\n        return None\n\n    df_filtered = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n\n    if df_filtered.empty:\n        return None\n\n    X = df_filtered[columns[1:]]\n    y = df_filtered[columns[0]]\n\n    X = sm.add_constant(X)  # Adds a constant term to the predictor variables\n    model = sm.OLS(y, X).fit()\n\n    return model", "raw_solution": "import pandas as pd\nimport statsmodels.api as sm\n\ndef task_func(df: pd.DataFrame, height: int, weight: int, columns: list) -> sm.regression.linear_model.RegressionResultsWrapper:\n    if df.empty or not all(column in df.columns for column in columns):\n        return None\n\n    df_filtered = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n\n    if df_filtered.empty:\n        return None\n\n    X = df_filtered[columns[1:]]\n    y = df_filtered[columns[0]]\n\n    X = sm.add_constant(X)  # Adds a constant term to the predictor variables\n    model = sm.OLS(y, X).fit()\n\n    return model\n```\n"}
{"task_id": "BigCodeBench/751", "solution": "import random\nfrom collections import Counter\ndef task_func(values, weights, n_samples):\n    \"\"\"\n    Diye gaye bharit distribution ke aadhar par random numbers sample lein aur samples ka ek histogram lautaien.\n\n    Parameters:\n    - values (list): sample karne ke liye values ki list.\n    - weights (list): values se jude bharon ki list.\n    - n_samples (int): khinche jaane waale samples ki number.\n\n    Returns:\n        histogram (dict): values ko keys ke roop mein aur counts ko values ke roop mein ek dictionary ke roop mein ek histogram.\n\n    Requirements:\n    - collections.Counter\n    - random\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func([1, 2, 3], [3, 2, 1], 1000)\n    {2: 342, 1: 480, 3: 178}\n\n    \"\"\"\n    # Ensure the weights sum up to 1 for proper probability distribution\n    total_weight = sum(weights)\n    normalized_weights = [w / total_weight for w in weights]\n\n    # Generate random samples based on the given weights\n    samples = random.choices(values, weights=normalized_weights, k=n_samples)\n\n    # Count the occurrences of each value in the samples\n    histogram = Counter(samples)\n\n    return dict(histogram)", "raw_solution": "import random\nfrom collections import Counter\n\ndef task_func(values, weights, n_samples):\n    \"\"\"\n    Diye gaye bharit distribution ke aadhar par random numbers sample lein aur samples ka ek histogram lautaien.\n\n    Parameters:\n    - values (list): sample karne ke liye values ki list.\n    - weights (list): values se jude bharon ki list.\n    - n_samples (int): khinche jaane waale samples ki number.\n\n    Returns:\n        histogram (dict): values ko keys ke roop mein aur counts ko values ke roop mein ek dictionary ke roop mein ek histogram.\n\n    Requirements:\n    - collections.Counter\n    - random\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func([1, 2, 3], [3, 2, 1], 1000)\n    {2: 342, 1: 480, 3: 178}\n\n    \"\"\"\n    # Ensure the weights sum up to 1 for proper probability distribution\n    total_weight = sum(weights)\n    normalized_weights = [w / total_weight for w in weights]\n\n    # Generate random samples based on the given weights\n    samples = random.choices(values, weights=normalized_weights, k=n_samples)\n\n    # Count the occurrences of each value in the samples\n    histogram = Counter(samples)\n\n    return dict(histogram)\n\n# Example usage:\n# random.seed(42)\n# print(task_func([1, 2, 3], [3, 2, 1], 1000))\n```\n"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data should not be empty.\")\n    if target_column not in data.columns:\n        raise ValueError(\"Target column should be in the data.\")\n    if not all(np.issubdtype(dtype, np.number) for dtype in data.dtypes):\n        raise ValueError(\"All columns in data should be numeric.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state should be an integer.\")\n    if not (0 < test_size < 1):\n        raise ValueError(\"Test size should be between 0 and 1.\")\n\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)\nrng = np.random.default_rng(seed=42)\ndata = pd.DataFrame({\n    'x1': rng.random(100),\n    'x2': rng.random(100),\n    'y': rng.random(100)\n})", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data should not be empty.\")\n    if target_column not in data.columns:\n        raise ValueError(\"Target column should be in the data.\")\n    if not all(np.issubdtype(dtype, np.number) for dtype in data.dtypes):\n        raise ValueError(\"All columns in data should be numeric.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state should be an integer.\")\n    if not (0 < test_size < 1):\n        raise ValueError(\"Test size should be between 0 and 1.\")\n\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)\n\n# Example usage:\nrng = np.random.default_rng(seed=42)\ndata = pd.DataFrame({\n    'x1': rng.random(100),\n    'x2': rng.random(100),\n    'y': rng.random(100)\n})\nresult = task_func(data, 'y', random_state=2, test_size=0.3)"}
{"task_id": "BigCodeBench/753", "solution": "import math\nimport random\nimport statistics\nRADIUS = 5\ndef task_func(n):\n    \"\"\"\n    RADIUS trijya (default maan 5 hai) ke circle ke andar n random bindu banaen and center se unki average distance lautaaen.\n\n    Parameters:\n    - n (int): utpann kie jaane vaale points ki number.\n\n    Returns:\n        float: circle ke center se average distance.\n\n    Requirements:\n    - math\n    - random\n    - statistics\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func(100)\n    3.2406\n    >>> task_func(50)\n    3.4443\n\n    \"\"\"\n    points = []\n    for _ in range(n):\n        angle = random.uniform(0, 2 * math.pi)\n        distance = random.uniform(0, RADIUS)\n        x = RADIUS * math.cos(angle) + RADIUS\n        y = RADIUS * math.sin(angle) + RADIUS\n        points.append((x, y))\n\n    distances = [math.sqrt(x**2 + y**2) for x, y in points]\n    average_distance = statistics.mean(distances)\n\n    return average_distance", "raw_solution": "import math\nimport random\nimport statistics\n\n# Constants\nRADIUS = 5\n\ndef task_func(n):\n    \"\"\"\n    RADIUS trijya (default maan 5 hai) ke circle ke andar n random bindu banaen and center se unki average distance lautaaen.\n\n    Parameters:\n    - n (int): utpann kie jaane vaale points ki number.\n\n    Returns:\n        float: circle ke center se average distance.\n\n    Requirements:\n    - math\n    - random\n    - statistics\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func(100)\n    3.2406\n    >>> task_func(50)\n    3.4443\n\n    \"\"\"\n    points = []\n    for _ in range(n):\n        angle = random.uniform(0, 2 * math.pi)\n        distance = random.uniform(0, RADIUS)\n        x = RADIUS * math.cos(angle) + RADIUS\n        y = RADIUS * math.sin(angle) + RADIUS\n        points.append((x, y))\n\n    distances = [math.sqrt(x**2 + y**2) for x, y in points]\n    average_distance = statistics.mean(distances)\n\n    return average_distance\n\n# Test cases"}
{"task_id": "BigCodeBench/754", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(result):\n    # Initialize lists to store data\n    from_user_values = []\n    current_time = datetime.now().strftime(DATE_FORMAT)\n\n    for item in result:\n        if \"from_user\" in item:\n            from_user_values.append(item[\"from_user\"])\n        else:\n            from_user_values.append(np.nan)\n\n    # Convert list to numpy array for efficient computations\n    from_user_array = np.array(from_user_values)\n\n    # Calculate statistics\n    mean_value = np.nanmean(from_user_array)\n    median_value = np.nanmedian(from_user_array)\n    min_value = np.nanmin(from_user_array)\n    max_value = np.nanmax(from_user_array)\n    std_value = np.nanstd(from_user_array)\n\n    # Create a pandas Series with the statistics\n    stats_series = pd.Series({\n        'mean': mean_value,\n        'median': median_value,\n        'min': min_value,\n        'max': max_value,\n        'std': std_value,\n        'current_time': current_time\n    })\n\n    return stats_series\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(result):\n    # Initialize lists to store data\n    from_user_values = []\n    current_time = datetime.now().strftime(DATE_FORMAT)\n\n    for item in result:\n        if \"from_user\" in item:\n            from_user_values.append(item[\"from_user\"])\n        else:\n            from_user_values.append(np.nan)\n\n    # Convert list to numpy array for efficient computations\n    from_user_array = np.array(from_user_values)\n\n    # Calculate statistics\n    mean_value = np.nanmean(from_user_array)\n    median_value = np.nanmedian(from_user_array)\n    min_value = np.nanmin(from_user_array)\n    max_value = np.nanmax(from_user_array)\n    std_value = np.nanstd(from_user_array)\n\n    # Create a pandas Series with the statistics\n    stats_series = pd.Series({\n        'mean': mean_value,\n        'median': median_value,\n        'min': min_value,\n        'max': max_value,\n        'std': std_value,\n        'current_time': current_time\n    })\n\n    return stats_series\n\n# Example usage:\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]\nstats = task_func(result)"}
{"task_id": "BigCodeBench/755", "solution": "import os\nimport glob\ndef task_func(directory_path):\n    new_filenames = []\n    for filename in glob.glob(os.path.join(directory_path, '*')):\n        name, ext = os.path.splitext(filename)\n        name_parts = name.split('.')\n        name_parts.reverse()\n        new_filename = '.'.join(name_parts) + ext\n        new_filenames.append(new_filename)\n    return new_filenames", "raw_solution": "import os\nimport glob\n\ndef task_func(directory_path):\n    new_filenames = []\n    for filename in glob.glob(os.path.join(directory_path, '*')):\n        name, ext = os.path.splitext(filename)\n        name_parts = name.split('.')\n        name_parts.reverse()\n        new_filename = '.'.join(name_parts) + ext\n        new_filenames.append(new_filename)\n    return new_filenames\n```\n"}
{"task_id": "BigCodeBench/756", "solution": "import shutil\nfrom pathlib import Path\nfrom typing import List\ndef task_func(source_dir: str, target_dir: str, extensions: List[str]) -> int:\n    source = Path(source_dir)\n    target = Path(target_dir)\n\n    if not source.exists() or not target.exists():\n        raise ValueError(\"source_dir or target_dir does not exist.\")\n\n    moved_files_count = 0\n\n    for ext in extensions:\n        for file in source.glob(f'*{ext}'):\n            shutil.move(str(file), str(target / file.name))\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import shutil\nfrom pathlib import Path\nfrom typing import List\n\ndef task_func(source_dir: str, target_dir: str, extensions: List[str]) -> int:\n    source = Path(source_dir)\n    target = Path(target_dir)\n\n    if not source.exists() or not target.exists():\n        raise ValueError(\"source_dir or target_dir does not exist.\")\n\n    moved_files_count = 0\n\n    for ext in extensions:\n        for file in source.glob(f'*{ext}'):\n            shutil.move(str(file), str(target / file.name))\n            moved_files_count += 1\n\n    return moved_files_count\n```\n"}
{"task_id": "BigCodeBench/757", "solution": "import numpy as np\nimport datetime\ndef task_func(arr):\n    \"\"\"\n    Numpy array (array) ke sabhi strings (strings) mein \". \" se alag kiye gaye words (words) ke order (order) ko ulta karein.\n\n    Parameters:\n    - arr (numpy array): numpy array (array).\n\n    Returns:\n        numpy.ndarray: numpy array (array) jismein strings (strings) ulte hain.\n\n    Requirements:\n    - numpy\n    - datetime\n\n    Example:\n    >>> arr = np.array(['apple.orange', 'red.green.yellow'])\n    >>> reversed_arr = task_func(arr)\n    >>> print(reversed_arr)\n    ['orange.apple' 'yellow.green.red']\n\n    \"\"\"\n    reversed_arr = np.array([word[::-1] for word in arr])\n    return reversed_arr", "raw_solution": "import numpy as np\nimport datetime\n\ndef task_func(arr):\n    \"\"\"\n    Numpy array (array) ke sabhi strings (strings) mein \". \" se alag kiye gaye words (words) ke order (order) ko ulta karein.\n\n    Parameters:\n    - arr (numpy array): numpy array (array).\n\n    Returns:\n        numpy.ndarray: numpy array (array) jismein strings (strings) ulte hain.\n\n    Requirements:\n    - numpy\n    - datetime\n\n    Example:\n    >>> arr = np.array(['apple.orange', 'red.green.yellow'])\n    >>> reversed_arr = task_func(arr)\n    >>> print(reversed_arr)\n    ['orange.apple' 'yellow.green.red']\n\n    \"\"\"\n    reversed_arr = np.array([word[::-1] for word in arr])\n    return reversed_arr\n\n# Test cases"}
{"task_id": "BigCodeBench/758", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(num_samples, countries=['Russia', 'China', 'USA', 'India', 'Brazil'], \n           ages=np.arange(18, 60), genders=['Male', 'Female'], rng_seed=None):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples must be an integer.\")\n    \n    rng = np.random.default_rng(seed=rng_seed)\n    country = rng.choice(countries, num_samples, replace=True)\n    age = rng.choice(ages, num_samples, replace=True)\n    gender = rng.choice(genders, num_samples, replace=True)\n    \n    gender_encoder = LabelEncoder()\n    gender_encoded = gender_encoder.fit_transform(gender)\n    \n    data = pd.DataFrame({\n        'Country': country,\n        'Age': age,\n        'Gender': gender_encoded\n    })\n    \n    return data", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(num_samples, countries=['Russia', 'China', 'USA', 'India', 'Brazil'], \n           ages=np.arange(18, 60), genders=['Male', 'Female'], rng_seed=None):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples must be an integer.\")\n    \n    rng = np.random.default_rng(seed=rng_seed)\n    country = rng.choice(countries, num_samples, replace=True)\n    age = rng.choice(ages, num_samples, replace=True)\n    gender = rng.choice(genders, num_samples, replace=True)\n    \n    gender_encoder = LabelEncoder()\n    gender_encoded = gender_encoder.fit_transform(gender)\n    \n    data = pd.DataFrame({\n        'Country': country,\n        'Age': age,\n        'Gender': gender_encoded\n    })\n    \n    return data\n\n# Example usage:\n# demographics = task_func(5, rng_seed=31)\n# print(demographics)\n```\n"}
{"task_id": "BigCodeBench/759", "solution": "import os\nimport shutil\nimport fnmatch\ndef task_func(source_directory, destination_directory, file_pattern):\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    moved_files = []\n\n    for root, _, files in os.walk(source_directory):\n        for filename in fnmatch.filter(files, file_pattern):\n            source_file = os.path.join(root, filename)\n            destination_file = os.path.join(destination_directory, filename)\n            shutil.move(source_file, destination_file)\n            moved_files.append(destination_file)\n\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport fnmatch\n\ndef task_func(source_directory, destination_directory, file_pattern):\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    moved_files = []\n\n    for root, _, files in os.walk(source_directory):\n        for filename in fnmatch.filter(files, file_pattern):\n            source_file = os.path.join(root, filename)\n            destination_file = os.path.join(destination_directory, filename)\n            shutil.move(source_file, destination_file)\n            moved_files.append(destination_file)\n\n    return moved_files\n\n# Example usage:\n# moved_files = task_func('/path/to/source', '/path/to/destination', '*.txt')\n# print(moved_files)\n```\n"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Generate random birth years\n    birth_years = np.random.randint(start_year, end_year, size=100)\n    birth_dates = [datetime.datetime(year, np.random.randint(1, 13), np.random.randint(1, 29)) for year in birth_years]\n\n    # Generate random names\n    names = np.random.choice(latin_names + other_names, 100)\n\n    # Generate emails\n    emails = [f\"{name.lower()}{year}@{email_domain}\" for name, year in zip(names, birth_years)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': np.arange(1, 101),\n        'Name': names,\n        'Date of Birth': [date.strftime('%Y-%m-%d') for date in birth_dates],\n        'Email': emails\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Generate random birth years\n    birth_years = np.random.randint(start_year, end_year, size=100)\n    birth_dates = [datetime.datetime(year, np.random.randint(1, 13), np.random.randint(1, 29)) for year in birth_years]\n\n    # Generate random names\n    names = np.random.choice(latin_names + other_names, 100)\n\n    # Generate emails\n    emails = [f\"{name.lower()}{year}@{email_domain}\" for name, year in zip(names, birth_years)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': np.arange(1, 101),\n        'Name': names,\n        'Date of Birth': [date.strftime('%Y-%m-%d') for date in birth_dates],\n        'Email': emails\n    })\n\n    return df\n\n# Example usage:\n# df = task_func(start_year=0, rng_seed=1)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/761", "solution": "import json\nimport re\nfrom collections import Counter\nREPLACE_NONE = \"None\"\ndef task_func(json_str):\n    # Load the JSON data\n    data = json.loads(json_str)\n\n    # Replace None values with REPLACE_NONE\n    for key, value in data.items():\n        if value is None:\n            data[key] = REPLACE_NONE\n\n    # Extract email addresses and replace them with REPLACE_NONE\n    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n    for key, value in data.items():\n        if isinstance(value, str) and re.match(email_pattern, value):\n            data[key] = REPLACE_NONE\n\n    # Count the frequency of each unique value\n    value_counts = Counter(data.values())\n\n    return {\n        \"data\": data,\n        \"value_counts\": value_counts\n    }\njson_str = '{\"name\": \"John\", \"age\": null, \"email\": \"john@example.com\"}'", "raw_solution": "import json\nimport re\nfrom collections import Counter\n\n# Constants\nREPLACE_NONE = \"None\"\n\ndef task_func(json_str):\n    # Load the JSON data\n    data = json.loads(json_str)\n\n    # Replace None values with REPLACE_NONE\n    for key, value in data.items():\n        if value is None:\n            data[key] = REPLACE_NONE\n\n    # Extract email addresses and replace them with REPLACE_NONE\n    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n    for key, value in data.items():\n        if isinstance(value, str) and re.match(email_pattern, value):\n            data[key] = REPLACE_NONE\n\n    # Count the frequency of each unique value\n    value_counts = Counter(data.values())\n\n    return {\n        \"data\": data,\n        \"value_counts\": value_counts\n    }\n\n# Example usage\njson_str = '{\"name\": \"John\", \"age\": null, \"email\": \"john@example.com\"}'"}
{"task_id": "BigCodeBench/762", "solution": "import codecs\nimport os\nimport zipfile\ndef task_func(directory_name=\"latin_files\",\n              content='Sopet\u00f3n',\n              file_names=['file1.txt', 'file2.txt', 'file3.txt'],\n              encoding=\"latin-1\"):\n    # Create the directory if it doesn't exist\n    if not os.path.exists(directory_name):\n        os.makedirs(directory_name)\n\n    # Create and write content to each file\n    for file_name in file_names:\n        file_path = os.path.join(directory_name, file_name)\n        with codecs.open(file_path, 'w', encoding) as file:\n            file.write(content)\n\n    # Zip the directory\n    zip_file_name = f\"{directory_name}.zip\"\n    with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for file_name in file_names:\n            file_path = os.path.join(directory_name, file_name)\n            zip_file.write(file_path, arcname=file_name)\n\n    return zip_file_name", "raw_solution": "import codecs\nimport os\nimport zipfile\n\ndef task_func(directory_name=\"latin_files\",\n              content='Sopet\u00f3n',\n              file_names=['file1.txt', 'file2.txt', 'file3.txt'],\n              encoding=\"latin-1\"):\n    # Create the directory if it doesn't exist\n    if not os.path.exists(directory_name):\n        os.makedirs(directory_name)\n\n    # Create and write content to each file\n    for file_name in file_names:\n        file_path = os.path.join(directory_name, file_name)\n        with codecs.open(file_path, 'w', encoding) as file:\n            file.write(content)\n\n    # Zip the directory\n    zip_file_name = f\"{directory_name}.zip\"\n    with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for file_name in file_names:\n            file_path = os.path.join(directory_name, file_name)\n            zip_file.write(file_path, arcname=file_name)\n\n    return zip_file_name\n```\n"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    stats = defaultdict(lambda: {'values': []})\n\n    for entry in data:\n        for key, value in entry.items():\n            stats[key]['values'].append(value)\n\n    results = {key: {'mean': np.mean(values), 'median': np.median(values)} for key, values in stats.items()}\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Key', 'Mean', 'Median'])\n        for key, values in results.items():\n            writer.writerow([key, values['mean'], values['median']])\n\n    return results", "raw_solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    stats = defaultdict(lambda: {'values': []})\n\n    for entry in data:\n        for key, value in entry.items():\n            stats[key]['values'].append(value)\n\n    results = {key: {'mean': np.mean(values), 'median': np.median(values)} for key, values in stats.items()}\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Key', 'Mean', 'Median'])\n        for key, values in results.items():\n            writer.writerow([key, values['mean'], values['median']])\n\n    return results\n```\n"}
{"task_id": "BigCodeBench/764", "solution": "import csv\nimport random\ndef task_func(csv_file='names.csv', \n          latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n          names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'],\n          encoding='latin-1', rng_seed=None):\n    if not isinstance(csv_file, str):\n        raise TypeError(\"csv_file should be a string.\")\n    if not isinstance(latin_names, list):\n        raise TypeError(\"latin_names should be a list.\")\n    if not isinstance(names, list):\n        raise TypeError(\"names should be a list.\")\n\n    random.seed(rng_seed)\n\n    with open(csv_file, 'w', newline='', encoding=encoding) as csvfile:\n        fieldnames = ['Name', 'Age']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n\n        for _ in range(100):\n            name = random.choice(latin_names) if random.random() < 0.5 else random.choice(names)\n            age = random.randint(20, 50)\n            writer.writerow({'Name': name, 'Age': age})\n\n    return csv_file", "raw_solution": "import csv\nimport random\n\ndef task_func(csv_file='names.csv', \n          latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n          names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'],\n          encoding='latin-1', rng_seed=None):\n    if not isinstance(csv_file, str):\n        raise TypeError(\"csv_file should be a string.\")\n    if not isinstance(latin_names, list):\n        raise TypeError(\"latin_names should be a list.\")\n    if not isinstance(names, list):\n        raise TypeError(\"names should be a list.\")\n\n    random.seed(rng_seed)\n\n    with open(csv_file, 'w', newline='', encoding=encoding) as csvfile:\n        fieldnames = ['Name', 'Age']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n\n        for _ in range(100):\n            name = random.choice(latin_names) if random.random() < 0.5 else random.choice(names)\n            age = random.randint(20, 50)\n            writer.writerow({'Name': name, 'Age': age})\n\n    return csv_file\n```\n"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n\n    # Create target directory if it doesn't exist\n    if not target_path.exists():\n        target_path.mkdir(parents=True)\n\n    for file_path, content in kwargs.items():\n        if content is not None:\n            file_name = Path(file_path).name\n            new_file_path = target_path / file_name\n\n            # Write content to new file\n            with open(new_file_path, 'w') as new_file:\n                new_file.write(content)\n\n            copied_files.append(str(new_file_path))\n\n    return copied_files", "raw_solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n\n    # Create target directory if it doesn't exist\n    if not target_path.exists():\n        target_path.mkdir(parents=True)\n\n    for file_path, content in kwargs.items():\n        if content is not None:\n            file_name = Path(file_path).name\n            new_file_path = target_path / file_name\n\n            # Write content to new file\n            with open(new_file_path, 'w') as new_file:\n                new_file.write(content)\n\n            copied_files.append(str(new_file_path))\n\n    return copied_files\n\n# Example usage:\nfiles = {'/path/to/file1.txt': 'Hello', '/path/to/file2.txt': None, '/path/to/file3.txt': 'World'}"}
{"task_id": "BigCodeBench/766", "solution": "import re\nimport collections\ndef task_func(string, patterns=['nnn', 'aaa', 'sss', 'ddd', 'fff']):\n    if not isinstance(string, str):\n        raise TypeError(\"Input must be a string.\")\n    if not isinstance(patterns, list) or not all(isinstance(p, str) for p in patterns):\n        raise TypeError(\"Patterns must be a list of strings.\")\n\n    pattern_counts = collections.defaultdict(int)\n    for pattern in patterns:\n        pattern_counts[pattern] = len(re.findall(pattern, string))\n\n    return dict(pattern_counts)", "raw_solution": "import re\nimport collections\n\ndef task_func(string, patterns=['nnn', 'aaa', 'sss', 'ddd', 'fff']):\n    if not isinstance(string, str):\n        raise TypeError(\"Input must be a string.\")\n    if not isinstance(patterns, list) or not all(isinstance(p, str) for p in patterns):\n        raise TypeError(\"Patterns must be a list of strings.\")\n\n    pattern_counts = collections.defaultdict(int)\n    for pattern in patterns:\n        pattern_counts[pattern] = len(re.findall(pattern, string))\n\n    return dict(pattern_counts)\n\n# Test cases"}
{"task_id": "BigCodeBench/767", "solution": "from collections import Counter\nimport random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(list_of_lists):\n    random_letter = random.choice(LETTERS)\n    flat_list = [item for sublist in list_of_lists for item in sublist]\n    letter_count = Counter(flat_list)\n    return {random_letter: letter_count[random_letter]}", "raw_solution": "from collections import Counter\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef task_func(list_of_lists):\n    random_letter = random.choice(LETTERS)\n    flat_list = [item for sublist in list_of_lists for item in sublist]\n    letter_count = Counter(flat_list)\n    return {random_letter: letter_count[random_letter]}\n\n# Example usage:\nrandom.seed(42)"}
{"task_id": "BigCodeBench/768", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path):\n    if not os.path.isdir(dir_path):\n        raise ValueError(f\"The path {dir_path} is not a valid directory.\")\n\n    error_count = {}\n    pattern = re.compile(r'\\berror\\b', re.IGNORECASE)\n\n    for file_path in glob.glob(os.path.join(dir_path, '**', '*.txt'), recursive=True):\n        relative_path = os.path.relpath(file_path, dir_path)\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            count = len(pattern.findall(content))\n            error_count[relative_path] = count\n\n    return error_count", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(dir_path):\n    if not os.path.isdir(dir_path):\n        raise ValueError(f\"The path {dir_path} is not a valid directory.\")\n\n    error_count = {}\n    pattern = re.compile(r'\\berror\\b', re.IGNORECASE)\n\n    for file_path in glob.glob(os.path.join(dir_path, '**', '*.txt'), recursive=True):\n        relative_path = os.path.relpath(file_path, dir_path)\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            count = len(pattern.findall(content))\n            error_count[relative_path] = count\n\n    return error_count\n```\n"}
{"task_id": "BigCodeBench/769", "solution": "from collections import Counter\nimport itertools\nimport operator\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Ek menu items ki ek jatil list ka samna karte hue, list ko samtal kare and most common menu items lautaye.\n\n    Parameters:\n    - list_of_menuitems (list): Ek menu items ki ek jatil list.\n\n    Returns:\n        str: most common menu items.\n\n    Requirements:\n    - collections\n    - itertools\n    - operator\n\n    Example:\n    >>> task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    'Pizza'\n\n    \"\"\"\n    # Flatten the list of lists\n    flattened_list = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count the frequency of each menu item\n    counter = Counter(flattened_list)\n    \n    # Find the most common menu item\n    most_common_item, _ = counter.most_common(1)[0]\n    \n    return most_common_item", "raw_solution": "from collections import Counter\nimport itertools\nimport operator\n\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Ek menu items ki ek jatil list ka samna karte hue, list ko samtal kare and most common menu items lautaye.\n\n    Parameters:\n    - list_of_menuitems (list): Ek menu items ki ek jatil list.\n\n    Returns:\n        str: most common menu items.\n\n    Requirements:\n    - collections\n    - itertools\n    - operator\n\n    Example:\n    >>> task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    'Pizza'\n\n    \"\"\"\n    # Flatten the list of lists\n    flattened_list = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count the frequency of each menu item\n    counter = Counter(flattened_list)\n    \n    # Find the most common menu item\n    most_common_item, _ = counter.most_common(1)[0]\n    \n    return most_common_item\n```\n"}
{"task_id": "BigCodeBench/770", "solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndef task_func(num_samples=500, noise_strength=1, random_seed=None, test_size=0.2):\n    if test_size < 0.2:\n        raise ValueError(\"Test size must be at least 0.2\")\n\n    np.random.seed(random_seed)\n\n    # Generate random data\n    X = np.random.rand(num_samples, 1) * 10  # single feature\n    noise = np.random.randn(num_samples, 1) * noise_strength\n    y = 2 * X + 3 + noise  # target with noise\n\n    # Split the dataset\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n\n    # Train the model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Calculate the R-squared score\n    r_squared = model.score(X_test, y_test)\n\n    return r_squared, model", "raw_solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(num_samples=500, noise_strength=1, random_seed=None, test_size=0.2):\n    if test_size < 0.2:\n        raise ValueError(\"Test size must be at least 0.2\")\n\n    np.random.seed(random_seed)\n\n    # Generate random data\n    X = np.random.rand(num_samples, 1) * 10  # single feature\n    noise = np.random.randn(num_samples, 1) * noise_strength\n    y = 2 * X + 3 + noise  # target with noise\n\n    # Split the dataset\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n\n    # Train the model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Calculate the R-squared score\n    r_squared = model.score(X_test, y_test)\n\n    return r_squared, model\n```\n"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    path = Path(directory)\n\n    for file in path.glob('*.csv'):\n        file_name = file.name\n        if re.match(pattern, file_name):\n            new_file_name = re.sub(r'-\\d+', '', file_name)\n            new_file_path = path / new_file_name\n            if not new_file_path.exists():\n                new_files.append(new_file_name)\n                with open(file, mode='r', newline='', encoding='utf-8') as f:\n                    reader = csv.reader(f)\n                    with open(new_file_path, mode='w', newline='', encoding='utf-8') as nf:\n                        writer = csv.writer(nf)\n                        for row in reader:\n                            writer.writerow(row)\n\n    return new_files", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    path = Path(directory)\n\n    for file in path.glob('*.csv'):\n        file_name = file.name\n        if re.match(pattern, file_name):\n            new_file_name = re.sub(r'-\\d+', '', file_name)\n            new_file_path = path / new_file_name\n            if not new_file_path.exists():\n                new_files.append(new_file_name)\n                with open(file, mode='r', newline='', encoding='utf-8') as f:\n                    reader = csv.reader(f)\n                    with open(new_file_path, mode='w', newline='', encoding='utf-8') as nf:\n                        writer = csv.writer(nf)\n                        for row in reader:\n                            writer.writerow(row)\n\n    return new_files\n```\n"}
{"task_id": "BigCodeBench/772", "solution": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\ndef task_func(num_samples=1000, k=5, d=2, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random samples\n    X = np.random.randn(num_samples, 1)\n\n    # Apply linear transformation\n    Y = k * X + d\n\n    # Standardize the data\n    scaler = StandardScaler()\n    X_standardized = scaler.fit_transform(X)\n    Y_standardized = scaler.transform(Y.reshape(-1, 1))\n\n    # Calculate mean squared error\n    mse = mean_squared_error(X_standardized, Y_standardized)\n\n    return mse", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(num_samples=1000, k=5, d=2, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random samples\n    X = np.random.randn(num_samples, 1)\n\n    # Apply linear transformation\n    Y = k * X + d\n\n    # Standardize the data\n    scaler = StandardScaler()\n    X_standardized = scaler.fit_transform(X)\n    Y_standardized = scaler.transform(Y.reshape(-1, 1))\n\n    # Calculate mean squared error\n    mse = mean_squared_error(X_standardized, Y_standardized)\n\n    return mse\n\n# Test cases"}
{"task_id": "BigCodeBench/773", "solution": "import os\nimport re\nimport shutil\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\ndef task_func():\n    \"\"\"\n    Ek source directory mein maujood sabhi json ko ek target directory mein le jaen and filename ko last time \"-\" aane par vibhajit karke and filename ke prefix part ko rakhkar unka naam badlen.\n\n    Parameters:\n    - None\n\n    Returns:\n        None\n\n    Requirements:\n    - os\n    - re\n    - shutil\n\n    Example:\n    >>> task_func()\n\n    \"\"\"\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    for filename in os.listdir(SOURCE_DIR):\n        match = FILE_PATTERN.match(filename)\n        if match:\n            base_name = match.group(1)\n            new_filename = f\"{base_name}.json\"\n            shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename))", "raw_solution": "import os\nimport re\nimport shutil\n\n# Constants\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\n\ndef task_func():\n    \"\"\"\n    Ek source directory mein maujood sabhi json ko ek target directory mein le jaen and filename ko last time \"-\" aane par vibhajit karke and filename ke prefix part ko rakhkar unka naam badlen.\n\n    Parameters:\n    - None\n\n    Returns:\n        None\n\n    Requirements:\n    - os\n    - re\n    - shutil\n\n    Example:\n    >>> task_func()\n\n    \"\"\"\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    for filename in os.listdir(SOURCE_DIR):\n        match = FILE_PATTERN.match(filename)\n        if match:\n            base_name = match.group(1)\n            new_filename = f\"{base_name}.json\"\n            shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename))\n\n# Example usage\ntask_func()\n```\nThis script will move all files in the source directory that match the pattern `^(.*?)-\\d+\\.json$` to the target directory, renaming them to just the base name without the number and extension. It will create the target directory if it does not exist.\n"}
{"task_id": "BigCodeBench/774", "solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ndef task_func(num_samples=100, n_estimators=100, random_seed=None, cv=5):\n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv must be at least 2\")\n\n    np.random.seed(random_seed)\n\n    # Generate random data\n    X = np.random.randn(num_samples, 5)\n    y = np.sum(X, axis=1) + np.random.randn(num_samples)\n\n    # Initialize and train the model\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_seed)\n    model.fit(X, y)\n\n    # Perform cross-validation\n    scores = cross_val_score(model, X, y, cv=cv)\n\n    return np.mean(scores), model", "raw_solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef task_func(num_samples=100, n_estimators=100, random_seed=None, cv=5):\n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv must be at least 2\")\n\n    np.random.seed(random_seed)\n\n    # Generate random data\n    X = np.random.randn(num_samples, 5)\n    y = np.sum(X, axis=1) + np.random.randn(num_samples)\n\n    # Initialize and train the model\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_seed)\n    model.fit(X, y)\n\n    # Perform cross-validation\n    scores = cross_val_score(model, X, y, cv=cv)\n\n    return np.mean(scores), model\n\n# Example usage:\n# results = task_func(random_seed=1)\n# print(results)\n```\n"}
{"task_id": "BigCodeBench/775", "solution": "from string import ascii_lowercase\nimport re\nfrom collections import Counter\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\nLETTERS = ascii_lowercase\ndef task_func(string):\n    \"\"\"\n    Yadi ek **string** maujood hai, to ise antim baar \"-\" aane par vibhajit karein aur **string** ke **prefix** mein pratyek **lowercase letter** ki **frequency** ko ginein.\n\n    Parameters:\n    - string (str): The **input** **string**.\n\n    Returns:\n        dict: ek **dictionary** jisme pratyek **lowercase letter** ki **frequency** ho.\n\n    Requirements:\n    - string\n    - re\n    - collections\n\n    Example:\n    >>> task_func('abc-def-ghij')\n    {'a': 1, 'b': 1, 'c': 1, 'd': 1, 'e': 1, 'f': 1, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 0, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'u': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0}\n\n    \"\"\"\n    # Extract the prefix before the hyphen\n    match = LETTERS_PATTERN.match(string)\n    prefix = match.group(1) if match else ''\n\n    # Count the frequency of each letter in the prefix\n    letter_count = Counter(prefix)\n\n    # Create a dictionary with all lowercase letters initialized to 0\n    result = {letter: letter_count.get(letter, 0) for letter in LETTERS}\n\n    return result", "raw_solution": "from string import ascii_lowercase\nimport re\nfrom collections import Counter\n\n# Constants\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\nLETTERS = ascii_lowercase\n\ndef task_func(string):\n    \"\"\"\n    Yadi ek **string** maujood hai, to ise antim baar \"-\" aane par vibhajit karein aur **string** ke **prefix** mein pratyek **lowercase letter** ki **frequency** ko ginein.\n\n    Parameters:\n    - string (str): The **input** **string**.\n\n    Returns:\n        dict: ek **dictionary** jisme pratyek **lowercase letter** ki **frequency** ho.\n\n    Requirements:\n    - string\n    - re\n    - collections\n\n    Example:\n    >>> task_func('abc-def-ghij')\n    {'a': 1, 'b': 1, 'c': 1, 'd': 1, 'e': 1, 'f': 1, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 0, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'u': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0}\n\n    \"\"\"\n    # Extract the prefix before the hyphen\n    match = LETTERS_PATTERN.match(string)\n    prefix = match.group(1) if match else ''\n\n    # Count the frequency of each letter in the prefix\n    letter_count = Counter(prefix)\n\n    # Create a dictionary with all lowercase letters initialized to 0\n    result = {letter: letter_count.get(letter, 0) for letter in LETTERS}\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/776", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(file_path, output_path=None, sort_key='title', linear_regression=False, x_column=None, y_column=None):\n    try:\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise Exception(f\"Error reading the file: {e}\")\n\n    try:\n        df = df.sort_values(by=sort_key)\n    except Exception as e:\n        raise Exception(f\"Error sorting the dataframe: {e}\")\n\n    if linear_regression:\n        if x_column not in df.columns or y_column not in df.columns:\n            raise ValueError(\"Linear regression columns not found in dataframe\")\n        try:\n            model = LinearRegression().fit(df[[x_column]], df[y_column])\n        except Exception as e:\n            raise Exception(f\"Error fitting the linear regression model: {e}\")\n        return model\n    else:\n        if output_path:\n            try:\n                df.to_csv(output_path, index=False)\n            except Exception as e:\n                raise Exception(f\"Error writing the output file: {e}\")\n            return output_path\n        else:\n            return df", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(file_path, output_path=None, sort_key='title', linear_regression=False, x_column=None, y_column=None):\n    try:\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise Exception(f\"Error reading the file: {e}\")\n\n    try:\n        df = df.sort_values(by=sort_key)\n    except Exception as e:\n        raise Exception(f\"Error sorting the dataframe: {e}\")\n\n    if linear_regression:\n        if x_column not in df.columns or y_column not in df.columns:\n            raise ValueError(\"Linear regression columns not found in dataframe\")\n        try:\n            model = LinearRegression().fit(df[[x_column]], df[y_column])\n        except Exception as e:\n            raise Exception(f\"Error fitting the linear regression model: {e}\")\n        return model\n    else:\n        if output_path:\n            try:\n                df.to_csv(output_path, index=False)\n            except Exception as e:\n                raise Exception(f\"Error writing the output file: {e}\")\n            return output_path\n        else:\n            return df\n```\n"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.zip'):\n            match = re.match(pattern, filename)\n            if match:\n                prefix = match.group(1)\n                new_dir = os.path.join(directory, prefix)\n                if not os.path.exists(new_dir):\n                    os.makedirs(new_dir)\n                with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                    zip_ref.extractall(new_dir)\n                extracted_dirs.append(new_dir)\n\n    return extracted_dirs", "raw_solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.zip'):\n            match = re.match(pattern, filename)\n            if match:\n                prefix = match.group(1)\n                new_dir = os.path.join(directory, prefix)\n                if not os.path.exists(new_dir):\n                    os.makedirs(new_dir)\n                with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                    zip_ref.extractall(new_dir)\n                extracted_dirs.append(new_dir)\n\n    return extracted_dirs\n\n# Example usage:\n# print(task_func('/tmp/my_data'))\n```\nThis script will create a new directory for each zip file that matches the given pattern, and extract the contents of the zip file into that directory. The function returns a list of the created directories.\n"}
{"task_id": "BigCodeBench/778", "solution": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\ndef task_func(news_articles):\n    if not all(isinstance(article, dict) for article in news_articles):\n        raise ValueError(\"All items in news_articles must be dictionaries.\")\n\n    required_keys = {'title', 'title_url', 'id', 'category'}\n    if not all(required_keys.issubset(article.keys()) for article in news_articles):\n        raise ValueError(\"Each article must contain 'title', 'title_url', 'id', and 'category' keys.\")\n\n    # Sort articles by category\n    sorted_articles = sorted(news_articles, key=itemgetter('category'))\n\n    # Group articles by category\n    grouped_articles = groupby(sorted_articles, key=itemgetter('category'))\n\n    # Create a defaultdict to store articles by category\n    category_articles = defaultdict(list)\n    for category, articles in grouped_articles:\n        category_articles[category].extend(articles)\n\n    return category_articles\narticles = [\n    {'title': 'Der Standard', 'title_url': 'standard', 'id': 2, 'category': 'climate'},\n    {'title': 'tecky', 'title_url': 'tecky', 'id': 4, 'category': 'climate'},\n    {'title': 'earth magazine', 'title_url': 'earth', 'id': 4, 'category': 'environment'}\n]", "raw_solution": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\n\ndef task_func(news_articles):\n    if not all(isinstance(article, dict) for article in news_articles):\n        raise ValueError(\"All items in news_articles must be dictionaries.\")\n\n    required_keys = {'title', 'title_url', 'id', 'category'}\n    if not all(required_keys.issubset(article.keys()) for article in news_articles):\n        raise ValueError(\"Each article must contain 'title', 'title_url', 'id', and 'category' keys.\")\n\n    # Sort articles by category\n    sorted_articles = sorted(news_articles, key=itemgetter('category'))\n\n    # Group articles by category\n    grouped_articles = groupby(sorted_articles, key=itemgetter('category'))\n\n    # Create a defaultdict to store articles by category\n    category_articles = defaultdict(list)\n    for category, articles in grouped_articles:\n        category_articles[category].extend(articles)\n\n    return category_articles\n\n# Test cases\narticles = [\n    {'title': 'Der Standard', 'title_url': 'standard', 'id': 2, 'category': 'climate'},\n    {'title': 'tecky', 'title_url': 'tecky', 'id': 4, 'category': 'climate'},\n    {'title': 'earth magazine', 'title_url': 'earth', 'id': 4, 'category': 'environment'}\n]\n\nsorted_articles = task_func(articles)"}
{"task_id": "BigCodeBench/779", "solution": "import os\nimport shutil\nimport time\nBACKUP_DIR = '/tmp/backup'\ndef task_func(directory):\n    \"\"\"\n    Ek directory ka backup banaen and afterwards directory ko saaf karen.\n\n    Notes:\n        Function backup directory path and errors ki ek list return karega (yadi koi ho).\n\n    Parameters:\n    - directory (str): backup and saaf ki jaane wali directory path.\n\n    Returns:\n        tuple: Ek tuple jismein shaamil hain:\n        str: backup directory path.\n        list: Operation ke dauraan aane wali kisi bhi errors ki ek list (agar koi errors nahi hain to khaali list).\n\n    Requirements:\n    - os\n    - shutil\n    - time\n\n    Example:\n    >>> task_func('/tmp/my_data')\n    ('/tmp/backup/backup_20230827010101', [])\n\n    \"\"\"\n    errors = []\n    try:\n        # Create a unique backup directory name\n        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n        backup_path = os.path.join(BACKUP_DIR, f'backup_{timestamp}')\n        \n        # Create the backup directory\n        os.makedirs(backup_path, exist_ok=True)\n        \n        # Copy the directory to the backup location\n        shutil.copytree(directory, backup_path)\n        \n        # Safing the directory\n        shutil.rmtree(directory)\n        os.makedirs(directory, exist_ok=True)\n        \n        return backup_path, errors\n    except Exception as e:\n        errors.append(str(e))\n        return backup_path, errors", "raw_solution": "import os\nimport shutil\nimport time\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef get_unique_backup_dir():\n    return \"/fake/backup/path\"\n\ndef task_func(directory):\n    \"\"\"\n    Ek directory ka backup banaen and afterwards directory ko saaf karen.\n\n    Notes:\n        Function backup directory path and errors ki ek list return karega (yadi koi ho).\n\n    Parameters:\n    - directory (str): backup and saaf ki jaane wali directory path.\n\n    Returns:\n        tuple: Ek tuple jismein shaamil hain:\n        str: backup directory path.\n        list: Operation ke dauraan aane wali kisi bhi errors ki ek list (agar koi errors nahi hain to khaali list).\n\n    Requirements:\n    - os\n    - shutil\n    - time\n\n    Example:\n    >>> task_func('/tmp/my_data')\n    ('/tmp/backup/backup_20230827010101', [])\n\n    \"\"\"\n    errors = []\n    try:\n        # Create a unique backup directory name\n        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n        backup_path = os.path.join(BACKUP_DIR, f'backup_{timestamp}')\n        \n        # Create the backup directory\n        os.makedirs(backup_path, exist_ok=True)\n        \n        # Copy the directory to the backup location\n        shutil.copytree(directory, backup_path)\n        \n        # Safing the directory\n        shutil.rmtree(directory)\n        os.makedirs(directory, exist_ok=True)\n        \n        return backup_path, errors\n    except Exception as e:\n        errors.append(str(e))\n        return backup_path, errors\n```\n"}
{"task_id": "BigCodeBench/780", "solution": "import pandas as pd\nimport pytz\ndef task_func(articles, timezone):\n    if not isinstance(articles, list) or not all(isinstance(article, dict) for article in articles):\n        raise TypeError(\"Articles should be a list of dictionaries.\")\n    \n    required_keys = {'title', 'title_url', 'id', 'category', 'published_time'}\n    for article in articles:\n        if not required_keys.issubset(article.keys()):\n            raise ValueError(\"Each article dictionary must contain the required keys.\")\n    \n    if not isinstance(timezone, str):\n        raise TypeError(\"Timezone should be a string.\")\n    \n    try:\n        tz = pytz.timezone(timezone)\n    except pytz.UnknownTimeZoneError:\n        raise ValueError(\"Unknown timezone provided.\")\n    \n    # Convert 'published_time' to the specified timezone\n    for article in articles:\n        article['published_time'] = article['published_time'].astimezone(tz)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(articles)\n    \n    # Group by 'category' and calculate required statistics\n    grouped = df.groupby('category')['published_time'].agg(['count', 'mean', 'min', 'max'])\n    \n    return grouped", "raw_solution": "import pandas as pd\nimport pytz\n\ndef task_func(articles, timezone):\n    if not isinstance(articles, list) or not all(isinstance(article, dict) for article in articles):\n        raise TypeError(\"Articles should be a list of dictionaries.\")\n    \n    required_keys = {'title', 'title_url', 'id', 'category', 'published_time'}\n    for article in articles:\n        if not required_keys.issubset(article.keys()):\n            raise ValueError(\"Each article dictionary must contain the required keys.\")\n    \n    if not isinstance(timezone, str):\n        raise TypeError(\"Timezone should be a string.\")\n    \n    try:\n        tz = pytz.timezone(timezone)\n    except pytz.UnknownTimeZoneError:\n        raise ValueError(\"Unknown timezone provided.\")\n    \n    # Convert 'published_time' to the specified timezone\n    for article in articles:\n        article['published_time'] = article['published_time'].astimezone(tz)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(articles)\n    \n    # Group by 'category' and calculate required statistics\n    grouped = df.groupby('category')['published_time'].agg(['count', 'mean', 'min', 'max'])\n    \n    return grouped\n```\n"}
{"task_id": "BigCodeBench/781", "solution": "import os\nfrom datetime import datetime\ndef task_func(filepath: str) -> dict:\n    \"\"\"\n    Ek file ke size and last modification ki date nirdharit karein.\n\n    Parameters:\n    - filepath (str): The path to the file.\n\n    Returns:\n        dict: Ek dictionary jismein size (bytes mein) and file ki last modification\n        date '%Y-%m-%d %H:%M:%S' format mein shaamil hai.\n\n    Requirements:\n    - os\n    - datetime\n\n    Example:\n    >>> task_func('/path/to/file.txt')\n    {'size': '1024 bytes', 'last_modified': '2022-01-01 12:30:45'}\n\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n    \n    file_size = os.path.getsize(filepath)\n    last_modified_time = os.path.getmtime(filepath)\n    last_modified_date = datetime.fromtimestamp(last_modified_time).strftime('%Y-%m-%d %H:%M:%S')\n\n    return {'size': f\"{file_size} bytes\", 'last_modified': last_modified_date}", "raw_solution": "import os\nfrom datetime import datetime\n\ndef task_func(filepath: str) -> dict:\n    \"\"\"\n    Ek file ke size and last modification ki date nirdharit karein.\n\n    Parameters:\n    - filepath (str): The path to the file.\n\n    Returns:\n        dict: Ek dictionary jismein size (bytes mein) and file ki last modification\n        date '%Y-%m-%d %H:%M:%S' format mein shaamil hai.\n\n    Requirements:\n    - os\n    - datetime\n\n    Example:\n    >>> task_func('/path/to/file.txt')\n    {'size': '1024 bytes', 'last_modified': '2022-01-01 12:30:45'}\n\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n    \n    file_size = os.path.getsize(filepath)\n    last_modified_time = os.path.getmtime(filepath)\n    last_modified_date = datetime.fromtimestamp(last_modified_time).strftime('%Y-%m-%d %H:%M:%S')\n\n    return {'size': f\"{file_size} bytes\", 'last_modified': last_modified_date}\n```\n"}
{"task_id": "BigCodeBench/782", "solution": "import random\nimport pandas as pd\nimport numpy as np\ndef task_func(n,\n          domain=\"samplewebsite.com\",\n          categories=['Sports', 'Technology', 'Health', 'Science', 'Business'],\n          random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n        np.random.seed(random_seed)\n\n    data = {\n        'title': [],\n        'title_url': [],\n        'id': [],\n        'category': [],\n        'views': []\n    }\n\n    for i in range(n):\n        title = f\"Article {i}\"\n        title_url = f\"{domain}/Article_{i}\"\n        category = random.choice(categories)\n        views = np.random.poisson(1000)\n        data['title'].append(title)\n        data['title_url'].append(title_url)\n        data['id'].append(i)\n        data['category'].append(category)\n        data['views'].append(views)\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import random\nimport pandas as pd\nimport numpy as np\n\ndef task_func(n,\n          domain=\"samplewebsite.com\",\n          categories=['Sports', 'Technology', 'Health', 'Science', 'Business'],\n          random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n        np.random.seed(random_seed)\n\n    data = {\n        'title': [],\n        'title_url': [],\n        'id': [],\n        'category': [],\n        'views': []\n    }\n\n    for i in range(n):\n        title = f\"Article {i}\"\n        title_url = f\"{domain}/Article_{i}\"\n        category = random.choice(categories)\n        views = np.random.poisson(1000)\n        data['title'].append(title)\n        data['title_url'].append(title_url)\n        data['id'].append(i)\n        data['category'].append(category)\n        data['views'].append(views)\n\n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\n# df = task_func(5, random_seed=1)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/783", "solution": "import os\nimport shutil\ndef task_func(src_dir, dest_dir, extension):\n    \"\"\"\n    Ek directory se doosri mein ek particular extension wali sabhi files ko le jaen.\n\n    Parameters:\n    - src_dir (str): source directory.\n    - dest_dir (str): destination directory.\n    - extension (str): file extension.\n\n    Returns:\n        files_moved (int): Le jaya gaya files ki number.\n\n    Requirements:\n    - os\n    - shutil\n\n    Example:\n    >>> task_func('/path/to/src', '/path/to/dest', '.txt')\n\n    \"\"\"\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    files_moved = 0\n    for filename in os.listdir(src_dir):\n        if filename.endswith(extension):\n            shutil.move(os.path.join(src_dir, filename), dest_dir)\n            files_moved += 1\n\n    return files_moved", "raw_solution": "import os\nimport shutil\n\ndef task_func(src_dir, dest_dir, extension):\n    \"\"\"\n    Ek directory se doosri mein ek particular extension wali sabhi files ko le jaen.\n\n    Parameters:\n    - src_dir (str): source directory.\n    - dest_dir (str): destination directory.\n    - extension (str): file extension.\n\n    Returns:\n        files_moved (int): Le jaya gaya files ki number.\n\n    Requirements:\n    - os\n    - shutil\n\n    Example:\n    >>> task_func('/path/to/src', '/path/to/dest', '.txt')\n\n    \"\"\"\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    files_moved = 0\n    for filename in os.listdir(src_dir):\n        if filename.endswith(extension):\n            shutil.move(os.path.join(src_dir, filename), dest_dir)\n            files_moved += 1\n\n    return files_moved\n```\n"}
{"task_id": "BigCodeBench/784", "solution": "import pandas as pd\nimport random\nimport csv\ndef task_func(n, \n           categories=['Sports', 'Technology', 'Business', 'Politics', 'Entertainment'],\n           news_sites=['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC'],\n           likert_scale=['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree'],\n           file_path='news_survey_data.csv',\n           random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1\n        data.append([site, category, response, value])\n\n    df = pd.DataFrame(data, columns=['site', 'category', 'response', 'value'])\n\n    df.to_csv(file_path, index=False, quoting=csv.QUOTE_NONNUMERIC)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport csv\n\ndef task_func(n, \n           categories=['Sports', 'Technology', 'Business', 'Politics', 'Entertainment'],\n           news_sites=['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC'],\n           likert_scale=['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree'],\n           file_path='news_survey_data.csv',\n           random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1\n        data.append([site, category, response, value])\n\n    df = pd.DataFrame(data, columns=['site', 'category', 'response', 'value'])\n\n    df.to_csv(file_path, index=False, quoting=csv.QUOTE_NONNUMERIC)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    files ko archive karein jo ek vishesh *pattern* se mel khate hain aur phir mool files ko hata dein.\n\n    Notes:\n        Yah *function* archive *file* *path* lautaega.\n\n    Parameters:\n    - pattern (str): *pattern* files se mel khane ke liye.\n\n    Returns:\n        *archive_file* (str): archive *file* *path*.\n\n    Requirements:\n    - subprocess\n    - os\n    - glob\n\n    Example:\n    >>> task_func('*.txt')\n\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find files matching the pattern\n    files = glob.glob(pattern)\n    \n    # Archive files\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar.gz')\n    subprocess.run(['tar', '-czf', archive_file] + files, check=True)\n\n    # Remove files\n    for file in files:\n        os.remove(file)\n\n    return archive_file", "raw_solution": "import subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    files ko archive karein jo ek vishesh *pattern* se mel khate hain aur phir mool files ko hata dein.\n\n    Notes:\n        Yah *function* archive *file* *path* lautaega.\n\n    Parameters:\n    - pattern (str): *pattern* files se mel khane ke liye.\n\n    Returns:\n        *archive_file* (str): archive *file* *path*.\n\n    Requirements:\n    - subprocess\n    - os\n    - glob\n\n    Example:\n    >>> task_func('*.txt')\n\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find files matching the pattern\n    files = glob.glob(pattern)\n    \n    # Archive files\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar.gz')\n    subprocess.run(['tar', '-czf', archive_file] + files, check=True)\n\n    # Remove files\n    for file in files:\n        os.remove(file)\n\n    return archive_file\n```\n"}
{"task_id": "BigCodeBench/786", "solution": "import pandas as pd\nimport csv\nimport random\ndef task_func(\n    n, \n    countries=['USA', 'UK', 'China', 'India', 'Germany'], \n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'], \n    output_path=None,\n    random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        country = random.choice(countries)\n        product = random.choice(products)\n        sales = random.randint(1, 100)\n        data.append([country, product, sales])\n\n    df = pd.DataFrame(data, columns=['Country', 'product', 'Sales'])\n\n    if output_path:\n        df.to_csv(output_path, index=False)\n\n    return df", "raw_solution": "import pandas as pd\nimport csv\nimport random\n\ndef task_func(\n    n, \n    countries=['USA', 'UK', 'China', 'India', 'Germany'], \n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'], \n    output_path=None,\n    random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        country = random.choice(countries)\n        product = random.choice(products)\n        sales = random.randint(1, 100)\n        data.append([country, product, sales])\n\n    df = pd.DataFrame(data, columns=['Country', 'product', 'Sales'])\n\n    if output_path:\n        df.to_csv(output_path, index=False)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/787", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(array1, array2):\n    if array1.size == 0 or array2.size == 0:\n        return 0.0\n\n    if array1.size != array2.size:\n        raise ValueError(\"Both arrays must have the same length.\")\n\n    max_distance = 0.0\n    for (x1, y1), (x2, y2) in combinations(zip(array1, array2), 2):\n        distance = np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n        max_distance = max(max_distance, distance)\n\n    return max_distance\narray1 = np.array([2, 3, 4])\narray2 = np.array([1, 5, 2])", "raw_solution": "import numpy as np\nfrom itertools import combinations\n\ndef task_func(array1, array2):\n    if array1.size == 0 or array2.size == 0:\n        return 0.0\n\n    if array1.size != array2.size:\n        raise ValueError(\"Both arrays must have the same length.\")\n\n    max_distance = 0.0\n    for (x1, y1), (x2, y2) in combinations(zip(array1, array2), 2):\n        distance = np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n        max_distance = max(max_distance, distance)\n\n    return max_distance\n\n# Example usage:\narray1 = np.array([2, 3, 4])\narray2 = np.array([1, 5, 2])"}
{"task_id": "BigCodeBench/788", "solution": "import heapq\nfrom scipy import stats\nimport pandas as pd\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the DataFrame.\")\n    if N <= 1:\n        raise ValueError(\"N must be greater than 1.\")\n    \n    # Calculate absolute differences and get the N largest\n    differences = df[col1] - df[col2]\n    largest_diffs = heapq.nlargest(N, differences, key=abs)\n    \n    # Perform t-test on the N largest differences\n    t_stat, p_value = stats.ttest_1samp(largest_diffs, 0)\n    \n    return p_value", "raw_solution": "import heapq\nfrom scipy import stats\nimport pandas as pd\n\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the DataFrame.\")\n    if N <= 1:\n        raise ValueError(\"N must be greater than 1.\")\n    \n    # Calculate absolute differences and get the N largest\n    differences = df[col1] - df[col2]\n    largest_diffs = heapq.nlargest(N, differences, key=abs)\n    \n    # Perform t-test on the N largest differences\n    t_stat, p_value = stats.ttest_1samp(largest_diffs, 0)\n    \n    return p_value\n\n# Example usage:\n# df = pd.DataFrame({\n#     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81],\n#     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n# })\n# p_value = task_func(df, 'col1', 'col2', N=5)\n# print(p_value)\n```\n"}
{"task_id": "BigCodeBench/789", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nARRAY_LENGTH = 10\ndef task_func():\n    # Generate a random array\n    random_array = np.random.rand(ARRAY_LENGTH, 1)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Scale the array\n    scaled_array = scaler.fit_transform(random_array)\n\n    return scaled_array", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nARRAY_LENGTH = 10\n\ndef task_func():\n    # Generate a random array\n    random_array = np.random.rand(ARRAY_LENGTH, 1)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Scale the array\n    scaled_array = scaler.fit_transform(random_array)\n\n    return scaled_array\n\n# Example usage"}
{"task_id": "BigCodeBench/790", "solution": "import heapq\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the DataFrame\")\n\n    scaler = StandardScaler()\n    df[[col1, col2]] = scaler.fit_transform(df[[col1, col2]])\n\n    differences = []\n    for i in range(len(df)):\n        diff = abs(df.loc[i, col1] - df.loc[i, col2])\n        differences.append((diff, i))\n\n    largest_diff_indices = heapq.nlargest(N, differences, key=lambda x: x[0])\n    largest_diff_indices = [index for _, index in largest_diff_indices]\n\n    return largest_diff_indices", "raw_solution": "import heapq\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the DataFrame\")\n\n    scaler = StandardScaler()\n    df[[col1, col2]] = scaler.fit_transform(df[[col1, col2]])\n\n    differences = []\n    for i in range(len(df)):\n        diff = abs(df.loc[i, col1] - df.loc[i, col2])\n        differences.append((diff, i))\n\n    largest_diff_indices = heapq.nlargest(N, differences, key=lambda x: x[0])\n    largest_diff_indices = [index for _, index in largest_diff_indices]\n\n    return largest_diff_indices\n\n# Example usage:\n# df = pd.DataFrame({\n#     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81, 1, 2],\n#     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37, 3, 4]\n# })\n# print(indices = task_func(df, 'col1', 'col2', N=6))\n# print(indices)\n```\nThis script first standardizes the two columns using `StandardScaler` from `sklearn.preprocessing`. Then it calculates the absolute difference between the two columns for each row and stores these differences along with their indices in a list. The `heapq.nlargest` function is used to find the N largest differences, and their corresponding indices are returned.\n"}
{"task_id": "BigCodeBench/791", "solution": "from collections import Counter\nimport random\nfrom itertools import cycle\ndef task_func(l):\n    # Create a cycle and shuffle the first 30 elements\n    cycled_l = cycle(l)\n    shuffled_l = random.sample(list(cycled_l)[:30], 30)\n\n    # Create a counter from the shuffled list\n    counter = Counter(shuffled_l)\n\n    return counter", "raw_solution": "from collections import Counter\nimport random\nfrom itertools import cycle\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef task_func(l):\n    # Create a cycle and shuffle the first 30 elements\n    cycled_l = cycle(l)\n    shuffled_l = random.sample(list(cycled_l)[:30], 30)\n\n    # Create a counter from the shuffled list\n    counter = Counter(shuffled_l)\n\n    return counter\n\n# Example usage"}
{"task_id": "BigCodeBench/792", "solution": "import heapq\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\ndef task_func(df, feature, target, n=10):\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(\"Columns specified do not exist in the DataFrame\")\n\n    X = df[[feature]].values\n    y = df[target].values\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    residuals = model.predict(X) - y\n    largest_residual_indices = heapq.nlargest(n, range(len(residuals)), key=residuals.__getitem__)\n\n    return largest_residual_indices, model", "raw_solution": "import heapq\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\ndef task_func(df, feature, target, n=10):\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(\"Columns specified do not exist in the DataFrame\")\n\n    X = df[[feature]].values\n    y = df[target].values\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    residuals = model.predict(X) - y\n    largest_residual_indices = heapq.nlargest(n, range(len(residuals)), key=residuals.__getitem__)\n\n    return largest_residual_indices, model\n\n# Example usage:\n# df = pd.DataFrame({\n#     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81],\n#     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n# })\n# print(indices, model = task_func(df, 'col1', 'col2', n=5))\n# print(print(indices))\n# print(model)\n```\n"}
{"task_id": "BigCodeBench/793", "solution": "import numpy as np\nimport random\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\ndef task_func(l=None):\n    if l is None:\n        l = [random.choice(ELEMENTS) for _ in range(10)]\n\n    arr = np.array(l)\n    last_three = arr[-3:]\n    arr = np.concatenate((arr[:-3], last_three))\n\n    return arr", "raw_solution": "import numpy as np\nimport random\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef task_func(l=None):\n    if l is None:\n        l = [random.choice(ELEMENTS) for _ in range(10)]\n\n    arr = np.array(l)\n    last_three = arr[-3:]\n    arr = np.concatenate((arr[:-3], last_three))\n\n    return arr\n\n# Example usage:\nrandom.seed(42)"}
{"task_id": "BigCodeBench/794", "solution": "import string\nimport random\nBRACKETS = \"(){}[]\"\ndef task_func(length, random_seed=None):\n    \"\"\"\n    Generate a random string of a given length, where each character can be:\n    - An opening or closing bracket from the BRACKETS set\n    - A small letter from the English alphabet\n\n    The function uses an optional `random_seed` to control the randomness of the characters.\n\n    Parameters:\n    - length (int): The length of the string to be generated.\n    - random_seed (int): The seed for the random number generator. Defaults to None.\n\n    Returns:\n    - str: The generated random string.\n    \"\"\"\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    characters = BRACKETS + string.ascii_lowercase\n    random_string = ''.join(random.choice(characters) for _ in range(length))\n\n    return random_string", "raw_solution": "import string\nimport random\n\n# Define the set of characters to be used\nBRACKETS = \"(){}[]\"\n\ndef task_func(length, random_seed=None):\n    \"\"\"\n    Generate a random string of a given length, where each character can be:\n    - An opening or closing bracket from the BRACKETS set\n    - A small letter from the English alphabet\n\n    The function uses an optional `random_seed` to control the randomness of the characters.\n\n    Parameters:\n    - length (int): The length of the string to be generated.\n    - random_seed (int): The seed for the random number generator. Defaults to None.\n\n    Returns:\n    - str: The generated random string.\n    \"\"\"\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    characters = BRACKETS + string.ascii_lowercase\n    random_string = ''.join(random.choice(characters) for _ in range(length))\n\n    return random_string\n\n# Test cases"}
{"task_id": "BigCodeBench/795", "solution": "from collections import deque\nimport math\ndef task_func(l):\n    dq = deque(l)\n    dq.rotate(3)\n    sum_of_elements = sum(map(ord, dq))\n    sqrt_of_sum = math.sqrt(sum_of_elements)\n    print(f\"The square root of the sum of numeric elements: {sqrt_of_sum}\")\n    return dq", "raw_solution": "from collections import deque\nimport math\n\ndef task_func(l):\n    dq = deque(l)\n    dq.rotate(3)\n    sum_of_elements = sum(map(ord, dq))\n    sqrt_of_sum = math.sqrt(sum_of_elements)\n    print(f\"The square root of the sum of numeric elements: {sqrt_of_sum}\")\n    return dq\n\n# Test cases"}
{"task_id": "BigCodeBench/796", "solution": "import os\nimport re\ndef task_func(directory):\n    BRACKET_PATTERN = '[(){}\\\\[\\\\]]'\n    bracket_files = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(BRACKET_PATTERN, file):\n                bracket_files.append(os.path.join(root, file))\n\n    return bracket_files", "raw_solution": "import os\nimport re\n\ndef task_func(directory):\n    BRACKET_PATTERN = '[(){}\\\\[\\\\]]'\n    bracket_files = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(BRACKET_PATTERN, file):\n                bracket_files.append(os.path.join(root, file))\n\n    return bracket_files\n\n# Example usage:\n# print(task_func('./some_directory/'))\n```\nThis script uses the `os` and `re` modules to walk through a directory and its subdirectories, checking each file name for the presence of any of the specified brackets. If a file name contains a bracket, the file path is added to the `bracket_files` list, which is then returned by the function.\n"}
{"task_id": "BigCodeBench/797", "solution": "import re\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> int:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    # Initialize a counter for brackets\n    bracket_count = 0\n\n    # Iterate over each cell in the DataFrame\n    for cell in df.values.flatten():\n        # Find all brackets in the cell\n        brackets = re.findall(r'[(){}[\\]]', str(cell))\n        # Increment the counter by the number of brackets found\n        bracket_count += len(brackets)\n\n    return bracket_count", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> int:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    # Initialize a counter for brackets\n    bracket_count = 0\n\n    # Iterate over each cell in the DataFrame\n    for cell in df.values.flatten():\n        # Find all brackets in the cell\n        brackets = re.findall(r'[(){}[\\]]', str(cell))\n        # Increment the counter by the number of brackets found\n        bracket_count += len(brackets)\n\n    return bracket_count\n\n# Example usage:"}
{"task_id": "BigCodeBench/798", "solution": "import os\nimport shutil\nBACKUP_DIR = '/tmp/backup'\ndef task_func(directory):\n    if not os.path.exists(BACKUP_DIR):\n        return f'backup directory {BACKUP_DIR} nonexistent hai. update ko rollback nahin ki or jaa sakta hai.'\n\n    backup_path = os.path.join(BACKUP_DIR, os.path.basename(directory))\n    if os.path.exists(backup_path):\n        shutil.copytree(backup_path, directory)\n        return directory\n    else:\n        return f'backup directory {backup_path} nonexistent hai. update ko rollback nahin ki or jaa sakta hai.'", "raw_solution": "import os\nimport shutil\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef task_func(directory):\n    if not os.path.exists(BACKUP_DIR):\n        return f'backup directory {BACKUP_DIR} nonexistent hai. update ko rollback nahin ki or jaa sakta hai.'\n\n    backup_path = os.path.join(BACKUP_DIR, os.path.basename(directory))\n    if os.path.exists(backup_path):\n        shutil.copytree(backup_path, directory)\n        return directory\n    else:\n        return f'backup directory {backup_path} nonexistent hai. update ko rollback nahin ki or jaa sakta hai.'\n```\n"}
{"task_id": "BigCodeBench/799", "solution": "import pandas as pd\nfrom random import seed, choices\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if not L:\n        return pd.DataFrame(), []\n\n    column_names = [''.join(choices('abcdefghijklmnopqrstuvwxyz', k=1)) for _ in range(3)]\n    df_list = []\n    for _ in range(num_dataframes):\n        df = pd.DataFrame(L, columns=column_names)\n        df_list.append(df)\n\n    common_rows = df_list[0].copy()\n    for df in df_list[1:]:\n        common_rows = common_rows.merge(df, how='inner', suffixes=('', '_y'))\n\n    return common_rows, df_list", "raw_solution": "import pandas as pd\nfrom random import seed, choices\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if not L:\n        return pd.DataFrame(), []\n\n    column_names = [''.join(choices('abcdefghijklmnopqrstuvwxyz', k=1)) for _ in range(3)]\n    df_list = []\n    for _ in range(num_dataframes):\n        df = pd.DataFrame(L, columns=column_names)\n        df_list.append(df)\n\n    common_rows = df_list[0].copy()\n    for df in df_list[1:]:\n        common_rows = common_rows.merge(df, how='inner', suffixes=('', '_y'))\n\n    return common_rows, df_list\n```\n"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    CSV file se kul **goals** aur **penalties** ki kul **number** ginen aur diye gaye **goals** aur **penalties** ke saath ise update karein.\n\n    Parameters:\n    - goals (dict): ek **dictionary** jahan **keys** team ke **naam** hain aur **maan** **goals** ki **number** hain. \n    - penalties (dict): ek **dictionary** jahan **keys** team ke **naam** hain aur **maan** dand ki **number** hain.\n\n    Returns:\n        count (Counter.collections): **goals** aur **penalties** ki kul **count** ke saath ek Counter **object**.\n\n    Requirements:\n    - csv\n    - os\n    - collections.Counter\n\n    Example:\n    >>> goals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\n    >>> penalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\n    >>> counts = task_func(goals, penalties)\n    >>> print(counts)\n    Counter({'goals': 8, 'penalties': 7})\n\n    \"\"\"\n    total_goals = sum(goals.values())\n    total_penalties = sum(penalties.values())\n    count = Counter({'goals': total_goals, 'penalties': total_penalties})\n    return count", "raw_solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    CSV file se kul **goals** aur **penalties** ki kul **number** ginen aur diye gaye **goals** aur **penalties** ke saath ise update karein.\n\n    Parameters:\n    - goals (dict): ek **dictionary** jahan **keys** team ke **naam** hain aur **maan** **goals** ki **number** hain. \n    - penalties (dict): ek **dictionary** jahan **keys** team ke **naam** hain aur **maan** dand ki **number** hain.\n\n    Returns:\n        count (Counter.collections): **goals** aur **penalties** ki kul **count** ke saath ek Counter **object**.\n\n    Requirements:\n    - csv\n    - os\n    - collections.Counter\n\n    Example:\n    >>> goals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\n    >>> penalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\n    >>> counts = task_func(goals, penalties)\n    >>> print(counts)\n    Counter({'goals': 8, 'penalties': 7})\n\n    \"\"\"\n    total_goals = sum(goals.values())\n    total_penalties = sum(penalties.values())\n    count = Counter({'goals': total_goals, 'penalties': total_penalties})\n    return count\n```\n"}
{"task_id": "BigCodeBench/801", "solution": "import collections\nimport numpy as np\ndef task_func(file_name):\n    common_values = {}\n    try:\n        data = np.genfromtxt(file_name, delimiter=',', names=True, dtype=None, encoding='utf-8')\n    except FileNotFoundError:\n        return common_values\n\n    for column in data.dtype.names:\n        column_data = data[column]\n        if column_data.size == 0:\n            continue\n        most_common = collections.Counter(column_data).most_common()\n        if len(most_common) > 0:\n            common_values[column] = most_common[0][0]\n\n    return common_values", "raw_solution": "import collections\nimport numpy as np\n\ndef task_func(file_name):\n    common_values = {}\n    try:\n        data = np.genfromtxt(file_name, delimiter=',', names=True, dtype=None, encoding='utf-8')\n    except FileNotFoundError:\n        return common_values\n\n    for column in data.dtype.names:\n        column_data = data[column]\n        if column_data.size == 0:\n            continue\n        most_common = collections.Counter(column_data).most_common()\n        if len(most_common) > 0:\n            common_values[column] = most_common[0][0]\n\n    return common_values\n```\nThis script uses the `numpy.genfromtxt` function to read the CSV file and `collections.Counter` to find the most common values in each column. It handles empty columns and files by skipping them. The function returns a dictionary with column names as keys and the most common values as values.\n"}
{"task_id": "BigCodeBench/802", "solution": "import numpy as np\nimport itertools\ndef task_func(dimension, seed=42):\n    if dimension <= 0:\n        raise ValueError(\"Dimension must be a positive integer.\")\n    \n    np.random.seed(seed)\n    \n    # Generate a random 2D matrix of given dimension\n    matrix = np.random.randint(1, 101, size=(dimension, dimension))\n    \n    # Flatten the matrix into a list\n    flat_list = list(itertools.chain.from_iterable(matrix))\n    \n    return matrix, flat_list", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(dimension, seed=42):\n    if dimension <= 0:\n        raise ValueError(\"Dimension must be a positive integer.\")\n    \n    np.random.seed(seed)\n    \n    # Generate a random 2D matrix of given dimension\n    matrix = np.random.randint(1, 101, size=(dimension, dimension))\n    \n    # Flatten the matrix into a list\n    flat_list = list(itertools.chain.from_iterable(matrix))\n    \n    return matrix, flat_list\n\n# Example usage:\n# matrix, flat_list = task_func(3)\n# print(matrix)\n# print(flat_list)\n```\n"}
{"task_id": "BigCodeBench/803", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(file_name: str) -> pd.DataFrame:\n    df = pd.read_csv(file_name)\n    numeric_cols = df.select_dtypes(include=['number']).columns\n\n    if len(numeric_cols) == 0:\n        raise ValueError(\"Input DataFrame does not contain any numeric columns.\")\n\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(file_name: str) -> pd.DataFrame:\n    df = pd.read_csv(file_name)\n    numeric_cols = df.select_dtypes(include=['number']).columns\n\n    if len(numeric_cols) == 0:\n        raise ValueError(\"Input DataFrame does not contain any numeric columns.\")\n\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df\n\n# Example usage:\n# normalized_data = task_func(\"sample.csv\")\n# print(normalized_data.head())\n```\n"}
{"task_id": "BigCodeBench/804", "solution": "import os\nfrom datetime import datetime\nLOG_DIR = './logs'\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    \"\"\"\n    This function writes metrics to a log file with a timestamp for each entry.\n\n    Parameters:\n    - metrics (dict): A dictionary where keys are metric names and values are their corresponding values.\n    - filename (str): The name of the file where metrics will be logged.\n    - log_dir (str, optional): The directory where the log file will be saved. Default is './logs'.\n\n    Returns:\n        bool: True if metrics were successfully logged, otherwise False.\n\n    Requirements:\n    - os\n    - datetime\n\n    Example:\n    >>> metrics = {'accuracy': 0.98, 'loss': 0.05}\n    >>> task_func(metrics, 'metrics.log')\n    True\n    \"\"\"\n    try:\n        # Ensure the log directory exists\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n\n        # Create a log file path\n        file_path = os.path.join(log_dir, filename)\n\n        # Open the file in append mode\n        with open(file_path, 'a') as file:\n            # Get the current timestamp\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            # Write the metrics and timestamp to the file\n            for metric, value in metrics.items():\n                file.write(f\"{timestamp} - {metric}: {value}\\n\")\n\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False", "raw_solution": "import os\nfrom datetime import datetime\n\n# Constants\nLOG_DIR = './logs'\n\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    \"\"\"\n    This function writes metrics to a log file with a timestamp for each entry.\n\n    Parameters:\n    - metrics (dict): A dictionary where keys are metric names and values are their corresponding values.\n    - filename (str): The name of the file where metrics will be logged.\n    - log_dir (str, optional): The directory where the log file will be saved. Default is './logs'.\n\n    Returns:\n        bool: True if metrics were successfully logged, otherwise False.\n\n    Requirements:\n    - os\n    - datetime\n\n    Example:\n    >>> metrics = {'accuracy': 0.98, 'loss': 0.05}\n    >>> task_func(metrics, 'metrics.log')\n    True\n    \"\"\"\n    try:\n        # Ensure the log directory exists\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n\n        # Create a log file path\n        file_path = os.path.join(log_dir, filename)\n\n        # Open the file in append mode\n        with open(file_path, 'a') as file:\n            # Get the current timestamp\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            # Write the metrics and timestamp to the file\n            for metric, value in metrics.items():\n                file.write(f\"{timestamp} - {metric}: {value}\\n\")\n\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```\n"}
{"task_id": "BigCodeBench/805", "solution": "import pandas as pd\nimport random\ndef task_func(dictionary, item, seed):\n    df = pd.DataFrame(dictionary)\n    locations = []\n\n    for col in df.columns:\n        for index, value in df[col].iteritems():\n            if value == item:\n                locations.append((index, col))\n\n    random.seed(seed)\n    random_number = random.randint(0, 9)\n    occurrences = len(locations)\n\n    return locations, occurrences, df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(dictionary, item, seed):\n    df = pd.DataFrame(dictionary)\n    locations = []\n\n    for col in df.columns:\n        for index, value in df[col].iteritems():\n            if value == item:\n                locations.append((index, col))\n\n    random.seed(seed)\n    random_number = random.randint(0, 9)\n    occurrences = len(locations)\n\n    return locations, occurrences, df\n\n# Example usage:\ndict = {'A': ['apple', 'banana'], 'B': ['orange', 'apple']}"}
{"task_id": "BigCodeBench/806", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text, n=2):\n    \"\"\"\n    Ek \"text\" string se duplicate and nltk.corpus.stopwords hataen.\n    Phir, text mein n-gram ki count utpann karen.\n\n    Parameters:\n    - text (str): vishleshan karne ke liye text string.\n    - n (int): n-gram ka size.\n\n    Returns:\n        dict: text mein n-gram ki count.\n\n    Requirements:\n    - re\n    - nltk.corpus.stopwords\n    - collections.Counter\n\n    Example:\n    >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    >>> ngrams = task_func(text)\n    >>> print(ngrams)\n    Counter({('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumps'): 1, ('jumps', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', 'dog'): 1, ('dog', 'quick'): 1, ('quick', 'respond'): 1})\n\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Generate n-grams\n    ngrams = zip(*[words[i:] for i in range(n)])\n\n    # Count n-grams\n    ngram_counts = Counter(ngrams)\n\n    return ngram_counts", "raw_solution": "import re\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\n\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(text, n=2):\n    \"\"\"\n    Ek \"text\" string se duplicate and nltk.corpus.stopwords hataen.\n    Phir, text mein n-gram ki count utpann karen.\n\n    Parameters:\n    - text (str): vishleshan karne ke liye text string.\n    - n (int): n-gram ka size.\n\n    Returns:\n        dict: text mein n-gram ki count.\n\n    Requirements:\n    - re\n    - nltk.corpus.stopwords\n    - collections.Counter\n\n    Example:\n    >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    >>> ngrams = task_func(text)\n    >>> print(ngrams)\n    Counter({('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumps'): 1, ('jumps', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', 'dog'): 1, ('dog', 'quick'): 1, ('quick', 'respond'): 1})\n\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Generate n-grams\n    ngrams = zip(*[words[i:] for i in range(n)])\n\n    # Count n-grams\n    ngram_counts = Counter(ngrams)\n\n    return ngram_counts\n```\n"}
{"task_id": "BigCodeBench/807", "solution": "import numpy as np\nfrom scipy.stats import norm\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    if data.size == 0:\n        return [], 0.0, 0.0\n\n    mean, var = norm.fit(data)\n    z_scores = np.abs((data - mean) / np.sqrt(var))\n\n    outliers = np.where(z_scores > threshold)[0].tolist()\n\n    return outliers, mean, var\ndata = np.array([1, 2, 3, 4, 5, 6, 100])", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\n\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    if data.size == 0:\n        return [], 0.0, 0.0\n\n    mean, var = norm.fit(data)\n    z_scores = np.abs((data - mean) / np.sqrt(var))\n\n    outliers = np.where(z_scores > threshold)[0].tolist()\n\n    return outliers, mean, var\n\n# Test cases\ndata = np.array([1, 2, 3, 4, 5, 6, 100])\noutliers, mean, var = task_func(data)"}
{"task_id": "BigCodeBench/808", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text):\n    # Remove duplicates\n    text = re.sub(r'\\b(\\w+)(\\s+\\1\\b)+', r'\\1', text)\n\n    # Remove stopwords\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in STOPWORDS]\n    cleaned_text = ' '.join(filtered_words)\n\n    # Get sentiment\n    blob = TextBlob(cleaned_text)\n    sentiment = blob.sentiment\n\n    return sentiment", "raw_solution": "import re\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(text):\n    # Remove duplicates\n    text = re.sub(r'\\b(\\w+)(\\s+\\1\\b)+', r'\\1', text)\n\n    # Remove stopwords\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in STOPWORDS]\n    cleaned_text = ' '.join(filtered_words)\n\n    # Get sentiment\n    blob = TextBlob(cleaned_text)\n    sentiment = blob.sentiment\n\n    return sentiment\n\n# Example usage"}
{"task_id": "BigCodeBench/809", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    \n    clusters = {}\n    for i, label in enumerate(labels):\n        if label not in clusters:\n            clusters[label] = []\n        clusters[label].append(i)\n    \n    return clusters\ndata = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    \n    clusters = {}\n    for i, label in enumerate(labels):\n        if label not in clusters:\n            clusters[label] = []\n        clusters[label].append(i)\n    \n    return clusters\n\n# Test cases\ndata = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ncluster = task_func(data, 2)\ncluster_list = list(cluster.values())\ncluster_list.sort(key=lambda x: x[0])"}
{"task_id": "BigCodeBench/810", "solution": "import re\nimport os\nimport subprocess\ndef task_func(dir_path, exe_pattern, execute_files=True):\n    if not os.path.isdir(dir_path):\n        raise ValueError(\"Provided directory path is not valid.\")\n    \n    exe_files = []\n    for root, _, files in os.walk(dir_path):\n        for file in files:\n            if re.search(exe_pattern, file):\n                exe_files.append(os.path.join(root, file))\n    \n    results = []\n    for exe_file in exe_files:\n        if execute_files:\n            try:\n                result = subprocess.run([exe_file], capture_output=True, text=True)\n                results.append(result.stdout)\n            except Exception as e:\n                results.append(f\"Error executing {exe_file}: {str(e)}\")\n        else:\n            results.append(exe_file)\n    \n    return results", "raw_solution": "import re\nimport os\nimport subprocess\n\ndef task_func(dir_path, exe_pattern, execute_files=True):\n    if not os.path.isdir(dir_path):\n        raise ValueError(\"Provided directory path is not valid.\")\n    \n    exe_files = []\n    for root, _, files in os.walk(dir_path):\n        for file in files:\n            if re.search(exe_pattern, file):\n                exe_files.append(os.path.join(root, file))\n    \n    results = []\n    for exe_file in exe_files:\n        if execute_files:\n            try:\n                result = subprocess.run([exe_file], capture_output=True, text=True)\n                results.append(result.stdout)\n            except Exception as e:\n                results.append(f\"Error executing {exe_file}: {str(e)}\")\n        else:\n            results.append(exe_file)\n    \n    return results\n\n# Example usage:\n# print(task_func(\"C:\\\\SomeDir\", r\"(?<!Distillr)\\\\AcroTray\\\\.exe\"))\n# print(task_func(\"C:\\\\SomeDir\", r\"(?<!Distillr)\\\\AcroTray\\\\.exe\", execute_files=False))\n```\nThis script walks through the directory and its subdirectories, searching for files matching the provided pattern. If `execute_files` is set to True, it attempts to execute each file and captures the standard output. If `execute_files` is set to False, it simply appends the file paths to the results list. The function returns a list of results, with each result corresponding to the standard output or file path of an executable file.\n"}
{"task_id": "BigCodeBench/811", "solution": "import pandas as pd\nfrom random import randint, seed\ndef task_func(dictionary, item, sample_size=None, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n    \n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(dictionary)\n    \n    # Find positions of the item\n    positions = [(index, col) for index, row in df.iterrows() for col, value in row.items() if value == item]\n    \n    # Optionally take a random sample\n    if sample_size is not None:\n        sample = []\n        for _ in range(sample_size):\n            index = randint(0, len(positions) - 1)\n            sample.append(positions[index])\n        positions = sample\n    \n    return positions, df\ndictionary = {\n    1: ['road', 'car', 'traffic'],\n    2: ['car', 'light', 'candle']\n}", "raw_solution": "import pandas as pd\nfrom random import randint, seed\n\ndef task_func(dictionary, item, sample_size=None, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n    \n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(dictionary)\n    \n    # Find positions of the item\n    positions = [(index, col) for index, row in df.iterrows() for col, value in row.items() if value == item]\n    \n    # Optionally take a random sample\n    if sample_size is not None:\n        sample = []\n        for _ in range(sample_size):\n            index = randint(0, len(positions) - 1)\n            sample.append(positions[index])\n        positions = sample\n    \n    return positions, df\n\n# Example usage:\ndictionary = {\n    1: ['road', 'car', 'traffic'],\n    2: ['car', 'light', 'candle']\n}\npositions = task_func(dictionary, 'car')"}
{"task_id": "BigCodeBench/812", "solution": "import re\nfrom pathlib import Path\nimport tarfile\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\ndef task_func(directory=DIRECTORY, file_pattern=PATTERN):\n    \"\"\"\n    'C:\\ SomeDir\\' directory mein re expression '(? <! Distillr)\\\\\\\\ AcroTray\\\\.exe' ke pattern se mel khaane waali files ko dhoondhe. Yadi mili, to in files ko ek tar file mein archive karein.\n\n    Parameters:\n    - directory: us directory ko jahaan nirdisht pattern se mel khaane waali files ko khoja jaana hai. Function is directory ke sabhi files mein, sub-directory sahit, dohraaega.\n    - file_pattern: file_pattern se mel khaane ke liye upayog kiya jaane waala re expression pattern. Jin files ke naam is pattern se mel khaate hain unhein ek archive (tar file) mein joda jaaega.\n\n    Returns:\n        str: str tar file ka path.\n\n    Requirements:\n    - re\n    - pathlib\n    - tarfile\n\n    Example:\n    >>> f_680('/path/to/source', '/path/to/target')\n\n    \"\"\"\n    # Convert directory to Path object\n    dir_path = Path(directory)\n\n    # Find all files matching the pattern\n    matched_files = list(dir_path.rglob('*').filter(lambda f: re.search(file_pattern, str(f))))\n\n    # Create a tar file\n    tar_file_path = dir_path / \"archive.tar\"\n    with tarfile.open(tar_file_path, \"w\") as tar:\n        for file in matched_files:\n            tar.add(file, arcname=file.name)\n\n    return str(tar_file_path)", "raw_solution": "import re\nfrom pathlib import Path\nimport tarfile\n\n# Constants\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\n\ndef task_func(directory=DIRECTORY, file_pattern=PATTERN):\n    \"\"\"\n    'C:\\ SomeDir\\' directory mein re expression '(? <! Distillr)\\\\\\\\ AcroTray\\\\.exe' ke pattern se mel khaane waali files ko dhoondhe. Yadi mili, to in files ko ek tar file mein archive karein.\n\n    Parameters:\n    - directory: us directory ko jahaan nirdisht pattern se mel khaane waali files ko khoja jaana hai. Function is directory ke sabhi files mein, sub-directory sahit, dohraaega.\n    - file_pattern: file_pattern se mel khaane ke liye upayog kiya jaane waala re expression pattern. Jin files ke naam is pattern se mel khaate hain unhein ek archive (tar file) mein joda jaaega.\n\n    Returns:\n        str: str tar file ka path.\n\n    Requirements:\n    - re\n    - pathlib\n    - tarfile\n\n    Example:\n    >>> f_680('/path/to/source', '/path/to/target')\n\n    \"\"\"\n    # Convert directory to Path object\n    dir_path = Path(directory)\n\n    # Find all files matching the pattern\n    matched_files = list(dir_path.rglob('*').filter(lambda f: re.search(file_pattern, str(f))))\n\n    # Create a tar file\n    tar_file_path = dir_path / \"archive.tar\"\n    with tarfile.open(tar_file_path, \"w\") as tar:\n        for file in matched_files:\n            tar.add(file, arcname=file.name)\n\n    return str(tar_file_path)\n```\n"}
{"task_id": "BigCodeBench/813", "solution": "from itertools import combinations\nimport pandas as pd\ndef task_func(number_list, element):\n    # Generate all combinations of 3 numbers from the list\n    comb = combinations(number_list, 3)\n    \n    # Filter combinations that add up to the given element\n    valid_combinations = [combo for combo in comb if sum(combo) <= element]\n    \n    # Create a DataFrame from the valid combinations\n    df = pd.DataFrame(valid_combinations, columns=['Combinations'])\n    \n    # Return the DataFrame\n    return df", "raw_solution": "from itertools import combinations\nimport pandas as pd\n\ndef task_func(number_list, element):\n    # Generate all combinations of 3 numbers from the list\n    comb = combinations(number_list, 3)\n    \n    # Filter combinations that add up to the given element\n    valid_combinations = [combo for combo in comb if sum(combo) <= element]\n    \n    # Create a DataFrame from the valid combinations\n    df = pd.DataFrame(valid_combinations, columns=['Combinations'])\n    \n    # Return the DataFrame\n    return df\n\n# Test cases"}
{"task_id": "BigCodeBench/814", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    matched_files = []\n    for root, _, files in os.walk(source_dir):\n        for file in files:\n            if re.search(file_pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    config_file_path = os.path.join(target_dir, 'config.txt')\n    with open(config_file_path, 'w') as config_file:\n        for file_path in matched_files:\n            config_file.write(file_path + '\\n')\n\n    return config_file_path", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    matched_files = []\n    for root, _, files in os.walk(source_dir):\n        for file in files:\n            if re.search(file_pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    config_file_path = os.path.join(target_dir, 'config.txt')\n    with open(config_file_path, 'w') as config_file:\n        for file_path in matched_files:\n            config_file.write(file_path + '\\n')\n\n    return config_file_path\n\n# Example usage:\n# print(task_func('/path/to/source', '/path/to/target'))\n```\nThis script walks through the source directory, finds all files matching the given pattern, and writes their paths to a configuration file in the target directory. The function returns the path to the configuration file.\n"}
{"task_id": "BigCodeBench/815", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(test_scores, student):\n    df = pd.DataFrame(test_scores)\n    \n    if student not in df['Student'].values:\n        raise ValueError(f\"Student {student} is not present in the test_scores dataframe\")\n    \n    student_scores = df[df['Student'] == student]\n    average_score = student_scores['Score'].mean()\n    std_dev = student_scores['Score'].std()\n    \n    return np.array([average_score, std_dev]), student_scores", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(test_scores, student):\n    df = pd.DataFrame(test_scores)\n    \n    if student not in df['Student'].values:\n        raise ValueError(f\"Student {student} is not present in the test_scores dataframe\")\n    \n    student_scores = df[df['Student'] == student]\n    average_score = student_scores['Score'].mean()\n    std_dev = student_scores['Score'].std()\n    \n    return np.array([average_score, std_dev]), student_scores\n\n# Example usage:\nSTUDENTS = range(1, 101)\nnp.random.seed(10)\nscores = {'Student': list(np.random.choice(STUDENTS, 50, replace=True)),\n         'Score': np.random.randint(50, 101, size=50)}"}
{"task_id": "BigCodeBench/816", "solution": "from collections import Counter\nimport random\nHAND_RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\ndef task_func():\n    hand = random.choices(HAND_RANKS, k=5)\n    rank_count = Counter(hand)\n    return hand, rank_count", "raw_solution": "from collections import Counter\nimport random\n\n# Constants\nHAND_RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\nSUITS = ['H', 'D', 'C', 'S']\n\ndef task_func():\n    hand = random.choices(HAND_RANKS, k=5)\n    rank_count = Counter(hand)\n    return hand, rank_count\n```\n"}
{"task_id": "BigCodeBench/817", "solution": "from collections import Counter\nimport logging\ndef task_func(letter_list, element, log_path):\n    # Set up logging\n    logging.basicConfig(filename=f'{log_path}/task_func.log', \n                        filemode='w', \n                        level=logging.DEBUG, \n                        format='%(levelname)s:%(message)s')\n\n    # Log the function call\n    logging.info(f\"Function ko list ke saath bulaya gaya: {letter_list} and element: {element}\")\n\n    # Check if the element is in the list\n    if element not in letter_list:\n        logging.error(f\"'{element}' letter list mein nahim hai\")\n        raise ValueError(f\"'{element}' letter list mein nahim hai\")\n\n    # Calculate the frequency of the element\n    element_frequency = Counter(letter_list)[element]\n    logging.info(f\"'{element}' ki frequency {element_frequency}\")\n\n    return element_frequency", "raw_solution": "from collections import Counter\nimport logging\n\ndef task_func(letter_list, element, log_path):\n    # Set up logging\n    logging.basicConfig(filename=f'{log_path}/task_func.log', \n                        filemode='w', \n                        level=logging.DEBUG, \n                        format='%(levelname)s:%(message)s')\n\n    # Log the function call\n    logging.info(f\"Function ko list ke saath bulaya gaya: {letter_list} and element: {element}\")\n\n    # Check if the element is in the list\n    if element not in letter_list:\n        logging.error(f\"'{element}' letter list mein nahim hai\")\n        raise ValueError(f\"'{element}' letter list mein nahim hai\")\n\n    # Calculate the frequency of the element\n    element_frequency = Counter(letter_list)[element]\n    logging.info(f\"'{element}' ki frequency {element_frequency}\")\n\n    return element_frequency\n\n# Example usage"}
{"task_id": "BigCodeBench/818", "solution": "import re\nimport string\nPUNCTUATION = string.punctuation\ndef task_func(text):\n    \"\"\"\n    Ek text ko words mein vibhajit karein, punctuation ko hata dein and unhein chhote letters mein badlein.\n\n    Parameters:\n    - text (str): input text.\n\n    Returns:\n        cleaned_words (list): cleaned_words ki ek list.\n\n    Requirements:\n    - re\n    - string\n\n    Example:\n    >>> task_func(\"Hello, world! This is a test.\")\n    ['hello', 'world', 'this', 'is', 'a', 'test']\n\n    \"\"\"\n    # Remove punctuation\n    text = re.sub(f'[{re.escape(PUNCTUATION)}]', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Split into words\n    words = text.split()\n\n    return words", "raw_solution": "import re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef task_func(text):\n    \"\"\"\n    Ek text ko words mein vibhajit karein, punctuation ko hata dein and unhein chhote letters mein badlein.\n\n    Parameters:\n    - text (str): input text.\n\n    Returns:\n        cleaned_words (list): cleaned_words ki ek list.\n\n    Requirements:\n    - re\n    - string\n\n    Example:\n    >>> task_func(\"Hello, world! This is a test.\")\n    ['hello', 'world', 'this', 'is', 'a', 'test']\n\n    \"\"\"\n    # Remove punctuation\n    text = re.sub(f'[{re.escape(PUNCTUATION)}]', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Split into words\n    words = text.split()\n\n    return words\n\n# Test cases"}
{"task_id": "BigCodeBench/819", "solution": "import time\nimport random\ndef task_func(iterations=5, min_delay=1.0, max_delay=2.0, seed=None):\n    if not isinstance(iterations, int) or iterations <= 0:\n        raise ValueError(\"iterations must be a positive integer.\")\n    if not (isinstance(min_delay, float) and isinstance(max_delay, float)) or min_delay <= 0 or max_delay <= 0:\n        raise ValueError(\"min_delay and max_delay must be positive float values.\")\n\n    random.seed(seed)\n    messages = []\n    total_delay = 0.0\n\n    for _ in range(iterations):\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n        message = f\"{delay:.2f} seconds have passed\"\n        messages.append(message)\n        total_delay += delay\n\n    return messages, total_delay", "raw_solution": "import time\nimport random\n\n\ndef task_func(iterations=5, min_delay=1.0, max_delay=2.0, seed=None):\n    if not isinstance(iterations, int) or iterations <= 0:\n        raise ValueError(\"iterations must be a positive integer.\")\n    if not (isinstance(min_delay, float) and isinstance(max_delay, float)) or min_delay <= 0 or max_delay <= 0:\n        raise ValueError(\"min_delay and max_delay must be positive float values.\")\n\n    random.seed(seed)\n    messages = []\n    total_delay = 0.0\n\n    for _ in range(iterations):\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n        message = f\"{delay:.2f} seconds have passed\"\n        messages.append(message)\n        total_delay += delay\n\n    return messages, total_delay\n```\n"}
{"task_id": "BigCodeBench/820", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(num_words, word_length):\n    if num_words < 0 or word_length < 0:\n        raise ValueError(\"num_words and word_length must be non-negative\")\n\n    words = []\n    for _ in range(num_words):\n        word = ''.join(random.choice(LETTERS) for _ in range(word_length))\n        words.append(word)\n\n    return words", "raw_solution": "import random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef task_func(num_words, word_length):\n    if num_words < 0 or word_length < 0:\n        raise ValueError(\"num_words and word_length must be non-negative\")\n\n    words = []\n    for _ in range(num_words):\n        word = ''.join(random.choice(LETTERS) for _ in range(word_length))\n        words.append(word)\n\n    return words\n\n# Example usage:\n# print(task_func(5, 3))\n```\n"}
{"task_id": "BigCodeBench/821", "solution": "import time\nimport threading\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    def delay_thread(thread_id):\n        time.sleep(delay_time)\n        completion_messages.append(f'Delay in thread {thread_id} completed')\n\n    completion_messages = []\n    threads = []\n\n    for i in range(num_threads):\n        thread = threading.Thread(target=delay_thread, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return completion_messages", "raw_solution": "import time\nimport threading\n\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    def delay_thread(thread_id):\n        time.sleep(delay_time)\n        completion_messages.append(f'Delay in thread {thread_id} completed')\n\n    completion_messages = []\n    threads = []\n\n    for i in range(num_threads):\n        thread = threading.Thread(target=delay_thread, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return completion_messages\n\n# Example usage:\n# print(task_func(0.1, 3))\n# print(task_func(1, 10))\n```\n"}
{"task_id": "BigCodeBench/822", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\nDIGITS = string.digits\ndef task_func(length, num_digits):\n    if length <= 0:\n        raise ValueError(\"Length must be a positive integer.\")\n    if num_digits < 0 or num_digits > length:\n        raise ValueError(\"Number of digits must be a non-negative integer and less than or equal to length.\")\n\n    # Generate random letters\n    password_letters = ''.join(random.choice(LETTERS) for _ in range(length - num_digits))\n\n    # Generate random digits\n    password_digits = ''.join(random.choice(DIGITS) for _ in range(num_digits))\n\n    # Combine and shuffle\n    password_list = list(password_letters + password_digits)\n    random.shuffle(password_list)\n\n    # Return the final password\n    return ''.join(password_list)", "raw_solution": "import random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\nDIGITS = string.digits\n\ndef task_func(length, num_digits):\n    if length <= 0:\n        raise ValueError(\"Length must be a positive integer.\")\n    if num_digits < 0 or num_digits > length:\n        raise ValueError(\"Number of digits must be a non-negative integer and less than or equal to length.\")\n\n    # Generate random letters\n    password_letters = ''.join(random.choice(LETTERS) for _ in range(length - num_digits))\n\n    # Generate random digits\n    password_digits = ''.join(random.choice(DIGITS) for _ in range(num_digits))\n\n    # Combine and shuffle\n    password_list = list(password_letters + password_digits)\n    random.shuffle(password_list)\n\n    # Return the final password\n    return ''.join(password_list)\n\n# Test cases"}
{"task_id": "BigCodeBench/823", "solution": "import time\nimport numpy as np\ndef task_func(samples=10, delay=0.1):\n    delays = []\n\n    for _ in range(samples):\n        start_time = time.time()\n        time.sleep(delay)\n        end_time = time.time()\n        actual_delay = end_time - start_time\n        delays.append(actual_delay)\n\n    mean_delay = np.mean(delays)\n    std_delay = np.std(delays)\n\n    return mean_delay, std_delay", "raw_solution": "import time\nimport numpy as np\n\ndef task_func(samples=10, delay=0.1):\n    delays = []\n\n    for _ in range(samples):\n        start_time = time.time()\n        time.sleep(delay)\n        end_time = time.time()\n        actual_delay = end_time - start_time\n        delays.append(actual_delay)\n\n    mean_delay = np.mean(delays)\n    std_delay = np.std(delays)\n\n    return mean_delay, std_delay\n\n# Example usage:"}
{"task_id": "BigCodeBench/824", "solution": "import re\nimport string\nPUNCTUATION = string.punctuation\ndef task_func(text):\n    \"\"\"\n    Ek string mein words aur punctuation marks ki number ginen.\n\n    Parameters:\n    - text (str): text string.\n\n    Returns:\n        tuple: words aur punctuation marks ki number wala ek tuple.\n\n    Requirements:\n    - re\n    - string\n\n    Example:\n    >>> task_func(\"Hello, world! This is a test.\")\n    (6, 3)\n\n    \"\"\"\n    # Remove punctuation from the text\n    text_no_punct = text.translate(str.maketrans('', '', PUNCTUATION))\n    \n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text_no_punct)\n    \n    # Count the number of words and punctuation marks\n    num_words = len(words)\n    num_punct = len(text) - len(text_no_punct)\n    \n    return num_words, num_punct", "raw_solution": "import re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef task_func(text):\n    \"\"\"\n    Ek string mein words aur punctuation marks ki number ginen.\n\n    Parameters:\n    - text (str): text string.\n\n    Returns:\n        tuple: words aur punctuation marks ki number wala ek tuple.\n\n    Requirements:\n    - re\n    - string\n\n    Example:\n    >>> task_func(\"Hello, world! This is a test.\")\n    (6, 3)\n\n    \"\"\"\n    # Remove punctuation from the text\n    text_no_punct = text.translate(str.maketrans('', '', PUNCTUATION))\n    \n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text_no_punct)\n    \n    # Count the number of words and punctuation marks\n    num_words = len(words)\n    num_punct = len(text) - len(text_no_punct)\n    \n    return num_words, num_punct\n\n# Test cases"}
{"task_id": "BigCodeBench/825", "solution": "import numpy as np\nfrom itertools import product\nimport string\ndef task_func(length, seed=None, alphabets=list(string.ascii_lowercase)):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Generate all possible combinations of the given length\n    all_combinations = product(alphabets, repeat=length)\n\n    # Convert the combinations to strings\n    all_strings = [''.join(combination) for combination in all_combinations]\n\n    # Randomly select 10 strings\n    selected_strings = np.random.choice(all_strings, size=10, replace=False).tolist()\n\n    return selected_strings", "raw_solution": "import numpy as np\nfrom itertools import product\nimport string\n\ndef task_func(length, seed=None, alphabets=list(string.ascii_lowercase)):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Generate all possible combinations of the given length\n    all_combinations = product(alphabets, repeat=length)\n\n    # Convert the combinations to strings\n    all_strings = [''.join(combination) for combination in all_combinations]\n\n    # Randomly select 10 strings\n    selected_strings = np.random.choice(all_strings, size=10, replace=False).tolist()\n\n    return selected_strings\n\n# Example usage:\n# print(task_func(2, 123))\n```\n"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count\n```\n"}
{"task_id": "BigCodeBench/827", "solution": "import math\nfrom sympy import isprime\ndef task_func(input_list):\n    # Filter the list to only include prime numbers\n    prime_numbers = [num for num in input_list if isprime(num)]\n    \n    # Sort the prime numbers in ascending order\n    prime_numbers.sort()\n    \n    # Convert the sorted prime numbers to their degree values\n    degree_values = [math.degrees(math.atan(num)) for num in prime_numbers]\n    \n    # Sort the degree values in ascending order\n    degree_values.sort()\n    \n    # Map the degree values back to the original prime numbers\n    sorted_primes = [prime_numbers[degree_values.index(math.degrees(math.atan(num)))] for num in degree_values]\n    \n    return sorted_primes", "raw_solution": "import math\nfrom sympy import isprime\n\ndef task_func(input_list):\n    # Filter the list to only include prime numbers\n    prime_numbers = [num for num in input_list if isprime(num)]\n    \n    # Sort the prime numbers in ascending order\n    prime_numbers.sort()\n    \n    # Convert the sorted prime numbers to their degree values\n    degree_values = [math.degrees(math.atan(num)) for num in prime_numbers]\n    \n    # Sort the degree values in ascending order\n    degree_values.sort()\n    \n    # Map the degree values back to the original prime numbers\n    sorted_primes = [prime_numbers[degree_values.index(math.degrees(math.atan(num)))] for num in degree_values]\n    \n    return sorted_primes\n\n# Test cases"}
{"task_id": "BigCodeBench/828", "solution": "import os\nimport errno\nimport shutil\ndef task_func(filename, dest_dir):\n    try:\n        # Ensure the destination directory exists\n        os.makedirs(dest_dir, exist_ok=True)\n\n        # Copy the file to the destination directory\n        dest_file = os.path.join(dest_dir, os.path.basename(filename))\n        shutil.copy2(filename, dest_file)\n\n        # Truncate the original file\n        with open(filename, 'w') as f:\n            f.truncate(0)\n\n        return os.path.abspath(dest_file)\n\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            raise FileNotFoundError(f\"The file {filename} does not exist.\")\n        elif e.errno == errno.EACCES:\n            raise PermissionError(f\"Permission denied: {dest_dir}\")\n        else:\n            raise e", "raw_solution": "import os\nimport errno\nimport shutil\n\ndef task_func(filename, dest_dir):\n    try:\n        # Ensure the destination directory exists\n        os.makedirs(dest_dir, exist_ok=True)\n\n        # Copy the file to the destination directory\n        dest_file = os.path.join(dest_dir, os.path.basename(filename))\n        shutil.copy2(filename, dest_file)\n\n        # Truncate the original file\n        with open(filename, 'w') as f:\n            f.truncate(0)\n\n        return os.path.abspath(dest_file)\n\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            raise FileNotFoundError(f\"The file {filename} does not exist.\")\n        elif e.errno == errno.EACCES:\n            raise PermissionError(f\"Permission denied: {dest_dir}\")\n        else:\n            raise e\n\n# Example usage:\n# print(task_func('/path/to/original/test.txt', '/path/to/destination'))\n```\n"}
{"task_id": "BigCodeBench/829", "solution": "import pandas as pd\nfrom statistics import mean\ndef task_func(df: pd.DataFrame) -> dict:\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Name' and 'Score' columns.\")\n    \n    grouped = df.groupby('Name')['Score'].apply(lambda x: mean(x)).reset_index()\n    grouped.columns = ['Name', 'AverageScore']\n    \n    generator_dict = {}\n    for _, row in grouped.iterrows():\n        generator_dict[row['Name']] = (row['Name'], row['AverageScore'])\n    \n    return generator_dict", "raw_solution": "import pandas as pd\nfrom statistics import mean\n\ndef task_func(df: pd.DataFrame) -> dict:\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Name' and 'Score' columns.\")\n    \n    grouped = df.groupby('Name')['Score'].apply(lambda x: mean(x)).reset_index()\n    grouped.columns = ['Name', 'AverageScore']\n    \n    generator_dict = {}\n    for _, row in grouped.iterrows():\n        generator_dict[row['Name']] = (row['Name'], row['AverageScore'])\n    \n    return generator_dict\n\n# Example usage:\ndf_sample = pd.DataFrame({\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John'],\n    'Score': [85, 79, 90, 88, 82]\n})\n\ngen_dict = task_func(df_sample)"}
{"task_id": "BigCodeBench/830", "solution": "import json\nimport os\ndef task_func(filename, data):\n    # Check if file exists\n    file_exists = os.path.exists(filename)\n    \n    # Write data to file\n    try:\n        with open(filename, 'w') as file:\n            json.dump(data, file)\n        success = True\n    except Exception as e:\n        success = False\n        print(f\"An error occurred: {e}\")\n    \n    # Read data back from file\n    try:\n        with open(filename, 'r') as file:\n            written_data = json.load(file)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        written_data = None\n\n    return success, written_data", "raw_solution": "import json\nimport os\n\ndef task_func(filename, data):\n    # Check if file exists\n    file_exists = os.path.exists(filename)\n    \n    # Write data to file\n    try:\n        with open(filename, 'w') as file:\n            json.dump(data, file)\n        success = True\n    except Exception as e:\n        success = False\n        print(f\"An error occurred: {e}\")\n    \n    # Read data back from file\n    try:\n        with open(filename, 'r') as file:\n            written_data = json.load(file)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        written_data = None\n\n    return success, written_data\n\n# Example usage:\n# result, written_data = task_func('data.json', {'key': 'value'})\n# print(result)  # This should print: true\n# print(written_data)  # This should print: {'key': 'value'}\n```\n"}
{"task_id": "BigCodeBench/831", "solution": "import random\nimport math\ndef task_func(range_start=1, range_end=100, pairs_count=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    def generate_pairs():\n        for _ in range(pairs_count):\n            num1 = random.randint(range_start, range_end)\n            num2 = random.randint(range_start, range_end)\n            abs_diff = abs(num1 - num2)\n            yield (num1, num2, math.sqrt(abs_diff))\n\n    return generate_pairs()", "raw_solution": "import random\nimport math\n\n\ndef task_func(range_start=1, range_end=100, pairs_count=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    def generate_pairs():\n        for _ in range(pairs_count):\n            num1 = random.randint(range_start, range_end)\n            num2 = random.randint(range_start, range_end)\n            abs_diff = abs(num1 - num2)\n            yield (num1, num2, math.sqrt(abs_diff))\n\n    return generate_pairs()\n\n\n# Example usage:\n# pairs = task_func(random_seed=1)\n# print(next(pairs))\n```\n"}
{"task_id": "BigCodeBench/832", "solution": "import pickle\nimport os\ndef task_func(filename, data):\n    # Check if directory exists, if not, create it\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Serialize the data and write it to the file\n    try:\n        with open(filename, 'wb') as file:\n            pickle.dump(data, file)\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False", "raw_solution": "import pickle\nimport os\n\ndef task_func(filename, data):\n    # Check if directory exists, if not, create it\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Serialize the data and write it to the file\n    try:\n        with open(filename, 'wb') as file:\n            pickle.dump(data, file)\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n\n# Example usage:\n# result = task_func('data.pkl', {'key': 'value'})\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/833", "solution": "import random\nfrom collections import Counter\nfrom statistics import mode\ndef task_func(list_length=1000, range_start=1, range_end=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    random_list = [random.randint(range_start, range_end) for _ in range(list_length)]\n    frequency = Counter(random_list)\n\n    mode_value = mode(random_list)\n\n    def number_generator():\n        for number, count in frequency.items():\n            yield (number, count)\n\n    return mode_value, number_generator()", "raw_solution": "import random\nfrom collections import Counter\nfrom statistics import mode\n\n\ndef task_func(list_length=1000, range_start=1, range_end=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    random_list = [random.randint(range_start, range_end) for _ in range(list_length)]\n    frequency = Counter(random_list)\n\n    mode_value = mode(random_list)\n\n    def number_generator():\n        for number, count in frequency.items():\n            yield (number, count)\n\n    return mode_value, number_generator()\n\n\n# Example usage:\n# mode, numbers = task_func(100, 1, 5, random_seed=1)\n# print(mode)  # prints the mode e.g. 3\n# print(next(numbers))  # prints a tuple like (1, 25)\n```\n"}
{"task_id": "BigCodeBench/834", "solution": "import binascii\nimport io\nimport gzip\ndef task_func(compressed_hex):\n    try:\n        # Convert the hexadecimal string to bytes\n        compressed_bytes = binascii.unhexlify(compressed_hex)\n        \n        # Use io.BytesIO to read the bytes as a stream\n        with io.BytesIO(compressed_bytes) as byte_stream:\n            # Decompress the bytes using gzip\n            with gzip.GzipFile(fileobj=byte_stream) as decompressed_file:\n                # Read the decompressed data\n                decompressed_data = decompressed_file.read()\n                # Decode the data to utf-8\n                decoded_string = decompressed_data.decode('utf-8')\n                return decoded_string\n    except gzip.BadCrcError as e:\n        return f'Error during decompression: {str(e)}'\n    except Exception as e:\n        return f'Error during decompression: {str(e)}'", "raw_solution": "import binascii\nimport io\nimport gzip\n\ndef task_func(compressed_hex):\n    try:\n        # Convert the hexadecimal string to bytes\n        compressed_bytes = binascii.unhexlify(compressed_hex)\n        \n        # Use io.BytesIO to read the bytes as a stream\n        with io.BytesIO(compressed_bytes) as byte_stream:\n            # Decompress the bytes using gzip\n            with gzip.GzipFile(fileobj=byte_stream) as decompressed_file:\n                # Read the decompressed data\n                decompressed_data = decompressed_file.read()\n                # Decode the data to utf-8\n                decoded_string = decompressed_data.decode('utf-8')\n                return decoded_string\n    except gzip.BadCrcError as e:\n        return f'Error during decompression: {str(e)}'\n    except Exception as e:\n        return f'Error during decompression: {str(e)}'\n```\n"}
{"task_id": "BigCodeBench/835", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    data = np.random.randint(0, 100, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    df = df.drop(df.columns[remove_cols], axis=1)\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    data = np.random.randint(0, 100, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    df = df.drop(df.columns[remove_cols], axis=1)\n\n    return df\n\n# Example usage:\n# df = task_func(10, [1, 3], columns=['test', 'rem1', 'apple', 'remove'], random_seed=12)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/836", "solution": "import os\nimport shutil\nimport csv\ndef task_func(target_value='332', csv_dir='./csv_files/', processed_dir='./processed_files/', simulate=False):\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    result = {}\n\n    for file_name in os.listdir(csv_dir):\n        if not file_name.endswith('.csv'):\n            continue\n\n        file_path = os.path.join(csv_dir, file_name)\n        with open(file_path, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            for index, row in enumerate(reader):\n                if row and row[0] == target_value:\n                    result[file_name] = index\n                    if not simulate:\n                        shutil.move(file_path, os.path.join(processed_dir, file_name))\n                    break\n\n    return result", "raw_solution": "import os\nimport shutil\nimport csv\n\ndef task_func(target_value='332', csv_dir='./csv_files/', processed_dir='./processed_files/', simulate=False):\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    result = {}\n\n    for file_name in os.listdir(csv_dir):\n        if not file_name.endswith('.csv'):\n            continue\n\n        file_path = os.path.join(csv_dir, file_name)\n        with open(file_path, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            for index, row in enumerate(reader):\n                if row and row[0] == target_value:\n                    result[file_name] = index\n                    if not simulate:\n                        shutil.move(file_path, os.path.join(processed_dir, file_name))\n                    break\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/837", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(n_rows, scale_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    data = np.random.randint(0, 100, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    scaler = StandardScaler()\n    df[df.columns[scale_cols]] = scaler.fit_transform(df[df.columns[scale_cols]])\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(n_rows, scale_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    data = np.random.randint(0, 100, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    scaler = StandardScaler()\n    df[df.columns[scale_cols]] = scaler.fit_transform(df[df.columns[scale_cols]])\n\n    return df\n\n# Example usage:\n# df = task_func(3, [1], columns=['test', 'scale'], random_seed=1)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/838", "solution": "import re\nfrom nltk.stem import PorterStemmer\nimport pandas as pd\ndef task_func(text_series):\n    stemmer = PorterStemmer()\n    \n    def process_text(text):\n        # Convert to lowercase\n        text = text.lower()\n        # Remove non-alphanumeric characters except spaces\n        text = re.sub(r'[^a-z\\s]', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Stem words\n        words = text.split()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return ' '.join(stemmed_words)\n    \n    processed_series = text_series.apply(process_text)\n    return processed_series", "raw_solution": "import re\nfrom nltk.stem import PorterStemmer\nimport pandas as pd\n\ndef task_func(text_series):\n    stemmer = PorterStemmer()\n    \n    def process_text(text):\n        # Convert to lowercase\n        text = text.lower()\n        # Remove non-alphanumeric characters except spaces\n        text = re.sub(r'[^a-z\\s]', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Stem words\n        words = text.split()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return ' '.join(stemmed_words)\n    \n    processed_series = text_series.apply(process_text)\n    return processed_series\n\n# Example usage:\ninput_series = pd.Series([\"Yeh ek sample text hai.\", \"Ek aur udharan!\"])\noutput_series = task_func(input_series)"}
{"task_id": "BigCodeBench/839", "solution": "import csv\nimport random\nimport string\ndef task_func(file_path,\n              num_rows,\n              gender=['Male', 'Female', 'Non-Binary'],\n              countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    fieldnames = ['Name', 'Age', 'Gender', 'Country']\n    with open(file_path, mode='w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for _ in range(num_rows):\n            name = ''.join(random.choices(string.ascii_uppercase, k=5))\n            age = random.randint(20, 60)\n            gender_choice = random.choice(gender)\n            country_choice = random.choice(countries)\n            writer.writerow({'Name': name, 'Age': age, 'Gender': gender_choice, 'Country': country_choice})\n\n    return file_path", "raw_solution": "import csv\nimport random\nimport string\n\ndef task_func(file_path,\n              num_rows,\n              gender=['Male', 'Female', 'Non-Binary'],\n              countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    fieldnames = ['Name', 'Age', 'Gender', 'Country']\n    with open(file_path, mode='w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for _ in range(num_rows):\n            name = ''.join(random.choices(string.ascii_uppercase, k=5))\n            age = random.randint(20, 60)\n            gender_choice = random.choice(gender)\n            country_choice = random.choice(countries)\n            writer.writerow({'Name': name, 'Age': age, 'Gender': gender_choice, 'Country': country_choice})\n\n    return file_path\n```\n"}
{"task_id": "BigCodeBench/840", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(file_path, num_rows, data_dimensions=5, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random data\n    data = np.random.rand(num_rows, data_dimensions)\n\n    # Create column names\n    columns = [f'Feature_{i+1}' for i in range(data_dimensions)]\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=columns)\n\n    # Save DataFrame to CSV\n    df.to_csv(file_path, index=False)\n\n    return file_path", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(file_path, num_rows, data_dimensions=5, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random data\n    data = np.random.rand(num_rows, data_dimensions)\n\n    # Create column names\n    columns = [f'Feature_{i+1}' for i in range(data_dimensions)]\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=columns)\n\n    # Save DataFrame to CSV\n    df.to_csv(file_path, index=False)\n\n    return file_path\n\n# Example usage:\n# print(task_func('/tmp/data.csv', 100))\n```\n"}
{"task_id": "BigCodeBench/841", "solution": "import re\nimport json\nfrom collections import defaultdict\nimport string\ndef task_func(json_string):\n    try:\n        data = json.loads(json_string)\n        text = data.get('text', '')\n    except (json.JSONDecodeError, KeyError):\n        return {}\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Split text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count word frequencies\n    word_count = defaultdict(int)\n    for word in words:\n        word_count[word] += 1\n\n    return dict(word_count)", "raw_solution": "import re\nimport json\nfrom collections import defaultdict\nimport string\n\ndef task_func(json_string):\n    try:\n        data = json.loads(json_string)\n        text = data.get('text', '')\n    except (json.JSONDecodeError, KeyError):\n        return {}\n\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Split text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count word frequencies\n    word_count = defaultdict(int)\n    for word in words:\n        word_count[word] += 1\n\n    return dict(word_count)\n\n# Example usage:\njson_input = '{\"text\": \"Hello world! Hello universe. World, meet universe.\"}'"}
{"task_id": "BigCodeBench/842", "solution": "import sqlite3\nimport random\ndef task_func(db_path,\n          num_entries,\n          users=['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'],\n          countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n          random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute('''\n        CREATE TABLE users (\n            id INTEGER PRIMARY KEY,\n            name TEXT,\n            age INTEGER,\n            country TEXT\n        )\n    ''')\n\n    for i in range(num_entries):\n        name = random.choice(users)\n        age = random.randint(20, 60)\n        country = random.choice(countries)\n        c.execute('INSERT INTO users (name, age, country) VALUES (?, ?, ?)', (name, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path", "raw_solution": "import sqlite3\nimport random\n\ndef task_func(db_path,\n          num_entries,\n          users=['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'],\n          countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n          random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute('''\n        CREATE TABLE users (\n            id INTEGER PRIMARY KEY,\n            name TEXT,\n            age INTEGER,\n            country TEXT\n        )\n    ''')\n\n    for i in range(num_entries):\n        name = random.choice(users)\n        age = random.randint(20, 60)\n        country = random.choice(countries)\n        c.execute('INSERT INTO users (name, age, country) VALUES (?, ?, ?)', (name, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path\n```\n"}
{"task_id": "BigCodeBench/843", "solution": "import random\nimport re\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\ndef task_func(n_sentences):\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choices(WORD_LIST, k=random.randint(1, 5)))\n        sentence += '.'\n        sentences.append(sentence)\n\n    # Join all sentences into a single string\n    text = ' '.join(sentences)\n\n    # Remove non-alphanumeric characters except for spaces\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    return text", "raw_solution": "import random\nimport re\n\n# Constants\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\n\ndef task_func(n_sentences):\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choices(WORD_LIST, k=random.randint(1, 5)))\n        sentence += '.'\n        sentences.append(sentence)\n\n    # Join all sentences into a single string\n    text = ' '.join(sentences)\n\n    # Remove non-alphanumeric characters except for spaces\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    return text\n\n# Example usage:\nrandom.seed(42)\nresult = task_func(2)"}
{"task_id": "BigCodeBench/844", "solution": "import csv\nimport random\nfrom faker import Faker\ndef task_func(file_path, num_rows, random_seed=None):\n    if not isinstance(num_rows, int) or num_rows < 0:\n        raise ValueError(\"num_rows must be a non-negative integer.\")\n    \n    fake = Faker()\n    random.seed(random_seed)\n    Faker.seed(random_seed)\n    \n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Address', 'Email'])\n        \n        for _ in range(num_rows):\n            name = fake.first_name() + ' ' + fake.last_name()\n            age = random.randint(20, 60)\n            address = fake.address().replace('\\n', ', ')\n            email = fake.email()\n            writer.writerow([name, age, address, email])\n    \n    return file_path", "raw_solution": "import csv\nimport random\nfrom faker import Faker\n\ndef task_func(file_path, num_rows, random_seed=None):\n    if not isinstance(num_rows, int) or num_rows < 0:\n        raise ValueError(\"num_rows must be a non-negative integer.\")\n    \n    fake = Faker()\n    random.seed(random_seed)\n    Faker.seed(random_seed)\n    \n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Address', 'Email'])\n        \n        for _ in range(num_rows):\n            name = fake.first_name() + ' ' + fake.last_name()\n            age = random.randint(20, 60)\n            address = fake.address().replace('\\n', ', ')\n            email = fake.email()\n            writer.writerow([name, age, address, email])\n    \n    return file_path\n```\n"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Preprocess the texts by removing non-alphanumeric characters and converting to lowercase\n    text1 = ALPHANUMERIC.sub('', text1).lower()\n    text2 = ALPHANUMERIC.sub('', text2).lower()\n\n    # Tokenize the texts into words\n    words1 = text1.split()\n    words2 = text2.split()\n\n    # Calculate term frequency for each text\n    tf1 = Counter(words1)\n    tf2 = Counter(words2)\n\n    # Calculate cosine similarity\n    common_words = set(tf1.keys()) & set(tf2.keys())\n    dot_product = sum(tf1[word] * tf2[word] for word in common_words)\n    magnitude1 = np.sqrt(sum(tf1[word] ** 2 for word in tf1.keys()))\n    magnitude2 = np.sqrt(sum(tf2[word] ** 2 for word in tf2.keys()))\n    cosine_similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0\n\n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return cosine_similarity, levenshtein_ratio", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    # Preprocess the texts by removing non-alphanumeric characters and converting to lowercase\n    text1 = ALPHANUMERIC.sub('', text1).lower()\n    text2 = ALPHANUMERIC.sub('', text2).lower()\n\n    # Tokenize the texts into words\n    words1 = text1.split()\n    words2 = text2.split()\n\n    # Calculate term frequency for each text\n    tf1 = Counter(words1)\n    tf2 = Counter(words2)\n\n    # Calculate cosine similarity\n    common_words = set(tf1.keys()) & set(tf2.keys())\n    dot_product = sum(tf1[word] * tf2[word] for word in common_words)\n    magnitude1 = np.sqrt(sum(tf1[word] ** 2 for word in tf1.keys()))\n    magnitude2 = np.sqrt(sum(tf2[word] ** 2 for word in tf2.keys()))\n    cosine_similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0\n\n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return cosine_similarity, levenshtein_ratio\n\n# Example usage"}
{"task_id": "BigCodeBench/846", "solution": "import collections\nimport pandas as pd\nclass ExampleObject:\n    def __init__(self, animal, shape):\n        self.animal = animal\n        self.shape = shape\ndef task_func(obj_list, attr):\n    # Extract the attribute values from the objects in the list\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n    \n    # Count the frequency of each attribute value\n    attr_counter = collections.Counter(attr_values)\n    \n    # Convert the counter to a pandas DataFrame\n    df = pd.DataFrame(attr_counter.items(), columns=[attr, 'count'])\n    \n    return df\nobj_list = [ExampleObject('tiger', 'Square'), ExampleObject('leopard', 'Circle'), ExampleObject('cat', 'Rectangle'), ExampleObject('elephant', 'Rectangle')]", "raw_solution": "import collections\nimport pandas as pd\n\nclass ExampleObject:\n    def __init__(self, animal, shape):\n        self.animal = animal\n        self.shape = shape\n\ndef task_func(obj_list, attr):\n    # Extract the attribute values from the objects in the list\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n    \n    # Count the frequency of each attribute value\n    attr_counter = collections.Counter(attr_values)\n    \n    # Convert the counter to a pandas DataFrame\n    df = pd.DataFrame(attr_counter.items(), columns=[attr, 'count'])\n    \n    return df\n\n# Example usage:\nobj_list = [ExampleObject('tiger', 'Square'), ExampleObject('leopard', 'Circle'), ExampleObject('cat', 'Rectangle'), ExampleObject('elephant', 'Rectangle')]\ncount = task_func(obj_list, 'shape')"}
{"task_id": "BigCodeBench/847", "solution": "import re\nimport os\nimport string\nimport random\ndef task_func(input_string, directory='./text_files'):\n    # Create directory if it doesn't exist\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Remove special characters and split the string into lines\n    cleaned_lines = re.sub(r'[^\\w\\s]', '', input_string).split('\\n')\n\n    # Generate random file names\n    file_paths = []\n    for line in cleaned_lines:\n        if line.strip():  # Check if line is not empty\n            random_number = random.randint(10000, 99999)\n            file_name = f\"{random_number}.txt\"\n            file_path = os.path.join(directory, file_name)\n            with open(file_path, 'w') as file:\n                file.write(line)\n            file_paths.append(file_path)\n\n    return file_paths", "raw_solution": "import re\nimport os\nimport string\nimport random\n\ndef task_func(input_string, directory='./text_files'):\n    # Create directory if it doesn't exist\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Remove special characters and split the string into lines\n    cleaned_lines = re.sub(r'[^\\w\\s]', '', input_string).split('\\n')\n\n    # Generate random file names\n    file_paths = []\n    for line in cleaned_lines:\n        if line.strip():  # Check if line is not empty\n            random_number = random.randint(10000, 99999)\n            file_name = f\"{random_number}.txt\"\n            file_path = os.path.join(directory, file_name)\n            with open(file_path, 'w') as file:\n                file.write(line)\n            file_paths.append(file_path)\n\n    return file_paths\n\n# Example usage:\n# print(task_func('line a\\\\nfollows by line b\\\\n...bye\\\\n'))\n```\n"}
{"task_id": "BigCodeBench/848", "solution": "import heapq\nimport random\nclass Object:\n    def __init__(self, value):\n        self.value = value\ndef task_func(obj_list, attr, top_n=5, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if not obj_list or not hasattr(obj_list[0], attr):\n        return [], None\n\n    # Extract the attribute values from the objects\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n\n    # Find the top N values using a heap\n    top_values = heapq.nlargest(top_n, attr_values)\n\n    # Randomly choose a value from all attribute values\n    random_value = random.choice(attr_values)\n\n    return top_values, random_value\nobj_list = [Object(random.randint(1, 100)) for _ in range(33)]", "raw_solution": "import heapq\nimport random\n\nclass Object:\n    def __init__(self, value):\n        self.value = value\n\ndef task_func(obj_list, attr, top_n=5, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if not obj_list or not hasattr(obj_list[0], attr):\n        return [], None\n\n    # Extract the attribute values from the objects\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n\n    # Find the top N values using a heap\n    top_values = heapq.nlargest(top_n, attr_values)\n\n    # Randomly choose a value from all attribute values\n    random_value = random.choice(attr_values)\n\n    return top_values, random_value\n\n# Example usage:\nrandom.seed(1)\nobj_list = [Object(random.randint(1, 100)) for _ in range(33)]\ntop_values, random_value = task_func(obj_list, 'value', 5, seed=1)"}
{"task_id": "BigCodeBench/849", "solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(input_string):\n    # Remove newline characters and split the string into words\n    words = re.sub(r'\\n', ' ', input_string).split()\n\n    # Remove stopwords and count word frequencies\n    filtered_words = [word for word in words if word.lower() not in STOPWORDS]\n    word_counts = Counter(filtered_words)\n\n    return dict(word_counts)", "raw_solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(input_string):\n    # Remove newline characters and split the string into words\n    words = re.sub(r'\\n', ' ', input_string).split()\n\n    # Remove stopwords and count word frequencies\n    filtered_words = [word for word in words if word.lower() not in STOPWORDS]\n    word_counts = Counter(filtered_words)\n\n    return dict(word_counts)\n```\n"}
{"task_id": "BigCodeBench/850", "solution": "import pandas as pd\nimport statistics\nimport random\ndef task_func(students, subjects, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    report_data = []\n\n    for student in students:\n        grades = {subject: random.randint(0, 100) for subject in subjects}\n        average_grade = statistics.mean(grades.values())\n        report_data.append([student] + list(grades.values()) + [average_grade])\n\n    report_columns = ['student'] + subjects + ['average_grade']\n    report_df = pd.DataFrame(report_data, columns=report_columns)\n\n    return report_df\nstudent = ['Alice', 'Bob', 'Charlie']\nsubjects = ['Math', 'Physics', 'English']", "raw_solution": "import pandas as pd\nimport statistics\nimport random\n\ndef task_func(students, subjects, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    report_data = []\n\n    for student in students:\n        grades = {subject: random.randint(0, 100) for subject in subjects}\n        average_grade = statistics.mean(grades.values())\n        report_data.append([student] + list(grades.values()) + [average_grade])\n\n    report_columns = ['student'] + subjects + ['average_grade']\n    report_df = pd.DataFrame(report_data, columns=report_columns)\n\n    return report_df\n\n# Example usage:\nstudent = ['Alice', 'Bob', 'Charlie']\nsubjects = ['Math', 'Physics', 'English']\nreport = task_func(student, subjects, seed=123)"}
{"task_id": "BigCodeBench/851", "solution": "import textwrap\nimport re\ndef task_func(input_string, width):\n    \"\"\"\n    Ek multi string ko separate strings mein vibhajit karein aur pratyek line ko ek certain width tak lapetein.\n\n    Parameters:\n    - input_string (str): Vah multi string jise lapetne ki aavashyakta hai.\n    - width (int): Pratyek line ko lapetne ki width.\n\n    Returns:\n        str: Lapeta gaya string jahaan pratyek line ko specified width tak lapeta gaya hai.\n\n    Requirements:\n    - textwrap\n    - re\n\n    Example:\n    >>> task_func('Another line\\\\\\\\nWith wrapping', 8)\n    'Another\\\\\\\\nline\\\\\\\\nWith\\\\\\\\nwrapping'\n\n    \"\"\"\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n    \n    # Wrap each line to the specified width\n    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n    \n    # Join the wrapped lines back into a single string\n    result = '\\n'.join(wrapped_lines)\n    \n    # Replace double backslashes with single backslashes\n    result = re.sub(r'\\\\\\\\n', '\\\\n', result)\n    \n    return result", "raw_solution": "import textwrap\nimport re\n\ndef task_func(input_string, width):\n    \"\"\"\n    Ek multi string ko separate strings mein vibhajit karein aur pratyek line ko ek certain width tak lapetein.\n\n    Parameters:\n    - input_string (str): Vah multi string jise lapetne ki aavashyakta hai.\n    - width (int): Pratyek line ko lapetne ki width.\n\n    Returns:\n        str: Lapeta gaya string jahaan pratyek line ko specified width tak lapeta gaya hai.\n\n    Requirements:\n    - textwrap\n    - re\n\n    Example:\n    >>> task_func('Another line\\\\\\\\nWith wrapping', 8)\n    'Another\\\\\\\\nline\\\\\\\\nWith\\\\\\\\nwrapping'\n\n    \"\"\"\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n    \n    # Wrap each line to the specified width\n    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n    \n    # Join the wrapped lines back into a single string\n    result = '\\n'.join(wrapped_lines)\n    \n    # Replace double backslashes with single backslashes\n    result = re.sub(r'\\\\\\\\n', '\\\\n', result)\n    \n    return result\n\n# Test cases"}
{"task_id": "BigCodeBench/852", "solution": "import random\nimport string\ndef task_func(max_length, n_samples, seed=None):\n    if max_length < 1:\n        raise ValueError(\"max_length must be at least 1\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    letters = string.ascii_lowercase\n    random_strings = [''.join(random.choices(letters, k=length)) for length in range(1, max_length + 1) for _ in range(n_samples // max_length)]\n\n    return random_strings[:n_samples]", "raw_solution": "import random\nimport string\n\ndef task_func(max_length, n_samples, seed=None):\n    if max_length < 1:\n        raise ValueError(\"max_length must be at least 1\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    letters = string.ascii_lowercase\n    random_strings = [''.join(random.choices(letters, k=length)) for length in range(1, max_length + 1) for _ in range(n_samples // max_length)]\n\n    return random_strings[:n_samples]\n\n# Example usage:\n# print(task_func(3, 12, seed=12))\n# print(task_func(5, n_samples=8, seed=1))\n```\n"}
{"task_id": "BigCodeBench/853", "solution": "import os\nimport shutil\nimport string\nINVALID_CHARACTERS = string.punctuation + string.whitespace\ndef task_func(directory_path):\n    summary = {}\n    invalid_files = []\n\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            if any(char in file for char in INVALID_CHARACTERS):\n                invalid_files.append(file)\n\n    if invalid_files:\n        invalid_dir = os.path.join(directory_path, 'Invalid.')\n        os.makedirs(invalid_dir, exist_ok=True)\n        for file in invalid_files:\n            shutil.move(os.path.join(directory_path, file), invalid_dir)\n            file_extension = file.split('.')[-1]\n            summary[file_extension] = summary.get(file_extension, 0) + 1\n\n    return summary", "raw_solution": "import os\nimport shutil\nimport string\n\n# Constants\nINVALID_CHARACTERS = string.punctuation + string.whitespace\n\ndef task_func(directory_path):\n    summary = {}\n    invalid_files = []\n\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            if any(char in file for char in INVALID_CHARACTERS):\n                invalid_files.append(file)\n\n    if invalid_files:\n        invalid_dir = os.path.join(directory_path, 'Invalid.')\n        os.makedirs(invalid_dir, exist_ok=True)\n        for file in invalid_files:\n            shutil.move(os.path.join(directory_path, file), invalid_dir)\n            file_extension = file.split('.')[-1]\n            summary[file_extension] = summary.get(file_extension, 0) + 1\n\n    return summary\n\n# Example usage:\n# print(task_func('path_to_directory'))\n```\nThis script walks through the directory and its subdirectories, checks for files with invalid characters in their names, and moves them to a new directory named 'Invalid.'. It also keeps a count of the number of files moved for each file extension.\n"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    if not isinstance(numbers, list) or not all(isinstance(n, int) for n in numbers):\n        raise TypeError(\"Input must be a list of integers.\")\n    if any(n < 0 for n in numbers):\n        raise ValueError(\"Input numbers must be non-negative.\")\n\n    # Generate all permutations of the list\n    perms = list(permutations(numbers))\n\n    # Calculate the sum of the factorials for each permutation\n    factorial_sums = [reduce(lambda x, y: x + math.factorial(y), perm) for perm in perms]\n\n    return factorial_sums, perms", "raw_solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    if not isinstance(numbers, list) or not all(isinstance(n, int) for n in numbers):\n        raise TypeError(\"Input must be a list of integers.\")\n    if any(n < 0 for n in numbers):\n        raise ValueError(\"Input numbers must be non-negative.\")\n\n    # Generate all permutations of the list\n    perms = list(permutations(numbers))\n\n    # Calculate the sum of the factorials for each permutation\n    factorial_sums = [reduce(lambda x, y: x + math.factorial(y), perm) for perm in perms]\n\n    return factorial_sums, perms\n\n# Example usage:\n# fac, perm = task_func([1, 2, 3])\n# print(fac)  # Output: [9, 9, 9, 9, 9, 9]\n# print(perm)  # Output: [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]\n\n# fac, perm = task_func([0, 4])\n# print(fac)  # Output: [25, 25]\n# print(perm)  # Output: [(0, 4), (4, 0)]\n```\n"}
{"task_id": "BigCodeBench/855", "solution": "import random\nimport string\nimport collections\nVALID_CHARACTERS = string.ascii_letters + string.digits\ndef task_func(n_strings, string_length):\n    \"\"\"\n    n random string ki specified length utpann karein, sabhi string mein pratyek character ki frequency ginein, aur result ko ek dictionary ke roop mein vaapas karein.\n    Constants:\n    - VALID_CHARACTERS: ek string jismein sabhi valid characters (ASCII letters aur digits) shaamil hain jinka upyog random string mein kiya ja sakta hai.\n\n    Parameters:\n    - n_strings (int): utpann karne ke liye random string ki sankhya.\n    - string_length (int): pratyek random string ki length.\n\n    Returns:\n        dict: ek dictionary jismein character counts shaamil hain, jismein characters keys ke roop mein aur unki frequency values ke roop mein hain.\n\n    Requirements:\n    - random\n    - string\n    - collections\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func(2, 3)\n    {'O': 1, 'h': 1, 'b': 1, 'V': 1, 'r': 1, 'p': 1}\n\n    \"\"\"\n    all_chars = []\n    for _ in range(n_strings):\n        random_string = ''.join(random.choice(VALID_CHARACTERS) for _ in range(string_length))\n        all_chars.extend(random_string)\n\n    char_count = collections.Counter(all_chars)\n    return dict(char_count)", "raw_solution": "import random\nimport string\nimport collections\n\n# Constants\nVALID_CHARACTERS = string.ascii_letters + string.digits\n\ndef task_func(n_strings, string_length):\n    \"\"\"\n    n random string ki specified length utpann karein, sabhi string mein pratyek character ki frequency ginein, aur result ko ek dictionary ke roop mein vaapas karein.\n    Constants:\n    - VALID_CHARACTERS: ek string jismein sabhi valid characters (ASCII letters aur digits) shaamil hain jinka upyog random string mein kiya ja sakta hai.\n\n    Parameters:\n    - n_strings (int): utpann karne ke liye random string ki sankhya.\n    - string_length (int): pratyek random string ki length.\n\n    Returns:\n        dict: ek dictionary jismein character counts shaamil hain, jismein characters keys ke roop mein aur unki frequency values ke roop mein hain.\n\n    Requirements:\n    - random\n    - string\n    - collections\n\n    Example:\n    >>> random.seed(42)\n    >>> task_func(2, 3)\n    {'O': 1, 'h': 1, 'b': 1, 'V': 1, 'r': 1, 'p': 1}\n\n    \"\"\"\n    all_chars = []\n    for _ in range(n_strings):\n        random_string = ''.join(random.choice(VALID_CHARACTERS) for _ in range(string_length))\n        all_chars.extend(random_string)\n\n    char_count = collections.Counter(all_chars)\n    return dict(char_count)\n```\n"}
{"task_id": "BigCodeBench/856", "solution": "from functools import reduce\nfrom itertools import combinations\nimport numpy as np\ndef task_func(shape=(3, 3), low=1, high=10, seed=None):\n    if high <= low:\n        raise ValueError(\"high must be greater than low\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    matrix = np.random.randint(low, high, size=shape)\n    flat_matrix = matrix.flatten()\n    pairs = combinations(flat_matrix, 2)\n    products = [reduce(lambda x, y: x * y, pair) for pair in pairs]\n    total_product_sum = sum(products)\n\n    return total_product_sum, matrix", "raw_solution": "from functools import reduce\nfrom itertools import combinations\nimport numpy as np\n\ndef task_func(shape=(3, 3), low=1, high=10, seed=None):\n    if high <= low:\n        raise ValueError(\"high must be greater than low\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    matrix = np.random.randint(low, high, size=shape)\n    flat_matrix = matrix.flatten()\n    pairs = combinations(flat_matrix, 2)\n    products = [reduce(lambda x, y: x * y, pair) for pair in pairs]\n    total_product_sum = sum(products)\n\n    return total_product_sum, matrix\n\n# Example usage:\n# print(task_func((5, 4), seed=1))\n```\n"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n\n    # Ensure destination directory exists\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    for ext in EXTENSIONS:\n        # Find all files with the specified extensions in the source directory\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*{ext}'))\n        \n        for file in files:\n            try:\n                # Copy file to destination directory\n                shutil.copy(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                # Warn about any issues encountered during the file transfer\n                warnings.warn(f\"Failed to transfer {file}: {e}\")\n\n        # Wait for a second before processing the next extension\n        time.sleep(1)\n\n    return transferred_files", "raw_solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n\n    # Ensure destination directory exists\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    for ext in EXTENSIONS:\n        # Find all files with the specified extensions in the source directory\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*{ext}'))\n        \n        for file in files:\n            try:\n                # Copy file to destination directory\n                shutil.copy(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                # Warn about any issues encountered during the file transfer\n                warnings.warn(f\"Failed to transfer {file}: {e}\")\n\n        # Wait for a second before processing the next extension\n        time.sleep(1)\n\n    return transferred_files\n```\n"}
{"task_id": "BigCodeBench/858", "solution": "import string\nimport random\nfrom collections import Counter\ndef task_func(n, seed=None):\n    # Set the seed for reproducibility\n    random.seed(seed)\n    \n    # Generate a list of random lowercase letters\n    random_letters = [random.choice(string.ascii_lowercase) for _ in range(n)]\n    \n    # Count the occurrences of each letter\n    letter_counts = Counter(random_letters)\n    \n    return letter_counts", "raw_solution": "import string\nimport random\nfrom collections import Counter\n\ndef task_func(n, seed=None):\n    # Set the seed for reproducibility\n    random.seed(seed)\n    \n    # Generate a list of random lowercase letters\n    random_letters = [random.choice(string.ascii_lowercase) for _ in range(n)]\n    \n    # Count the occurrences of each letter\n    letter_counts = Counter(random_letters)\n    \n    return letter_counts\n\n# Example usage:\n# letter_counts = task_func(1000, seed=123)\n# print(letter_counts)\n```\n"}
{"task_id": "BigCodeBench/859", "solution": "import warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\ndef task_func():\n    # Load the iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Set the warning action to 'always'\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n        model = svm.SVC()\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        accuracy = metrics.accuracy_score(y_test, y_pred)\n\n        # Check if accuracy is less than 0.9\n        if accuracy < 0.9:\n            warning_msg = f\"Warning: Accuracy is less than 0.9. Current accuracy is {accuracy}.\"\n        else:\n            warning_msg = None\n\n    return accuracy, warning_msg", "raw_solution": "import warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\n\ndef task_func():\n    # Load the iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Set the warning action to 'always'\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n        model = svm.SVC()\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        accuracy = metrics.accuracy_score(y_test, y_pred)\n\n        # Check if accuracy is less than 0.9\n        if accuracy < 0.9:\n            warning_msg = f\"Warning: Accuracy is less than 0.9. Current accuracy is {accuracy}.\"\n        else:\n            warning_msg = None\n\n    return accuracy, warning_msg\n```\n"}
{"task_id": "BigCodeBench/860", "solution": "import re\nimport random\nimport string\ndef task_func(n, pattern, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def generate_random_string(length):\n        return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n\n    matches = []\n    for _ in range(n):\n        random_string = generate_random_string(random.randint(5, 10))\n        matches.extend(re.findall(pattern, random_string))\n\n    return matches", "raw_solution": "import re\nimport random\nimport string\n\ndef task_func(n, pattern, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def generate_random_string(length):\n        return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n\n    matches = []\n    for _ in range(n):\n        random_string = generate_random_string(random.randint(5, 10))\n        matches.extend(re.findall(pattern, random_string))\n\n    return matches\n\n# Example usage:\n# print(task_func(100, r'[A-Za-z]{5}', seed=12345))\n```\n"}
{"task_id": "BigCodeBench/861", "solution": "from collections import Counter\nfrom random import choice, seed\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\ndef task_func(list_of_lists):\n    seed(42)  # For reproducibility\n    baskets = []\n\n    for basket in list_of_lists:\n        cart = Counter()\n        for item in basket:\n            cart[choice(POSSIBLE_ITEMS)] += 1\n        baskets.append(cart)\n\n    return baskets", "raw_solution": "from collections import Counter\nfrom random import choice, seed\n\n# Constants\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef task_func(list_of_lists):\n    seed(42)  # For reproducibility\n    baskets = []\n\n    for basket in list_of_lists:\n        cart = Counter()\n        for item in basket:\n            cart[choice(POSSIBLE_ITEMS)] += 1\n        baskets.append(cart)\n\n    return baskets\n\n# Example usage:\nbaskets = task_func([[1, 2, 3], [4, 5]])"}
{"task_id": "BigCodeBench/862", "solution": "import random\nimport string\nfrom collections import defaultdict\ndef task_func(n, seed=None):\n    random.seed(seed)\n    letters = string.ascii_lowercase\n    letter_dict = defaultdict(list)\n\n    for _ in range(n):\n        letter = random.choice(letters)\n        letter_dict[letter].append(letter)\n\n    return letter_dict", "raw_solution": "import random\nimport string\nfrom collections import defaultdict\n\ndef task_func(n, seed=None):\n    random.seed(seed)\n    letters = string.ascii_lowercase\n    letter_dict = defaultdict(list)\n\n    for _ in range(n):\n        letter = random.choice(letters)\n        letter_dict[letter].append(letter)\n\n    return letter_dict\n\n# Example usage:\n# print(task_func(30, seed=1))\n```\n"}
{"task_id": "BigCodeBench/863", "solution": "import numpy as np\nimport math\ndef task_func(list_of_lists):\n    sums = []\n    for lst in list_of_lists:\n        sum_of_squares = sum(math.pow(num, 2) for num in lst)\n        sums.append(sum_of_squares)\n    return sums", "raw_solution": "import numpy as np\nimport math\n\n# Constants\nPOSSIBLE_NUMBERS = np.arange(1, 11)\n\ndef task_func(list_of_lists):\n    sums = []\n    for lst in list_of_lists:\n        sum_of_squares = sum(math.pow(num, 2) for num in lst)\n        sums.append(sum_of_squares)\n    return sums\n\n# Test cases"}
{"task_id": "BigCodeBench/864", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(fruit_data):\n    if not fruit_data:\n        return pd.DataFrame(columns=['Total Count', 'Average Count'])\n\n    # Create a DataFrame from the list of tuples\n    df = pd.DataFrame(fruit_data, columns=['Fruit', 'Count'])\n\n    # Group by 'Fruit' and calculate total and average count\n    grouped = df.groupby('Fruit').agg(\n        Total_Count=('Count', 'sum'),\n        Average_Count=('Count', 'mean')\n    ).reset_index()\n\n    # Round the average count to the nearest integer\n    grouped['Average_Count'] = grouped['Average_Count'].round()\n\n    return grouped", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(fruit_data):\n    if not fruit_data:\n        return pd.DataFrame(columns=['Total Count', 'Average Count'])\n\n    # Create a DataFrame from the list of tuples\n    df = pd.DataFrame(fruit_data, columns=['Fruit', 'Count'])\n\n    # Group by 'Fruit' and calculate total and average count\n    grouped = df.groupby('Fruit').agg(\n        Total_Count=('Count', 'sum'),\n        Average_Count=('Count', 'mean')\n    ).reset_index()\n\n    # Round the average count to the nearest integer\n    grouped['Average_Count'] = grouped['Average_Count'].round()\n\n    return grouped\n\n# Example usage:\nfruit_list = [('apple', 5), ('banana', 3), ('apple', 6), ('banana', 4), ('cherry', 5), ('banana', 2), ('apple', 4), ('cherry', 5)]\nreport = task_func(fruit_list)\nreport.sort_index(inplace=True)"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=['item', 'count', 'weight'])\n\n    # Normalize the count column using z-score\n    df['normalized_count'] = zscore(df['count'])\n\n    # Normalize the weight column using min-max scaling\n    scaler = MinMaxScaler()\n    df['normalized_weight'] = scaler.fit_transform(df[['weight']])\n\n    # Select the relevant columns for the final report\n    report = df[['item', 'normalized_count', 'normalized_weight']]\n\n    return report\ndata = [('A', 100, 0.5), ('B', 200, 0.6), ('C', 150, 0.7)]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=['item', 'count', 'weight'])\n\n    # Normalize the count column using z-score\n    df['normalized_count'] = zscore(df['count'])\n\n    # Normalize the weight column using min-max scaling\n    scaler = MinMaxScaler()\n    df['normalized_weight'] = scaler.fit_transform(df[['weight']])\n\n    # Select the relevant columns for the final report\n    report = df[['item', 'normalized_count', 'normalized_weight']]\n\n    return report\n\n# Example usage:\ndata = [('A', 100, 0.5), ('B', 200, 0.6), ('C', 150, 0.7)]\nreport = task_func(data)"}
{"task_id": "BigCodeBench/866", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=2, random_state=0):\n    # Extracting coordinates from the data\n    coordinates = np.array([point[1:] for point in data])\n    \n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(coordinates)\n    \n    # Getting the cluster labels\n    labels = kmeans.labels_\n    \n    return labels\ndata = [('T1', 1, 1), ('T2', 1, 1.1), ('T2', 1.1, 1), ('C1', 400, 400), ('C2', 401, 401), ('B1', 35, 35)]", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters=2, random_state=0):\n    # Extracting coordinates from the data\n    coordinates = np.array([point[1:] for point in data])\n    \n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(coordinates)\n    \n    # Getting the cluster labels\n    labels = kmeans.labels_\n    \n    return labels\n\n# Example usage:\ndata = [('T1', 1, 1), ('T2', 1, 1.1), ('T2', 1.1, 1), ('C1', 400, 400), ('C2', 401, 401), ('B1', 35, 35)]\nlabels = task_func(data, n_clusters=3, random_state=42)"}
{"task_id": "BigCodeBench/867", "solution": "import re\nimport string\ndef task_func(text1, text2):\n    # Define the punctuation characters to be removed\n    punctuation_chars = string.punctuation\n\n    # Create a regex pattern to match any of the punctuation characters\n    pattern = \"[\" + re.escape(punctuation_chars) + \"]\"\n\n    # Remove punctuation from both texts\n    cleaned_text1 = re.sub(pattern, \"\", text1)\n    cleaned_text2 = re.sub(pattern, \"\", text2)\n\n    return cleaned_text1, cleaned_text2", "raw_solution": "import re\nimport string\n\ndef task_func(text1, text2):\n    # Define the punctuation characters to be removed\n    punctuation_chars = string.punctuation\n\n    # Create a regex pattern to match any of the punctuation characters\n    pattern = \"[\" + re.escape(punctuation_chars) + \"]\"\n\n    # Remove punctuation from both texts\n    cleaned_text1 = re.sub(pattern, \"\", text1)\n    cleaned_text2 = re.sub(pattern, \"\", text2)\n\n    return cleaned_text1, cleaned_text2\n\n# Test cases\ncleaned_text1, cleaned_text2 = task_func(\"test (with parenthesis []!!)\", \"And, other; stuff ^_`\")"}
{"task_id": "BigCodeBench/868", "solution": "from itertools import cycle\nfrom random import choice, seed\ndef task_func(n_colors, colors=['Red', 'Green', 'Blue', 'Yellow', 'Purple'], rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    color_cycle = cycle(colors)\n    result = []\n\n    for _ in range(n_colors):\n        result.append(next(color_cycle))\n        if len(result) % len(colors) == 0:\n            result.append(choice(result))\n\n    return result", "raw_solution": "from itertools import cycle\nfrom random import choice, seed\n\ndef task_func(n_colors, colors=['Red', 'Green', 'Blue', 'Yellow', 'Purple'], rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    color_cycle = cycle(colors)\n    result = []\n\n    for _ in range(n_colors):\n        result.append(next(color_cycle))\n        if len(result) % len(colors) == 0:\n            result.append(choice(result))\n\n    return result\n\n# Example usage:\n# color_pattern = task_func(4, rng_seed=123)\n# print(color_pattern)\n```\n"}
{"task_id": "BigCodeBench/869", "solution": "import pandas as pd\nfrom itertools import cycle\nfrom random import randint, seed\ndef task_func(\n    n_grades,\n    students=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    grade_range=range(1, 11),\n    rng_seed=None\n):\n    if not students:\n        raise ValueError(\"students list cannot be empty\")\n\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    student_cycle = cycle(students)\n    grades = []\n\n    for _ in range(n_grades):\n        student = next(student_cycle)\n        grade = randint(*grade_range)\n        grades.append({'Student': student, 'Grade': grade})\n\n    df = pd.DataFrame(grades)\n\n    return df", "raw_solution": "import pandas as pd\nfrom itertools import cycle\nfrom random import randint, seed\n\ndef task_func(\n    n_grades,\n    students=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    grade_range=range(1, 11),\n    rng_seed=None\n):\n    if not students:\n        raise ValueError(\"students list cannot be empty\")\n\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    student_cycle = cycle(students)\n    grades = []\n\n    for _ in range(n_grades):\n        student = next(student_cycle)\n        grade = randint(*grade_range)\n        grades.append({'Student': student, 'Grade': grade})\n\n    df = pd.DataFrame(grades)\n\n    return df\n\n# Example usage:\n# grade_report = task_func(5, rng_seed=12)\n# print(grade_report)\n```\n"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store mean values\n    mean_values = []\n\n    # Iterate over the data list\n    for i, data in enumerate(data_list):\n        # Filter out non-numerical values\n        numerical_values = [x for x in data if isinstance(x, (int, float))]\n\n        # Calculate the mean of numerical values\n        mean_value = np.mean(numerical_values) if numerical_values else np.nan\n\n        # Append the mean value to the list\n        mean_values.append(mean_value)\n\n    # Create a DataFrame with the mean values\n    df = pd.DataFrame(mean_values, columns=['Mean Value'], index=[f'Position {i}' for i in range(len(data_list))])\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store mean values\n    mean_values = []\n\n    # Iterate over the data list\n    for i, data in enumerate(data_list):\n        # Filter out non-numerical values\n        numerical_values = [x for x in data if isinstance(x, (int, float))]\n\n        # Calculate the mean of numerical values\n        mean_value = np.mean(numerical_values) if numerical_values else np.nan\n\n        # Append the mean value to the list\n        mean_values.append(mean_value)\n\n    # Create a DataFrame with the mean values\n    df = pd.DataFrame(mean_values, columns=['Mean Value'], index=[f'Position {i}' for i in range(len(data_list))])\n\n    return df\n\n# Example usage\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)"}
{"task_id": "BigCodeBench/871", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list, file_name):\n    if not data_list:\n        return []\n\n    # Initialize an empty list to store mean values\n    mean_values = []\n\n    # Open the file in write mode\n    with open(file_name, 'w') as file:\n        for position, data in enumerate(data_list, start=1):\n            # Filter out non-numeric values and convert to a numpy array\n            numeric_data = np.array([x for x in data[1:] if isinstance(x, (int, float))])\n            \n            # Calculate the mean using numpy\n            mean_value = np.mean(numeric_data)\n            \n            # Write the mean value to the file\n            file.write(f\"Position {position}: {mean_value}\\n\")\n            \n            # Append the mean value to the list\n            mean_values.append(mean_value)\n\n    return mean_values", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(data_list, file_name):\n    if not data_list:\n        return []\n\n    # Initialize an empty list to store mean values\n    mean_values = []\n\n    # Open the file in write mode\n    with open(file_name, 'w') as file:\n        for position, data in enumerate(data_list, start=1):\n            # Filter out non-numeric values and convert to a numpy array\n            numeric_data = np.array([x for x in data[1:] if isinstance(x, (int, float))])\n            \n            # Calculate the mean using numpy\n            mean_value = np.mean(numeric_data)\n            \n            # Write the mean value to the file\n            file.write(f\"Position {position}: {mean_value}\\n\")\n            \n            # Append the mean value to the list\n            mean_values.append(mean_value)\n\n    return mean_values\n```\n"}
{"task_id": "BigCodeBench/872", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        return []\n\n    # Unzip the list of tuples\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=0))\n\n    # Initialize the list to store mean values\n    mean_values = []\n\n    for position in unzipped_data:\n        numeric_values = [val for val in position if isinstance(val, (int, float))]\n        if numeric_values:\n            mean_values.append(np.mean(numeric_values))\n        else:\n            mean_values.append(np.nan)\n\n    return mean_values", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(data_list):\n    if not data_list:\n        return []\n\n    # Unzip the list of tuples\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=0))\n\n    # Initialize the list to store mean values\n    mean_values = []\n\n    for position in unzipped_data:\n        numeric_values = [val for val in position if isinstance(val, (int, float))]\n        if numeric_values:\n            mean_values.append(np.mean(numeric_values))\n        else:\n            mean_values.append(np.nan)\n\n    return mean_values\n\n# Test cases"}
{"task_id": "BigCodeBench/873", "solution": "import csv\nimport os\ndef task_func(data, file_path, headers):\n    if file_path is None:\n        raise ValueError(\"file_path cannot be None\")\n\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n    # Write data to CSV file\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        for row in data:\n            writer.writerow(row)\n\n    return os.path.abspath(file_path)", "raw_solution": "import csv\nimport os\n\ndef task_func(data, file_path, headers):\n    if file_path is None:\n        raise ValueError(\"file_path cannot be None\")\n\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n    # Write data to CSV file\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        for row in data:\n            writer.writerow(row)\n\n    return os.path.abspath(file_path)\n```\n"}
{"task_id": "BigCodeBench/874", "solution": "from itertools import zip_longest\nfrom scipy.spatial import distance\ndef task_func(points):\n    distances = []\n    for point1, point2 in zip_longest(points, points[1:], fillvalue=None):\n        if point1 is not None and point2 is not None:\n            dist = distance.euclidean(point1, point2)\n            distances.append(dist)\n    return distances", "raw_solution": "from itertools import zip_longest\nfrom scipy.spatial import distance\n\ndef task_func(points):\n    distances = []\n    for point1, point2 in zip_longest(points, points[1:], fillvalue=None):\n        if point1 is not None and point2 is not None:\n            dist = distance.euclidean(point1, point2)\n            distances.append(dist)\n    return distances\n\n# Test cases"}
{"task_id": "BigCodeBench/875", "solution": "import pandas as pd\nimport random\ndef task_func(data, columns=['Name', 'Age', 'Occupation'], fill_missing=False, num_range=(0, 100), seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a list to store the rows\n    rows = []\n\n    # Iterate over the data and fill missing values\n    for row in data:\n        new_row = []\n        for i, value in enumerate(row):\n            if value is None and fill_missing:\n                if isinstance(columns[i], str) and columns[i].lower() == 'age':\n                    new_row.append(random.randint(num_range[0], num_range[1]))\n                else:\n                    new_row.append(None)\n            else:\n                new_row.append(value)\n        rows.append(new_row)\n\n    # Create a DataFrame\n    df = pd.DataFrame(rows, columns=columns)\n\n    return df\ndata = [('john', 25, 'engineer'), ('alice', ), ('bob', )]", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(data, columns=['Name', 'Age', 'Occupation'], fill_missing=False, num_range=(0, 100), seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a list to store the rows\n    rows = []\n\n    # Iterate over the data and fill missing values\n    for row in data:\n        new_row = []\n        for i, value in enumerate(row):\n            if value is None and fill_missing:\n                if isinstance(columns[i], str) and columns[i].lower() == 'age':\n                    new_row.append(random.randint(num_range[0], num_range[1]))\n                else:\n                    new_row.append(None)\n            else:\n                new_row.append(value)\n        rows.append(new_row)\n\n    # Create a DataFrame\n    df = pd.DataFrame(rows, columns=columns)\n\n    return df\n\n# Example usage:\ndata = [('john', 25, 'engineer'), ('alice', ), ('bob', )]\ndf = task_func(data, fill_missing=True, num_range=(0, 10), seed=42)"}
{"task_id": "BigCodeBench/876", "solution": "import collections\nimport operator\nimport os\nimport shutil\ndef task_func(data_dict, source_directory, backup_directory):\n    # Add a key 'a' with value 1 to the dictionary\n    data_dict['a'] = 1\n\n    # Sort the dictionary by values' frequency in descending order\n    value_frequencies = sorted(collections.Counter(data_dict.values()).items(), key=operator.itemgetter(1), reverse=True)\n\n    # Backup all files from the source directory to the backup directory\n    backup_status = True\n    try:\n        if not os.path.exists(backup_directory):\n            os.makedirs(backup_directory)\n        for filename in os.listdir(source_directory):\n            shutil.copy(os.path.join(source_directory, filename), backup_directory)\n    except Exception as e:\n        backup_status = False\n\n    return data_dict, value_frequencies, backup_status", "raw_solution": "import collections\nimport operator\nimport os\nimport shutil\n\ndef task_func(data_dict, source_directory, backup_directory):\n    # Add a key 'a' with value 1 to the dictionary\n    data_dict['a'] = 1\n\n    # Sort the dictionary by values' frequency in descending order\n    value_frequencies = sorted(collections.Counter(data_dict.values()).items(), key=operator.itemgetter(1), reverse=True)\n\n    # Backup all files from the source directory to the backup directory\n    backup_status = True\n    try:\n        if not os.path.exists(backup_directory):\n            os.makedirs(backup_directory)\n        for filename in os.listdir(source_directory):\n            shutil.copy(os.path.join(source_directory, filename), backup_directory)\n    except Exception as e:\n        backup_status = False\n\n    return data_dict, value_frequencies, backup_status\n```\n"}
{"task_id": "BigCodeBench/877", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    if not all(data.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        raise ValueError(\"All columns in the DataFrame must be numeric.\")\n    if n_components > len(data.columns):\n        raise ValueError(\"n_components cannot be greater than the number of columns in the DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Input data cannot be empty.\")\n\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    pca = PCA(n_components=n_components)\n    principal_components = pca.fit_transform(scaled_data)\n\n    result_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n    return result_df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    if not all(data.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        raise ValueError(\"All columns in the DataFrame must be numeric.\")\n    if n_components > len(data.columns):\n        raise ValueError(\"n_components cannot be greater than the number of columns in the DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Input data cannot be empty.\")\n\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    pca = PCA(n_components=n_components)\n    principal_components = pca.fit_transform(scaled_data)\n\n    result_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n    return result_df\n```\n"}
{"task_id": "BigCodeBench/878", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\ndef task_func(data, target, test_size=0.2, random_state=None):\n    df = pd.DataFrame(data)\n    if target not in df.columns:\n        raise ValueError(f\"Target column '{target}' not found in DataFrame.\")\n    \n    X = df.drop(columns=[target])\n    y = df[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse, model, df", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(data, target, test_size=0.2, random_state=None):\n    df = pd.DataFrame(data)\n    if target not in df.columns:\n        raise ValueError(f\"Target column '{target}' not found in DataFrame.\")\n    \n    X = df.drop(columns=[target])\n    y = df[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse, model, df\n```\n"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    if data.empty:\n        raise ValueError(\"DataFrame should not be empty.\")\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Columns {col1} or {col2} not found in DataFrame.\")\n    if data[col1].nunique() < 2 or data[col2].nunique() < 2:\n        raise ValueError(\"Both columns should have at least two categories.\")\n    if data[col1].dtype not in [np.object, np.str] or data[col2].dtype not in [np.object, np.str]:\n        raise TypeError(\"Both columns should contain categorical data.\")\n\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p\ndata = pd.DataFrame({\n    'a': np.random.choice(['a', 'B'], size=100),\n    'b': np.random.choice(['x', 'Y'], size=100)\n})", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    if data.empty:\n        raise ValueError(\"DataFrame should not be empty.\")\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Columns {col1} or {col2} not found in DataFrame.\")\n    if data[col1].nunique() < 2 or data[col2].nunique() < 2:\n        raise ValueError(\"Both columns should have at least two categories.\")\n    if data[col1].dtype not in [np.object, np.str] or data[col2].dtype not in [np.object, np.str]:\n        raise TypeError(\"Both columns should contain categorical data.\")\n\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p\n\n# Example usage:\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'a': np.random.choice(['a', 'B'], size=100),\n    'b': np.random.choice(['x', 'Y'], size=100)\n})"}
{"task_id": "BigCodeBench/880", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=3, seed=None):\n    if not data.select_dtypes(include=[float, int]).shape[1] == data.shape[1]:\n        raise ValueError(\"DataFrame should only contain numerical values.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n    labels = kmeans.fit_predict(data)\n\n    return labels, kmeans\ndata = pd.DataFrame({\n    'a': [1, 20, 2, 22, 100],\n    'b': [1, 20, 2, 22, 100]\n})", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters=3, seed=None):\n    if not data.select_dtypes(include=[float, int]).shape[1] == data.shape[1]:\n        raise ValueError(\"DataFrame should only contain numerical values.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n    labels = kmeans.fit_predict(data)\n\n    return labels, kmeans\n\n# Example usage:\ndata = pd.DataFrame({\n    'a': [1, 20, 2, 22, 100],\n    'b': [1, 20, 2, 22, 100]\n})\nlabels, model = task_func(data, seed=213)"}
{"task_id": "BigCodeBench/881", "solution": "import pandas as pd\nimport random\ndef task_func(csv_file, column_name='data', pattern='\\d+[xX]', sample_size=None, seed=42):\n    df = pd.read_csv(csv_file)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in the CSV file.\")\n    \n    # Find all matches in the specified column\n    matches = df[column_name].apply(lambda x: pattern.search(x) is not None)\n    \n    # Filter the DataFrame to only include rows with matches\n    matches_df = df[matches]\n    \n    # If sample_size is provided, take a random sample\n    if sample_size is not None:\n        random.seed(seed)\n        indices = random.sample(matches_df.index.tolist(), min(sample_size, len(matches_df)))\n        sample_df = matches_df.loc[indices]\n    else:\n        sample_df = matches_df\n    \n    return sample_df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(csv_file, column_name='data', pattern='\\d+[xX]', sample_size=None, seed=42):\n    df = pd.read_csv(csv_file)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in the CSV file.\")\n    \n    # Find all matches in the specified column\n    matches = df[column_name].apply(lambda x: pattern.search(x) is not None)\n    \n    # Filter the DataFrame to only include rows with matches\n    matches_df = df[matches]\n    \n    # If sample_size is provided, take a random sample\n    if sample_size is not None:\n        random.seed(seed)\n        indices = random.sample(matches_df.index.tolist(), min(sample_size, len(matches_df)))\n        sample_df = matches_df.loc[indices]\n    else:\n        sample_df = matches_df\n    \n    return sample_df\n```\n"}
{"task_id": "BigCodeBench/882", "solution": "import sqlite3\nimport pandas as pd\nimport os\nimport re\ndef task_func(db_file, table_name, column_name, pattern='\\d+[xX]'):\n    if not os.path.exists(db_file):\n        raise ValueError(f\"The database file {db_file} does not exist.\")\n\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    query = f\"SELECT {column_name} FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    # Find matches using regex\n    matches = df[column_name].apply(lambda x: re.findall(pattern, x))\n    matches = [item for sublist in matches for item in sublist]\n\n    # Create a DataFrame with matches\n    result_df = pd.DataFrame(matches, columns=[column_name])\n\n    return result_df", "raw_solution": "import sqlite3\nimport pandas as pd\nimport os\nimport re\n\ndef task_func(db_file, table_name, column_name, pattern='\\d+[xX]'):\n    if not os.path.exists(db_file):\n        raise ValueError(f\"The database file {db_file} does not exist.\")\n\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    query = f\"SELECT {column_name} FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    # Find matches using regex\n    matches = df[column_name].apply(lambda x: re.findall(pattern, x))\n    matches = [item for sublist in matches for item in sublist]\n\n    # Create a DataFrame with matches\n    result_df = pd.DataFrame(matches, columns=[column_name])\n\n    return result_df\n```\n"}
{"task_id": "BigCodeBench/883", "solution": "import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\ndef task_func(df: pd.DataFrame, column_a: str, column_b: str, column_c: str) -> bool:\n    # Filter the DataFrame based on conditions\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] > 900)]\n\n    # Check if the filtered DataFrame is empty\n    if filtered_df.empty:\n        return False\n\n    # Perform Augmented Dickey-Fuller test on column_a\n    adf_test = adfuller(filtered_df[column_a])\n\n    # Check if the p-value is less than 0.05\n    if adf_test[1] < 0.05:\n        return True\n\n    return False", "raw_solution": "import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\n\ndef task_func(df: pd.DataFrame, column_a: str, column_b: str, column_c: str) -> bool:\n    # Filter the DataFrame based on conditions\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] > 900)]\n\n    # Check if the filtered DataFrame is empty\n    if filtered_df.empty:\n        return False\n\n    # Perform Augmented Dickey-Fuller test on column_a\n    adf_test = adfuller(filtered_df[column_a])\n\n    # Check if the p-value is less than 0.05\n    if adf_test[1] < 0.05:\n        return True\n\n    return False\n\n# Example usage"}
{"task_id": "BigCodeBench/884", "solution": "import pandas as pd\nfrom scipy.stats import chi2_contingency\ndef task_func(df, columns=['A', 'B', 'C'], larger=50, equal=900):\n    if df.empty or not all(col in df.columns for col in columns):\n        raise ValueError(\"Insufficient data or columns not present in DataFrame.\")\n    \n    if len(columns) != 3:\n        raise ValueError(\"Exactly three columns are required for the test.\")\n    \n    # Filter rows based on conditions\n    df_filtered = df[(df[columns[1]] > larger) & (df[columns[2]] == equal)]\n    \n    if df_filtered.empty:\n        raise ValueError(\"No rows meet the specified conditions.\")\n    \n    # Create contingency table\n    contingency_table = pd.crosstab(df_filtered[columns[0]], df_filtered[columns[1]])\n    \n    # Perform chi-square test\n    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n    \n    return p_value\ndf = pd.DataFrame({\n    'A': ['Yes', 'No', 'Yes', 'No'],\n    'B': [55, 70, 40, 85],\n    'C': [900, 900, 800, 900]\n})", "raw_solution": "import pandas as pd\nfrom scipy.stats import chi2_contingency\n\ndef task_func(df, columns=['A', 'B', 'C'], larger=50, equal=900):\n    if df.empty or not all(col in df.columns for col in columns):\n        raise ValueError(\"Insufficient data or columns not present in DataFrame.\")\n    \n    if len(columns) != 3:\n        raise ValueError(\"Exactly three columns are required for the test.\")\n    \n    # Filter rows based on conditions\n    df_filtered = df[(df[columns[1]] > larger) & (df[columns[2]] == equal)]\n    \n    if df_filtered.empty:\n        raise ValueError(\"No rows meet the specified conditions.\")\n    \n    # Create contingency table\n    contingency_table = pd.crosstab(df_filtered[columns[0]], df_filtered[columns[1]])\n    \n    # Perform chi-square test\n    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n    \n    return p_value\n\n# Example usage:\ndf = pd.DataFrame({\n    'A': ['Yes', 'No', 'Yes', 'No'],\n    'B': [55, 70, 40, 85],\n    'C': [900, 900, 800, 900]\n})\n"}
{"task_id": "BigCodeBench/885", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, col_a='A', col_b='B', col_c='C', seed=None):\n    if df.empty or not all(df.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        return None, None\n\n    if col_a not in df.columns or col_b not in df.columns or col_c not in df.columns:\n        return None, None\n\n    df_filtered = df[(df[col_b] > 50) & (df[col_c] == 900)]\n\n    if df_filtered.empty:\n        return None, None\n\n    X = df_filtered[[col_a]]\n    y = df_filtered[col_b]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    return predictions, model", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, col_a='A', col_b='B', col_c='C', seed=None):\n    if df.empty or not all(df.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        return None, None\n\n    if col_a not in df.columns or col_b not in df.columns or col_c not in df.columns:\n        return None, None\n\n    df_filtered = df[(df[col_b] > 50) & (df[col_c] == 900)]\n\n    if df_filtered.empty:\n        return None, None\n\n    X = df_filtered[[col_a]]\n    y = df_filtered[col_b]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    return predictions, model\n```\n"}
{"task_id": "BigCodeBench/886", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(data):\n    if not all(key in data for key in ['Name', 'Age', 'Score']):\n        raise ValueError(\"Data dictionary must contain 'Name', 'Age', and 'Score' keys.\")\n\n    df = pd.DataFrame(data)\n    df = df.sort_values(by=['Name', 'Age'])\n\n    avg_scores = df.groupby('Name')['Score'].mean()\n\n    age_counter = Counter(df['Age'])\n    common_age = age_counter.most_common(1)[0][0] if age_counter else None\n\n    return df, avg_scores, common_age\ndata = {\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John', 'John', 'Nick', 'Tom', 'John', 'Tom'],\n    'Age': [20, 21, 19, 20, 19, 19, 21, 20, 19, 20],\n    'Score': [85, 79, 92, 88, 90, 92, 81, 86, 90, 85]\n}", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(data):\n    if not all(key in data for key in ['Name', 'Age', 'Score']):\n        raise ValueError(\"Data dictionary must contain 'Name', 'Age', and 'Score' keys.\")\n\n    df = pd.DataFrame(data)\n    df = df.sort_values(by=['Name', 'Age'])\n\n    avg_scores = df.groupby('Name')['Score'].mean()\n\n    age_counter = Counter(df['Age'])\n    common_age = age_counter.most_common(1)[0][0] if age_counter else None\n\n    return df, avg_scores, common_age\n\n# Example usage:\ndata = {\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John', 'John', 'Nick', 'Tom', 'John', 'Tom'],\n    'Age': [20, 21, 19, 20, 19, 19, 21, 20, 19, 20],\n    'Score': [85, 79, 92, 88, 90, 92, 81, 86, 90, 85]\n}\ndf, avg_scores, common_age = task_func(data)"}
{"task_id": "BigCodeBench/887", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(T1, row_num=50, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Convert T1 to integers\n    T1 = [tuple(map(int, t)) for t in T1]\n    \n    # Calculate the maximum number of columns\n    max_cols = max(len(t) for t in T1)\n    \n    # Create a list of random numbers for each column\n    random_data = []\n    for i in range(max_cols):\n        col_data = [np.random.randint(0, 100) for _ in range(row_num)]\n        random_data.append(col_data)\n    \n    # Create a DataFrame from the random data\n    df = pd.DataFrame(random_data).T\n    df.columns = [f'Col_{i+1}' for i in range(max_cols)]\n    \n    return df\nT1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\ndef task_func(T1, row_num=50, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Convert T1 to integers\n    T1 = [tuple(map(int, t)) for t in T1]\n    \n    # Calculate the maximum number of columns\n    max_cols = max(len(t) for t in T1)\n    \n    # Create a list of random numbers for each column\n    random_data = []\n    for i in range(max_cols):\n        col_data = [np.random.randint(0, 100) for _ in range(row_num)]\n        random_data.append(col_data)\n    \n    # Create a DataFrame from the random data\n    df = pd.DataFrame(random_data).T\n    df.columns = [f'Col_{i+1}' for i in range(max_cols)]\n    \n    return df\n\n# Example usage:\nT1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\ndf = task_func(T1, row_num=5, seed=2022)"}
{"task_id": "BigCodeBench/888", "solution": "import pandas as pd\nimport os\ndef task_func(data_dir: str, csv_files: list) -> pd.DataFrame:\n    if not csv_files:\n        return pd.DataFrame()\n\n    dataframes = []\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        if os.path.exists(file_path):\n            df = pd.read_csv(file_path)\n            dataframes.append(df)\n        else:\n            print(f\"File {file} does not exist in directory {data_dir}\")\n\n    if dataframes:\n        merged_df = pd.concat(dataframes, ignore_index=True)\n        return merged_df\n    else:\n        return pd.DataFrame()", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(data_dir: str, csv_files: list) -> pd.DataFrame:\n    if not csv_files:\n        return pd.DataFrame()\n\n    dataframes = []\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        if os.path.exists(file_path):\n            df = pd.read_csv(file_path)\n            dataframes.append(df)\n        else:\n            print(f\"File {file} does not exist in directory {data_dir}\")\n\n    if dataframes:\n        merged_df = pd.concat(dataframes, ignore_index=True)\n        return merged_df\n    else:\n        return pd.DataFrame()\n\n# Example usage:\n# df = task_func('/path/to/data/directory', ['file1.csv', 'file2.csv', 'file3.csv'])\n# print(df.head())\n```\n"}
{"task_id": "BigCodeBench/889", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    file_path = os.path.join(data_dir, csv_file)\n    \n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {csv_file} does not exist in the directory {data_dir}.\")\n\n    df = pd.read_csv(file_path)\n\n    if 'Taste' in df.columns:\n        df['Taste'] = df['Taste'].fillna(df['Taste'].mean())\n\n    return df", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    file_path = os.path.join(data_dir, csv_file)\n    \n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {csv_file} does not exist in the directory {data_dir}.\")\n\n    df = pd.read_csv(file_path)\n\n    if 'Taste' in df.columns:\n        df['Taste'] = df['Taste'].fillna(df['Taste'].mean())\n\n    return df\n\n# Example usage:\n# df = task_func(\"/path/to/data/directory\", \"file.csv\")\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/890", "solution": "import os\nimport random\nimport pandas as pd\ndef task_func(data_dir,\n              csv_files=['file1.csv', 'file2.csv', 'file3.csv'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if not csv_files:\n        return '', pd.DataFrame()\n\n    selected_file = random.choice(csv_files)\n    file_path = os.path.join(data_dir, selected_file)\n\n    if not os.path.exists(file_path):\n        return '', pd.DataFrame()\n\n    df = pd.read_csv(file_path)\n    if df.empty:\n        return selected_file, pd.DataFrame()\n\n    selected_rows = random.sample(range(len(df)), 3)\n    selected_df = df.iloc[selected_rows]\n\n    return selected_file, selected_df", "raw_solution": "import os\nimport random\nimport pandas as pd\n\ndef task_func(data_dir,\n              csv_files=['file1.csv', 'file2.csv', 'file3.csv'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if not csv_files:\n        return '', pd.DataFrame()\n\n    selected_file = random.choice(csv_files)\n    file_path = os.path.join(data_dir, selected_file)\n\n    if not os.path.exists(file_path):\n        return '', pd.DataFrame()\n\n    df = pd.read_csv(file_path)\n    if df.empty:\n        return selected_file, pd.DataFrame()\n\n    selected_rows = random.sample(range(len(df)), 3)\n    selected_df = df.iloc[selected_rows]\n\n    return selected_file, selected_df\n\n# Example usage:\n# file_name, df = task_func('test_data')\n# print(file_name)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/891", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\ndef task_func(csv_file_path, attribute, test_size=0.2, random_state=42):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n    \n    # Check if the attribute exists in the dataframe\n    if attribute not in df.columns:\n        raise ValueError(f\"Attribute '{attribute}' does not exist in the dataset.\")\n    \n    # Split the data into features and target\n    X = df.drop(columns=[attribute])\n    y = df[attribute]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n    \n    return model, predictions", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(csv_file_path, attribute, test_size=0.2, random_state=42):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n    \n    # Check if the attribute exists in the dataframe\n    if attribute not in df.columns:\n        raise ValueError(f\"Attribute '{attribute}' does not exist in the dataset.\")\n    \n    # Split the data into features and target\n    X = df.drop(columns=[attribute])\n    y = df[attribute]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n    \n    return model, predictions\n```\n"}
{"task_id": "BigCodeBench/892", "solution": "import random\nfrom collections import Counter\ndef task_func(strings: list) -> dict:\n    \"\"\"\n    This function takes a list of strings and analyzes the occurrence of a specific pattern.\n    The pattern is randomly generated and consists of two parts: a prefix and a suffix.\n    The function counts the occurrences of this pattern in the provided list of strings.\n\n    Parameters:\n    - strings (list): A list of strings to be analyzed.\n\n    Returns:\n    - dict: A dictionary with the count of occurrences of the pattern.\n\n    Requirements:\n    - random\n    - collections\n\n    Example:\n    >>> task_func(['abcd}def}', 'pqrs}tuv}', 'wxyz}123}', '456}789}', '0ab}cde}'])\n    Counter({2: 10})\n    \"\"\"\n    # Generate a random pattern\n    prefix = random.choice(strings)\n    suffix = random.choice(strings)", "raw_solution": "import random\nfrom collections import Counter\n\ndef task_func(strings: list) -> dict:\n    \"\"\"\n    This function takes a list of strings and analyzes the occurrence of a specific pattern.\n    The pattern is randomly generated and consists of two parts: a prefix and a suffix.\n    The function counts the occurrences of this pattern in the provided list of strings.\n\n    Parameters:\n    - strings (list): A list of strings to be analyzed.\n\n    Returns:\n    - dict: A dictionary with the count of occurrences of the pattern.\n\n    Requirements:\n    - random\n    - collections\n\n    Example:\n    >>> task_func(['abcd}def}', 'pqrs}tuv}', 'wxyz}123}', '456}789}', '0ab}cde}'])\n    Counter({2: 10})\n    \"\"\"\n    # Generate a random pattern\n    prefix = random.choice(strings)\n    suffix = random.choice(strings)\n    pattern = f\"{prefix}}{suffix}\"\n\n    # Count occurrences of the pattern in the list of strings\n    pattern_count = Counter(s for s in strings if pattern in s)\n\n    return pattern_count\n```\n"}
{"task_id": "BigCodeBench/893", "solution": "import re\nfrom datetime import datetime, time\ndef task_func(logs: list):\n    error_times = []\n    total_seconds = 0\n    error_count = 0\n\n    for log in logs:\n        match = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) ERROR:', log)\n        if match:\n            error_count += 1\n            error_time = datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S').time()\n            error_times.append(error_time)\n            total_seconds += error_time.hour * 3600 + error_time.minute * 60 + error_time.second\n\n    average_time = time(hour=total_seconds // error_count // 3600, minute=(total_seconds // error_count // 60) % 60, second=total_seconds // error_count % 60)\n\n    return error_times, average_time\nlogs = [\n    '2021-06-15 09:45:00 ERROR: Failed to connect to database',\n    '2021-06-15 10:15:00 WARNING: Low disk space',\n    '2021-06-15 10:35:00 INFO: Backup completed successfully'\n]", "raw_solution": "import re\nfrom datetime import datetime, time\n\ndef task_func(logs: list):\n    error_times = []\n    total_seconds = 0\n    error_count = 0\n\n    for log in logs:\n        match = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) ERROR:', log)\n        if match:\n            error_count += 1\n            error_time = datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S').time()\n            error_times.append(error_time)\n            total_seconds += error_time.hour * 3600 + error_time.minute * 60 + error_time.second\n\n    average_time = time(hour=total_seconds // error_count // 3600, minute=(total_seconds // error_count // 60) % 60, second=total_seconds // error_count % 60)\n\n    return error_times, average_time\n\n# Test cases\nlogs = [\n    '2021-06-15 09:45:00 ERROR: Failed to connect to database',\n    '2021-06-15 10:15:00 WARNING: Low disk space',\n    '2021-06-15 10:35:00 INFO: Backup completed successfully'\n]\n"}
{"task_id": "BigCodeBench/894", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Generate random integers between 1 and 100\n    array = np.random.randint(1, 101, size=ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(array)\n    std = np.std(array)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=20, color='blue', alpha=0.7, edgecolor='black')\n    \n    # Add labels and title\n    ax.set_xlabel('Mean')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Random Integers Histogram')\n    \n    # Plot mean and standard deviation lines\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.4f}')\n    ax.axvline(mean + std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean + 1 Std: {mean + std:.4f}')\n    ax.axvline(mean - std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean - 1 Std: {mean - std:.4f}')\n    \n    # Add legend\n    ax.legend()\n    \n    return array, mean, std, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef task_func():\n    # Generate random integers between 1 and 100\n    array = np.random.randint(1, 101, size=ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(array)\n    std = np.std(array)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=20, color='blue', alpha=0.7, edgecolor='black')\n    \n    # Add labels and title\n    ax.set_xlabel('Mean')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Random Integers Histogram')\n    \n    # Plot mean and standard deviation lines\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.4f}')\n    ax.axvline(mean + std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean + 1 Std: {mean + std:.4f}')\n    ax.axvline(mean - std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean - 1 Std: {mean - std:.4f}')\n    \n    # Add legend\n    ax.legend()\n    \n    return array, mean, std, ax\n```\n"}
{"task_id": "BigCodeBench/895", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Generate a random array of integers\n    array = np.random.randint(1, 100, ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(array)\n    std = np.std(array)\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=30, color='blue', alpha=0.7)\n    \n    # Plot mean and standard deviation lines\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.2f}')\n    ax.axvline(mean + std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean + 1 std: {mean + std:.2f}')\n    ax.axvline(mean - std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean - 1 std: {mean - std:.2f}')\n    \n    # Set labels and title\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n    ax.set_title('Random values distribution')\n    ax.legend()\n    \n    return array, mean, std, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef task_func():\n    # Generate a random array of integers\n    array = np.random.randint(1, 100, ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(array)\n    std = np.std(array)\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=30, color='blue', alpha=0.7)\n    \n    # Plot mean and standard deviation lines\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.2f}')\n    ax.axvline(mean + std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean + 1 std: {mean + std:.2f}')\n    ax.axvline(mean - std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean - 1 std: {mean - std:.2f}')\n    \n    # Set labels and title\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n    ax.set_title('Random values distribution')\n    ax.legend()\n    \n    return array, mean, std, ax\n```\n"}
{"task_id": "BigCodeBench/896", "solution": "from collections import Counter\nimport random\nimport itertools\ndef task_func(length, count, seed=0):\n    letters = ['a', 'b', 'c', 'd', 'e']\n    random.seed(seed)\n    \n    if length < 0 or count < 0:\n        raise ValueError(\"Length and count must be non-negative integers.\")\n    \n    # Generate random strings\n    random_strings = [''.join(random.choices(letters, k=length)) for _ in range(count)]\n    \n    # Count frequency of each letter in all strings\n    all_letters = itertools.chain.from_iterable(random_strings)\n    letter_count = Counter(all_letters)\n    \n    return letter_count", "raw_solution": "from collections import Counter\nimport random\nimport itertools\n\ndef task_func(length, count, seed=0):\n    letters = ['a', 'b', 'c', 'd', 'e']\n    random.seed(seed)\n    \n    if length < 0 or count < 0:\n        raise ValueError(\"Length and count must be non-negative integers.\")\n    \n    # Generate random strings\n    random_strings = [''.join(random.choices(letters, k=length)) for _ in range(count)]\n    \n    # Count frequency of each letter in all strings\n    all_letters = itertools.chain.from_iterable(random_strings)\n    letter_count = Counter(all_letters)\n    \n    return letter_count\n\n# Example usage:\n# print(task_func(5, 2, seed=1))\n# print(task_func(0, 100, seed=2))\n```\n"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    outcomes = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequency = np.array([outcomes.count(i) for i in NUMBERS])\n\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequency, tick_label=NUMBERS)\n    ax.set_title(\"Dice Rolls Histogram\")\n    ax.set_xlabel(\"Dice Face\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequency, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    outcomes = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequency = np.array([outcomes.count(i) for i in NUMBERS])\n\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequency, tick_label=NUMBERS)\n    ax.set_title(\"Dice Rolls Histogram\")\n    ax.set_xlabel(\"Dice Face\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequency, ax\n```\n"}
{"task_id": "BigCodeBench/898", "solution": "from collections import Counter\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(count, seed=0):\n    random.seed(seed)\n    pairs = []\n\n    for _ in range(count):\n        first_letter = random.choice(LETTERS)\n        second_letter = random.choice(LETTERS)\n        pairs.append((first_letter, second_letter))\n\n    return Counter(pairs)", "raw_solution": "from collections import Counter\nimport random\n\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef task_func(count, seed=0):\n    random.seed(seed)\n    pairs = []\n\n    for _ in range(count):\n        first_letter = random.choice(LETTERS)\n        second_letter = random.choice(LETTERS)\n        pairs.append((first_letter, second_letter))\n\n    return Counter(pairs)\n\n# Example usage:\n# print(task_func(5, seed=42))\n```\n"}
{"task_id": "BigCodeBench/899", "solution": "import numpy as np\nimport random\ndef task_func(length=10000, seed=0):\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative integer.\")\n    \n    random.seed(seed)\n    walk = np.zeros(length, dtype=int)\n    \n    for i in range(1, length):\n        step = random.choice([-1, 1])\n        walk[i] = walk[i-1] + step\n    \n    return walk", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(length=10000, seed=0):\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative integer.\")\n    \n    random.seed(seed)\n    walk = np.zeros(length, dtype=int)\n    \n    for i in range(1, length):\n        step = random.choice([-1, 1])\n        walk[i] = walk[i-1] + step\n    \n    return walk\n\n# Example usage:\n# random.seed(0)     # For reproducibility in doctest\n# walk = task_func(5)\n# print(walk.tolist())\n```\n"}
{"task_id": "BigCodeBench/900", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(d):\n    if not isinstance(d, list) or not all(isinstance(i, dict) for i in d):\n        raise ValueError(\"Input must be a list of dictionaries.\")\n\n    df = pd.DataFrame(d)\n\n    result = {}\n    for col in df.columns:\n        if col not in ['x', 'y', 'z']:\n            continue\n        result[col] = {\n            'mean': np.mean(df[col]),\n            'sum': np.sum(df[col]),\n            'max': np.max(df[col]),\n            'min': np.min(df[col]),\n            'std': np.std(df[col])\n        }\n\n    return result", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(d):\n    if not isinstance(d, list) or not all(isinstance(i, dict) for i in d):\n        raise ValueError(\"Input must be a list of dictionaries.\")\n\n    df = pd.DataFrame(d)\n\n    result = {}\n    for col in df.columns:\n        if col not in ['x', 'y', 'z']:\n            continue\n        result[col] = {\n            'mean': np.mean(df[col]),\n            'sum': np.sum(df[col]),\n            'max': np.max(df[col]),\n            'min': np.min(df[col]),\n            'std': np.std(df[col])\n        }\n\n    return result\n\n# Test cases\ndata = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]"}
{"task_id": "BigCodeBench/901", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n    scaled_values = scaler.fit_transform(df[['x', 'y', 'z']])\n    df_scaled = pd.DataFrame(scaled_values, columns=['x', 'y', 'z'])\n\n    return df_scaled", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n    scaled_values = scaler.fit_transform(df[['x', 'y', 'z']])\n    df_scaled = pd.DataFrame(scaled_values, columns=['x', 'y', 'z'])\n\n    return df_scaled\n\n# Test cases\ndata = [{'x': -1, 'y': 0, 'z': 5}, {'x': 3, 'y': -15, 'z': 0}, {'x': 0, 'y': 1, 'z': -7}]"}
{"task_id": "BigCodeBench/902", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(d):\n    \"\"\"\n    list 'd' mein se \"\"x,\",\" \"y\" and \"z\" keys ke saath values ki occurrence ginein.\n\n    Parameters:\n    - d (list): dictionaries ki ek list.\n\n    Returns:\n        dict: ek dictionary jiski kunjiyaan '\"x,\"', 'y', and 'z' hon and maan counter object hon.\n\n    Requirements:\n    - pandas\n    - collections.counter\n\n    \"\"\"\n    # Initialize a Counter object to count occurrences of 'x,', 'y', and 'z'\n    counter = Counter()\n\n    # Iterate over each dictionary in the list\n    for dictionary in d:\n        # Check if 'x,', 'y', and 'z' keys exist in the dictionary\n        if 'x,' in dictionary and 'y' in dictionary and 'z' in dictionary:\n            # Increment the counter for each occurrence of 'x,', 'y', and 'z'\n            counter['x,'] += 1\n            counter['y'] += 1\n            counter['z'] += 1\n\n    # Convert the Counter object to a dictionary and return it\n    return dict(counter)", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(d):\n    \"\"\"\n    list 'd' mein se \"\"x,\",\" \"y\" and \"z\" keys ke saath values ki occurrence ginein.\n\n    Parameters:\n    - d (list): dictionaries ki ek list.\n\n    Returns:\n        dict: ek dictionary jiski kunjiyaan '\"x,\"', 'y', and 'z' hon and maan counter object hon.\n\n    Requirements:\n    - pandas\n    - collections.counter\n\n    \"\"\"\n    # Initialize a Counter object to count occurrences of 'x,', 'y', and 'z'\n    counter = Counter()\n\n    # Iterate over each dictionary in the list\n    for dictionary in d:\n        # Check if 'x,', 'y', and 'z' keys exist in the dictionary\n        if 'x,' in dictionary and 'y' in dictionary and 'z' in dictionary:\n            # Increment the counter for each occurrence of 'x,', 'y', and 'z'\n            counter['x,'] += 1\n            counter['y'] += 1\n            counter['z'] += 1\n\n    # Convert the Counter object to a dictionary and return it\n    return dict(counter)\n```\n"}
{"task_id": "BigCodeBench/903", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(d, target='z'):\n    df = pd.DataFrame(d)\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\ndata = [{'x': 4, 'y': 20, 'z': 10}, {'x': 5, 'y': 25, 'z': 15}, {'x': 6, 'y': 5, 'z': 20}]", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(d, target='z'):\n    df = pd.DataFrame(d)\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\n\n# Test cases\ndata = [{'x': 4, 'y': 20, 'z': 10}, {'x': 5, 'y': 25, 'z': 15}, {'x': 6, 'y': 5, 'z': 20}]\nmodel = task_func(data, target='y')"}
{"task_id": "BigCodeBench/904", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(d, keys=['x', 'y', 'z']):\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(d)\n\n    # Check if all specified keys are in the DataFrame\n    for key in keys:\n        if key not in df.columns:\n            raise ValueError(f\"Key '{key}' not found in the data.\")\n\n    # Plot the specified keys\n    ax = df.plot(x=keys[0], y=keys[1:], kind='line')\n\n    # Return the Axes object\n    return ax\ndata = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(d, keys=['x', 'y', 'z']):\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(d)\n\n    # Check if all specified keys are in the DataFrame\n    for key in keys:\n        if key not in df.columns:\n            raise ValueError(f\"Key '{key}' not found in the data.\")\n\n    # Plot the specified keys\n    ax = df.plot(x=keys[0], y=keys[1:], kind='line')\n\n    # Return the Axes object\n    return ax\n\n# Example usage:\ndata = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]\nax = task_func(data)\ntype(ax)  # Should return <class 'matplotlib.axes._axes.Axes'>\n```\n"}
{"task_id": "BigCodeBench/905", "solution": "import os\nimport glob\nimport csv\ndef task_func(directory_path, file_extension='.csv'):\n    data = {}\n\n    # Find all files with the given extension in the directory\n    file_paths = glob.glob(os.path.join(directory_path, f'*{file_extension}'))\n\n    for file_path in file_paths:\n        # Extract the file name without extension\n        file_name = os.path.splitext(os.path.basename(file_path))[0]\n        \n        # Read the CSV file\n        with open(file_path, newline='', encoding='utf-8') as csvfile:\n            reader = csv.reader(csvfile)\n            data[file_name] = list(reader)\n\n    return data", "raw_solution": "import os\nimport glob\nimport csv\n\ndef task_func(directory_path, file_extension='.csv'):\n    data = {}\n\n    # Find all files with the given extension in the directory\n    file_paths = glob.glob(os.path.join(directory_path, f'*{file_extension}'))\n\n    for file_path in file_paths:\n        # Extract the file name without extension\n        file_name = os.path.splitext(os.path.basename(file_path))[0]\n        \n        # Read the CSV file\n        with open(file_path, newline='', encoding='utf-8') as csvfile:\n            reader = csv.reader(csvfile)\n            data[file_name] = list(reader)\n\n    return data\n\n# Example usage:\n# data = task_func('/home/user/data')\n# print(data)\n```\n"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n\n    # List all files in the source directory\n    files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n\n    # Create a zip file in the target directory\n    archive_path = os.path.join(target_dir, archive_name)", "raw_solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n\n    # List all files in the source directory\n    files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n\n    # Create a zip file in the target directory\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        for file in files:\n            # Add files to the archive and rename them with '_processed' suffix\n            archive.write(os.path.join(source_dir, file), arcname=f\"{re.sub(r'(?<!_processed)\\.txt$', '_processed.txt', file)}\")\n\n    return archive_path\n\n# Example usage:\n# print(task_func('./data/', './data_processed/'))\n# print(task_func('./data/', './data_processed/', 'my_archive.zip'))\n```\n"}
{"task_id": "BigCodeBench/907", "solution": "import os\nimport re\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    try:\n        # Check if the directory exists\n        if not os.path.exists(directory):\n            return False\n\n        # Iterate over all files in the directory\n        for filename in os.listdir(directory):\n            # Check if the filename matches the pattern\n            if re.search(pattern, filename):\n                # Replace the pattern with the replacement\n                new_filename = re.sub(pattern, replacement, filename)\n                # Rename the file\n                os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False", "raw_solution": "import os\nimport re\n\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    try:\n        # Check if the directory exists\n        if not os.path.exists(directory):\n            return False\n\n        # Iterate over all files in the directory\n        for filename in os.listdir(directory):\n            # Check if the filename matches the pattern\n            if re.search(pattern, filename):\n                # Replace the pattern with the replacement\n                new_filename = re.sub(pattern, replacement, filename)\n                # Rename the file\n                os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```\n"}
{"task_id": "BigCodeBench/908", "solution": "import os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\ndef task_func(directory: str, pattern: str) -> list:\n    # List to store Axes objects\n    axes_list = []\n\n    # Compile the regular expression pattern\n    regex = re.compile(pattern)\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the filename matches the pattern\n        if regex.match(filename):\n            # Read the CSV file\n            df = pd.read_csv(os.path.join(directory, filename))\n\n            # Check if the required columns exist\n            if 'mahina' in df.columns and 'sales' in df.columns:\n                # Plot the sales data\n                ax = df.plot(x='mahina', y='sales', title=filename)\n                axes_list.append(ax)\n\n    return axes_list", "raw_solution": "import os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\ndef task_func(directory: str, pattern: str) -> list:\n    # List to store Axes objects\n    axes_list = []\n\n    # Compile the regular expression pattern\n    regex = re.compile(pattern)\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the filename matches the pattern\n        if regex.match(filename):\n            # Read the CSV file\n            df = pd.read_csv(os.path.join(directory, filename))\n\n            # Check if the required columns exist\n            if 'mahina' in df.columns and 'sales' in df.columns:\n                # Plot the sales data\n                ax = df.plot(x='mahina', y='sales', title=filename)\n                axes_list.append(ax)\n\n    return axes_list\n```\n"}
{"task_id": "BigCodeBench/909", "solution": "import pandas as pd\nimport itertools\nfrom random import shuffle\ndef task_func(letters=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], categories=['Category 1', 'Category 2', 'Category 3']):\n    # Shuffle the categories list\n    shuffle(categories)\n\n    # Create all possible combinations of letters and categories\n    combinations = list(itertools.product(letters, categories))\n\n    # Convert the combinations to a DataFrame\n    df = pd.DataFrame(combinations, columns=['Letter', 'Category'])\n\n    return df", "raw_solution": "import pandas as pd\nimport itertools\nfrom random import shuffle\n\ndef task_func(letters=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], categories=['Category 1', 'Category 2', 'Category 3']):\n    # Shuffle the categories list\n    shuffle(categories)\n\n    # Create all possible combinations of letters and categories\n    combinations = list(itertools.product(letters, categories))\n\n    # Convert the combinations to a DataFrame\n    df = pd.DataFrame(combinations, columns=['Letter', 'Category'])\n\n    return df\n\n# Example usage:\n# df = task_func(['A', 'B'], ['Cat 1', 'Cat 2'])\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/910", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(letters, repetitions, colors):\n    if len(letters) != len(repetitions) or len(letters) != len(colors):\n        raise ValueError(\"All input lists must have the same length.\")\n    if not letters or not repetitions or not colors:\n        raise ValueError(\"Input lists must not be empty.\")\n\n    # Calculate the frequency of each letter\n    frequency = np.array(repetitions)\n    letters_array = np.array(letters)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    bars = ax.bar(letters_array, frequency, color=colors)\n\n    # Add labels and title\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letters Frequency')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(letters, repetitions, colors):\n    if len(letters) != len(repetitions) or len(letters) != len(colors):\n        raise ValueError(\"All input lists must have the same length.\")\n    if not letters or not repetitions or not colors:\n        raise ValueError(\"Input lists must not be empty.\")\n\n    # Calculate the frequency of each letter\n    frequency = np.array(repetitions)\n    letters_array = np.array(letters)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    bars = ax.bar(letters_array, frequency, color=colors)\n\n    # Add labels and title\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letters Frequency')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/911", "solution": "from functools import reduce\nimport operator\nimport string\ndef task_func(letters):\n    \"\"\"\n    letter list ke liye sangat numbers ke product ki gnnnaa karein,\n    jahan \\\"A\\\" 1 ke sangat hai, \\\"B\\\" 2 ke sangat hai, ityadi.\n    >>> task_func([\\\"A\\\", \\\"E\\\", \\\"I\\\"])\n    45\n\n    Notes:\n        function pratyek bade letter ko uski sangat number se map karne ke liye ek purvanirdharit dictionary ka upayog karta hai.\n\n    Parameters:\n    - letters (list of str): A list of uppercase letters.\n\n    Returns:\n        int: int letters ke sangat numbers ka product.\n\n    Requirements:\n    - functools.reduce\n    - operator\n    - string\n\n    Example:\n    Examples:\n    >>> task_func([\\\\\"A\\\\\", \\\\\"B\\\\\", \\\\\"C\\\\\"])\n    6\n\n    \"\"\"\n    # Create a dictionary mapping each letter to its corresponding value\n    letter_values = {letter: index + 1 for index, letter in enumerate(string.ascii_uppercase)}\n    \n    # Map each letter in the input list to its corresponding value\n    letter_values_list = [letter_values[letter] for letter in letters]\n    \n    # Calculate the product of all values using reduce and operator.mul\n    product = reduce(operator.mul, letter_values_list, 1)\n    \n    return product", "raw_solution": "from functools import reduce\nimport operator\nimport string\n\ndef task_func(letters):\n    \"\"\"\n    letter list ke liye sangat numbers ke product ki gnnnaa karein,\n    jahan \\\"A\\\" 1 ke sangat hai, \\\"B\\\" 2 ke sangat hai, ityadi.\n    >>> task_func([\\\"A\\\", \\\"E\\\", \\\"I\\\"])\n    45\n\n    Notes:\n        function pratyek bade letter ko uski sangat number se map karne ke liye ek purvanirdharit dictionary ka upayog karta hai.\n\n    Parameters:\n    - letters (list of str): A list of uppercase letters.\n\n    Returns:\n        int: int letters ke sangat numbers ka product.\n\n    Requirements:\n    - functools.reduce\n    - operator\n    - string\n\n    Example:\n    Examples:\n    >>> task_func([\\\\\"A\\\\\", \\\\\"B\\\\\", \\\\\"C\\\\\"])\n    6\n\n    \"\"\"\n    # Create a dictionary mapping each letter to its corresponding value\n    letter_values = {letter: index + 1 for index, letter in enumerate(string.ascii_uppercase)}\n    \n    # Map each letter in the input list to its corresponding value\n    letter_values_list = [letter_values[letter] for letter in letters]\n    \n    # Calculate the product of all values using reduce and operator.mul\n    product = reduce(operator.mul, letter_values_list, 1)\n    \n    return product\n\n# Test cases"}
{"task_id": "BigCodeBench/912", "solution": "from collections import Counter\nimport itertools\ndef task_func(letters: list, repetitions: int) -> dict:\n    \"\"\"\n    Dee gaee number mein repetitions ke baad aak list mein pratyek letter kee frequency ginen.\n\n    Parameters:\n    - letters (list): letter darsane vaalee single-character strings kee aak list.\n    - repetitions (int): list ko repetitions kee number.\n\n    Returns:\n        Aak dictionary lautaata hai jahaan keys letter hain and maan unakee frequencyyaan hain.\n\n    Requirements:\n    - collections.Counter\n    - itertools\n\n    Example:\n    >>> task_func(['A', 'B', 'C'], 2)\n    {'A': 2, 'B': 2, 'C': 2}\n    >>> task_func(['A', 'B'], 3)\n    {'A': 3, 'B': 3}\n\n    \"\"\"\n    # Repeat the list for the given number of repetitions\n    repeated_letters = list(itertools.chain.from_iterable(letters for _ in range(repetitions)))\n    \n    # Count the frequency of each letter\n    letter_counts = Counter(repeated_letters)\n    \n    return dict(letter_counts)", "raw_solution": "from collections import Counter\nimport itertools\n\ndef task_func(letters: list, repetitions: int) -> dict:\n    \"\"\"\n    Dee gaee number mein repetitions ke baad aak list mein pratyek letter kee frequency ginen.\n\n    Parameters:\n    - letters (list): letter darsane vaalee single-character strings kee aak list.\n    - repetitions (int): list ko repetitions kee number.\n\n    Returns:\n        Aak dictionary lautaata hai jahaan keys letter hain and maan unakee frequencyyaan hain.\n\n    Requirements:\n    - collections.Counter\n    - itertools\n\n    Example:\n    >>> task_func(['A', 'B', 'C'], 2)\n    {'A': 2, 'B': 2, 'C': 2}\n    >>> task_func(['A', 'B'], 3)\n    {'A': 3, 'B': 3}\n\n    \"\"\"\n    # Repeat the list for the given number of repetitions\n    repeated_letters = list(itertools.chain.from_iterable(letters for _ in range(repetitions)))\n    \n    # Count the frequency of each letter\n    letter_counts = Counter(repeated_letters)\n    \n    return dict(letter_counts)\n\n# Test cases"}
{"task_id": "BigCodeBench/913", "solution": "from typing import List, Union\nimport numpy as np\nimport scipy.fft\ndef task_func(data: List[Union[int, str]], repetitions: int = 1):\n    # Convert data to numpy array\n    data_array = np.array(data)\n    \n    # Calculate mode and count\n    unique, counts = np.unique(data_array, return_counts=True)\n    mode = unique[np.argmax(counts)]\n    count = counts[np.argmax(counts)]\n    \n    # Repeat data based on repetitions\n    repeated_data = np.repeat(data_array, repetitions)\n    \n    # Calculate FFT\n    fft_result = scipy.fft.fft(repeated_data)\n    \n    # Prepare the result dictionary\n    result = {\n        'mode': mode,\n        'count': count,\n        'fft': fft_result\n    }\n    \n    return result", "raw_solution": "from typing import List, Union\nimport numpy as np\nimport scipy.fft\n\ndef task_func(data: List[Union[int, str]], repetitions: int = 1):\n    # Convert data to numpy array\n    data_array = np.array(data)\n    \n    # Calculate mode and count\n    unique, counts = np.unique(data_array, return_counts=True)\n    mode = unique[np.argmax(counts)]\n    count = counts[np.argmax(counts)]\n    \n    # Repeat data based on repetitions\n    repeated_data = np.repeat(data_array, repetitions)\n    \n    # Calculate FFT\n    fft_result = scipy.fft.fft(repeated_data)\n    \n    # Prepare the result dictionary\n    result = {\n        'mode': mode,\n        'count': count,\n        'fft': fft_result\n    }\n    \n    return result\n\n# Example usage:\n# result = task_func([1, '2', '2'], repetitions=1)\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Convert date to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Sort the dataframe by date\n    df = df.sort_values('date')\n    \n    # Extract date and closing price\n    dates = df['date'].values.reshape(-1, 1)\n    closing_prices = df['closing_price'].values\n    \n    # Create a linear regression model\n    model = LinearRegression()\n    model.fit(dates, closing_prices)\n    \n    # Predict next 7 days\n    future_dates = np.array([df['date'].max() + pd.Timedelta(days=i) for i in range(1, 8)]).reshape(-1, 1)\n    future_prices = model.predict(future_dates)\n    \n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.scatter(dates, closing_prices, color='blue', label='Historical Data')\n    plt.plot(future_dates, future_prices, color='red', label='Predicted Data')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n    return future_prices.tolist(), plt", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Convert date to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Sort the dataframe by date\n    df = df.sort_values('date')\n    \n    # Extract date and closing price\n    dates = df['date'].values.reshape(-1, 1)\n    closing_prices = df['closing_price'].values\n    \n    # Create a linear regression model\n    model = LinearRegression()\n    model.fit(dates, closing_prices)\n    \n    # Predict next 7 days\n    future_dates = np.array([df['date'].max() + pd.Timedelta(days=i) for i in range(1, 8)]).reshape(-1, 1)\n    future_prices = model.predict(future_dates)\n    \n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.scatter(dates, closing_prices, color='blue', label='Historical Data')\n    plt.plot(future_dates, future_prices, color='red', label='Predicted Data')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n    return future_prices.tolist(), plt\n```\n"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    # Check if 'closing_price' column exists in the dataframe\n    if 'closing_price' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'closing_price' column.\")\n    \n    # Calculate Z-Scores\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    # Plot outliers\n    fig, ax = plt.subplots()\n    ax.scatter(outliers.index, outliers['closing_price'], color='red')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    return outliers, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    # Check if 'closing_price' column exists in the dataframe\n    if 'closing_price' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'closing_price' column.\")\n    \n    # Calculate Z-Scores\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    # Plot outliers\n    fig, ax = plt.subplots()\n    ax.scatter(outliers.index, outliers['closing_price'], color='red')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    return outliers, ax\n```\n"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Box plot\n    sns.boxplot(x=df['closing_price'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n    axes[0].set_xlabel('Closing Price')\n\n    # Histogram\n    sns.histplot(df['closing_price'], bins=10, ax=axes[1], kde=True)\n    axes[1].set_title('Histogram of Closing Prices')\n    axes[1].set_xlabel('Closing Price')\n\n    plt.tight_layout()\n    return axes[0], axes[1]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Box plot\n    sns.boxplot(x=df['closing_price'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n    axes[0].set_xlabel('Closing Price')\n\n    # Histogram\n    sns.histplot(df['closing_price'], bins=10, ax=axes[1], kde=True)\n    axes[1].set_title('Histogram of Closing Prices')\n    axes[1].set_xlabel('Closing Price')\n\n    plt.tight_layout()\n    return axes[0], axes[1]\n\n# Example usage:\n# df = pd.DataFrame({\n#     'closing_price': [100, 101, 102, 103, 104, 150]\n# })\n# boxplot_ax, histplot_ax = task_func(df)\n# print(boxplot_ax.get_title())\n# print(histplot_ax.get_title())\n```\n"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the dataframe has the correct columns and types\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df['closing_price'] = df['closing_price'].astype(float)\n\n    # Sort the dataframe by date\n    df = df.sort_values('date')\n\n    # Fit the ARIMA model\n    model = ARIMA(df['closing_price'], order=(1, 1, 1))\n    model_fit = model.fit()\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot the forecast\n    fig, ax = plt.subplots()\n    ax.plot(df['date'], df['closing_price'], label='Historical Data')\n    ax.plot(pd.date_range(start=df['date'].iloc[-1], periods=8, freq='D')[1:], forecast, label='Forecast', marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('ARIMA Forecast for Next 7 Days')\n    ax.legend()\n\n    return forecast.tolist(), ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the dataframe has the correct columns and types\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df['closing_price'] = df['closing_price'].astype(float)\n\n    # Sort the dataframe by date\n    df = df.sort_values('date')\n\n    # Fit the ARIMA model\n    model = ARIMA(df['closing_price'], order=(1, 1, 1))\n    model_fit = model.fit()\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot the forecast\n    fig, ax = plt.subplots()\n    ax.plot(df['date'], df['closing_price'], label='Historical Data')\n    ax.plot(pd.date_range(start=df['date'].iloc[-1], periods=8, freq='D')[1:], forecast, label='Forecast', marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('ARIMA Forecast for Next 7 Days')\n    ax.legend()\n\n    return forecast.tolist(), ax\n```\n"}
{"task_id": "BigCodeBench/918", "solution": "import pandas as pd\nimport re\ndef task_func(data, mapping):\n    df = pd.DataFrame(data)\n    for column in df.columns:\n        df[column] = df[column].apply(lambda x: ' '.join([mapping.get(word, word) for word in re.findall(r'\\b\\w+\\b', x)]))\n    return df\ndata = {'text': ['NASA is great', 'I live in the USA']}\nmapping = {'NASA': 'National Aeronautics and Space Administration', 'USA': 'United States of America'}", "raw_solution": "import pandas as pd\nimport re\n\n# Function to replace acronyms in DataFrame\ndef task_func(data, mapping):\n    df = pd.DataFrame(data)\n    for column in df.columns:\n        df[column] = df[column].apply(lambda x: ' '.join([mapping.get(word, word) for word in re.findall(r'\\b\\w+\\b', x)]))\n    return df\n\n# Example usage:\ndata = {'text': ['NASA is great', 'I live in the USA']}\nmapping = {'NASA': 'National Aeronautics and Space Administration', 'USA': 'United States of America'}"}
{"task_id": "BigCodeBench/919", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, column):\n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n    \n    category_counts = df[column].value_counts()\n    ax = category_counts.plot(kind='bar', color='skyblue')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of {column}')\n    \n    return ax\ndata = {'Category': ['A', 'B', 'B', 'C', 'A', 'D', 'E', 'E', 'D']}", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column):\n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n    \n    category_counts = df[column].value_counts()\n    ax = category_counts.plot(kind='bar', color='skyblue')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of {column}')\n    \n    return ax\n\n# Example usage:\ndata = {'Category': ['A', 'B', 'B', 'C', 'A', 'D', 'E', 'E', 'D']}\nax = task_func(data, 'Category')\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/920", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    df = pd.DataFrame(data)\n    corr = df.corr()\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n    ax.set_title('Correlation Matrix')\n\n    return ax\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    df = pd.DataFrame(data)\n    corr = df.corr()\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n    ax.set_title('Correlation Matrix')\n\n    return ax\n\n# Example usage:\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\nax = task_func(data)\nplt.show()\n```\nThis script imports the necessary libraries, defines a function that takes a dictionary of data, converts it into a pandas DataFrame, calculates the correlation matrix, and then plots a heatmap of the correlation matrix using seaborn. The heatmap is annotated with the correlation values and has a title 'Correlation Matrix'. The function returns the axes object of the heatmap for further manipulation if needed.\n"}
{"task_id": "BigCodeBench/921", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data, columns):\n    df = pd.DataFrame(data)\n    scaler = MinMaxScaler()\n    df[columns] = scaler.fit_transform(df[columns])\n    return df\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data, columns):\n    df = pd.DataFrame(data)\n    scaler = MinMaxScaler()\n    df[columns] = scaler.fit_transform(df[columns])\n    return df\n\n# Example usage:\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}\nnormalized_df = task_func(data, ['a', 'b'])"}
{"task_id": "BigCodeBench/922", "solution": "import pandas as pd\nimport re\nSTOPWORDS = set([\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n    \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n    \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n    \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n    \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\",\n    \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\",\n    \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n    \"don\", \"should\", \"now\"\n])\ndef task_func(data, column):\n    df = pd.DataFrame(data)\n    df[column] = df[column].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in STOPWORDS]))\n    return df", "raw_solution": "import pandas as pd\nimport re\n\n# Constants\nSTOPWORDS = set([\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n    \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n    \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n    \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n    \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\",\n    \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\",\n    \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n    \"don\", \"should\", \"now\"\n])\n\ndef task_func(data, column):\n    df = pd.DataFrame(data)\n    df[column] = df[column].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in STOPWORDS]))\n    return df\n```\n"}
{"task_id": "BigCodeBench/923", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(person_names, email_domains, num_records=5):\n    if len(person_names) < num_records:\n        raise ValueError(\"Number of person names must be greater than or equal to num_records.\")\n    if not email_domains:\n        raise ValueError(\"At least one email domain must be provided.\")\n\n    data = []\n    for _ in range(num_records):\n        name = random.choice(person_names)\n        domain = random.choice(email_domains)\n        email = f\"{re.sub(r'@', '[at]', name.lower())}@{domain}\"\n        data.append({'Name': name, 'Email': email})\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(person_names, email_domains, num_records=5):\n    if len(person_names) < num_records:\n        raise ValueError(\"Number of person names must be greater than or equal to num_records.\")\n    if not email_domains:\n        raise ValueError(\"At least one email domain must be provided.\")\n\n    data = []\n    for _ in range(num_records):\n        name = random.choice(person_names)\n        domain = random.choice(email_domains)\n        email = f\"{re.sub(r'@', '[at]', name.lower())}@{domain}\"\n        data.append({'Name': name, 'Email': email})\n\n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\nrandom.seed(0)  # Initialize random seed"}
{"task_id": "BigCodeBench/924", "solution": "import pandas as pd\nimport os\nimport sys\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    df = pd.read_csv(file_path)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n    \n    df[column_name] = df[column_name].apply(lambda x: str(x).replace('\\n', '<br>'))\n    \n    return df", "raw_solution": "import pandas as pd\nimport os\nimport sys\n\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    df = pd.read_csv(file_path)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n    \n    df[column_name] = df[column_name].apply(lambda x: str(x).replace('\\n', '<br>'))\n    \n    return df\n\n# Example usage:\n# df = task_func('data.csv', 'Value')\n# print(df['Value'].iloc[0])\n# df = task_func('another_data.csv', 'Comments')\n# print(df['Comments'].iloc[1])\n```\n"}
{"task_id": "BigCodeBench/925", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_size=1000, column_names=['A', 'B', 'C', 'D', 'E'], seed=0):\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(data_size, len(column_names)))\n    df = pd.DataFrame(data, columns=column_names)\n    df[df < 10] = -1\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data_size=1000, column_names=['A', 'B', 'C', 'D', 'E'], seed=0):\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(data_size, len(column_names)))\n    df = pd.DataFrame(data, columns=column_names)\n    df[df < 10] = -1\n    return df\n\n# Example usage:\n# df = task_func(data_size=100, column_names=['X', 'Y', 'Z'], seed=42)\n# print(df.shape)  # Output: (100, 3)\n```\n"}
{"task_id": "BigCodeBench/926", "solution": "import pandas as pd\nimport sqlite3\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_path)\n    \n    # Load data from the specified table into a Pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Replace newline characters with HTML line breaks\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    \n    # Close the database connection\n    conn.close()\n    \n    return df", "raw_solution": "import pandas as pd\nimport sqlite3\n\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_path)\n    \n    # Load data from the specified table into a Pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Replace newline characters with HTML line breaks\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    \n    # Close the database connection\n    conn.close()\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/927", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Initialize the LabelEncoder\n    le = LabelEncoder()\n\n    # Encode the specified column\n    df[column_name] = le.fit_transform(df[column_name])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Initialize the LabelEncoder\n    le = LabelEncoder()\n\n    # Encode the specified column\n    df[column_name] = le.fit_transform(df[column_name])\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Ensure the word is in lowercase\n    word = word.lower()\n    \n    # Generate all possible two-letter combinations\n    two_letter_combinations = [''.join(p) for p in itertools.permutations(string.ascii_lowercase, 2)]\n    \n    # Count occurrences of each two-letter combination in the word\n    combination_counter = Counter()\n    for combination in two_letter_combinations:\n        combination_counter[combination] = word.count(combination)\n    \n    return combination_counter", "raw_solution": "from collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Ensure the word is in lowercase\n    word = word.lower()\n    \n    # Generate all possible two-letter combinations\n    two_letter_combinations = [''.join(p) for p in itertools.permutations(string.ascii_lowercase, 2)]\n    \n    # Count occurrences of each two-letter combination in the word\n    combination_counter = Counter()\n    for combination in two_letter_combinations:\n        combination_counter[combination] = word.count(combination)\n    \n    return combination_counter\n\n# Example usage:\n# print(task_func('abcdef'))\n```\n"}
{"task_id": "BigCodeBench/929", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(word: str) -> (np.ndarray, float):\n    ascii_diffs = np.diff(np.array([ord(c) for c in word]))\n    entropy = stats.entropy(ascii_diffs, base=2)\n    return ascii_diffs, entropy", "raw_solution": "import numpy as np\nfrom scipy import stats\n\ndef task_func(word: str) -> (np.ndarray, float):\n    ascii_diffs = np.diff(np.array([ord(c) for c in word]))\n    entropy = stats.entropy(ascii_diffs, base=2)\n    return ascii_diffs, entropy\n\n# Test cases"}
{"task_id": "BigCodeBench/930", "solution": "import random\nimport string\nPOSSIBLE_LETTERS = ['a', 'b', 'c']\ndef task_func(word):\n    if not word.isalpha():\n        raise ValueError(\"Input should only contain alphabetic characters.\")\n    \n    pairs = []\n    for i in range(0, len(word), 2):\n        if i + 1 < len(word):\n            pairs.append(word[i:i+2])\n        else:\n            pairs.append(word[i] + random.choice(POSSIBLE_LETTERS))\n    \n    while len(pairs) < len(POSSIBLE_LETTERS):\n        pairs.append('')\n\n    return pairs", "raw_solution": "import random\nimport string\n\nPOSSIBLE_LETTERS = ['a', 'b', 'c']\ndef task_func(word):\n    if not word.isalpha():\n        raise ValueError(\"Input should only contain alphabetic characters.\")\n    \n    pairs = []\n    for i in range(0, len(word), 2):\n        if i + 1 < len(word):\n            pairs.append(word[i:i+2])\n        else:\n            pairs.append(word[i] + random.choice(POSSIBLE_LETTERS))\n    \n    while len(pairs) < len(POSSIBLE_LETTERS):\n        pairs.append('')\n\n    return pairs\n\n# Test cases"}
{"task_id": "BigCodeBench/931", "solution": "from collections import defaultdict\nimport re\ndef task_func(word: str) -> dict:\n    \"\"\"\n    saaf kie gae shabd mein, har do-akshar sanyojan ki occurrences gyaat karen,\n    keval varnamaala ke aksharon ko dhyaan mein rakhate hue.\n\n    Parameters:\n    - word (str): input **string**.\n\n    Returns:\n        collections.defaultdict: ek **dictionary** jismein **keys** do-akshar **combinations** hain aur **values** saaf kie gae **word** mein unki **counts** hain.\n\n    Requirements:\n    - collections.defaultdict\n    - re\n\n    Example:\n    >>> task_func('abcdef')\n    defaultdict(<class 'int'>, {'ab': 1, 'bc': 1, 'cd': 1, 'de': 1, 'ef': 1})\n    >>> task_func('aabbcc')\n    defaultdict(<class 'int'>, {'aa': 1, 'ab': 1, 'bb': 1, 'bc': 1, 'cc': 1})\n    >>> task_func('a1!b@c#d$')\n    defaultdict(<class 'int'>, {'ab': 1, 'bc': 1, 'cd': 1})\n\n    \"\"\"\n    # Remove non-alphabetic characters\n    cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n    \n    # Initialize a defaultdict to count occurrences of each two-letter combination\n    combinations_count = defaultdict(int)\n    \n    # Iterate over the cleaned_word to count combinations\n    for i in range(len(cleaned_word) - 1):\n        combination = cleaned_word[i:i+2]\n        combinations_count[combination] += 1\n    \n    return combinations_count", "raw_solution": "from collections import defaultdict\nimport re\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    saaf kie gae shabd mein, har do-akshar sanyojan ki occurrences gyaat karen,\n    keval varnamaala ke aksharon ko dhyaan mein rakhate hue.\n\n    Parameters:\n    - word (str): input **string**.\n\n    Returns:\n        collections.defaultdict: ek **dictionary** jismein **keys** do-akshar **combinations** hain aur **values** saaf kie gae **word** mein unki **counts** hain.\n\n    Requirements:\n    - collections.defaultdict\n    - re\n\n    Example:\n    >>> task_func('abcdef')\n    defaultdict(<class 'int'>, {'ab': 1, 'bc': 1, 'cd': 1, 'de': 1, 'ef': 1})\n    >>> task_func('aabbcc')\n    defaultdict(<class 'int'>, {'aa': 1, 'ab': 1, 'bb': 1, 'bc': 1, 'cc': 1})\n    >>> task_func('a1!b@c#d$')\n    defaultdict(<class 'int'>, {'ab': 1, 'bc': 1, 'cd': 1})\n\n    \"\"\"\n    # Remove non-alphabetic characters\n    cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n    \n    # Initialize a defaultdict to count occurrences of each two-letter combination\n    combinations_count = defaultdict(int)\n    \n    # Iterate over the cleaned_word to count combinations\n    for i in range(len(cleaned_word) - 1):\n        combination = cleaned_word[i:i+2]\n        combinations_count[combination] += 1\n    \n    return combinations_count\n```\n"}
{"task_id": "BigCodeBench/932", "solution": "from collections import Counter\nimport re\ndef task_func(word: str) -> list:\n    # Remove non-alphabetic characters\n    cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n    \n    # Check if the cleaned word has less than 2 characters\n    if len(cleaned_word) < 2:\n        return []\n\n    # Generate all 2-letter combinations\n    combinations = [cleaned_word[i:i+2].lower() for i in range(len(cleaned_word) - 1)]\n\n    # Count the frequency of each combination\n    combination_counter = Counter(combinations)\n\n    # Find the most common combination\n    most_common_combination = combination_counter.most_common(1)\n\n    # Return the most common combination and its frequency\n    return [(most_common_combination[0][0], most_common_combination[0][1])] if most_common_combination else []", "raw_solution": "from collections import Counter\nimport re\n\ndef task_func(word: str) -> list:\n    # Remove non-alphabetic characters\n    cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n    \n    # Check if the cleaned word has less than 2 characters\n    if len(cleaned_word) < 2:\n        return []\n\n    # Generate all 2-letter combinations\n    combinations = [cleaned_word[i:i+2].lower() for i in range(len(cleaned_word) - 1)]\n\n    # Count the frequency of each combination\n    combination_counter = Counter(combinations)\n\n    # Find the most common combination\n    most_common_combination = combination_counter.most_common(1)\n\n    # Return the most common combination and its frequency\n    return [(most_common_combination[0][0], most_common_combination[0][1])] if most_common_combination else []\n\n# Test cases"}
{"task_id": "BigCodeBench/933", "solution": "import string\nimport wordninja\ndef task_func(word):\n    # Create a list of tuples with each letter and its position in the alphabet\n    letter_positions = [(letter, string.ascii_lowercase.index(letter) + 1) for letter in word]\n\n    # Split the word into words using wordninja\n    words = wordninja.split(word)\n\n    return letter_positions, words", "raw_solution": "import string\nimport wordninja\n\ndef task_func(word):\n    # Create a list of tuples with each letter and its position in the alphabet\n    letter_positions = [(letter, string.ascii_lowercase.index(letter) + 1) for letter in word]\n\n    # Split the word into words using wordninja\n    words = wordninja.split(word)\n\n    return letter_positions, words\n\n# Test cases"}
{"task_id": "BigCodeBench/934", "solution": "from collections import Counter\nimport hashlib\ndef task_func(word: str) -> str:\n    \"\"\"\n    Ek word mein baaye se right pratyek adjacent letter pair ki sankhya ginen and result ko MD5 hash ke roop mein encode karen.\n\n    Parameters:\n    - word (str): Vah word jismein adjacent letter pair ginane hain.\n\n    Returns:\n        str: Ek MD5 hash string jahaan adjacent letter pair ki counts hain.\n\n    Requirements:\n    - collections.Counter\n\n    Example:\n    udaaharan:\n    >>> task_func('abracadabra')\n    'bc9af285d87b312e61ab3661e66b741b'\n    >>> task_func('hello')\n    'dd5dec1a853625e2dc48f3d42665c337'\n\n    \"\"\"\n    # Create adjacent letter pairs\n    pairs = [word[i:i+2] for i in range(len(word) - 1)]\n    \n    # Count occurrences of each pair\n    pair_counts = Counter(pairs)\n    \n    # Convert counts to a string\n    counts_str = ''.join(f\"{pair}: {count}\" for pair, count in pair_counts.items())\n    \n    # Create an MD5 hash of the counts string\n    md5_hash = hashlib.md5(counts_str.encode()).hexdigest()\n    \n    return md5_hash", "raw_solution": "from collections import Counter\nimport hashlib\n\ndef task_func(word: str) -> str:\n    \"\"\"\n    Ek word mein baaye se right pratyek adjacent letter pair ki sankhya ginen and result ko MD5 hash ke roop mein encode karen.\n\n    Parameters:\n    - word (str): Vah word jismein adjacent letter pair ginane hain.\n\n    Returns:\n        str: Ek MD5 hash string jahaan adjacent letter pair ki counts hain.\n\n    Requirements:\n    - collections.Counter\n\n    Example:\n    udaaharan:\n    >>> task_func('abracadabra')\n    'bc9af285d87b312e61ab3661e66b741b'\n    >>> task_func('hello')\n    'dd5dec1a853625e2dc48f3d42665c337'\n\n    \"\"\"\n    # Create adjacent letter pairs\n    pairs = [word[i:i+2] for i in range(len(word) - 1)]\n    \n    # Count occurrences of each pair\n    pair_counts = Counter(pairs)\n    \n    # Convert counts to a string\n    counts_str = ''.join(f\"{pair}: {count}\" for pair, count in pair_counts.items())\n    \n    # Create an MD5 hash of the counts string\n    md5_hash = hashlib.md5(counts_str.encode()).hexdigest()\n    \n    return md5_hash\n```\n"}
{"task_id": "BigCodeBench/935", "solution": "import pandas as pd\nimport string\ndef task_func(word):\n    if not word.islower() or not word.isalpha():\n        raise ValueError(\"The input word should be in lowercase and consist of alphabetic characters only.\")\n    \n    # Create a list of tuples with each letter and its position\n    letter_positions = [(letter, string.ascii_lowercase.index(letter) + 1) for letter in word]\n    \n    # Create a DataFrame from the list of tuples\n    df = pd.DataFrame(letter_positions, columns=['Letter', 'Position'])\n    \n    return df", "raw_solution": "import pandas as pd\nimport string\n\ndef task_func(word):\n    if not word.islower() or not word.isalpha():\n        raise ValueError(\"The input word should be in lowercase and consist of alphabetic characters only.\")\n    \n    # Create a list of tuples with each letter and its position\n    letter_positions = [(letter, string.ascii_lowercase.index(letter) + 1) for letter in word]\n    \n    # Create a DataFrame from the list of tuples\n    df = pd.DataFrame(letter_positions, columns=['Letter', 'Position'])\n    \n    return df\n\n# Example usage:\n# df = task_func('abc')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/936", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport string\nALPHABET = list(string.ascii_lowercase)\ndef task_func(word):\n    \"\"\"\n    Dee gaye word mein pratyek letter kee position ko darshaane vaala ek bar chart banaen\n    numpy and matplotlib.pyplot ka upayog karake english alphabet ke bheetar.\n    constants:\n    - ALPHABET: english alphabet ke sabhee chhote letters kee ek list.\n\n    Notes:\n        function english alphabet mein pratyek letter kee position ko darshaane ke lie pratyek letter ke index ka upayog karata hai.\n        example ke lie, 'a' ko 1 dvaara darshaaya jaega, 'b' ko 2 se, and isee tarah.\n\n    Parameters:\n    - word (str): vah word jisakee letters kee position plot kee jaegee.\n    - only chhote alphabet characters mein hona chaahie.\n\n    Returns:\n        axes: ek matplotlib.axes._axes.Axes object jo generated plot ka pratinidhitv karata hai.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    example:\n    >>> ax = task_func('abc')\n    >>> ax = task_func('hello')\n\n    \"\"\"\n    # Initialize an array to store the positions of the letters\n    positions = np.zeros(len(word), dtype=int)\n\n    # Get the positions of the letters in the word\n    for i, letter in enumerate(word):\n        if letter in ALPHABET:\n            positions[i] = ALPHABET.index(letter) + 1\n\n    # Create a bar chart of the positions\n    fig, ax = plt.subplots()\n    ax.bar(range(len(word)), positions, tick_label=word)\n\n    # Set the title and labels\n    ax.set_title('Position of Letters in Word')\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Position')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport string\n\n# Constants\nALPHABET = list(string.ascii_lowercase)\n\ndef task_func(word):\n    \"\"\"\n    Dee gaye word mein pratyek letter kee position ko darshaane vaala ek bar chart banaen\n    numpy and matplotlib.pyplot ka upayog karake english alphabet ke bheetar.\n    constants:\n    - ALPHABET: english alphabet ke sabhee chhote letters kee ek list.\n\n    Notes:\n        function english alphabet mein pratyek letter kee position ko darshaane ke lie pratyek letter ke index ka upayog karata hai.\n        example ke lie, 'a' ko 1 dvaara darshaaya jaega, 'b' ko 2 se, and isee tarah.\n\n    Parameters:\n    - word (str): vah word jisakee letters kee position plot kee jaegee.\n    - only chhote alphabet characters mein hona chaahie.\n\n    Returns:\n        axes: ek matplotlib.axes._axes.Axes object jo generated plot ka pratinidhitv karata hai.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    example:\n    >>> ax = task_func('abc')\n    >>> ax = task_func('hello')\n\n    \"\"\"\n    # Initialize an array to store the positions of the letters\n    positions = np.zeros(len(word), dtype=int)\n\n    # Get the positions of the letters in the word\n    for i, letter in enumerate(word):\n        if letter in ALPHABET:\n            positions[i] = ALPHABET.index(letter) + 1\n\n    # Create a bar chart of the positions\n    fig, ax = plt.subplots()\n    ax.bar(range(len(word)), positions, tick_label=word)\n\n    # Set the title and labels\n    ax.set_title('Position of Letters in Word')\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Position')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/937", "solution": "import re\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove non-alphanumeric characters\n    cleaned_str = re.sub(r'\\W+', '', input_str)\n\n    # Count the frequency of each character\n    char_count = Counter(cleaned_str.lower())\n\n    return char_count", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(input_str):\n    # Remove non-alphanumeric characters\n    cleaned_str = re.sub(r'\\W+', '', input_str)\n\n    # Count the frequency of each character\n    char_count = Counter(cleaned_str.lower())\n\n    return char_count\n\n# Example usage:"}
{"task_id": "BigCodeBench/938", "solution": "import re\nimport pandas as pd\ndef task_func(input_df):\n    # Check if the input is a pandas DataFrame\n    if not isinstance(input_df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n\n    # Check if the DataFrame contains a 'text' column\n    if 'text' not in input_df.columns:\n        raise ValueError(\"DataFrame must contain a 'text' column\")\n\n    # Function to clean text\n    def clean_text(text):\n        # Remove special characters, punctuation marks, and spaces\n        cleaned_text = re.sub(r'[^a-zA-Z0-9]', '', text)\n        return cleaned_text\n\n    # Apply the clean_text function to the 'text' column\n    input_df['clean_text'] = input_df['text'].apply(clean_text)\n\n    # Calculate the length of the cleaned text\n    input_df['text_length'] = input_df['clean_text'].apply(len)\n\n    return input_df", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(input_df):\n    # Check if the input is a pandas DataFrame\n    if not isinstance(input_df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n\n    # Check if the DataFrame contains a 'text' column\n    if 'text' not in input_df.columns:\n        raise ValueError(\"DataFrame must contain a 'text' column\")\n\n    # Function to clean text\n    def clean_text(text):\n        # Remove special characters, punctuation marks, and spaces\n        cleaned_text = re.sub(r'[^a-zA-Z0-9]', '', text)\n        return cleaned_text\n\n    # Apply the clean_text function to the 'text' column\n    input_df['clean_text'] = input_df['text'].apply(clean_text)\n\n    # Calculate the length of the cleaned text\n    input_df['text_length'] = input_df['clean_text'].apply(len)\n\n    return input_df\n\n# Example usage:\n# df = pd.DataFrame({'text': ['Special $#! characters   spaces 888323']})\n# print(task_func(df))\n```\n"}
{"task_id": "BigCodeBench/939", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path: str) -> list:\n    # Get all files in the directory\n    files = glob.glob(os.path.join(dir_path, '*'))\n\n    # List to store new file names\n    new_file_names = []\n\n    for file in files:\n        # Extract the file name\n        file_name = os.path.basename(file)\n\n        # Remove special characters, punctuation marks, and spaces\n        new_file_name = re.sub(r'[^a-zA-Z0-9]', '', file_name)\n\n        # Add the new file name to the list\n        new_file_names.append(new_file_name)\n\n    return new_file_names", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(dir_path: str) -> list:\n    # Get all files in the directory\n    files = glob.glob(os.path.join(dir_path, '*'))\n\n    # List to store new file names\n    new_file_names = []\n\n    for file in files:\n        # Extract the file name\n        file_name = os.path.basename(file)\n\n        # Remove special characters, punctuation marks, and spaces\n        new_file_name = re.sub(r'[^a-zA-Z0-9]', '', file_name)\n\n        # Add the new file name to the list\n        new_file_names.append(new_file_name)\n\n    return new_file_names\n\n# Example usage:\n# print(task_func('path/to/directory'))\n# print(task_func('another/directory/path'))\n```\n"}
{"task_id": "BigCodeBench/940", "solution": "import re\nfrom nltk import word_tokenize\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove special characters, punctuation marks, and spaces\n    cleaned_str = re.sub(r'[^\\w\\s]', '', input_str)\n    \n    # Tokenize the cleaned string into words\n    words = word_tokenize(cleaned_str)\n    \n    # Count the frequency of each word\n    word_count = Counter(words)\n    \n    return word_count", "raw_solution": "import re\nfrom nltk import word_tokenize\nfrom collections import Counter\n\ndef task_func(input_str):\n    # Remove special characters, punctuation marks, and spaces\n    cleaned_str = re.sub(r'[^\\w\\s]', '', input_str)\n    \n    # Tokenize the cleaned string into words\n    words = word_tokenize(cleaned_str)\n    \n    # Count the frequency of each word\n    word_count = Counter(words)\n    \n    return word_count\n\n# Example usage:\n# print(task_func('Special $#! characters   spaces 888323'))\n```\n"}
{"task_id": "BigCodeBench/941", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_date: str, periods: int, freq: str, random_seed: int = 0) -> (pd.DataFrame, plt.Axes):\n    np.random.seed(random_seed)\n    \n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random sales data\n    sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date: str, periods: int, freq: str, random_seed: int = 0) -> (pd.DataFrame, plt.Axes):\n    np.random.seed(random_seed)\n    \n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random sales data\n    sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate dates based on frequency\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Initialize sales data\n    sales_data = []\n    \n    for date in dates:\n        for category in categories:\n            # Generate random sales data\n            sales = np.random.randint(100, 1000)\n            sales_data.append([date, category, sales])\n    \n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=['Date', 'Category', 'Sales'])\n    \n    # Plot sales data\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Data Over Time')\n    ax.legend()\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate dates based on frequency\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Initialize sales data\n    sales_data = []\n    \n    for date in dates:\n        for category in categories:\n            # Generate random sales data\n            sales = np.random.randint(100, 1000)\n            sales_data.append([date, category, sales])\n    \n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=['Date', 'Category', 'Sales'])\n    \n    # Plot sales data\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Data Over Time')\n    ax.legend()\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/943", "solution": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(start_date='2016-01-01', periods=24, freq='M', model='additive'):\n    # Generate a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random sales data\n    np.random.seed(0)\n    sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Sales': sales_data}, index=dates)\n    \n    # Perform seasonal decomposition\n    decomposition = seasonal_decompose(df['Sales'], model=model)\n    \n    # Extract trend, seasonal, and residual components\n    trend = decomposition.trend\n    seasonal = decomposition.seasonal\n    residual = decomposition.resid\n    \n    # Return the results as a dictionary\n    return {'trend': trend, 'seasonal': seasonal, 'residual': residual}", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(start_date='2016-01-01', periods=24, freq='M', model='additive'):\n    # Generate a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random sales data\n    np.random.seed(0)\n    sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Sales': sales_data}, index=dates)\n    \n    # Perform seasonal decomposition\n    decomposition = seasonal_decompose(df['Sales'], model=model)\n    \n    # Extract trend, seasonal, and residual components\n    trend = decomposition.trend\n    seasonal = decomposition.seasonal\n    residual = decomposition.resid\n    \n    # Return the results as a dictionary\n    return {'trend': trend, 'seasonal': seasonal, 'residual': residual}\n\n# Example usage:\nresult = task_func('2016-01-01', 24, 'M')\nassert all(key in result for key in ['trend', 'seasonal', 'residual'])\nassert len(result['seasonal']) == 24\n```\n"}
{"task_id": "BigCodeBench/944", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    np.random.seed(seed)\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    prices = np.random.uniform(low=100, high=500, size=periods)\n\n    df = pd.DataFrame({'Date': date_range, 'Price': prices})\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Price'], marker='o')\n    ax.set_title('Stock Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    np.random.seed(seed)\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    prices = np.random.uniform(low=100, high=500, size=periods)\n\n    df = pd.DataFrame({'Date': date_range, 'Price': prices})\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Price'], marker='o')\n    ax.set_title('Stock Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate sales data if not provided\n    if sales_data is None:\n        sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n    \n    # Prepare data for regression\n    X = np.arange(len(df)).reshape(-1, 1)\n    y = df['Sales'].values\n    \n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Predict future sales\n    future_dates = pd.date_range(start=date_range[-1] + pd.Timedelta(1, unit='W'), periods=periods, freq=freq)\n    future_X = np.arange(len(df) + periods).reshape(-1, 1)\n    future_sales = model.predict(future_X)\n    \n    return future_sales[-periods:]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate sales data if not provided\n    if sales_data is None:\n        sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n    \n    # Prepare data for regression\n    X = np.arange(len(df)).reshape(-1, 1)\n    y = df['Sales'].values\n    \n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Predict future sales\n    future_dates = pd.date_range(start=date_range[-1] + pd.Timedelta(1, unit='W'), periods=periods, freq=freq)\n    future_X = np.arange(len(df) + periods).reshape(-1, 1)\n    future_sales = model.predict(future_X)\n    \n    return future_sales[-periods:]\n```\n"}
{"task_id": "BigCodeBench/946", "solution": "import numpy as np\nimport pandas as pd\nimport random\ndef task_func(rows=3, cols=2, min_val=0, max_val=100, seed=0):\n    \"\"\"\n    Ek diye gaye range mein random integers ke saath specified dimensions ka ek matrix banata hai,\n    and then ise ek Panda DataFrame mein badal deta hai.\n\n    Parameters:\n    - rows (int): matrix mein rows ki number. Default 3 hai.\n    - cols (int): matrix mein columns ki number. Default 2 hai.\n    - min_val (int): random integers ke liye min_val int value. Default 0 hai.\n    - max_val (int): random integers ke liye max_val int value. Default 100 hai.\n\n    Returns:\n        DataFrame: Ek Panda DataFrame jisme specified range mein random int shaamil hain.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n\n    Example:\n    >>> df = task_func(3, 2, 0, 100)\n    >>> print(type(df))\n    <class 'pandas.core.frame.DataFrame'>\n    >>> print(df.shape)\n    (3, 2)\n\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate a random matrix\n    random_matrix = np.random.randint(min_val, max_val, size=(rows, cols))\n\n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(random_matrix, columns=[f'col_{i}' for i in range(cols)])\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nimport random\n\ndef task_func(rows=3, cols=2, min_val=0, max_val=100, seed=0):\n    \"\"\"\n    Ek diye gaye range mein random integers ke saath specified dimensions ka ek matrix banata hai,\n    and then ise ek Panda DataFrame mein badal deta hai.\n\n    Parameters:\n    - rows (int): matrix mein rows ki number. Default 3 hai.\n    - cols (int): matrix mein columns ki number. Default 2 hai.\n    - min_val (int): random integers ke liye min_val int value. Default 0 hai.\n    - max_val (int): random integers ke liye max_val int value. Default 100 hai.\n\n    Returns:\n        DataFrame: Ek Panda DataFrame jisme specified range mein random int shaamil hain.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n\n    Example:\n    >>> df = task_func(3, 2, 0, 100)\n    >>> print(type(df))\n    <class 'pandas.core.frame.DataFrame'>\n    >>> print(df.shape)\n    (3, 2)\n\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate a random matrix\n    random_matrix = np.random.randint(min_val, max_val, size=(rows, cols))\n\n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(random_matrix, columns=[f'col_{i}' for i in range(cols)])\n\n    return df\n\n# Example usage:\n# df = task_func(3, 2, 0, 100)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/947", "solution": "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\nfrom itertools import islice\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate a list of unique dates between start_date and end_date\n    date_range = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n    unique_dates = random.sample(date_range, min(rows * columns, len(date_range)))\n\n    # Create a matrix with unique dates\n    matrix = np.array(list(islice(cycle(unique_dates), rows * columns))).reshape(rows, columns)\n\n    # Convert datetime objects to strings\n    matrix = np.array([[str(date) for date in row] for row in matrix])\n\n    return matrix\ndef cycle(iterable):\n    while True:\n        for item in iterable:\n            yield item", "raw_solution": "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\nfrom itertools import islice\n\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate a list of unique dates between start_date and end_date\n    date_range = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n    unique_dates = random.sample(date_range, min(rows * columns, len(date_range)))\n\n    # Create a matrix with unique dates\n    matrix = np.array(list(islice(cycle(unique_dates), rows * columns))).reshape(rows, columns)\n\n    # Convert datetime objects to strings\n    matrix = np.array([[str(date) for date in row] for row in matrix])\n\n    return matrix\n\n# Helper function to create an infinite iterator from a list\ndef cycle(iterable):\n    while True:\n        for item in iterable:\n            yield item\n```\n"}
{"task_id": "BigCodeBench/948", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler()\n    scaled_matrix = scaler.fit_transform(matrix)\n    return scaled_matrix", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler()\n    scaled_matrix = scaler.fit_transform(matrix)\n    return scaled_matrix\n\n# Test cases"}
{"task_id": "BigCodeBench/949", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(rows, columns, seed=None):\n    \"\"\"\n    Ek specified range ke bheetar random values ke saath ek DataFrame banaen.\n    Yeh function diye gaye dimensions ka ek matrix banata hai jo 0 and 1 ke beech random values se bhara hota hai and ise ek Pandas DataFrame ke roop mein lautata hai. users ke paas reproducible parinaamon int ek random seed set karne ka option hota hai.\n\n    Parameters:\n    - rows (int): matrix int rows ki number.\n    - columns (int): matrix int columns ki number.\n    - seed (int, optional): random number generator int seed. Default None hai.\n\n    Returns:\n        DataFrame: Ek Pandas DataFrame jismein utpann random maan shaamil hain.\n\n    Requirements:\n    - numpy\n    - pandas\n\n    Example:\n    Examples:\n    >>> df = task_func(3, 2, seed=42)\n    >>> print(df.shape)\n    (3, 2)\n    >>> df = task_func(1, 1, seed=24)\n    >>> print(df.shape)\n    (1, 1)\n\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    random_matrix = np.random.rand(rows, columns)\n    df = pd.DataFrame(random_matrix, columns=[f'col_{i}' for i in range(columns)])\n    \n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(rows, columns, seed=None):\n    \"\"\"\n    Ek specified range ke bheetar random values ke saath ek DataFrame banaen.\n    Yeh function diye gaye dimensions ka ek matrix banata hai jo 0 and 1 ke beech random values se bhara hota hai and ise ek Pandas DataFrame ke roop mein lautata hai. users ke paas reproducible parinaamon int ek random seed set karne ka option hota hai.\n\n    Parameters:\n    - rows (int): matrix int rows ki number.\n    - columns (int): matrix int columns ki number.\n    - seed (int, optional): random number generator int seed. Default None hai.\n\n    Returns:\n        DataFrame: Ek Pandas DataFrame jismein utpann random maan shaamil hain.\n\n    Requirements:\n    - numpy\n    - pandas\n\n    Example:\n    Examples:\n    >>> df = task_func(3, 2, seed=42)\n    >>> print(df.shape)\n    (3, 2)\n    >>> df = task_func(1, 1, seed=24)\n    >>> print(df.shape)\n    (1, 1)\n\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    random_matrix = np.random.rand(rows, columns)\n    df = pd.DataFrame(random_matrix, columns=[f'col_{i}' for i in range(columns)])\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/950", "solution": "import numpy as np\nfrom scipy.linalg import svd\ndef task_func(rows=3, columns=2, seed=0):\n    \"\"\"\n    random values ka ek matrix banaen, jismein specified dimensions hon, aur us par singular value decomposition (SVD) karen.\n\n    Parameters:\n    - rows (int): random matrix ke liye rows ki number. Default 3 hai.\n    - columns (int): random matrix ke liye columns ki number. Default 2 hai.\n    - seed (int, optional): reproducibility sunishchit karne ke liye random number generator ke liye seed. Default None hai.\n\n    Returns:\n        tuple: ek tuple jismein teen elements hain:\n        U (ndarray): unitary matrix U.\n        s (ndarray): singular maan, avrohi order mein chhaante gae.\n        Vh (ndarray): unitary matrix V ka conjugate transpose.\n\n    Requirements:\n    - numpy\n    - scipy.linalg.svd\n\n    Example:\n    >>> U, s, Vh = task_func(3, 2, seed=42)\n    >>> print('U shape:', U.shape)\n    U shape: (3, 3)\n    >>> print('s shape:', s.shape)\n    s shape: (2,)\n    >>> print('Vh shape:', Vh.shape)\n    Vh shape: (2, 2)\n\n    \"\"\"\n    np.random.seed(seed)\n    random_matrix = np.random.rand(rows, columns)\n    U, s, Vh = svd(random_matrix)\n    return U, s, Vh", "raw_solution": "import numpy as np\nfrom scipy.linalg import svd\n\ndef task_func(rows=3, columns=2, seed=0):\n    \"\"\"\n    random values ka ek matrix banaen, jismein specified dimensions hon, aur us par singular value decomposition (SVD) karen.\n\n    Parameters:\n    - rows (int): random matrix ke liye rows ki number. Default 3 hai.\n    - columns (int): random matrix ke liye columns ki number. Default 2 hai.\n    - seed (int, optional): reproducibility sunishchit karne ke liye random number generator ke liye seed. Default None hai.\n\n    Returns:\n        tuple: ek tuple jismein teen elements hain:\n        U (ndarray): unitary matrix U.\n        s (ndarray): singular maan, avrohi order mein chhaante gae.\n        Vh (ndarray): unitary matrix V ka conjugate transpose.\n\n    Requirements:\n    - numpy\n    - scipy.linalg.svd\n\n    Example:\n    >>> U, s, Vh = task_func(3, 2, seed=42)\n    >>> print('U shape:', U.shape)\n    U shape: (3, 3)\n    >>> print('s shape:', s.shape)\n    s shape: (2,)\n    >>> print('Vh shape:', Vh.shape)\n    Vh shape: (2, 2)\n\n    \"\"\"\n    np.random.seed(seed)\n    random_matrix = np.random.rand(rows, columns)\n    U, s, Vh = svd(random_matrix)\n    return U, s, Vh\n```\n"}
{"task_id": "BigCodeBench/951", "solution": "import pandas as pd\nimport numpy as np\nimport random\nfrom random import randint, seed\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\ndef task_func(mystrings, n_products, seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    data = []\n    for i in range(n_products):\n        product_name = mystrings[i].replace(' ', '_')\n        category = random.choice(CATEGORIES)\n        price = round(np.random.normal(50, 10), 2)\n        data.append([product_name, category, price])\n\n    df = pd.DataFrame(data, columns=['Product Name', 'Category', 'Price'])\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\nfrom random import randint, seed\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\n\ndef task_func(mystrings, n_products, seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    data = []\n    for i in range(n_products):\n        product_name = mystrings[i].replace(' ', '_')\n        category = random.choice(CATEGORIES)\n        price = round(np.random.normal(50, 10), 2)\n        data.append([product_name, category, price])\n\n    df = pd.DataFrame(data, columns=['Product Name', 'Category', 'Price'])\n    return df\n\n# Example usage:\n# df = task_func(['Mobile Phone', 'T Shirt', 'Coffee Maker', 'python_book', 'Toy Car'], 2)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if seed is not None:\n        random.seed(seed)\n\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative.\")\n\n    # Generate due date\n    due_date = datetime.now() + timedelta(days=random.randint(1, 30))\n\n    # Create a list of tasks to assign\n    tasks_to_assign = task_list[:n_tasks] if n_tasks <= len(task_list) else task_list\n\n    # Assign tasks to employees randomly\n    assignments = []\n    for task in tasks_to_assign:\n        assigned_to = random.choice(employees)\n        task_name = task.replace(\" \", \"_\")\n        assignments.append([task_name, assigned_to, due_date.strftime(\"%Y-%m-%d\")])\n\n    # Create a DataFrame\n    df = pd.DataFrame(assignments, columns=[\"Task_Name\", \"Assigned_To\", \"Due_Date\"])\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if seed is not None:\n        random.seed(seed)\n\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative.\")\n\n    # Generate due date\n    due_date = datetime.now() + timedelta(days=random.randint(1, 30))\n\n    # Create a list of tasks to assign\n    tasks_to_assign = task_list[:n_tasks] if n_tasks <= len(task_list) else task_list\n\n    # Assign tasks to employees randomly\n    assignments = []\n    for task in tasks_to_assign:\n        assigned_to = random.choice(employees)\n        task_name = task.replace(\" \", \"_\")\n        assignments.append([task_name, assigned_to, due_date.strftime(\"%Y-%m-%d\")])\n\n    # Create a DataFrame\n    df = pd.DataFrame(assignments, columns=[\"Task_Name\", \"Assigned_To\", \"Due_Date\"])\n\n    return df\n\n# Example usage:\n# df = task_func(['Clean Office', 'Prepare Report', 'Client Meeting'], 2, seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/953", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(mystrings, folder_path, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n\n    filenames = []\n    for i, title in enumerate(mystrings):\n        # Generate random data points\n        data_points = np.random.rand(10)\n        \n        # Create a bar chart\n        plt.figure()\n        plt.bar(range(10), data_points, color='blue')\n        plt.title(title)\n        plt.xlabel('Bar Index')\n        plt.ylabel('Random Value')\n        \n        # Save the plot as a PNG file\n        filename = f\"{title.replace(' ', '_')}.png\"\n        filepath = os.path.join(folder_path, filename)\n        plt.savefig(filepath)\n        plt.close()\n        \n        filenames.append(filename)\n\n    return filenames", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(mystrings, folder_path, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n\n    filenames = []\n    for i, title in enumerate(mystrings):\n        # Generate random data points\n        data_points = np.random.rand(10)\n        \n        # Create a bar chart\n        plt.figure()\n        plt.bar(range(10), data_points, color='blue')\n        plt.title(title)\n        plt.xlabel('Bar Index')\n        plt.ylabel('Random Value')\n        \n        # Save the plot as a PNG file\n        filename = f\"{title.replace(' ', '_')}.png\"\n        filepath = os.path.join(folder_path, filename)\n        plt.savefig(filepath)\n        plt.close()\n        \n        filenames.append(filename)\n\n    return filenames\n```\n"}
{"task_id": "BigCodeBench/954", "solution": "import random\nimport re\ndef task_func(target_words, n_sentences, vocabulary):\n    if n_sentences < 0:\n        raise ValueError(\"n_sentences must be non-negative.\")\n    if not vocabulary:\n        raise ValueError(\"vocabulary must not be empty.\")\n\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choices(vocabulary, k=10))\n        sentence = sentence.lower()\n        for target in target_words:\n            target = target.replace(' ', '_')\n            sentence = re.sub(r'\\b' + re.escape(target) + r'\\b', target, sentence, flags=re.IGNORECASE)\n        sentences.append(sentence)\n\n    return sentences", "raw_solution": "import random\nimport re\n\ndef task_func(target_words, n_sentences, vocabulary):\n    if n_sentences < 0:\n        raise ValueError(\"n_sentences must be non-negative.\")\n    if not vocabulary:\n        raise ValueError(\"vocabulary must not be empty.\")\n\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choices(vocabulary, k=10))\n        sentence = sentence.lower()\n        for target in target_words:\n            target = target.replace(' ', '_')\n            sentence = re.sub(r'\\b' + re.escape(target) + r'\\b', target, sentence, flags=re.IGNORECASE)\n        sentences.append(sentence)\n\n    return sentences\n\n# Example usage:\nrandom.seed(42)"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text cannot be empty.\")\n\n    # Replace spaces with underscores in the list of strings\n    modified_strings = [s.replace(' ', '_') for s in mystrings]\n\n    # Combine all modified strings into a single string\n    combined_text = ' '.join(modified_strings)\n\n    # Find all words in the text (case-insensitive)\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Prepare data for plotting\n    words = list(word_counts.keys())\n    frequencies = np.array(list(word_counts.values()))\n\n    # Plot the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency Plot')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text cannot be empty.\")\n\n    # Replace spaces with underscores in the list of strings\n    modified_strings = [s.replace(' ', '_') for s in mystrings]\n\n    # Combine all modified strings into a single string\n    combined_text = ' '.join(modified_strings)\n\n    # Find all words in the text (case-insensitive)\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Prepare data for plotting\n    words = list(word_counts.keys())\n    frequencies = np.array(list(word_counts.values()))\n\n    # Plot the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency Plot')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:\n# ax = task_func(['Lorem ipsum', 'consectetur adipiscing'], 'Lorem ipsum dolor sit amet lorem Ipsum')\n# print(type(ax))\n```\n"}
{"task_id": "BigCodeBench/956", "solution": "import re\nimport string\nimport random\ndef task_func(text: str, seed=None) -> str:\n    if seed is not None:\n        random.seed(seed)\n\n    # Replace special characters with underscores\n    text = re.sub(r'[{}\\t\\n]', lambda m: '_' * len(m.group()), text)\n\n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n\n    # Convert to uppercase with 50% probability\n    text = ''.join(random.choice([c.upper(), c.lower()]) for c in text)\n\n    return text", "raw_solution": "import re\nimport string\nimport random\n\ndef task_func(text: str, seed=None) -> str:\n    if seed is not None:\n        random.seed(seed)\n\n    # Replace special characters with underscores\n    text = re.sub(r'[{}\\t\\n]', lambda m: '_' * len(m.group()), text)\n\n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n\n    # Convert to uppercase with 50% probability\n    text = ''.join(random.choice([c.upper(), c.lower()]) for c in text)\n\n    return text\n\n# Test cases"}
{"task_id": "BigCodeBench/957", "solution": "import string\nimport re\ndef task_func(text: str) -> tuple:\n    # Remove punctuation using string.punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Split text into words using whitespace as delimiter\n    words = re.findall(r'\\S+', text)\n    \n    # Count characters and unique characters\n    characters = len(text)\n    unique_characters = len(set(text))\n    \n    return len(words), characters, unique_characters", "raw_solution": "import string\nimport re\n\ndef task_func(text: str) -> tuple:\n    # Remove punctuation using string.punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Split text into words using whitespace as delimiter\n    words = re.findall(r'\\S+', text)\n    \n    # Count characters and unique characters\n    characters = len(text)\n    unique_characters = len(set(text))\n    \n    return len(words), characters, unique_characters\n\n# Test cases"}
{"task_id": "BigCodeBench/958", "solution": "import random\nimport re\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def scramble_word(word):\n        if len(word) <= 3:\n            return word\n        else:\n            first, last = word[0], word[-1]\n            middle = ''.join(random.sample(word[1:-1], len(word) - 2))\n            return first + middle + last\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    scrambled_words = [scramble_word(word) for word in words]\n\n    return re.sub(r'\\b\\w+\\b', lambda m: scrambled_words.pop(0), text)", "raw_solution": "import random\nimport re\n\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def scramble_word(word):\n        if len(word) <= 3:\n            return word\n        else:\n            first, last = word[0], word[-1]\n            middle = ''.join(random.sample(word[1:-1], len(word) - 2))\n            return first + middle + last\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    scrambled_words = [scramble_word(word) for word in words]\n\n    return re.sub(r'\\b\\w+\\b', lambda m: scrambled_words.pop(0), text)\n\n# Test cases"}
{"task_id": "BigCodeBench/959", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a translation table that maps each alphabetic character to a random character\n    translation_table = str.maketrans(\n        string.ascii_letters, \n        ''.join(random.choices(string.ascii_letters, k=len(string.ascii_letters)))\n    )\n\n    # Translate the text using the translation table\n    translated_text = text.translate(translation_table)\n\n    return translated_text", "raw_solution": "import string\nimport random\n\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a translation table that maps each alphabetic character to a random character\n    translation_table = str.maketrans(\n        string.ascii_letters, \n        ''.join(random.choices(string.ascii_letters, k=len(string.ascii_letters)))\n    )\n\n    # Translate the text using the translation table\n    translated_text = text.translate(translation_table)\n\n    return translated_text\n\n# Example usage:\n# text = 'Hello, world!'\n# print(task_func(text, 0))  # Output: 'Mynbi, qpmzj!'\n```\n"}
{"task_id": "BigCodeBench/960", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    if not text:\n        raise ValueError(\"Input text cannot be empty.\")\n\n    random.seed(seed)\n\n    result = []\n    for char in text:\n        if char.isalpha():\n            result.append(random.choice(string.ascii_lowercase))\n        elif char.isdigit():\n            result.append(str(random.randint(0, 9)))\n        elif char.isspace():\n            result.append(random.choice(string.ascii_lowercase) if random.random() < 0.5 else ' ')\n        else:\n            result.append(char)\n\n    return ''.join(result)", "raw_solution": "import string\nimport random\n\ndef task_func(text, seed=None):\n    if not text:\n        raise ValueError(\"Input text cannot be empty.\")\n\n    random.seed(seed)\n\n    result = []\n    for char in text:\n        if char.isalpha():\n            result.append(random.choice(string.ascii_lowercase))\n        elif char.isdigit():\n            result.append(str(random.randint(0, 9)))\n        elif char.isspace():\n            result.append(random.choice(string.ascii_lowercase) if random.random() < 0.5 else ' ')\n        else:\n            result.append(char)\n\n    return ''.join(result)\n\n# Test cases"}
{"task_id": "BigCodeBench/961", "solution": "import os\nimport glob\nfrom collections import Counter\ndef task_func(directory, extensions=[\".txt\", \".docx\", \".xlsx\", \".csv\"], keep_zero=True):\n    if not os.path.exists(directory):\n        raise OSError(f\"The directory {directory} does not exist.\")\n\n    file_counts = Counter()\n\n    for ext in extensions:\n        pattern = os.path.join(directory, '**', f'*{ext}')\n        files = glob.glob(pattern, recursive=True)\n        file_counts[ext] = len(files)\n\n    if not keep_zero:\n        file_counts = Counter({ext: count for ext, count in file_counts.items() if count > 0})\n\n    return file_counts", "raw_solution": "import os\nimport glob\nfrom collections import Counter\n\ndef task_func(directory, extensions=[\".txt\", \".docx\", \".xlsx\", \".csv\"], keep_zero=True):\n    if not os.path.exists(directory):\n        raise OSError(f\"The directory {directory} does not exist.\")\n\n    file_counts = Counter()\n\n    for ext in extensions:\n        pattern = os.path.join(directory, '**', f'*{ext}')\n        files = glob.glob(pattern, recursive=True)\n        file_counts[ext] = len(files)\n\n    if not keep_zero:\n        file_counts = Counter({ext: count for ext, count in file_counts.items() if count > 0})\n\n    return file_counts\n```\n"}
{"task_id": "BigCodeBench/962", "solution": "import os\nfrom pathlib import Path\nimport glob\nimport shutil\ndef task_func(source_directory: str, target_directory: str):\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Ensure target directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    # Define the file extensions to look for\n    extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    files_moved = 0\n\n    # Iterate over each file extension\n    for ext in extensions:\n        # Use glob to find all files with the current extension\n        for file_path in glob.glob(os.path.join(source_directory, '**', f'*{ext}'), recursive=True):\n            # Create a Path object for the file\n            file = Path(file_path)\n            # Define the target file path\n            target_file = Path(target_directory) / file.name\n\n            # Handle naming conflicts\n            counter = 1\n            while target_file.exists():\n                target_file = Path(target_directory) / f\"{os.path.splitext(file.name)[0]}-{counter}{ext}\"\n                counter += 1\n\n            # Move the file to the target directory\n            shutil.move(str(file), str(target_file))\n            files_moved += 1\n\n    return files_moved", "raw_solution": "import os\nfrom pathlib import Path\nimport glob\nimport shutil\n\ndef task_func(source_directory: str, target_directory: str):\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Ensure target directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    # Define the file extensions to look for\n    extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    files_moved = 0\n\n    # Iterate over each file extension\n    for ext in extensions:\n        # Use glob to find all files with the current extension\n        for file_path in glob.glob(os.path.join(source_directory, '**', f'*{ext}'), recursive=True):\n            # Create a Path object for the file\n            file = Path(file_path)\n            # Define the target file path\n            target_file = Path(target_directory) / file.name\n\n            # Handle naming conflicts\n            counter = 1\n            while target_file.exists():\n                target_file = Path(target_directory) / f\"{os.path.splitext(file.name)[0]}-{counter}{ext}\"\n                counter += 1\n\n            # Move the file to the target directory\n            shutil.move(str(file), str(target_file))\n            files_moved += 1\n\n    return files_moved\n```\n"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if source_directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Create target directory if it does not exist\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Find all files with valid extensions in the source directory\n    files_to_zip = []\n    for ext in valid_extensions:\n        files_to_zip.extend(glob.glob(os.path.join(source_directory, f'*{ext}')))\n\n    # Define the full path for the zip file\n    zip_path = Path(target_directory) / f\"{zip_name}.zip\"\n\n    # Create the zip file\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files_to_zip:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return str(zip_path)", "raw_solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if source_directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Create target directory if it does not exist\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Find all files with valid extensions in the source directory\n    files_to_zip = []\n    for ext in valid_extensions:\n        files_to_zip.extend(glob.glob(os.path.join(source_directory, f'*{ext}')))\n\n    # Define the full path for the zip file\n    zip_path = Path(target_directory) / f\"{zip_name}.zip\"\n\n    # Create the zip file\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files_to_zip:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return str(zip_path)\n\n# Example usage:\n# path = task_func('/path/to/source_directory', '/path/to/target_directory', 'zipped_files')\n# print(path)\n```\n"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\ndef task_func(source_directory: str, target_directory: str) -> int:\n    source_path = Path(source_directory)\n    target_path = Path(target_directory)\n\n    if not source_path.exists() or not source_path.is_dir():\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist or is not a directory.\")\n\n    if not target_path.exists():\n        os.makedirs(target_path)\n\n    converted_files = 0\n    for file in source_path.glob('*.txt *.docx *.xlsx *.csv'):\n        if file.suffix in ['.txt', '.docx', '.xlsx', '.csv']:\n            try:\n                if file.suffix == '.txt':\n                    with open(file, 'r') as f:\n                        data = f.read()\n                elif file.suffix == '.docx':\n                    doc = docx.Document(file)\n                    data = '\\n'.join([para.text for para in doc.paragraphs])\n                elif file.suffix == '.xlsx':\n                    df = pd.read_excel(file)\n                    data = df.to_csv(index=False)\n                elif file.suffix == '.csv':\n                    df = pd.read_csv(file)\n                    data = df.to_csv(index=False)\n                \n                target_file = target_path / file.name.replace(file.suffix, '.csv')\n                with open(target_file, 'w') as f:\n                    f.write(data)\n                converted_files += 1\n            except Exception as e:\n                print(f\"Error converting {file}: {e}\")\n\n    return converted_files", "raw_solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    source_path = Path(source_directory)\n    target_path = Path(target_directory)\n\n    if not source_path.exists() or not source_path.is_dir():\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist or is not a directory.\")\n\n    if not target_path.exists():\n        os.makedirs(target_path)\n\n    converted_files = 0\n    for file in source_path.glob('*.txt *.docx *.xlsx *.csv'):\n        if file.suffix in ['.txt', '.docx', '.xlsx', '.csv']:\n            try:\n                if file.suffix == '.txt':\n                    with open(file, 'r') as f:\n                        data = f.read()\n                elif file.suffix == '.docx':\n                    doc = docx.Document(file)\n                    data = '\\n'.join([para.text for para in doc.paragraphs])\n                elif file.suffix == '.xlsx':\n                    df = pd.read_excel(file)\n                    data = df.to_csv(index=False)\n                elif file.suffix == '.csv':\n                    df = pd.read_csv(file)\n                    data = df.to_csv(index=False)\n                \n                target_file = target_path / file.name.replace(file.suffix, '.csv')\n                with open(target_file, 'w') as f:\n                    f.write(data)\n                converted_files += 1\n            except Exception as e:\n                print(f\"Error converting {file}: {e}\")\n\n    return converted_files\n```\n"}
{"task_id": "BigCodeBench/965", "solution": "import os\nimport re\nimport shutil\ndef task_func(source_directory: str, target_directory: str, pattern: str = r\"\\d{4}\") -> int:\n    if not os.path.exists(source_directory) or not os.path.isdir(source_directory):\n        return 0\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    matched_files = []\n    for filename in os.listdir(source_directory):\n        if re.search(pattern, filename):\n            matched_files.append(filename)\n\n    for filename in matched_files:\n        shutil.move(os.path.join(source_directory, filename), target_directory)\n\n    return len(matched_files)", "raw_solution": "import os\nimport re\nimport shutil\n\ndef task_func(source_directory: str, target_directory: str, pattern: str = r\"\\d{4}\") -> int:\n    if not os.path.exists(source_directory) or not os.path.isdir(source_directory):\n        return 0\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    matched_files = []\n    for filename in os.listdir(source_directory):\n        if re.search(pattern, filename):\n            matched_files.append(filename)\n\n    for filename in matched_files:\n        shutil.move(os.path.join(source_directory, filename), target_directory)\n\n    return len(matched_files)\n```\n"}
{"task_id": "BigCodeBench/966", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty or not all(df.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        raise ValueError(\"DataFrame should not be empty and should contain only numeric data.\")\n\n    # Calculate cumulative sums\n    df_cumsum = df.cumsum()\n\n    # Plotting\n    fig, ax = plt.subplots()\n    for column in df_cumsum.columns:\n        ax.plot(df_cumsum.index, df_cumsum[column], label=column)\n\n    ax.set_title('Cumulative Sum per Column')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.legend()\n\n    return df_cumsum, fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty or not all(df.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        raise ValueError(\"DataFrame should not be empty and should contain only numeric data.\")\n\n    # Calculate cumulative sums\n    df_cumsum = df.cumsum()\n\n    # Plotting\n    fig, ax = plt.subplots()\n    for column in df_cumsum.columns:\n        ax.plot(df_cumsum.index, df_cumsum[column], label=column)\n\n    ax.set_title('Cumulative Sum per Column')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.legend()\n\n    return df_cumsum, fig\n```\n"}
{"task_id": "BigCodeBench/967", "solution": "import numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    x = np.linspace(x_range[0], x_range[1], num_points)\n    y = func(x)\n    integral, _ = integrate.quad(func, x_range[0], x_range[1])\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'{func.__name__}(x)')\n    ax.plot(x, np.cumsum(y) - y, label=f'Integral of {func.__name__}(x)', linestyle='--')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'Function and its integral: {func.__name__}(x)')\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    x = np.linspace(x_range[0], x_range[1], num_points)\n    y = func(x)\n    integral, _ = integrate.quad(func, x_range[0], x_range[1])\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'{func.__name__}(x)')\n    ax.plot(x, np.cumsum(y) - y, label=f'Integral of {func.__name__}(x)', linestyle='--')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'Function and its integral: {func.__name__}(x)')\n    ax.legend()\n\n    return ax\n\n# Example usage:\n# ax = task_func(np.sin)\n# print(type(ax))\n# print(ax.get_legend_handles_labels()[-1])\n```\n"}
{"task_id": "BigCodeBench/968", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    df = pd.DataFrame(data)\n    \n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    \n    numeric_df = df.select_dtypes(include='number')\n    if numeric_df.empty:\n        raise ValueError(\"No numeric columns present.\")\n    \n    cumulative_df = df.cumsum()\n    heatmap_data = cumulative_df.corr()\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Cumulative Sum Heatmap')\n    plt.show()\n\n    return ax\ndata = {'A': [1, 2, 3], 'B': [4, 5, 6]}", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    df = pd.DataFrame(data)\n    \n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    \n    numeric_df = df.select_dtypes(include='number')\n    if numeric_df.empty:\n        raise ValueError(\"No numeric columns present.\")\n    \n    cumulative_df = df.cumsum()\n    heatmap_data = cumulative_df.corr()\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Cumulative Sum Heatmap')\n    plt.show()\n\n    return ax\n\n# Example usage:\ndata = {'A': [1, 2, 3], 'B': [4, 5, 6]}\nax = task_func(data)\n```\n"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if not all(df.dtypes.apply(np.issubdtype, args=(np.number,))):\n        raise TypeError(\"All columns in the DataFrame must be numeric.\")\n    if df.empty or df.isnull().values.any():\n        raise ValueError(\"DataFrame must be non-empty and contain no NaN values.\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    df_cumsum = df_normalized.cumsum()\n\n    return df_cumsum", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if not all(df.dtypes.apply(np.issubdtype, args=(np.number,))):\n        raise TypeError(\"All columns in the DataFrame must be numeric.\")\n    if df.empty or df.isnull().values.any():\n        raise ValueError(\"DataFrame must be non-empty and contain no NaN values.\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    df_cumsum = df_normalized.cumsum()\n\n    return df_cumsum\n\n# Example usage:\n# input_df = pd.DataFrame({'A': [1, 2, 3], 'B': [3, 2, 1]})\n# output_df = task_func(input_df)\n# print(output_df)\n```\n"}
{"task_id": "BigCodeBench/970", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data: np.ndarray) -> plt.Axes:\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"Input must be a NumPy array.\")\n    if np.any(data < 0) or np.any(np.isnan(data)):\n        raise ValueError(\"Input array must not contain negative numbers or NaN values.\")\n    \n    sorted_data = np.sort(data)\n    cumulative_probability = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n\n    fig, ax = plt.subplots()\n    ax.plot(sorted_data, cumulative_probability, marker='o', linestyle='-', color='b')\n    ax.set_title('Cumulative Probability Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Probability')\n    ax.grid(True)\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data: np.ndarray) -> plt.Axes:\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"Input must be a NumPy array.\")\n    if np.any(data < 0) or np.any(np.isnan(data)):\n        raise ValueError(\"Input array must not contain negative numbers or NaN values.\")\n    \n    sorted_data = np.sort(data)\n    cumulative_probability = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n\n    fig, ax = plt.subplots()\n    ax.plot(sorted_data, cumulative_probability, marker='o', linestyle='-', color='b')\n    ax.set_title('Cumulative Probability Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Probability')\n    ax.grid(True)\n\n    return ax\n\n# Example usage:\n# ax = task_func(np.array([1, 2, 3, 4, 5]))\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The specified directory does not exist.\")\n\n    result = []\n    directory = Path(directory_path)\n\n    for file in directory.iterdir():\n        if file.is_file():\n            file_size = file.stat().st_size\n            creation_time = datetime.fromtimestamp(file.stat().st_ctime, tz=timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(file.stat().st_mtime, tz=timezone.utc).isoformat()\n            result.append((file.name, file_size, creation_time, modification_time))\n\n    return result", "raw_solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\n\ndef task_func(directory_path: str):\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The specified directory does not exist.\")\n\n    result = []\n    directory = Path(directory_path)\n\n    for file in directory.iterdir():\n        if file.is_file():\n            file_size = file.stat().st_size\n            creation_time = datetime.fromtimestamp(file.stat().st_ctime, tz=timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(file.stat().st_mtime, tz=timezone.utc).isoformat()\n            result.append((file.name, file_size, creation_time, modification_time))\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/972", "solution": "import pathlib\nimport os\ndef task_func(path: str, delimiter: str = os.path.sep) -> list:\n    invalid_chars = '<>:\"|?*'\n    if any(char in path for char in invalid_chars):\n        raise ValueError(\"Invalid characters in path\")\n\n    path = path.replace('\\\\\\\\', '\\\\')\n    path = pathlib.Path(path)\n    components = [str(component) for component in path.parts]\n\n    return components", "raw_solution": "import pathlib\nimport os\n\ndef task_func(path: str, delimiter: str = os.path.sep) -> list:\n    invalid_chars = '<>:\"|?*'\n    if any(char in path for char in invalid_chars):\n        raise ValueError(\"Invalid characters in path\")\n\n    path = path.replace('\\\\\\\\', '\\\\')\n    path = pathlib.Path(path)\n    components = [str(component) for component in path.parts]\n\n    return components\n\n# Test cases"}
{"task_id": "BigCodeBench/973", "solution": "import os\nimport shutil\ndef task_func(path, delimiter=\"/\"):\n    if not isinstance(path, str) or not path:\n        raise ValueError(\"The 'path' must be a non-empty string.\")\n    \n    if not all(component and not component.startswith(delimiter) for component in path.split(delimiter)):\n        raise ValueError(\"The 'path' contains invalid components.\")\n    \n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"The path '{path}' does not exist in the filesystem.\")\n    \n    usage_info = []\n    for component in path.split(delimiter):\n        if component:\n            usage = shutil.disk_usage(os.path.join(path, component))\n            usage_info.append((component, {'total': usage.total, 'used': usage.used, 'free': usage.free}))\n            path = os.path.join(path, component)\n    \n    return usage_info", "raw_solution": "import os\nimport shutil\n\n\ndef task_func(path, delimiter=\"/\"):\n    if not isinstance(path, str) or not path:\n        raise ValueError(\"The 'path' must be a non-empty string.\")\n    \n    if not all(component and not component.startswith(delimiter) for component in path.split(delimiter)):\n        raise ValueError(\"The 'path' contains invalid components.\")\n    \n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"The path '{path}' does not exist in the filesystem.\")\n    \n    usage_info = []\n    for component in path.split(delimiter):\n        if component:\n            usage = shutil.disk_usage(os.path.join(path, component))\n            usage_info.append((component, {'total': usage.total, 'used': usage.used, 'free': usage.free}))\n            path = os.path.join(path, component)\n    \n    return usage_info\n\n\n# Example usage:\n# print(task_func('Docs/src', '/'))\n```\n"}
{"task_id": "BigCodeBench/974", "solution": "import shutil\nimport pathlib\ndef task_func(source_path, destination_path):\n    # Convert to Path objects\n    source = pathlib.Path(source_path)\n    destination = pathlib.Path(destination_path)\n\n    # Check if source directory exists\n    if not source.exists() or not source.is_dir():\n        raise ValueError(f\"The source path {source_path} does not exist or is not a directory.\")\n\n    # Create destination directory if it does not exist\n    destination.mkdir(parents=True, exist_ok=True)\n\n    # List to store copied file names\n    copied_files = []\n\n    # Iterate over files in the source directory\n    for file in source.iterdir():\n        if file.is_file():\n            # Copy file to destination directory\n            shutil.copy(file, destination)\n            # Append file name to the list\n            copied_files.append(file.name)\n\n    return str(source.name), copied_files", "raw_solution": "import shutil\nimport pathlib\n\ndef task_func(source_path, destination_path):\n    # Convert to Path objects\n    source = pathlib.Path(source_path)\n    destination = pathlib.Path(destination_path)\n\n    # Check if source directory exists\n    if not source.exists() or not source.is_dir():\n        raise ValueError(f\"The source path {source_path} does not exist or is not a directory.\")\n\n    # Create destination directory if it does not exist\n    destination.mkdir(parents=True, exist_ok=True)\n\n    # List to store copied file names\n    copied_files = []\n\n    # Iterate over files in the source directory\n    for file in source.iterdir():\n        if file.is_file():\n            # Copy file to destination directory\n            shutil.copy(file, destination)\n            # Append file name to the list\n            copied_files.append(file.name)\n\n    return str(source.name), copied_files\n\n# Example usage:\n# result = task_func('/path/to/source', '/path/to/destination')\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/975", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(rows, columns=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=0) -> pd.DataFrame:\n    if rows < 0:\n        raise ValueError(\"Number of rows cannot be negative.\")\n    \n    np.random.seed(seed)\n    data = np.random.rand(rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    \n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(rows, columns=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=0) -> pd.DataFrame:\n    if rows < 0:\n        raise ValueError(\"Number of rows cannot be negative.\")\n    \n    np.random.seed(seed)\n    data = np.random.rand(rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    \n    return df\n\n# Example usage:\n# df = task_func(10)\n# print(df.head(2))\n```\n"}
{"task_id": "BigCodeBench/976", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    if records.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n\n    np.random.seed(random_seed)\n    feature_names = [f'f{i+1}' for i in range(records.shape[1])]\n    np.random.shuffle(feature_names)\n\n    scaler = StandardScaler()\n    scaled_records = scaler.fit_transform(records)\n\n    df = pd.DataFrame(scaled_records, columns=feature_names)\n    return df\ndata = np.array([[1, 2, 3], [4, 5, 6]])", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    if records.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n\n    np.random.seed(random_seed)\n    feature_names = [f'f{i+1}' for i in range(records.shape[1])]\n    np.random.shuffle(feature_names)\n\n    scaler = StandardScaler()\n    scaled_records = scaler.fit_transform(records)\n\n    df = pd.DataFrame(scaled_records, columns=feature_names)\n    return df\n\n# Example usage:\ndata = np.array([[1, 2, 3], [4, 5, 6]])\ndf = task_func(data, random_seed=42)"}
{"task_id": "BigCodeBench/977", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(array, features=None, seed=None):\n    if array.size == 0 or array.ndim != 2:\n        raise ValueError(\"Array must be a non-empty 2D array.\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    if features is None:\n        features = [f\"Feature {i+1}\" for i in range(array.shape[1])]\n\n    if len(features) != array.shape[1]:\n        raise ValueError(\"Number of features must match the number of columns in the array.\")\n\n    shuffled_indices = np.random.permutation(array.shape[1])\n    shuffled_array = array[:, shuffled_indices]\n    shuffled_features = [features[i] for i in shuffled_indices]\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(shuffled_array, annot=True, xticklabels=shuffled_features, yticklabels=shuffled_features)\n    plt.title(\"Shuffled Array Heatmap\")\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Features\")\n    plt.show()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(array, features=None, seed=None):\n    if array.size == 0 or array.ndim != 2:\n        raise ValueError(\"Array must be a non-empty 2D array.\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    if features is None:\n        features = [f\"Feature {i+1}\" for i in range(array.shape[1])]\n\n    if len(features) != array.shape[1]:\n        raise ValueError(\"Number of features must match the number of columns in the array.\")\n\n    shuffled_indices = np.random.permutation(array.shape[1])\n    shuffled_array = array[:, shuffled_indices]\n    shuffled_features = [features[i] for i in shuffled_indices]\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(shuffled_array, annot=True, xticklabels=shuffled_features, yticklabels=shuffled_features)\n    plt.title(\"Shuffled Array Heatmap\")\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Features\")\n    plt.show()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/978", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(array, seed=None):\n    if len(array.shape) != 2:\n        raise ValueError(\"Input array must be 2D.\")\n\n    pca = PCA(n_components=2, random_state=seed)\n    components = pca.fit_transform(array)\n\n    df = pd.DataFrame(components, columns=['PC1', 'PC2'])\n\n    return df\narray = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\ndef task_func(array, seed=None):\n    if len(array.shape) != 2:\n        raise ValueError(\"Input array must be 2D.\")\n\n    pca = PCA(n_components=2, random_state=seed)\n    components = pca.fit_transform(array)\n\n    df = pd.DataFrame(components, columns=['PC1', 'PC2'])\n\n    return df\n\n# Example usage:\narray = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\ndf = task_func(array, seed=42)"}
{"task_id": "BigCodeBench/979", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\ndef task_func(\n    feature_array,\n    target_array,\n    feature_names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n    target_name=\"target\",\n    seed=None,\n):\n    # Convert input arrays to pandas DataFrame\n    df = pd.DataFrame(feature_array, columns=feature_names)\n    df[target_name] = target_array\n\n    # Shuffle the DataFrame\n    if seed is not None:\n        np.random.seed(seed)\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # Extract features and target\n    X = df[feature_names]\n    y = df[target_name]\n\n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X, y)\n\n    return clf", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef task_func(\n    feature_array,\n    target_array,\n    feature_names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n    target_name=\"target\",\n    seed=None,\n):\n    # Convert input arrays to pandas DataFrame\n    df = pd.DataFrame(feature_array, columns=feature_names)\n    df[target_name] = target_array\n\n    # Shuffle the DataFrame\n    if seed is not None:\n        np.random.seed(seed)\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # Extract features and target\n    X = df[feature_names]\n    y = df[target_name]\n\n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X, y)\n\n    return clf\n```\n"}
{"task_id": "BigCodeBench/980", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(df):\n    if df.empty or not any(df.dtypes.apply(np.issubdtype, args=(np.number,))):\n        raise ValueError(\"DataFrame should not be empty and should contain at least one numeric column.\")\n\n    # Calculate correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n    ax.set_title(\"Correlation Matrix Heatmap\")\n\n    # Standardize numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    scaler = StandardScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df, fig", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\ndef task_func(df):\n    if df.empty or not any(df.dtypes.apply(np.issubdtype, args=(np.number,))):\n        raise ValueError(\"DataFrame should not be empty and should contain at least one numeric column.\")\n\n    # Calculate correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n    ax.set_title(\"Correlation Matrix Heatmap\")\n\n    # Standardize numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    scaler = StandardScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df, fig\n```\n"}
{"task_id": "BigCodeBench/981", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, num_series, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n    if start > end:\n        raise ValueError(\"start_date must be before end_date\")\n\n    if num_series < 1:\n        raise ValueError(\"num_series must be at least 1\")\n\n    date_range = pd.date_range(start=start, end=end, freq='D')\n    data = {f'series_{i+1}': [random.randint(0, 100) for _ in range(len(date_range))] for i in range(num_series)}\n    df = pd.DataFrame(data, index=date_range)\n\n    ax = df.plot(title='random time series', xlabel='date', ylabel='maan')\n    ax.legend(title='series')\n\n    return df, ax", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date, end_date, num_series, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n    if start > end:\n        raise ValueError(\"start_date must be before end_date\")\n\n    if num_series < 1:\n        raise ValueError(\"num_series must be at least 1\")\n\n    date_range = pd.date_range(start=start, end=end, freq='D')\n    data = {f'series_{i+1}': [random.randint(0, 100) for _ in range(len(date_range))] for i in range(num_series)}\n    df = pd.DataFrame(data, index=date_range)\n\n    ax = df.plot(title='random time series', xlabel='date', ylabel='maan')\n    ax.legend(title='series')\n\n    return df, ax\n\n# Example usage:\n# df, ax = task_func('2020-01-01', '2020-12-31', 3, 42)\n# df.head(2)\n```\n"}
{"task_id": "BigCodeBench/982", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport pandas as pd\ndef task_func(df, column, bins=30, density=True, alpha=0.6, color=\"g\", seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Extract the data from the specified column\n    data = df[column].dropna()\n\n    # Plot the histogram\n    count, bins, ignored = plt.hist(data, bins=bins, density=density, alpha=alpha, color=color, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n\n    # Plot the normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    # Set the title and labels\n    plt.title(f\"Normal Fit for '{column}'\")\n    plt.xlabel(column)\n    plt.ylabel('Density' if density else 'Frequency')\n\n    # Return the axes object\n    return plt.gca()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport pandas as pd\n\ndef task_func(df, column, bins=30, density=True, alpha=0.6, color=\"g\", seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Extract the data from the specified column\n    data = df[column].dropna()\n\n    # Plot the histogram\n    count, bins, ignored = plt.hist(data, bins=bins, density=density, alpha=alpha, color=color, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n\n    # Plot the normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    # Set the title and labels\n    plt.title(f\"Normal Fit for '{column}'\")\n    plt.xlabel(column)\n    plt.ylabel('Density' if density else 'Frequency')\n\n    # Return the axes object\n    return plt.gca()\n```\n"}
{"task_id": "BigCodeBench/983", "solution": "import seaborn as sns\nimport numpy as np\nimport pandas as pd\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    if not all(np.issubdtype(dtype, np.number) for dtype in df.dtypes):\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n    \n    covariance_df = df.cov()\n    pair_plot = sns.pairplot(df)\n    \n    return covariance_df, pair_plot", "raw_solution": "import seaborn as sns\nimport numpy as np\nimport pandas as pd\n\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    if not all(np.issubdtype(dtype, np.number) for dtype in df.dtypes):\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n    \n    covariance_df = df.cov()\n    pair_plot = sns.pairplot(df)\n    \n    return covariance_df, pair_plot\n```\n"}
{"task_id": "BigCodeBench/984", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, x_column, y_column):\n    # Extracting the data\n    X = df[[x_column]].values\n    y = df[y_column].values\n\n    # Creating and training the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predicting values\n    y_pred = model.predict(X)\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    ax.scatter(X, y, color='blue', label='Data points')\n    ax.plot(X, y_pred, color='red', label='Regression line')\n    ax.set_xlabel(x_column)\n    ax.set_ylabel(y_column)\n    ax.set_title('Scatter Plot with Regression Line')\n    ax.legend()\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, x_column, y_column):\n    # Extracting the data\n    X = df[[x_column]].values\n    y = df[y_column].values\n\n    # Creating and training the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predicting values\n    y_pred = model.predict(X)\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    ax.scatter(X, y, color='blue', label='Data points')\n    ax.plot(X, y_pred, color='red', label='Regression line')\n    ax.set_xlabel(x_column)\n    ax.set_ylabel(y_column)\n    ax.set_title('Scatter Plot with Regression Line')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        data = json.loads(json_data)\n        if not data or \"country\" not in data:\n            raise ValueError(\"Invalid JSON data\")\n\n        country_data = data[\"country\"]\n        if not isinstance(country_data, dict):\n            raise ValueError(\"Invalid country data format\")\n\n        report_data = []\n        for country, population in country_data.items():\n            if not isinstance(country, str) or not isinstance(population, (int, float)):\n                raise ValueError(\"Invalid country name or population format\")\n            if population < 0:\n                raise ValueError(\"Population cannot be negative\")\n            report_data.append({\"country\": country, \"population\": math.floor(population)})\n\n        df = pd.DataFrame(report_data)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        file_path = os.path.join(output_dir, file_name)\n        df.to_csv(file_path, index=False)\n\n        return file_path, df\n\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data\")\n    except OSError as e:\n        raise IOError(f\"Error writing to file: {e}\")", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        data = json.loads(json_data)\n        if not data or \"country\" not in data:\n            raise ValueError(\"Invalid JSON data\")\n\n        country_data = data[\"country\"]\n        if not isinstance(country_data, dict):\n            raise ValueError(\"Invalid country data format\")\n\n        report_data = []\n        for country, population in country_data.items():\n            if not isinstance(country, str) or not isinstance(population, (int, float)):\n                raise ValueError(\"Invalid country name or population format\")\n            if population < 0:\n                raise ValueError(\"Population cannot be negative\")\n            report_data.append({\"country\": country, \"population\": math.floor(population)})\n\n        df = pd.DataFrame(report_data)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        file_path = os.path.join(output_dir, file_name)\n        df.to_csv(file_path, index=False)\n\n        return file_path, df\n\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data\")\n    except OSError as e:\n        raise IOError(f\"Error writing to file: {e}\")\n```\n"}
{"task_id": "BigCodeBench/986", "solution": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ndef task_func(json_data: str, key_path: list):\n    # Convert JSON string to Python dictionary\n    data = json.loads(json_data)\n    \n    # Traverse the dictionary using the key path\n    for key in key_path:\n        if key not in data:\n            raise KeyError(f\"Key '{key}' not found in JSON data.\")\n        data = data[key]\n    \n    # Ensure the data is a list of numbers\n    if not isinstance(data, list) or not all(isinstance(i, (int, float)) for i in data):\n        raise ValueError(\"Data must be a list of numbers.\")\n    \n    # Convert list to numpy array for easy manipulation\n    data_array = np.array(data)\n    \n    # Create a pandas DataFrame for easier plotting\n    df = pd.DataFrame(data_array, columns=['Data'])\n    \n    # Create a boxplot using seaborn\n    fig, ax = plt.subplots()\n    sns.boxplot(x='Data', data=df, ax=ax)\n    \n    return fig\njson_data = '{\"level1\":{\"level2\":{\"data\":\"1,2,3,4\"}}}'\nkey_path = ['level1', 'level2', 'data']", "raw_solution": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef task_func(json_data: str, key_path: list):\n    # Convert JSON string to Python dictionary\n    data = json.loads(json_data)\n    \n    # Traverse the dictionary using the key path\n    for key in key_path:\n        if key not in data:\n            raise KeyError(f\"Key '{key}' not found in JSON data.\")\n        data = data[key]\n    \n    # Ensure the data is a list of numbers\n    if not isinstance(data, list) or not all(isinstance(i, (int, float)) for i in data):\n        raise ValueError(\"Data must be a list of numbers.\")\n    \n    # Convert list to numpy array for easy manipulation\n    data_array = np.array(data)\n    \n    # Create a pandas DataFrame for easier plotting\n    df = pd.DataFrame(data_array, columns=['Data'])\n    \n    # Create a boxplot using seaborn\n    fig, ax = plt.subplots()\n    sns.boxplot(x='Data', data=df, ax=ax)\n    \n    return fig\n\n# Example usage:\njson_data = '{\"level1\":{\"level2\":{\"data\":\"1,2,3,4\"}}}'\nkey_path = ['level1', 'level2', 'data']\nfig = task_func(json_data, key_path)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/987", "solution": "import json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(json_data: str, data_key: str):\n    # Load the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the numerical data using the provided key\n    try:\n        values = pd.Series(data)\n        for key in data_key.split('.'):\n            values = values.get(key, None)\n            if values is None:\n                raise KeyError(f\"Key '{key}' not found in the JSON data.\")\n        values = pd.Series(values)\n    except KeyError as e:\n        raise KeyError(f\"Error accessing data: {e}\")\n    \n    # Normalize the data using Min-Max scaling\n    scaler = MinMaxScaler()\n    normalized_values = scaler.fit_transform(values.values.reshape(-1, 1)).flatten()\n    \n    # Create a line plot of the original and normalized data\n    fig, ax = plt.subplots()\n    ax.plot(values, label='Original Data', color='blue')\n    ax.plot(normalized_values, label='Normalized Data', color='red')\n    ax.set_title('Comparison of Original and Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    return values, pd.Series(normalized_values), ax", "raw_solution": "import json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(json_data: str, data_key: str):\n    # Load the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the numerical data using the provided key\n    try:\n        values = pd.Series(data)\n        for key in data_key.split('.'):\n            values = values.get(key, None)\n            if values is None:\n                raise KeyError(f\"Key '{key}' not found in the JSON data.\")\n        values = pd.Series(values)\n    except KeyError as e:\n        raise KeyError(f\"Error accessing data: {e}\")\n    \n    # Normalize the data using Min-Max scaling\n    scaler = MinMaxScaler()\n    normalized_values = scaler.fit_transform(values.values.reshape(-1, 1)).flatten()\n    \n    # Create a line plot of the original and normalized data\n    fig, ax = plt.subplots()\n    ax.plot(values, label='Original Data', color='blue')\n    ax.plot(normalized_values, label='Normalized Data', color='red')\n    ax.set_title('Comparison of Original and Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    return values, pd.Series(normalized_values), ax\n\n# Example usage:\njson_str = '{\"data\": {\"values\": [5, 10, 15, 20, 25]}}'\noriginal_data, normalized_data, ax = task_func(json_str, 'data.values')\n```\n"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    if not os.path.exists(dir_path) or not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The directory {dir_path} does not exist or is not a directory.\")\n    \n    if not all(predicate in ['is_file', 'is_dir', 'has_special_chars', 'has_numbers'] for predicate in predicates):\n        raise ValueError(\"Invalid predicates provided.\")\n    \n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = Path(dir_path) / item\n        item_result = {}\n        for predicate in predicates:\n            if predicate == 'is_file':\n                item_result[predicate] = item_path.is_file()\n            elif predicate == 'is_dir':\n                item_result[predicate] = item_path.is_dir()\n            elif predicate == 'has_special_chars':\n                item_result[predicate] = bool(re.search(r'[^a-zA-Z0-9_]', item))\n            elif predicate == 'has_numbers':\n                item_result[predicate] = bool(re.search(r'\\d', item))\n        result[item] = item_result\n    \n    return result", "raw_solution": "import os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    if not os.path.exists(dir_path) or not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The directory {dir_path} does not exist or is not a directory.\")\n    \n    if not all(predicate in ['is_file', 'is_dir', 'has_special_chars', 'has_numbers'] for predicate in predicates):\n        raise ValueError(\"Invalid predicates provided.\")\n    \n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = Path(dir_path) / item\n        item_result = {}\n        for predicate in predicates:\n            if predicate == 'is_file':\n                item_result[predicate] = item_path.is_file()\n            elif predicate == 'is_dir':\n                item_result[predicate] = item_path.is_dir()\n            elif predicate == 'has_special_chars':\n                item_result[predicate] = bool(re.search(r'[^a-zA-Z0-9_]', item))\n            elif predicate == 'has_numbers':\n                item_result[predicate] = bool(re.search(r'\\d', item))\n        result[item] = item_result\n    \n    return result\n\n# Example usage:\n# print(task_func('/path/to/dir', ['is_file', 'has_numbers']))\n```\n"}
{"task_id": "BigCodeBench/989", "solution": "import random\nimport string\ndef task_func(length: int, predicates: list, seed: int = None):\n    if seed is not None:\n        random.seed(seed)\n\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative integer.\")\n\n    characters = string.ascii_letters + string.digits + string.punctuation\n\n    def has_uppercase(s):\n        return any(c.isupper() for c in s)\n\n    def has_lowercase(s):\n        return any(c.islower() for c in s)\n\n    def has_special_chars(s):\n        return any(c in string.punctuation for c in s)\n\n    def has_numbers(s):\n        return any(c.isdigit() for c in s)\n\n    predicates_map = {\n        'has_uppercase': has_uppercase,\n        'has_lowercase': has_lowercase,\n        'has_special_chars': has_special_chars,\n        'has_numbers': has_numbers\n    }\n\n    result = {}\n    for predicate in predicates:\n        if predicate not in predicates_map:\n            raise KeyError(f\"Invalid predicate: {predicate}\")\n        result[predicate] = predicates_map[predicate](''.join(random.choices(characters, k=length)))\n\n    random_string = ''.join(random.choices(characters, k=length))\n    return random_string, result", "raw_solution": "import random\nimport string\n\ndef task_func(length: int, predicates: list, seed: int = None):\n    if seed is not None:\n        random.seed(seed)\n\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative integer.\")\n\n    characters = string.ascii_letters + string.digits + string.punctuation\n\n    def has_uppercase(s):\n        return any(c.isupper() for c in s)\n\n    def has_lowercase(s):\n        return any(c.islower() for c in s)\n\n    def has_special_chars(s):\n        return any(c in string.punctuation for c in s)\n\n    def has_numbers(s):\n        return any(c.isdigit() for c in s)\n\n    predicates_map = {\n        'has_uppercase': has_uppercase,\n        'has_lowercase': has_lowercase,\n        'has_special_chars': has_special_chars,\n        'has_numbers': has_numbers\n    }\n\n    result = {}\n    for predicate in predicates:\n        if predicate not in predicates_map:\n            raise KeyError(f\"Invalid predicate: {predicate}\")\n        result[predicate] = predicates_map[predicate](''.join(random.choices(characters, k=length)))\n\n    random_string = ''.join(random.choices(characters, k=length))\n    return random_string, result\n\n# Example usage:\n# print(task_func(10, ['has_uppercase', 'has_numbers'], seed=42)[0])\n# print(task_func(5, ['has_lowercase'], seed=123))\n```\n"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    try:\n        # Convert hexadecimal string to bytes\n        byte_data = binascii.unhexlify(hex_string)\n        \n        # Convert bytes to utf-8 string\n        utf8_string = byte_data.decode('utf-8')\n        \n        # Encode in various formats\n        encoded_data = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('utf-8'),\n            'utf-8': utf8_string,\n            'utf-16': utf8_string.encode('utf-16').decode('utf-8'),\n            'utf-32': utf8_string.encode('utf-32').decode('utf-8'),\n            'ASCII': utf8_string.encode('ascii', 'ignore').decode('utf-8') if utf8_string.isascii() else 'ascii mein representable nahin',\n            'URL': urllib.parse.quote(utf8_string),\n            'ROT13': codecs.encode(utf8_string, 'rot_13')\n        }\n        \n        return encoded_data\n    except binascii.Error:\n        return {'error': 'Invalid hexadecimal string'}", "raw_solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    try:\n        # Convert hexadecimal string to bytes\n        byte_data = binascii.unhexlify(hex_string)\n        \n        # Convert bytes to utf-8 string\n        utf8_string = byte_data.decode('utf-8')\n        \n        # Encode in various formats\n        encoded_data = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('utf-8'),\n            'utf-8': utf8_string,\n            'utf-16': utf8_string.encode('utf-16').decode('utf-8'),\n            'utf-32': utf8_string.encode('utf-32').decode('utf-8'),\n            'ASCII': utf8_string.encode('ascii', 'ignore').decode('utf-8') if utf8_string.isascii() else 'ascii mein representable nahin',\n            'URL': urllib.parse.quote(utf8_string),\n            'ROT13': codecs.encode(utf8_string, 'rot_13')\n        }\n        \n        return encoded_data\n    except binascii.Error:\n        return {'error': 'Invalid hexadecimal string'}\n\n# Example usage:\n# print(task_func(\"4a4b4c\"))\n```\n"}
{"task_id": "BigCodeBench/991", "solution": "import binascii\nimport string\nimport random\ndef task_func(length):\n    if length < 1:\n        raise ValueError(\"Length must be at least 1\")\n\n    # Generate a random hexadecimal string of the given length\n    hex_string = ''.join(random.choice(string.hexdigits) for _ in range(length))\n\n    # Convert the hexadecimal string to ASCII\n    try:\n        ascii_string = binascii.unhexlify(hex_string).decode('utf-8', errors='ignore')\n    except binascii.Error:\n        raise ValueError(\"Invalid hexadecimal string\")\n\n    return ascii_string", "raw_solution": "import binascii\nimport string\nimport random\n\ndef task_func(length):\n    if length < 1:\n        raise ValueError(\"Length must be at least 1\")\n\n    # Generate a random hexadecimal string of the given length\n    hex_string = ''.join(random.choice(string.hexdigits) for _ in range(length))\n\n    # Convert the hexadecimal string to ASCII\n    try:\n        ascii_string = binascii.unhexlify(hex_string).decode('utf-8', errors='ignore')\n    except binascii.Error:\n        raise ValueError(\"Invalid hexadecimal string\")\n\n    return ascii_string\n\n# Example usage:\n# random.seed(0)\n# print(task_func(6))  # Output: '\\\\\\\\x18'\n# print(task_func(8))  # Output: 'P'\n```\n"}
{"task_id": "BigCodeBench/992", "solution": "import sys\nimport sqlite3\nPATH_TO_APPEND = \"path/to/whatever\"\nDATABASE = \"path/to/database.db\"\ndef task_func(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    \"\"\"\n    Yah function `sys.path` mein diye gaye `path` ko jorta hai aur `path` ke saath ek sqlite `database` ko update karta hai,\n    zaroorat parne par `table` banata hai aur `duplicates` se bachata hai.\n\n    Parameters:\n    - path_to_append (str): `sys.path` mein jorne aur sqlite `database` mein dalne ke liye ek ph'a'il `system` `path`. Yadi nirdisht nahin hai, to 'path/to/whatever' defaults hota hai.\n    - database (str): sqlite `database` ph'a'il ka ph'a'il `system` `path`. Yadi pradaan nahin kiya gaya, to 'path/to/database.db' defaults hota hai. function is `database` ke saath baatchit karta hai\n    - path sangrahit karne ke liye.\n\n    Returns:\n        str: Vah `path` jise `sys.path` mein jora gaya aur `database` mein dala gaya.\n\n    Requirements:\n    - sys\n    - sqlite3\n\n    Example:\n    udaharan:\n    >>> task_func('path/to/new_new_directory', 'path/to/new_database.db')\n    'path/to/new_new_directory'\n    >>> task_func()\n    'path/to/whatever'\n\n    \"\"\"\n    # Append the path to sys.path\n    sys_path = sys.path.append(path_to_append)\n\n    # Connect to the database\n    conn = sqlite3.connect(database)\n    cursor = conn.cursor()\n\n    # Create a table if it doesn't exist\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS my_table (\n            id INTEGER PRIMARY KEY,\n            data TEXT\n        )\n    ''')\n\n    # Insert data into the table\n    cursor.execute(\"INSERT INTO my_table (data) VALUES ('test_data')\")\n    conn.commit()\n\n    # Close the connection\n    conn.close()\n\n    return path_to_append", "raw_solution": "import sys\nimport sqlite3\n\n# Constants\nPATH_TO_APPEND = \"path/to/whatever\"\nDATABASE = \"path/to/database.db\"\n\n\ndef task_func(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    \"\"\"\n    Yah function `sys.path` mein diye gaye `path` ko jorta hai aur `path` ke saath ek sqlite `database` ko update karta hai,\n    zaroorat parne par `table` banata hai aur `duplicates` se bachata hai.\n\n    Parameters:\n    - path_to_append (str): `sys.path` mein jorne aur sqlite `database` mein dalne ke liye ek ph'a'il `system` `path`. Yadi nirdisht nahin hai, to 'path/to/whatever' defaults hota hai.\n    - database (str): sqlite `database` ph'a'il ka ph'a'il `system` `path`. Yadi pradaan nahin kiya gaya, to 'path/to/database.db' defaults hota hai. function is `database` ke saath baatchit karta hai\n    - path sangrahit karne ke liye.\n\n    Returns:\n        str: Vah `path` jise `sys.path` mein jora gaya aur `database` mein dala gaya.\n\n    Requirements:\n    - sys\n    - sqlite3\n\n    Example:\n    udaharan:\n    >>> task_func('path/to/new_new_directory', 'path/to/new_database.db')\n    'path/to/new_new_directory'\n    >>> task_func()\n    'path/to/whatever'\n\n    \"\"\"\n    # Append the path to sys.path\n    sys_path = sys.path.append(path_to_append)\n\n    # Connect to the database\n    conn = sqlite3.connect(database)\n    cursor = conn.cursor()\n\n    # Create a table if it doesn't exist\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS my_table (\n            id INTEGER PRIMARY KEY,\n            data TEXT\n        )\n    ''')\n\n    # Insert data into the table\n    cursor.execute(\"INSERT INTO my_table (data) VALUES ('test_data')\")\n    conn.commit()\n\n    # Close the connection\n    conn.close()\n\n    return path_to_append\n```\n"}
{"task_id": "BigCodeBench/993", "solution": "import re\nfrom scipy.stats import gaussian_kde\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram of word lengths\n    fig, ax = plt.subplots()\n    ax.hist(word_lengths, bins=range(1, max(word_lengths) + 2), alpha=0.5, label='Histogram')\n    \n    # Calculate KDE if there are more than 2 data points\n    if len(word_lengths) > 2:\n        kde = gaussian_kde(word_lengths)\n        x_range = range(1, max(word_lengths) + 1)\n        ax.plot(x_range, kde(x_range), label='KDE', linestyle='--')\n    \n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    ax.legend()\n    \n    return ax", "raw_solution": "import re\nfrom scipy.stats import gaussian_kde\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram of word lengths\n    fig, ax = plt.subplots()\n    ax.hist(word_lengths, bins=range(1, max(word_lengths) + 2), alpha=0.5, label='Histogram')\n    \n    # Calculate KDE if there are more than 2 data points\n    if len(word_lengths) > 2:\n        kde = gaussian_kde(word_lengths)\n        x_range = range(1, max(word_lengths) + 1)\n        ax.plot(x_range, kde(x_range), label='KDE', linestyle='--')\n    \n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    ax.legend()\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/994", "solution": "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\ndef task_func(url: str, csv_file_path: str) -> list:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise RuntimeError(f\"Error fetching url: {e}\")\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    articles = soup.find_all('div', class_='container')\n    data = []\n\n    for article in articles:\n        title = article.find('h1').text if article.find('h1') else 'No Title'\n        date = article.find('span', class_='date').text if article.find('span', class_='date') else 'No Date'\n        author = article.find('span', class_='author').text if article.find('span', class_='author') else 'No Author'\n        data.append({'Title': title, 'Date': date, 'Author': author})\n\n    df = pd.DataFrame(data)\n    df.to_csv(csv_file_path, index=False)\n\n    return data", "raw_solution": "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\ndef task_func(url: str, csv_file_path: str) -> list:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise RuntimeError(f\"Error fetching url: {e}\")\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    articles = soup.find_all('div', class_='container')\n    data = []\n\n    for article in articles:\n        title = article.find('h1').text if article.find('h1') else 'No Title'\n        date = article.find('span', class_='date').text if article.find('span', class_='date') else 'No Date'\n        author = article.find('span', class_='author').text if article.find('span', class_='author') else 'No Author'\n        data.append({'Title': title, 'Date': date, 'Author': author})\n\n    df = pd.DataFrame(data)\n    df.to_csv(csv_file_path, index=False)\n\n    return data\n```\n"}
{"task_id": "BigCodeBench/995", "solution": "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(file_path: str, plot_path: str) -> (float, float, str):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    # Reading the CSV file\n    df = pd.read_csv(file_path)\n    \n    # Checking if the DataFrame is empty\n    if df.empty:\n        return np.nan, np.nan, plot_path\n    \n    # Extracting the values from the DataFrame\n    values = df.values.flatten()\n    \n    # Checking if all values are numeric\n    if not all(isinstance(x, (int, float)) for x in values):\n        return np.nan, np.nan, plot_path\n    \n    # Calculating the average and median\n    average = np.mean(values)\n    median = np.median(values)\n    \n    # Plotting the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(values, marker='o', linestyle='-')\n    plt.title(\"Data Visualization\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n    plt.savefig(plot_path)\n    \n    return average, median, plot_path", "raw_solution": "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(file_path: str, plot_path: str) -> (float, float, str):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    # Reading the CSV file\n    df = pd.read_csv(file_path)\n    \n    # Checking if the DataFrame is empty\n    if df.empty:\n        return np.nan, np.nan, plot_path\n    \n    # Extracting the values from the DataFrame\n    values = df.values.flatten()\n    \n    # Checking if all values are numeric\n    if not all(isinstance(x, (int, float)) for x in values):\n        return np.nan, np.nan, plot_path\n    \n    # Calculating the average and median\n    average = np.mean(values)\n    median = np.median(values)\n    \n    # Plotting the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(values, marker='o', linestyle='-')\n    plt.title(\"Data Visualization\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n    plt.savefig(plot_path)\n    \n    return average, median, plot_path\n```\n"}
{"task_id": "BigCodeBench/996", "solution": "import requests\nimport json\nfrom bs4 import BeautifulSoup\ndef task_func(url: str, file_name: str = \"Output.txt\") -> str:\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    title = soup.find('title')\n\n    data = {'title': title.string if title else 'none'}\n    \n    with open(file_name, 'a') as f:\n        json.dump(data, f)\n        f.write('\\n')\n\n    return file_name", "raw_solution": "import requests\nimport json\nfrom bs4 import BeautifulSoup\n\ndef task_func(url: str, file_name: str = \"Output.txt\") -> str:\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    title = soup.find('title')\n\n    data = {'title': title.string if title else 'none'}\n    \n    with open(file_name, 'a') as f:\n        json.dump(data, f)\n        f.write('\\n')\n\n    return file_name\n\n# Example usage:\n# print(task_func(\"http://example.com\"))\n# print(task_func(\"http://another-example.com\", \"AnotherOutput.txt\"))\n```\nThis script uses the `requests` library to fetch the content of the web page, `BeautifulSoup` from `bs4` to parse the HTML content, and writes the title to a JSON file. If the title is not found, it writes 'none' to the file. The file is opened in append mode so that new data can be added without overwriting existing data.\n"}
{"task_id": "BigCodeBench/997", "solution": "import urllib.request\nimport os\nimport zipfile\nTARGET_DIR = \"downloaded_files\"\nTARGET_ZIP_FILE = \"downloaded_files.zip\"\ndef task_func(url):\n    try:\n        # Create the target directory if it doesn't exist\n        if not os.path.exists(TARGET_DIR):\n            os.makedirs(TARGET_DIR)\n\n        # Download the zip file\n        zip_path = os.path.join(TARGET_DIR, TARGET_ZIP_FILE)\n        urllib.request.urlretrieve(url, zip_path)\n\n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(TARGET_DIR)\n\n        # Remove the zip file after extraction\n        os.remove(zip_path)\n\n        return TARGET_DIR\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None", "raw_solution": "import urllib.request\nimport os\nimport zipfile\n\n# Constants\nTARGET_DIR = \"downloaded_files\"\nTARGET_ZIP_FILE = \"downloaded_files.zip\"\n\n\ndef task_func(url):\n    try:\n        # Create the target directory if it doesn't exist\n        if not os.path.exists(TARGET_DIR):\n            os.makedirs(TARGET_DIR)\n\n        # Download the zip file\n        zip_path = os.path.join(TARGET_DIR, TARGET_ZIP_FILE)\n        urllib.request.urlretrieve(url, zip_path)\n\n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(TARGET_DIR)\n\n        # Remove the zip file after extraction\n        os.remove(zip_path)\n\n        return TARGET_DIR\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n```\n"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    # Download the tar.gz file\n    try:\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Failed to download the file: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    checksum = md5_hash.hexdigest()\n\n    # Verify the checksum\n    if checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"MD5 checksum does not match.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Failed to extract the tar.gz file: {e}\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Clean up the downloaded file\n    os.remove(TARGET_TAR_FILE)\n    return True", "raw_solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\n\ndef task_func(url):\n    # Download the tar.gz file\n    try:\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Failed to download the file: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    checksum = md5_hash.hexdigest()\n\n    # Verify the checksum\n    if checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"MD5 checksum does not match.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Failed to extract the tar.gz file: {e}\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Clean up the downloaded file\n    os.remove(TARGET_TAR_FILE)\n    return True\n```\n"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    # Download the CSV file\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise Exception(f\"Error downloading the file: {e}\")\n\n    # Read the CSV file and count occurrences of the specified column\n    try:\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"Column '{column_name}' not found in the CSV file.\")\n            \n            counter = collections.Counter()\n            for row in reader:\n                counter[row[column_name]] += 1\n\n            # Remove the downloaded file\n            os.remove(csv_file_path)\n\n            return dict(counter)\n    except Exception as e:\n        raise Exception(f\"Error processing the file: {e}\")", "raw_solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    # Download the CSV file\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise Exception(f\"Error downloading the file: {e}\")\n\n    # Read the CSV file and count occurrences of the specified column\n    try:\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"Column '{column_name}' not found in the CSV file.\")\n            \n            counter = collections.Counter()\n            for row in reader:\n                counter[row[column_name]] += 1\n\n            # Remove the downloaded file\n            os.remove(csv_file_path)\n\n            return dict(counter)\n    except Exception as e:\n        raise Exception(f\"Error processing the file: {e}\")\n```\n"}
{"task_id": "BigCodeBench/1000", "solution": "import urllib.request\nimport os\nimport json\nimport pandas as pd\nTARGET_JSON_FILE = \"downloaded_file.json\"\ndef task_func(url):\n    # Download the JSON file\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n\n    # Read the JSON file\n    with open(TARGET_JSON_FILE, 'r') as file:\n        data = json.load(file)\n\n    # Convert JSON data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Remove the temporary JSON file\n    os.remove(TARGET_JSON_FILE)\n\n    return df", "raw_solution": "import urllib.request\nimport os\nimport json\nimport pandas as pd\n\n# Constants\nTARGET_JSON_FILE = \"downloaded_file.json\"\n\n\ndef task_func(url):\n    # Download the JSON file\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n\n    # Read the JSON file\n    with open(TARGET_JSON_FILE, 'r') as file:\n        data = json.load(file)\n\n    # Convert JSON data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Remove the temporary JSON file\n    os.remove(TARGET_JSON_FILE)\n\n    return df\n\n# Example usage:\n# df = task_func('http://example.com/employees.json')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/1001", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path: str):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Check if 'column1' exists in the DataFrame\n    if 'column1' not in df.columns:\n        raise ValueError(\"CSV file must contain 'column1' column\")\n\n    # Normalize the 'column1' data\n    df['column1'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n\n    # Plot the normalized data\n    fig, ax = plt.subplots()\n    ax.plot(df['column1'])\n\n    # Set the title and labels with formatting\n    title = \"{:<20} :  {:<20}\".format(\"Plot Title\", \"Normalized Column 1\")\n    xlabel = \"{:<20} :  {:<20}\".format(\"Index\", \"Normalized Value\")\n    ylabel = \"{:<20} :  {:<20}\".format(\"Frequency\", \"Normalized Value\")\n\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file_path: str):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Check if 'column1' exists in the DataFrame\n    if 'column1' not in df.columns:\n        raise ValueError(\"CSV file must contain 'column1' column\")\n\n    # Normalize the 'column1' data\n    df['column1'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n\n    # Plot the normalized data\n    fig, ax = plt.subplots()\n    ax.plot(df['column1'])\n\n    # Set the title and labels with formatting\n    title = \"{:<20} :  {:<20}\".format(\"Plot Title\", \"Normalized Column 1\")\n    xlabel = \"{:<20} :  {:<20}\".format(\"Index\", \"Normalized Value\")\n    ylabel = \"{:<20} :  {:<20}\".format(\"Frequency\", \"Normalized Value\")\n\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1002", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, column_name=\"target_column\"):\n    # Convert JSON data to Pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n\n    # Plot histogram of the specified column\n    ax = df[column_name].plot(kind='hist', title=f'Histogram of {column_name}', figsize=(10, 6))\n    ax.set_xlabel(column_name)\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column_name=\"target_column\"):\n    # Convert JSON data to Pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n\n    # Plot histogram of the specified column\n    ax = df[column_name].plot(kind='hist', title=f'Histogram of {column_name}', figsize=(10, 6))\n    ax.set_xlabel(column_name)\n\n    return df, ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n        if root.tag != 'root':\n            raise ValueError(\"xml structure expected format se mel nahi khati.\")\n\n        # Extract data from XML and create a DataFrame\n        data = []\n        for item in root.findall('item'):\n            row = {}\n            for child in item:\n                row[child.tag] = child.text\n            data.append(row)\n\n        df = pd.DataFrame(data)\n\n        return df\n\n    except urllib.error.URLError as e:\n        raise ValueError(f\"URL error: {e}\")\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"XML syntax error: {e}\")\n    except ValueError as e:\n        raise ValueError(f\"Value error: {e}\")", "raw_solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n        if root.tag != 'root':\n            raise ValueError(\"xml structure expected format se mel nahi khati.\")\n\n        # Extract data from XML and create a DataFrame\n        data = []\n        for item in root.findall('item'):\n            row = {}\n            for child in item:\n                row[child.tag] = child.text\n            data.append(row)\n\n        df = pd.DataFrame(data)\n\n        return df\n\n    except urllib.error.URLError as e:\n        raise ValueError(f\"URL error: {e}\")\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"XML syntax error: {e}\")\n    except ValueError as e:\n        raise ValueError(f\"Value error: {e}\")\n\n# Example usage:\n# df = task_func('http://example.com/sample_data.xml')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the text file from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Extract words using regular expressions\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # Plot the most common words\n    common_words = word_freq.most_common(10)\n    words, counts = zip(*common_words)\n    \n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 most common words')\n\n    return word_freq, ax", "raw_solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the text file from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Extract words using regular expressions\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # Plot the most common words\n    common_words = word_freq.most_common(10)\n    words, counts = zip(*common_words)\n    \n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 most common words')\n\n    return word_freq, ax\n```\n"}
{"task_id": "BigCodeBench/1005", "solution": "import urllib.request\nimport zipfile\nimport os\nimport urllib.error\ndef task_func(\n    url: str,\n    save_path: str = \"downloaded_file.zip\",\n    extract_path: str = \"extracted_files\",\n) -> str:\n    try:\n        # Download the zip file\n        urllib.request.urlretrieve(url, save_path)\n\n        # Create the extract directory if it doesn't exist\n        if not os.path.exists(extract_path):\n            os.makedirs(extract_path)\n\n        # Extract the zip file\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_path)\n\n        # Return the path to the extracted files\n        return extract_path\n\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"URL error: {e.reason}\")\n\n    except zipfile.BadZipFile:\n        raise zipfile.BadZipFile(\"The downloaded file is not a valid zip file.\")\n\n    except OSError as e:\n        raise OSError(f\"File I/O error: {e.strerror}\")\n\n    finally:\n        # Clean up the downloaded zip file\n        if os.path.exists(save_path):\n            os.remove(save_path)", "raw_solution": "import urllib.request\nimport zipfile\nimport os\nimport urllib.error\n\ndef task_func(\n    url: str,\n    save_path: str = \"downloaded_file.zip\",\n    extract_path: str = \"extracted_files\",\n) -> str:\n    try:\n        # Download the zip file\n        urllib.request.urlretrieve(url, save_path)\n\n        # Create the extract directory if it doesn't exist\n        if not os.path.exists(extract_path):\n            os.makedirs(extract_path)\n\n        # Extract the zip file\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_path)\n\n        # Return the path to the extracted files\n        return extract_path\n\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"URL error: {e.reason}\")\n\n    except zipfile.BadZipFile:\n        raise zipfile.BadZipFile(\"The downloaded file is not a valid zip file.\")\n\n    except OSError as e:\n        raise OSError(f\"File I/O error: {e.strerror}\")\n\n    finally:\n        # Clean up the downloaded zip file\n        if os.path.exists(save_path):\n            os.remove(save_path)\n```\n"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        if response.status_code != 200:\n            return \"Error: Diye gaye url se file downloads karane mein unable vaapas karta hai.\"\n\n        # Save the file\n        zip_path = os.path.join(download_path, \"downloaded.zip\")\n        with open(zip_path, \"wb\") as file:\n            file.write(response.content)\n\n        # Check if the file is a zip file\n        if not zipfile.is_zipfile(zip_path):\n            return \"Error: url zip file ki or ishara nahin karta.\"\n\n        # Extract the zip file\n        with ZipFile(zip_path, \"r\") as zip_ref:\n            zip_ref.extractall(download_path)\n\n        # Clean up the zip file\n        os.remove(zip_path)\n\n        return download_path\n\n    except requests.RequestException as e:\n        return f\"Error: Diye gaye url se file downloads karane mein unable vaapas karta hai. {str(e)}\"\n\n    except BadZipFile as e:\n        return \"Error: Download ki gayi file ek valid zip file nahin hai.\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"", "raw_solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        if response.status_code != 200:\n            return \"Error: Diye gaye url se file downloads karane mein unable vaapas karta hai.\"\n\n        # Save the file\n        zip_path = os.path.join(download_path, \"downloaded.zip\")\n        with open(zip_path, \"wb\") as file:\n            file.write(response.content)\n\n        # Check if the file is a zip file\n        if not zipfile.is_zipfile(zip_path):\n            return \"Error: url zip file ki or ishara nahin karta.\"\n\n        # Extract the zip file\n        with ZipFile(zip_path, \"r\") as zip_ref:\n            zip_ref.extractall(download_path)\n\n        # Clean up the zip file\n        os.remove(zip_path)\n\n        return download_path\n\n    except requests.RequestException as e:\n        return f\"Error: Diye gaye url se file downloads karane mein unable vaapas karta hai. {str(e)}\"\n\n    except BadZipFile as e:\n        return \"Error: Download ki gayi file ek valid zip file nahin hai.\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n```\n"}
{"task_id": "BigCodeBench/1007", "solution": "import requests\nimport pandas as pd\ndef task_func(url: str) -> pd.DataFrame:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raises HTTPError for bad responses\n    except requests.RequestException as e:\n        raise SystemError(f\"An error occurred while fetching data: {e}\")\n\n    try:\n        data = response.json()\n    except ValueError as e:\n        raise ValueError(\"The response is not valid JSON\") from e\n\n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise ValueError(\"The JSON data is not in a format suitable for conversion to a DataFrame\")\n\n    df = pd.DataFrame(data)\n\n    return df", "raw_solution": "import requests\nimport pandas as pd\n\ndef task_func(url: str) -> pd.DataFrame:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raises HTTPError for bad responses\n    except requests.RequestException as e:\n        raise SystemError(f\"An error occurred while fetching data: {e}\")\n\n    try:\n        data = response.json()\n    except ValueError as e:\n        raise ValueError(\"The response is not valid JSON\") from e\n\n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise ValueError(\"The JSON data is not in a format suitable for conversion to a DataFrame\")\n\n    df = pd.DataFrame(data)\n\n    return df\n\n# Example usage:\n# df = task_func('https://example.com/data.json')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        raise requests.exceptions.HTTPError(f\"HTTP error occurred: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', id=table_id)\n\n    if table is None:\n        raise ValueError(\"Table ID not found.\")\n\n    rows = table.find_all('tr')\n    data = []\n    for row in rows:\n        cols = row.find_all(['td', 'th'])\n        data.append([col.get_text(strip=True) for col in cols])\n\n    if not data:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(data[1:], columns=data[0])\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        raise requests.exceptions.HTTPError(f\"HTTP error occurred: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', id=table_id)\n\n    if table is None:\n        raise ValueError(\"Table ID not found.\")\n\n    rows = table.find_all('tr')\n    data = []\n    for row in rows:\n        cols = row.find_all(['td', 'th'])\n        data.append([col.get_text(strip=True) for col in cols])\n\n    if not data:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(data[1:], columns=data[0])\n\n    return df\n\n# Example usage:\n# df = task_func('https://example.com/data.html', 'table1')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/1009", "solution": "import xml.etree.ElementTree as ET\nimport csv\ndef task_func(xml_content, output_csv_path):\n    try:\n        root = ET.fromstring(xml_content)\n    except ET.ParseError as e:\n        raise ET.ParseError(f\"Error parsing XML content: {e}\")\n\n    try:\n        with open(output_csv_path, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            for element in root.findall('.//{*}element'):\n                writer.writerow([element.tag, element.text])\n    except IOError as e:\n        raise IOError(f\"Error writing to CSV file: {e}\")", "raw_solution": "import xml.etree.ElementTree as ET\nimport csv\n\n\ndef task_func(xml_content, output_csv_path):\n    try:\n        root = ET.fromstring(xml_content)\n    except ET.ParseError as e:\n        raise ET.ParseError(f\"Error parsing XML content: {e}\")\n\n    try:\n        with open(output_csv_path, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            for element in root.findall('.//{*}element'):\n                writer.writerow([element.tag, element.text])\n    except IOError as e:\n        raise IOError(f\"Error writing to CSV file: {e}\")\n\n# Example usage:\n# task_func('<root><element>data</element></root>', 'output.csv')\n```\n"}
{"task_id": "BigCodeBench/1010", "solution": "import requests\nfrom PIL import Image\nimport io\ndef task_func(url):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise ValueError(f\"Error downloading image: {e}\")\n\n    try:\n        image = Image.open(io.BytesIO(response.content))\n    except IOError as e:\n        raise ValueError(f\"Error processing image: {e}\")\n\n    return image", "raw_solution": "import requests\nfrom PIL import Image\nimport io\n\ndef task_func(url):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise ValueError(f\"Error downloading image: {e}\")\n\n    try:\n        image = Image.open(io.BytesIO(response.content))\n    except IOError as e:\n        raise ValueError(f\"Error processing image: {e}\")\n\n    return image\n\n# Example usage:\n# img = task_func('https://example.com/image.jpg')\n# print(isinstance(img, Image.Image))  # Should print True\n```\n"}
{"task_id": "BigCodeBench/1011", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the specified columns exist in the DataFrame\n    if col1_name not in df.columns or col2_name not in df.columns:\n        raise ValueError(f\"Columns {col1_name} or {col2_name} not found in the CSV file.\")\n\n    # Calculate the mean of col2_name grouped by col1_name\n    mean_values = df.groupby(col1_name)[col2_name].mean()\n\n    # Create a bar plot\n    ax = mean_values.plot(kind='bar', figsize=(10, 6), color='skyblue')\n\n    # Configure the plot\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    # Return the Axes object\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the specified columns exist in the DataFrame\n    if col1_name not in df.columns or col2_name not in df.columns:\n        raise ValueError(f\"Columns {col1_name} or {col2_name} not found in the CSV file.\")\n\n    # Calculate the mean of col2_name grouped by col1_name\n    mean_values = df.groupby(col1_name)[col2_name].mean()\n\n    # Create a bar plot\n    ax = mean_values.plot(kind='bar', figsize=(10, 6), color='skyblue')\n\n    # Configure the plot\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    # Return the Axes object\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    try:\n        # Ensure the download directory exists\n        DOWNLOAD_DIR.mkdir(exist_ok=True)\n        ZIP_DIR.mkdir(exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n\n        # Save the file\n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n\n        # Extract the zip file\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # List the extracted files\n        extracted_files = [f.name for f in ZIP_DIR.iterdir()]\n\n        return 'Download and extraction successful', extracted_files\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Network error occurred - {e}\", []\n    except zipfile.BadZipFile:\n        return \"Error: Bad zip file\", []\n    except Exception as e:\n        return f\"Error: {e}\", []", "raw_solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    try:\n        # Ensure the download directory exists\n        DOWNLOAD_DIR.mkdir(exist_ok=True)\n        ZIP_DIR.mkdir(exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n\n        # Save the file\n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n\n        # Extract the zip file\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # List the extracted files\n        extracted_files = [f.name for f in ZIP_DIR.iterdir()]\n\n        return 'Download and extraction successful', extracted_files\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Network error occurred - {e}\", []\n    except zipfile.BadZipFile:\n        return \"Error: Bad zip file\", []\n    except Exception as e:\n        return f\"Error: {e}\", []\n```\n"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    response = requests.get(urljoin(base_url, url))\n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = set()\n\n    for link in soup.find_all('a', href=True):\n        absolute_url = urljoin(base_url, link['href'])\n        links.add(absolute_url)\n\n    with open(csv_file, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for link in links:\n            writer.writerow([link])\n\n    return len(links)", "raw_solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    response = requests.get(urljoin(base_url, url))\n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = set()\n\n    for link in soup.find_all('a', href=True):\n        absolute_url = urljoin(base_url, link['href'])\n        links.add(absolute_url)\n\n    with open(csv_file, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for link in links:\n            writer.writerow([link])\n\n    return len(links)\n```\n"}
{"task_id": "BigCodeBench/1014", "solution": "import requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(api_url):\n    if not isinstance(api_url, str):\n        raise TypeError(\"api_url must be a string\")\n\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as err:\n        raise requests.exceptions.HTTPError(f\"HTTP error occurred: {err}\")\n\n    data = response.json()\n\n    if not data:\n        return pd.DataFrame(), None\n\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        return df, None\n\n    ax = df.plot(kind='line', title='Data Plot')\n    plt.xlabel('Index')\n    plt.ylabel('Values')\n    plt.show()\n\n    return df, ax", "raw_solution": "import requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(api_url):\n    if not isinstance(api_url, str):\n        raise TypeError(\"api_url must be a string\")\n\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as err:\n        raise requests.exceptions.HTTPError(f\"HTTP error occurred: {err}\")\n\n    data = response.json()\n\n    if not data:\n        return pd.DataFrame(), None\n\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        return df, None\n\n    ax = df.plot(kind='line', title='Data Plot')\n    plt.xlabel('Index')\n    plt.ylabel('Values')\n    plt.show()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n\n    tree = html.fromstring(response.content)\n    tables = tree.findall('.//table')\n\n    if not tables:\n        print(\"No tables found in the webpage.\")\n        return 0\n\n    table = tables[0]\n    df = pd.read_html(html.tostring(table))[0]\n\n    if df.empty:\n        print(\"No data found in the table.\")\n        return 0\n\n    try:\n        conn = sqlite3.connect(database_name)\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error storing data in the database: {e}\")\n        return 0\n    finally:\n        conn.close()\n\n    return len(df)", "raw_solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n\n    tree = html.fromstring(response.content)\n    tables = tree.findall('.//table')\n\n    if not tables:\n        print(\"No tables found in the webpage.\")\n        return 0\n\n    table = tables[0]\n    df = pd.read_html(html.tostring(table))[0]\n\n    if df.empty:\n        print(\"No data found in the table.\")\n        return 0\n\n    try:\n        conn = sqlite3.connect(database_name)\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error storing data in the database: {e}\")\n        return 0\n    finally:\n        conn.close()\n\n    return len(df)\n```\n"}
{"task_id": "BigCodeBench/1016", "solution": "import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error downloading image: {e}\")\n\n    try:\n        image = Image.open(BytesIO(response.content))\n    except IOError as e:\n        raise IOError(f\"Error processing image: {e}\")\n\n    # Convert image to grayscale\n    grayscale_image = image.convert(\"L\")\n    \n    # Convert image to numpy array\n    image_array = np.array(grayscale_image)\n    \n    # Calculate histogram\n    histogram = np.histogram(image_array, bins=256, range=(0, 255))[0]\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.plot(histogram, color='black')\n    ax.set_title('Grayscale Histogram')\n    ax.set_xlabel('Pixel Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax", "raw_solution": "import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error downloading image: {e}\")\n\n    try:\n        image = Image.open(BytesIO(response.content))\n    except IOError as e:\n        raise IOError(f\"Error processing image: {e}\")\n\n    # Convert image to grayscale\n    grayscale_image = image.convert(\"L\")\n    \n    # Convert image to numpy array\n    image_array = np.array(grayscale_image)\n    \n    # Calculate histogram\n    histogram = np.histogram(image_array, bins=256, range=(0, 255))[0]\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.plot(histogram, color='black')\n    ax.set_title('Grayscale Histogram')\n    ax.set_xlabel('Pixel Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/1017", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\ndef task_func(csv_file_path, target_column=\"target\", test_size=0.2, n_estimators=100):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n    \n    if target_column not in df.columns:\n        raise ValueError(f\"'{target_column}' column not found in the CSV file.\")\n    \n    # Split the dataset into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    \n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Predict the target for the test set\n    y_pred = clf.predict(X_test)\n    \n    # Generate and return the classification report\n    report = classification_report(y_test, y_pred, output_dict=True)\n    return report", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\ndef task_func(csv_file_path, target_column=\"target\", test_size=0.2, n_estimators=100):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n    \n    if target_column not in df.columns:\n        raise ValueError(f\"'{target_column}' column not found in the CSV file.\")\n    \n    # Split the dataset into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    \n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Predict the target for the test set\n    y_pred = clf.predict(X_test)\n    \n    # Generate and return the classification report\n    report = classification_report(y_test, y_pred, output_dict=True)\n    return report\n```\n"}
{"task_id": "BigCodeBench/1018", "solution": "from bs4 import BeautifulSoup\nimport requests\nURL = \"http://example.com\"\ndef task_func(url=URL, from_encoding=\"cp1251\", use_lxml=False):\n    if not url:\n        return None\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return None\n\n    if response.status_code != 200:\n        return None\n\n    try:\n        content = response.content.decode(from_encoding)\n    except (UnicodeDecodeError, LookupError):\n        return None\n\n    if use_lxml:\n        parser = \"lxml\"\n    else:\n        parser = \"html.parser\"\n\n    try:\n        soup = BeautifulSoup(content, parser)\n    except Exception:\n        return None\n\n    return soup", "raw_solution": "from bs4 import BeautifulSoup\nimport requests\n\n# Constants\nURL = \"http://example.com\"\n\n\ndef task_func(url=URL, from_encoding=\"cp1251\", use_lxml=False):\n    if not url:\n        return None\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return None\n\n    if response.status_code != 200:\n        return None\n\n    try:\n        content = response.content.decode(from_encoding)\n    except (UnicodeDecodeError, LookupError):\n        return None\n\n    if use_lxml:\n        parser = \"lxml\"\n    else:\n        parser = \"html.parser\"\n\n    try:\n        soup = BeautifulSoup(content, parser)\n    except Exception:\n        return None\n\n    return soup\n```\n"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        with Image.open(filename) as img:\n            # Use pytesseract to extract text from the image\n            extracted_text = pytesseract.image_to_string(img)\n\n        # Convert the extracted text from the original encoding to UTF-8\n        comment = codecs.decode(extracted_text, from_encoding).encode(to_encoding).decode(to_encoding)\n    except (UnicodeDecodeError, LookupError):\n        # If there's an error in decoding, return an empty string\n        comment = \"\"\n    except Exception as e:\n        # Handle any other exceptions\n        raise ValueError(f\"An error occurred: {e}\")\n\n    return comment", "raw_solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        with Image.open(filename) as img:\n            # Use pytesseract to extract text from the image\n            extracted_text = pytesseract.image_to_string(img)\n\n        # Convert the extracted text from the original encoding to UTF-8\n        comment = codecs.decode(extracted_text, from_encoding).encode(to_encoding).decode(to_encoding)\n    except (UnicodeDecodeError, LookupError):\n        # If there's an error in decoding, return an empty string\n        comment = \"\"\n    except Exception as e:\n        # Handle any other exceptions\n        raise ValueError(f\"An error occurred: {e}\")\n\n    return comment\n```\n"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n\n        content = response.content\n\n        if not content:\n            return {}\n\n        if from_encoding is None:\n            detected = chardet.detect(content)\n            from_encoding = detected['encoding']\n\n        if from_encoding != to_encoding:\n            content = content.decode(from_encoding).encode(to_encoding)\n\n        return json.loads(content.decode(to_encoding))\n\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error fetching data from {url}: {e}\")\n\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Error decoding JSON: {e}\")\n\n    except Exception as e:\n        raise ValueError(f\"An unexpected error occurred: {e}\")", "raw_solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n\n        content = response.content\n\n        if not content:\n            return {}\n\n        if from_encoding is None:\n            detected = chardet.detect(content)\n            from_encoding = detected['encoding']\n\n        if from_encoding != to_encoding:\n            content = content.decode(from_encoding).encode(to_encoding)\n\n        return json.loads(content.decode(to_encoding))\n\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error fetching data from {url}: {e}\")\n\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Error decoding JSON: {e}\")\n\n    except Exception as e:\n        raise ValueError(f\"An unexpected error occurred: {e}\")\n```\n"}
{"task_id": "BigCodeBench/1021", "solution": "import binascii\nimport hashlib\ndef task_func(input_string, verify_hash=None):\n    if not isinstance(input_string, str):\n        raise TypeError(\"input_string must be a string.\")\n    if verify_hash is not None and not isinstance(verify_hash, str):\n        raise TypeError(\"verify_hash must be a string or None.\")\n    \n    # Compute SHA256 hash of the input string\n    hash_object = hashlib.sha256(input_string.encode())\n    hex_dig = binascii.hexlify(hash_object.digest()).decode()\n    \n    # Verify the hash if provided\n    if verify_hash is not None:\n        return hex_dig == verify_hash, hex_dig\n    else:\n        return hex_dig", "raw_solution": "import binascii\nimport hashlib\n\ndef task_func(input_string, verify_hash=None):\n    if not isinstance(input_string, str):\n        raise TypeError(\"input_string must be a string.\")\n    if verify_hash is not None and not isinstance(verify_hash, str):\n        raise TypeError(\"verify_hash must be a string or None.\")\n    \n    # Compute SHA256 hash of the input string\n    hash_object = hashlib.sha256(input_string.encode())\n    hex_dig = binascii.hexlify(hash_object.digest()).decode()\n    \n    # Verify the hash if provided\n    if verify_hash is not None:\n        return hex_dig == verify_hash, hex_dig\n    else:\n        return hex_dig\n\n# Test cases"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the file.\")\n    \n    try:\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    except Exception as e:\n        raise ValueError(f\"Error converting {column_name} to datetime: {e}\")\n    \n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n    df = df.sort_values(by=column_name)\n    \n    return df", "raw_solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the file.\")\n    \n    try:\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    except Exception as e:\n        raise ValueError(f\"Error converting {column_name} to datetime: {e}\")\n    \n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n    df = df.sort_values(by=column_name)\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/1023", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(dataframe):\n    if dataframe.empty:\n        raise ValueError(\"Input dataframe is empty.\")\n    \n    if not all(np.issubdtype(dtype, np.number) for dtype in dataframe.dtypes):\n        raise TypeError(\"All columns in the dataframe must be numeric.\")\n    \n    if dataframe.shape[1] < 2:\n        raise ValueError(\"Dataframe must contain at least two columns.\")\n    \n    correlation_matrix = dataframe.corr()\n    highest_corr_column = correlation_matrix.abs().max().idxmax()\n    \n    fig, ax = plt.subplots()\n    ax.scatter(dataframe[highest_corr_column], dataframe.drop(columns=[highest_corr_column]).iloc[:, 0])\n    ax.set_title(f'Scatter plot of {highest_corr_column} vs {correlation_matrix.abs().max().idxmin()}')\n    ax.set_xlabel(highest_corr_column)\n    ax.set_ylabel(correlation_matrix.abs().max().idxmin())\n    \n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(dataframe):\n    if dataframe.empty:\n        raise ValueError(\"Input dataframe is empty.\")\n    \n    if not all(np.issubdtype(dtype, np.number) for dtype in dataframe.dtypes):\n        raise TypeError(\"All columns in the dataframe must be numeric.\")\n    \n    if dataframe.shape[1] < 2:\n        raise ValueError(\"Dataframe must contain at least two columns.\")\n    \n    correlation_matrix = dataframe.corr()\n    highest_corr_column = correlation_matrix.abs().max().idxmax()\n    \n    fig, ax = plt.subplots()\n    ax.scatter(dataframe[highest_corr_column], dataframe.drop(columns=[highest_corr_column]).iloc[:, 0])\n    ax.set_title(f'Scatter plot of {highest_corr_column} vs {correlation_matrix.abs().max().idxmin()}')\n    ax.set_xlabel(highest_corr_column)\n    ax.set_ylabel(correlation_matrix.abs().max().idxmin())\n    \n    return ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     'A': np.random.rand(100),\n#     'B': np.random.rand(100),\n#     'C': np.random.rand(100)\n# })\n# ax = task_func(df)\n# print(ax)\n```\n"}
{"task_id": "BigCodeBench/1024", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nPLOT_TITLE = \"Value Distribution\"\ndef task_func(data_dict):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(data_dict)\n    \n    # Drop rows with None values\n    df = df.dropna()\n    \n    # Check if DataFrame is empty or all values are the same\n    if df.empty or df.nunique().sum() == 1:\n        return df, None\n    \n    # Calculate bins dynamically\n    min_val = df.min().min()\n    max_val = df.max().max()\n    bins = np.arange(min_val, max_val + 1, (max_val - min_val) / 10).tolist()\n    bins = max(2, min(11, len(bins)))\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    sns.histplot(df.values.flatten(), bins=bins, kde=True, ax=ax)\n    ax.set_title(PLOT_TITLE)\n    \n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPLOT_TITLE = \"Value Distribution\"\n\ndef task_func(data_dict):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(data_dict)\n    \n    # Drop rows with None values\n    df = df.dropna()\n    \n    # Check if DataFrame is empty or all values are the same\n    if df.empty or df.nunique().sum() == 1:\n        return df, None\n    \n    # Calculate bins dynamically\n    min_val = df.min().min()\n    max_val = df.max().max()\n    bins = np.arange(min_val, max_val + 1, (max_val - min_val) / 10).tolist()\n    bins = max(2, min(11, len(bins)))\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    sns.histplot(df.values.flatten(), bins=bins, kde=True, ax=ax)\n    ax.set_title(PLOT_TITLE)\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/1025", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nPLOT_TITLE = \"Scaled Values\"\ndef task_func(data_dict):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Drop rows with None values\n    df = df.dropna()\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Scale the data\n    scaled_data = scaler.fit_transform(df)\n\n    # Create a new DataFrame with the scaled data\n    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n\n    # Plot the scaled data\n    ax = scaled_df.plot(kind='line', figsize=(10, 6))\n    ax.set_title(PLOT_TITLE)\n\n    return scaled_df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nPLOT_TITLE = \"Scaled Values\"\n\n\ndef task_func(data_dict):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Drop rows with None values\n    df = df.dropna()\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Scale the data\n    scaled_data = scaler.fit_transform(df)\n\n    # Create a new DataFrame with the scaled data\n    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n\n    # Plot the scaled data\n    ax = scaled_df.plot(kind='line', figsize=(10, 6))\n    ax.set_title(PLOT_TITLE)\n\n    return scaled_df, ax\n```\n"}
{"task_id": "BigCodeBench/1026", "solution": "import numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\ndef task_func(kwargs):\n    alpha = 0.05\n    nan_policy = 'omit'\n\n    group1 = kwargs.get('group1', [])\n    group2 = kwargs.get('group2', [])\n\n    if len(group1) < 2 or len(group2) < 2:\n        raise ValueError(\"Each group must have at least 2 non-nan_policy data points.\")\n\n    if np.var(group1, ddof=1) < 1e-8 or np.var(group2, ddof=1) < 1e-8:\n        raise ValueError(\"Each group must have sufficient variance.\")\n\n    group1 = np.array(group1)\n    group2 = np.array(group2)\n\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n\n    t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n\n    significant = p_value < alpha\n\n    group1_stats = {'mean': np.mean(group1), 'std': np.std(group1)}\n    group2_stats = {'mean': np.mean(group2), 'std': np.std(group2)}\n\n    fig, ax_boxplot = plt.subplots()\n    fig, ax_histogram = plt.subplots()\n\n    ax_boxplot.boxplot([group1, group2], labels=['Group 1', 'Group 2'])\n    ax_histogram.hist(group1, alpha=0.5, label='Group 1')\n    ax_histogram.hist(group2, alpha=0.5, label='Group 2')\n    ax_histogram.legend()\n\n    return {\n        'significant': significant,\n        'group1_stats': group1_stats,\n        'group2_stats': group2_stats,\n        'ax_boxplot': ax_boxplot,\n        'ax_histogram': ax_histogram\n    }", "raw_solution": "import numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\n\ndef task_func(kwargs):\n    alpha = 0.05\n    nan_policy = 'omit'\n\n    group1 = kwargs.get('group1', [])\n    group2 = kwargs.get('group2', [])\n\n    if len(group1) < 2 or len(group2) < 2:\n        raise ValueError(\"Each group must have at least 2 non-nan_policy data points.\")\n\n    if np.var(group1, ddof=1) < 1e-8 or np.var(group2, ddof=1) < 1e-8:\n        raise ValueError(\"Each group must have sufficient variance.\")\n\n    group1 = np.array(group1)\n    group2 = np.array(group2)\n\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n\n    t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n\n    significant = p_value < alpha\n\n    group1_stats = {'mean': np.mean(group1), 'std': np.std(group1)}\n    group2_stats = {'mean': np.mean(group2), 'std': np.std(group2)}\n\n    fig, ax_boxplot = plt.subplots()\n    fig, ax_histogram = plt.subplots()\n\n    ax_boxplot.boxplot([group1, group2], labels=['Group 1', 'Group 2'])\n    ax_histogram.hist(group1, alpha=0.5, label='Group 1')\n    ax_histogram.hist(group2, alpha=0.5, label='Group 2')\n    ax_histogram.legend()\n\n    return {\n        'significant': significant,\n        'group1_stats': group1_stats,\n        'group2_stats': group2_stats,\n        'ax_boxplot': ax_boxplot,\n        'ax_histogram': ax_histogram\n    }\n```\n"}
{"task_id": "BigCodeBench/1027", "solution": "import binascii\nimport urllib.parse\ndef task_func(url):\n    \"\"\"\n    Ek hexadecimal string ko url ke 'q' query parameter se decode karein.\n    Yeh function diye gaye url se 'q' query parameter nikaalta hai,\n    Yeh maan leta hai ki yeh ek hexadecimal string hai, and ise utf-8 string mein decode karta hai.\n    Yadi hexadecimal string invalid hai or ek valid utf-8 string mein decode nahi ki or jaa sakta hai, to none lauta or jaata hai.\n\n    Parameters:\n    - url (str): query parameter nikaalne ke liye url.\n\n    Returns:\n        str or none: Decode ki gayi string yadi 'q' parameter maujood hai and ek valid hexadecimal hai, otherwise none.\n\n    Requirements:\n    - binascii\n    - urllib.parse\n\n    Example:\n    >>> task_func('https://www.example.com?q=4a4b4c')\n    'JKL'\n\n    \"\"\"\n    # Parse the URL to extract the 'q' parameter\n    parsed_url = urllib.parse.urlparse(url)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n    \n    # Check if 'q' parameter exists\n    if 'q' in query_params:\n        hex_string = query_params['q'][0]\n        \n        try:\n            # Decode the hexadecimal string to utf-8\n            decoded_string = binascii.unhexlify(hex_string).decode('utf-8')\n            return decoded_string\n        except (binascii.Error, UnicodeDecodeError):\n            return None\n    else:\n        return None", "raw_solution": "import binascii\nimport urllib.parse\n\ndef task_func(url):\n    \"\"\"\n    Ek hexadecimal string ko url ke 'q' query parameter se decode karein.\n    Yeh function diye gaye url se 'q' query parameter nikaalta hai,\n    Yeh maan leta hai ki yeh ek hexadecimal string hai, and ise utf-8 string mein decode karta hai.\n    Yadi hexadecimal string invalid hai or ek valid utf-8 string mein decode nahi ki or jaa sakta hai, to none lauta or jaata hai.\n\n    Parameters:\n    - url (str): query parameter nikaalne ke liye url.\n\n    Returns:\n        str or none: Decode ki gayi string yadi 'q' parameter maujood hai and ek valid hexadecimal hai, otherwise none.\n\n    Requirements:\n    - binascii\n    - urllib.parse\n\n    Example:\n    >>> task_func('https://www.example.com?q=4a4b4c')\n    'JKL'\n\n    \"\"\"\n    # Parse the URL to extract the 'q' parameter\n    parsed_url = urllib.parse.urlparse(url)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n    \n    # Check if 'q' parameter exists\n    if 'q' in query_params:\n        hex_string = query_params['q'][0]\n        \n        try:\n            # Decode the hexadecimal string to utf-8\n            decoded_string = binascii.unhexlify(hex_string).decode('utf-8')\n            return decoded_string\n        except (binascii.Error, UnicodeDecodeError):\n            return None\n    else:\n        return None\n```\n"}
{"task_id": "BigCodeBench/1028", "solution": "import subprocess\nimport time\nimport json\nimport platform\nLOGFILE_PATH = \"logfile.log\"\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0:\n        raise ValueError(\"Interval and duration must be greater than 0.\")\n\n    if interval >= duration:\n        raise ValueError(\"Interval must be less than duration.\")\n\n    system = platform.system()\n    command = None\n\n    if system == \"Windows\":\n        command = \"wmic cpu get loadpercentage\"\n    else:\n        command = \"top -bn1 | grep 'Cpu(s)'\"\n\n    end_time = time.time() + duration\n    log_data = []\n\n    while time.time() < end_time:\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        output = result.stdout.strip()\n\n        if system == \"Windows\":\n            cpu_usage = int(output.split()[1])\n        else:\n            cpu_usage = int(output.split(\",\")[1].split(\":\")[1].strip())\n\n        log_data.append({\n            \"timestamp\": time.time(),\n            \"cpu_usage\": cpu_usage\n        })\n\n        time.sleep(interval)\n\n    try:\n        with open(LOGFILE_PATH, \"w\") as logfile:\n            json.dump(log_data, logfile)\n    except IOError as e:\n        print(f\"Error writing to log file: {e}\")\n        return None\n\n    return LOGFILE_PATH", "raw_solution": "import subprocess\nimport time\nimport json\nimport platform\n\nLOGFILE_PATH = \"logfile.log\"\n\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0:\n        raise ValueError(\"Interval and duration must be greater than 0.\")\n\n    if interval >= duration:\n        raise ValueError(\"Interval must be less than duration.\")\n\n    system = platform.system()\n    command = None\n\n    if system == \"Windows\":\n        command = \"wmic cpu get loadpercentage\"\n    else:\n        command = \"top -bn1 | grep 'Cpu(s)'\"\n\n    end_time = time.time() + duration\n    log_data = []\n\n    while time.time() < end_time:\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        output = result.stdout.strip()\n\n        if system == \"Windows\":\n            cpu_usage = int(output.split()[1])\n        else:\n            cpu_usage = int(output.split(\",\")[1].split(\":\")[1].strip())\n\n        log_data.append({\n            \"timestamp\": time.time(),\n            \"cpu_usage\": cpu_usage\n        })\n\n        time.sleep(interval)\n\n    try:\n        with open(LOGFILE_PATH, \"w\") as logfile:\n            json.dump(log_data, logfile)\n    except IOError as e:\n        print(f\"Error writing to log file: {e}\")\n        return None\n\n    return LOGFILE_PATH\n```\n"}
{"task_id": "BigCodeBench/1029", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(rows=100, columns=3):\n    # Generate random characters for each column\n    data = {chr(97 + i): np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=rows) for i in range(columns)}\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data)\n    \n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(rows=100, columns=3):\n    # Generate random characters for each column\n    data = {chr(97 + i): np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=rows) for i in range(columns)}\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data)\n    \n    return df\n\n# Example usage:\nnp.random.seed(0)\ndf = task_func(5, 3)"}
{"task_id": "BigCodeBench/1030", "solution": "import itertools\nimport string\nimport pandas as pd\ndef task_func():\n    # Generate all possible combinations of 3 letters from the alphabet\n    combinations = list(itertools.product(string.ascii_lowercase, repeat=3))\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Letter 1', 'Letter 2', 'Letter 3'])\n    \n    return df", "raw_solution": "import itertools\nimport string\nimport pandas as pd\n\ndef task_func():\n    # Generate all possible combinations of 3 letters from the alphabet\n    combinations = list(itertools.product(string.ascii_lowercase, repeat=3))\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Letter 1', 'Letter 2', 'Letter 3'])\n    \n    return df\n\n# Example usage"}
{"task_id": "BigCodeBench/1031", "solution": "import random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(n_rows=1000):\n    if n_rows <= 0:\n        raise ValueError(\"n_rows must be a positive integer\")\n\n    # Generate random 3-letter strings\n    random_strings = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(n_rows)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # Count frequencies\n    frequency = df['String'].value_counts().reset_index()\n    frequency.columns = ['String', 'Frequency']\n\n    # Get top 30 frequencies\n    top_30 = frequency.head(30)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.barh(top_30['String'], top_30['Frequency'], color='skyblue')\n    ax.set_title('Top 30 Frequencies of Random 3-Letter Strings')\n    ax.set_xlabel('Frequency')\n    ax.set_ylabel('String')\n\n    return ax", "raw_solution": "import random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(n_rows=1000):\n    if n_rows <= 0:\n        raise ValueError(\"n_rows must be a positive integer\")\n\n    # Generate random 3-letter strings\n    random_strings = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(n_rows)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # Count frequencies\n    frequency = df['String'].value_counts().reset_index()\n    frequency.columns = ['String', 'Frequency']\n\n    # Get top 30 frequencies\n    top_30 = frequency.head(30)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.barh(top_30['String'], top_30['Frequency'], color='skyblue')\n    ax.set_title('Top 30 Frequencies of Random 3-Letter Strings')\n    ax.set_xlabel('Frequency')\n    ax.set_ylabel('String')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1032", "solution": "import matplotlib.pyplot as plt\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\nLETTERS = list(string.ascii_lowercase)\ndef task_func(rows=1000, string_length=3):\n    if rows == 0:\n        print(\"heatmap generation karne ke liye koi data nahi hai.\")\n        return None\n\n    # Generate random strings\n    data = []\n    for _ in range(rows):\n        random_string = ''.join(random.choices(LETTERS, k=string_length))\n        data.append(random_string)\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['String'])\n\n    # One-hot encode the strings\n    encoded_df = pd.get_dummies(df['String'].apply(list))\n\n    # Calculate the frequency of each letter\n    frequency_df = encoded_df.sum()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(frequency_df.values.reshape(1, -1), annot=True, fmt='d', cmap='Blues',\n                     xticklabels=LETTERS, yticklabels=['Frequency'])\n    plt.title('Frequency of Each Letter')\n    plt.xlabel('Letters')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\n\ndef task_func(rows=1000, string_length=3):\n    if rows == 0:\n        print(\"heatmap generation karne ke liye koi data nahi hai.\")\n        return None\n\n    # Generate random strings\n    data = []\n    for _ in range(rows):\n        random_string = ''.join(random.choices(LETTERS, k=string_length))\n        data.append(random_string)\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['String'])\n\n    # One-hot encode the strings\n    encoded_df = pd.get_dummies(df['String'].apply(list))\n\n    # Calculate the frequency of each letter\n    frequency_df = encoded_df.sum()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(frequency_df.values.reshape(1, -1), annot=True, fmt='d', cmap='Blues',\n                     xticklabels=LETTERS, yticklabels=['Frequency'])\n    plt.title('Frequency of Each Letter')\n    plt.xlabel('Letters')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1033", "solution": "import itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate all possible 3-letter combinations\n    combinations = list(itertools.product(string.ascii_lowercase, repeat=3))\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['a', 'b', 'c'])\n    \n    # Calculate the frequency of the first letter in each combination\n    first_letter_counts = df['a'].value_counts()\n    \n    # Plot the histogram of the first letter frequencies\n    ax = first_letter_counts.plot(kind='bar', title='Frequency of First Letters in 3-Letter Combinations')\n    ax.set_xlabel('First Letter')\n    ax.set_ylabel('Frequency')\n    \n    return df, ax", "raw_solution": "import itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    # Generate all possible 3-letter combinations\n    combinations = list(itertools.product(string.ascii_lowercase, repeat=3))\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['a', 'b', 'c'])\n    \n    # Calculate the frequency of the first letter in each combination\n    first_letter_counts = df['a'].value_counts()\n    \n    # Plot the histogram of the first letter frequencies\n    ax = first_letter_counts.plot(kind='bar', title='Frequency of First Letters in 3-Letter Combinations')\n    ax.set_xlabel('First Letter')\n    ax.set_ylabel('Frequency')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/1034", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nCATEGORIES = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Automotive\", \"Books\"]\ndef task_func(s1, s2):\n    # Check if sales data is above a certain threshold\n    threshold = 200\n    above_threshold_s1 = s1[s1 > threshold]\n    above_threshold_s2 = s2[s2 > threshold]\n\n    # Plotting if there are categories above the threshold\n    if not above_threshold_s1.empty and not above_threshold_s2.empty:\n        fig, ax = plt.subplots()\n        above_threshold_s1.plot(kind='bar', ax=ax, label='Store 1')\n        above_threshold_s2.plot(kind='bar', ax=ax, label='Store 2')\n        ax.set_title('Sales Comparison Above Threshold in Categories')\n        ax.set_ylabel('Sales')\n        ax.set_xlabel('Categories')\n        ax.legend()\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n    else:\n        ax = None\n\n    # Calculate Euclidean distance\n    if not above_threshold_s1.empty and not above_threshold_s2.empty:\n        diff = above_threshold_s1 - above_threshold_s2\n        edit_distance = np.sqrt(np.sum(diff**2))\n    else:\n        edit_distance = 0.0\n\n    return ax, edit_distance\ns1 = pd.Series(np.random.randint(100, 500, size=5), index=CATEGORIES)\ns2 = pd.Series(np.random.randint(150, 600, size=5), index=CATEGORIES)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nCATEGORIES = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Automotive\", \"Books\"]\n\ndef task_func(s1, s2):\n    # Check if sales data is above a certain threshold\n    threshold = 200\n    above_threshold_s1 = s1[s1 > threshold]\n    above_threshold_s2 = s2[s2 > threshold]\n\n    # Plotting if there are categories above the threshold\n    if not above_threshold_s1.empty and not above_threshold_s2.empty:\n        fig, ax = plt.subplots()\n        above_threshold_s1.plot(kind='bar', ax=ax, label='Store 1')\n        above_threshold_s2.plot(kind='bar', ax=ax, label='Store 2')\n        ax.set_title('Sales Comparison Above Threshold in Categories')\n        ax.set_ylabel('Sales')\n        ax.set_xlabel('Categories')\n        ax.legend()\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n    else:\n        ax = None\n\n    # Calculate Euclidean distance\n    if not above_threshold_s1.empty and not above_threshold_s2.empty:\n        diff = above_threshold_s1 - above_threshold_s2\n        edit_distance = np.sqrt(np.sum(diff**2))\n    else:\n        edit_distance = 0.0\n\n    return ax, edit_distance\n\n# Example usage\nnp.random.seed(seed=32)\ns1 = pd.Series(np.random.randint(100, 500, size=5), index=CATEGORIES)\ns2 = pd.Series(np.random.randint(150, 600, size=5), index=CATEGORIES)\nax, edit_distance = task_func(s1, s2)"}
{"task_id": "BigCodeBench/1035", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(feature: pd.Series, target: pd.Series) -> (np.ndarray, plt.Axes):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(feature.values.reshape(-1, 1), target, test_size=0.2, random_state=42)\n\n    # Initialize and train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target variable for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Plot the confusion matrix\n    fig, ax = plt.subplots()\n    cax = ax.matshow(cm, cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    fig.colorbar(cax)\n\n    return cm, ax", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(feature: pd.Series, target: pd.Series) -> (np.ndarray, plt.Axes):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(feature.values.reshape(-1, 1), target, test_size=0.2, random_state=42)\n\n    # Initialize and train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target variable for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Plot the confusion matrix\n    fig, ax = plt.subplots()\n    cax = ax.matshow(cm, cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    fig.colorbar(cax)\n\n    return cm, ax\n```\n"}
{"task_id": "BigCodeBench/1036", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2):\n    # Create a DataFrame from the two Series\n    df = pd.DataFrame({'Series1': s1, 'Series2': s2})\n\n    # Calculate the intersection\n    intersection = df[df['Series1'].notna() & df['Series2'].notna()]\n\n    # Plot the swarm plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.swarmplot(data=df, palette=\"Set2\")\n    ax.set_title('Overlap Between {} and {}'.format(s1.name, s2.name))\n\n    # Highlight the intersection points\n    for index, row in intersection.iterrows():\n        plt.plot([index, index], [row['Series1'], row['Series2']], 'r--', lw=1)\n\n    # Calculate the number of intersections\n    intersection_count = len(intersection)\n\n    return ax, intersection_count", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(s1, s2):\n    # Create a DataFrame from the two Series\n    df = pd.DataFrame({'Series1': s1, 'Series2': s2})\n\n    # Calculate the intersection\n    intersection = df[df['Series1'].notna() & df['Series2'].notna()]\n\n    # Plot the swarm plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.swarmplot(data=df, palette=\"Set2\")\n    ax.set_title('Overlap Between {} and {}'.format(s1.name, s2.name))\n\n    # Highlight the intersection points\n    for index, row in intersection.iterrows():\n        plt.plot([index, index], [row['Series1'], row['Series2']], 'r--', lw=1)\n\n    # Calculate the number of intersections\n    intersection_count = len(intersection)\n\n    return ax, intersection_count\n\n# Example usage:\n# s1 = pd.Series([1, 2, 3, 4, 5], name='Series1')\n# s2 = pd.Series([4, 5, 6, 7, 8], name='Series2')\n# ax, count = task_func(s1, s2)\n# print(ax.get_title())\n# print(\"Intersection count:\", count)\n```\n"}
{"task_id": "BigCodeBench/1037", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2, n_clusters=3):\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"s1 and s2 pandas Series honi chahiye\")\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 ki length same honi chahiye\")\n\n    data = pd.DataFrame({'feature1': s1, 'feature2': s2})\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n    labels = kmeans.fit_predict(data)\n\n    ax = data.plot.scatter(x='feature1', y='feature2', c=labels, colormap='viridis', title='k-Means Clustering')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.legend(title='Cluster')\n\n    return labels, ax", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(s1, s2, n_clusters=3):\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"s1 and s2 pandas Series honi chahiye\")\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 ki length same honi chahiye\")\n\n    data = pd.DataFrame({'feature1': s1, 'feature2': s2})\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n    labels = kmeans.fit_predict(data)\n\n    ax = data.plot.scatter(x='feature1', y='feature2', c=labels, colormap='viridis', title='k-Means Clustering')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.legend(title='Cluster')\n\n    return labels, ax\n```\n"}
{"task_id": "BigCodeBench/1038", "solution": "from datetime import datetime\nimport json\ndef task_func(client_socket):\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    greeting = \"Hello, welcome to the server!\"\n\n    response = {\n        \"server_time\": current_time,\n        \"greeting\": greeting\n    }\n\n    response_json = json.dumps(response)\n    client_socket.sendall(response_json.encode('utf-8'))\n\n    # Close the client socket\n    client_socket.close()", "raw_solution": "from datetime import datetime\nimport json\n\nSERVER_ADDRESS = \"localhost\"\nBUFFER_SIZE = 1024\n\ndef task_func(client_socket):\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    greeting = \"Hello, welcome to the server!\"\n\n    response = {\n        \"server_time\": current_time,\n        \"greeting\": greeting\n    }\n\n    response_json = json.dumps(response)\n    client_socket.sendall(response_json.encode('utf-8'))\n\n    # Close the client socket\n    client_socket.close()\n```\n"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Create SSL context\n        context = ssl.create_default_context(ssl.FILETYPE_PEM)\n        context.load_cert_chain(cert_file, key_file)\n\n        # Wrap socket with SSL\n        ssl_socket = context.wrap_socket(client_socket, server_hostname='localhost')\n\n        # Receive file name\n        file_name = ssl_socket.recv(buffer_size).decode('utf-8')\n\n        # Check if file exists\n        if not os.path.exists(file_name):\n            ssl_socket.sendall('File not found'.encode('utf-8'))\n            return 'File not found'\n\n        # Calculate SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_name, 'rb') as f:\n            for byte_block in iter(lambda: f.read(buffer_size), b\"\"):\n                sha256_hash.update(byte_block)\n        \n        # Send the hash\n        ssl_socket.sendall(sha256_hash.hexdigest().encode('utf-8'))\n\n        # Close SSL socket\n        ssl_socket.close()\n\n        return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"", "raw_solution": "import ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Create SSL context\n        context = ssl.create_default_context(ssl.FILETYPE_PEM)\n        context.load_cert_chain(cert_file, key_file)\n\n        # Wrap socket with SSL\n        ssl_socket = context.wrap_socket(client_socket, server_hostname='localhost')\n\n        # Receive file name\n        file_name = ssl_socket.recv(buffer_size).decode('utf-8')\n\n        # Check if file exists\n        if not os.path.exists(file_name):\n            ssl_socket.sendall('File not found'.encode('utf-8'))\n            return 'File not found'\n\n        # Calculate SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_name, 'rb') as f:\n            for byte_block in iter(lambda: f.read(buffer_size), b\"\"):\n                sha256_hash.update(byte_block)\n        \n        # Send the hash\n        ssl_socket.sendall(sha256_hash.hexdigest().encode('utf-8'))\n\n        # Close SSL socket\n        ssl_socket.close()\n\n        return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n```\nThis script uses the `ssl` module to create an SSL context and wrap the client socket with SSL. It then receives the file name from the client, checks if the file exists, and calculates the SHA256 hash of the file if it does. The hash is then sent back to the client. If an error occurs, the error message is returned.\n"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(0)\n    server_address = (server_address, server_port)\n    server.bind(server_address)\n    server.listen(5)\n\n    # Sockets from which we expect to read\n    inputs = [server]\n\n    # Sockets to which we expect to write\n    outputs = []\n\n    # Message queues\n    message_queues = {}\n\n    # Start time\n    start_time = datetime.now()\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        # Wait for at least one of the sockets to be ready for processing\n        readable, writable, exceptional = select.select(inputs, outputs, inputs)\n\n        for s in readable:\n            if s is server:\n                # A \"readable\" server socket is ready to accept a connection\n                connection, client_address = s.accept()\n                connection.setblocking(0)\n                inputs.append(connection)\n\n                # Give the connection a queue for data we want to send\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    # A readable client socket has data\n                    message_queues[s].put(data)\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # Interpret empty result as closed connection\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                # Broadcast the message to all other connected clients\n                for output in outputs:\n                    if output != s:\n                        output.send(next_msg)\n\n        for s in exceptional:\n            # Handle \"exceptional conditions\"\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    return f'Server started on {server_address[0]}:{server_address[1]}. Ran for {run_duration} seconds.'", "raw_solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(0)\n    server_address = (server_address, server_port)\n    server.bind(server_address)\n    server.listen(5)\n\n    # Sockets from which we expect to read\n    inputs = [server]\n\n    # Sockets to which we expect to write\n    outputs = []\n\n    # Message queues\n    message_queues = {}\n\n    # Start time\n    start_time = datetime.now()\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        # Wait for at least one of the sockets to be ready for processing\n        readable, writable, exceptional = select.select(inputs, outputs, inputs)\n\n        for s in readable:\n            if s is server:\n                # A \"readable\" server socket is ready to accept a connection\n                connection, client_address = s.accept()\n                connection.setblocking(0)\n                inputs.append(connection)\n\n                # Give the connection a queue for data we want to send\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    # A readable client socket has data\n                    message_queues[s].put(data)\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # Interpret empty result as closed connection\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                # Broadcast the message to all other connected clients\n                for output in outputs:\n                    if output != s:\n                        output.send(next_msg)\n\n        for s in exceptional:\n            # Handle \"exceptional conditions\"\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    return f'Server started on {server_address[0]}:{server_address[1]}. Ran for {run_duration} seconds.'\n```\n"}
{"task_id": "BigCodeBench/1041", "solution": "import re\nimport os\ndef task_func(request):\n    # Regular expression to match the GET request format\n    request_pattern = re.compile(r\"GET /([^ ]+) HTTP/1.1\")\n    \n    # Match the request\n    match = request_pattern.match(request)\n    \n    if not match:\n        return \"HTTP/1.1 400 BAD REQUEST\\r\\n\\r\\nBad Request\"\n    \n    filename = match.group(1)\n    \n    try:\n        # Check if the file exists\n        if not os.path.exists(filename):\n            return \"HTTP/1.1 404 NOT FOUND\\r\\n\\r\\nFile Not Found\"\n        \n        # Read the file content\n        with open(filename, 'r') as file:\n            content = file.read()\n        \n        # Return the HTTP response with content length\n        response = f\"HTTP/1.1 200 OK\\r\\nContent-Length: {len(content)}\\r\\n\\r\\n{content}\"\n        return response\n    \n    except IOError:\n        return \"HTTP/1.1 500 INTERNAL SERVER ERROR\\r\\n\\r\\nInternal Server Error\"", "raw_solution": "import re\nimport os\n\ndef task_func(request):\n    # Regular expression to match the GET request format\n    request_pattern = re.compile(r\"GET /([^ ]+) HTTP/1.1\")\n    \n    # Match the request\n    match = request_pattern.match(request)\n    \n    if not match:\n        return \"HTTP/1.1 400 BAD REQUEST\\r\\n\\r\\nBad Request\"\n    \n    filename = match.group(1)\n    \n    try:\n        # Check if the file exists\n        if not os.path.exists(filename):\n            return \"HTTP/1.1 404 NOT FOUND\\r\\n\\r\\nFile Not Found\"\n        \n        # Read the file content\n        with open(filename, 'r') as file:\n            content = file.read()\n        \n        # Return the HTTP response with content length\n        response = f\"HTTP/1.1 200 OK\\r\\nContent-Length: {len(content)}\\r\\n\\r\\n{content}\"\n        return response\n    \n    except IOError:\n        return \"HTTP/1.1 500 INTERNAL SERVER ERROR\\r\\n\\r\\nInternal Server Error\"\n```\n"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    # Create an EmailMessage object\n    msg = EmailMessage()\n    \n    # Prompt the user for sender, recipient, and message\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient email: \")\n    password = getpass.getpass(\"Enter your password: \")\n    message = input(\"Enter your message: \")\n\n    # Set the email headers\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n    msg['Subject'] = 'Test Email'\n    msg.set_content(message)\n\n    # Connect to the SMTP server\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n        server.starttls()  # Secure the connection\n        server.login(sender_email, password)\n        server.send_message(msg)\n\n    print(\"Email sent successfully!\")", "raw_solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    # Create an EmailMessage object\n    msg = EmailMessage()\n    \n    # Prompt the user for sender, recipient, and message\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient email: \")\n    password = getpass.getpass(\"Enter your password: \")\n    message = input(\"Enter your message: \")\n\n    # Set the email headers\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n    msg['Subject'] = 'Test Email'\n    msg.set_content(message)\n\n    # Connect to the SMTP server\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n        server.starttls()  # Secure the connection\n        server.login(sender_email, password)\n        server.send_message(msg)\n\n    print(\"Email sent successfully!\")\n\n# Example usage:\n# import socket\n# server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n# server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n# server_socket.listen(5)\n# client_socket, addr = server_socket.accept()\n# task_func(client_socket)\n```\n"}
{"task_id": "BigCodeBench/1043", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"Data list is empty.\")\n\n    # Count occurrences of each category\n    category_counts = pd.Series(data_list).value_counts()\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the bar chart\n    all_categories = pd.Index(CATEGORIES + category_counts.index.tolist())\n    counts = category_counts.reindex(all_categories, fill_value=0)\n    ax.bar(all_categories, counts, width=0.8, align=\"center\")\n\n    # Set x-ticks and labels\n    ax.set_xticks(range(len(all_categories)))\n    ax.set_xticklabels(all_categories, rotation=45, ha=\"right\")\n\n    # Check if the distribution is uniform\n    if not counts.equals(pd.Series(counts).value_counts()):\n        print(\"Category distribution is not uniform.\")\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"Data list is empty.\")\n\n    # Count occurrences of each category\n    category_counts = pd.Series(data_list).value_counts()\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the bar chart\n    all_categories = pd.Index(CATEGORIES + category_counts.index.tolist())\n    counts = category_counts.reindex(all_categories, fill_value=0)\n    ax.bar(all_categories, counts, width=0.8, align=\"center\")\n\n    # Set x-ticks and labels\n    ax.set_xticks(range(len(all_categories)))\n    ax.set_xticklabels(all_categories, rotation=45, ha=\"right\")\n\n    # Check if the distribution is uniform\n    if not counts.equals(pd.Series(counts).value_counts()):\n        print(\"Category distribution is not uniform.\")\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1044", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nROOMS = [\"Room1\", \"Room2\", \"Room3\", \"Room4\", \"Room5\"]\ndef task_func(date_str, booking_data):\n    try:\n        # Validate the date string\n        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n        if date < datetime.now():\n            raise ValueError(\"The date must be a future date.\")\n    except ValueError as e:\n        raise ValueError(f\"Invalid date: {e}\") from e\n\n    # Create a DataFrame from the booking data\n    data = {room: booking_data.get(room, \"Not Listed\") for room in ROOMS}\n    df = pd.DataFrame(list(data.items()), columns=[\"Room\", \"Booking Status\"])\n\n    # Plot the booking status\n    ax = df.plot(kind='bar', x='Room', y='Booking Status', legend=False)\n    plt.xlabel(\"Room\")\n    plt.ylabel(\"Booking Status\")\n    plt.title(f\"Booking Status for {date_str}\")\n\n    return df, ax", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nROOMS = [\"Room1\", \"Room2\", \"Room3\", \"Room4\", \"Room5\"]\n\ndef task_func(date_str, booking_data):\n    try:\n        # Validate the date string\n        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n        if date < datetime.now():\n            raise ValueError(\"The date must be a future date.\")\n    except ValueError as e:\n        raise ValueError(f\"Invalid date: {e}\") from e\n\n    # Create a DataFrame from the booking data\n    data = {room: booking_data.get(room, \"Not Listed\") for room in ROOMS}\n    df = pd.DataFrame(list(data.items()), columns=[\"Room\", \"Booking Status\"])\n\n    # Plot the booking status\n    ax = df.plot(kind='bar', x='Room', y='Booking Status', legend=False)\n    plt.xlabel(\"Room\")\n    plt.ylabel(\"Booking Status\")\n    plt.title(f\"Booking Status for {date_str}\")\n\n    return df, ax\n\n# Example usage"}
{"task_id": "BigCodeBench/1045", "solution": "from datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\ndef task_func(date_str):\n    \"\"\"\n    Dee gayee date se current time tak beete hue total seconds kee ganana karen,\n    is period mein ghatit hone wale kisee bhee leap seconds sahit.\n\n    Notes:\n        Yah function datetime, numpy, aur dateutil.parser modules ka upayog karta hai.\n        LEAP_SECONDS array mein un years ko shaamil karana chaahie jab leap seconds jode gae the.\n\n    Parameters:\n    - date_str (str): date aur time jisase ganana karnee hai, \"yyyy-mm-dd hh:mm:ss\" format mein.\n\n    Returns:\n        int: die gae date se, leap seconds sahit, beete hue total seconds kee number.\n\n    Requirements:\n    - datetime.datetime\n    - numpy\n    - dateutil.parser.parse\n\n    Example:\n    >>> total_seconds = task_func('1970-01-01 00:00:00')\n    >>> print(total_seconds)\n    1702597276\n\n    \"\"\"\n    # Parse the input date string\n    input_date = parse(date_str)\n    \n    # Calculate the total seconds from the input date to the current time\n    current_time = datetime.now()\n    total_seconds = int((current_time - input_date).total_seconds())\n    \n    # Find the number of leap seconds that occurred in the period\n    leap_seconds_occurred = np.searchsorted(LEAP_SECONDS, input_date.year)\n    \n    # Subtract the number of leap seconds from the total seconds\n    total_seconds -= leap_seconds_occurred\n    \n    return total_seconds", "raw_solution": "from datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\n\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\n\n\ndef task_func(date_str):\n    \"\"\"\n    Dee gayee date se current time tak beete hue total seconds kee ganana karen,\n    is period mein ghatit hone wale kisee bhee leap seconds sahit.\n\n    Notes:\n        Yah function datetime, numpy, aur dateutil.parser modules ka upayog karta hai.\n        LEAP_SECONDS array mein un years ko shaamil karana chaahie jab leap seconds jode gae the.\n\n    Parameters:\n    - date_str (str): date aur time jisase ganana karnee hai, \"yyyy-mm-dd hh:mm:ss\" format mein.\n\n    Returns:\n        int: die gae date se, leap seconds sahit, beete hue total seconds kee number.\n\n    Requirements:\n    - datetime.datetime\n    - numpy\n    - dateutil.parser.parse\n\n    Example:\n    >>> total_seconds = task_func('1970-01-01 00:00:00')\n    >>> print(total_seconds)\n    1702597276\n\n    \"\"\"\n    # Parse the input date string\n    input_date = parse(date_str)\n    \n    # Calculate the total seconds from the input date to the current time\n    current_time = datetime.now()\n    total_seconds = int((current_time - input_date).total_seconds())\n    \n    # Find the number of leap seconds that occurred in the period\n    leap_seconds_occurred = np.searchsorted(LEAP_SECONDS, input_date.year)\n    \n    # Subtract the number of leap seconds from the total seconds\n    total_seconds -= leap_seconds_occurred\n    \n    return total_seconds\n```\n"}
{"task_id": "BigCodeBench/1046", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nfrom itertools import product\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\ndef task_func(date_str):\n    # Convert the input string to a datetime object\n    start_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Create a list of dates for the next 10 days\n    dates = [start_date + timedelta(days=i) for i in range(10)]\n    \n    # Generate all combinations of employees and dates\n    data = list(product(EMPLOYEES, dates))\n    \n    # Create a DataFrame from the generated data\n    df = pd.DataFrame(data, columns=[\"Employee\", \"Date\"])\n    \n    return df", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nfrom itertools import product\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef task_func(date_str):\n    # Convert the input string to a datetime object\n    start_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Create a list of dates for the next 10 days\n    dates = [start_date + timedelta(days=i) for i in range(10)]\n    \n    # Generate all combinations of employees and dates\n    data = list(product(EMPLOYEES, dates))\n    \n    # Create a DataFrame from the generated data\n    df = pd.DataFrame(data, columns=[\"Employee\", \"Date\"])\n    \n    return df\n\n# Example usage:\n# df = task_func('2023-06-15')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/1047", "solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    # Convert the date string to a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Generate random integers\n    random.seed(date.day)  # Seed the random number generator with the day of the month\n    random_integers = [random.randint(0, 100) for _ in range(date.day)]\n    \n    # Create a line plot of the random integers\n    fig, ax = plt.subplots()\n    ax.plot(random_integers, marker='o')\n    ax.set_title(f\"Random Integers for {date_str}\")\n    ax.set_xlabel(\"Index\")\n    ax.set_ylabel(\"Random Integer\")\n    \n    return ax", "raw_solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(date_str):\n    # Convert the date string to a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Generate random integers\n    random.seed(date.day)  # Seed the random number generator with the day of the month\n    random_integers = [random.randint(0, 100) for _ in range(date.day)]\n    \n    # Create a line plot of the random integers\n    fig, ax = plt.subplots()\n    ax.plot(random_integers, marker='o')\n    ax.set_title(f\"Random Integers for {date_str}\")\n    ax.set_xlabel(\"Index\")\n    ax.set_ylabel(\"Random Integer\")\n    \n    return ax\n\n# Example usage:\n# ax = task_func('2023-06-15')\n# print(type(ax))\n```\n"}
{"task_id": "BigCodeBench/1048", "solution": "from datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Calculate the frequency as the day of the month\n    frequency = date.day\n    \n    # Generate a time array from 0 to 2\u03c0 with a step size of 0.01\n    t = np.arange(0, 2 * np.pi, 0.01)\n    \n    # Generate a sine wave with the calculated frequency\n    y = np.sin(frequency * t)\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(t, y)\n    \n    # Set the title and labels\n    ax.set_title(f'Sine Wave for {date_str} (Frequency: {frequency})')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    \n    return ax", "raw_solution": "from datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(date_str):\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Calculate the frequency as the day of the month\n    frequency = date.day\n    \n    # Generate a time array from 0 to 2\u03c0 with a step size of 0.01\n    t = np.arange(0, 2 * np.pi, 0.01)\n    \n    # Generate a sine wave with the calculated frequency\n    y = np.sin(frequency * t)\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(t, y)\n    \n    # Set the title and labels\n    ax.set_title(f'Sine Wave for {date_str} (Frequency: {frequency})')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    \n    return ax\n\n# Example usage:\n# ax = task_func('2023-06-15')\n# print(ax.get_title())\n```\n"}
{"task_id": "BigCodeBench/1049", "solution": "import re\nimport pandas as pd\ndef task_func(input_string: str) -> pd.DataFrame:\n    # Remove empty lines and replace tabs with spaces\n    lines = [line.strip() for line in input_string.split('\\n') if line.strip()]\n    lines = [re.sub(r'\\t+', ' ', line) for line in lines]\n\n    # Create a DataFrame\n    df = pd.DataFrame(lines, columns=['Text'])\n\n    return df", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(input_string: str) -> pd.DataFrame:\n    # Remove empty lines and replace tabs with spaces\n    lines = [line.strip() for line in input_string.split('\\n') if line.strip()]\n    lines = [re.sub(r'\\t+', ' ', line) for line in lines]\n\n    # Create a DataFrame\n    df = pd.DataFrame(lines, columns=['Text'])\n\n    return df\n\n# Example usage:\n# df = task_func('line a\\\\nfollowed by line b with a\\\\ttab\\\\n\\\\n...bye\\\\n')\n# print(df.head())\n```\n"}
{"task_id": "BigCodeBench/1050", "solution": "import os\nimport hashlib\nDIRECTORY = \"./hashed_files\"\ndef task_func(input_string):\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    lines = input_string.strip().split('\\n')\n    file_paths = []\n\n    for i, line in enumerate(lines):\n        if line.strip() == '':\n            continue\n\n        # Create a SHA256 hash of the line\n        hash_object = hashlib.sha256(line.encode())\n        hash_hex = hash_object.hexdigest()\n\n        # Create a filename using the first 10 characters of the hash\n        filename = f\"{hash_hex[:10]}.txt\"\n        filepath = os.path.join(DIRECTORY, filename)\n\n        # Write the line to the file\n        with open(filepath, 'w') as file:\n            file.write(line)\n\n        file_paths.append(filepath)\n\n    return file_paths", "raw_solution": "import os\nimport hashlib\n\n# Constants\nDIRECTORY = \"./hashed_files\"\n\ndef task_func(input_string):\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    lines = input_string.strip().split('\\n')\n    file_paths = []\n\n    for i, line in enumerate(lines):\n        if line.strip() == '':\n            continue\n\n        # Create a SHA256 hash of the line\n        hash_object = hashlib.sha256(line.encode())\n        hash_hex = hash_object.hexdigest()\n\n        # Create a filename using the first 10 characters of the hash\n        filename = f\"{hash_hex[:10]}.txt\"\n        filepath = os.path.join(DIRECTORY, filename)\n\n        # Write the line to the file\n        with open(filepath, 'w') as file:\n            file.write(line)\n\n        file_paths.append(filepath)\n\n    return file_paths\n```\n"}
{"task_id": "BigCodeBench/1051", "solution": "import collections\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    if not data_dict:\n        return None, \"The distribution is uniform.\"\n\n    counts = np.array(list(data_dict.values()))\n    average_count = np.mean(counts)\n    if np.all(np.abs(counts - average_count) <= 1e-5):\n        message = \"The distribution is uniform.\"\n    else:\n        message = \"The distribution is not uniform.\"\n\n    fig, ax = plt.subplots()\n    ax.hist(counts, bins=min(10, len(set(counts))), edgecolor='black')\n    ax.set_xticks(list(data_dict.keys()))\n    ax.set_xticklabels(list(data_dict.keys()), rotation=45)\n    ax.set_xlabel('Categories')\n    ax.set_ylabel('Counts')\n    ax.set_title('Distribution of Categories')\n\n    return ax, message", "raw_solution": "import collections\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    if not data_dict:\n        return None, \"The distribution is uniform.\"\n\n    counts = np.array(list(data_dict.values()))\n    average_count = np.mean(counts)\n    if np.all(np.abs(counts - average_count) <= 1e-5):\n        message = \"The distribution is uniform.\"\n    else:\n        message = \"The distribution is not uniform.\"\n\n    fig, ax = plt.subplots()\n    ax.hist(counts, bins=min(10, len(set(counts))), edgecolor='black')\n    ax.set_xticks(list(data_dict.keys()))\n    ax.set_xticklabels(list(data_dict.keys()), rotation=45)\n    ax.set_xlabel('Categories')\n    ax.set_ylabel('Counts')\n    ax.set_title('Distribution of Categories')\n\n    return ax, message\n\n# Example usage:\ndata = {'A': 2, 'B': 3, 'C': 4, 'D': 1, 'E': 2}\nax, message = task_func(data)"}
{"task_id": "BigCodeBench/1052", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    # Load the data\n    df = pd.read_csv(file_path)\n    if df.empty or 'Text' not in df.columns:\n        print(\"The input file is empty or does not contain the 'Text' column.\")\n        return None\n\n    # Vectorize the text data\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    X = vectorizer.fit_transform(df['Text'])\n\n    # Get the most common words\n    word_counts = X.sum(axis=0)\n    words = vectorizer.get_feature_names_out()\n    most_common_words = [(words[i], word_counts[0, i]) for i in range(len(words))]\n\n    # Sort the words by frequency\n    most_common_words.sort(key=lambda x: x[1], reverse=True)\n\n    # Plot the most common words\n    if most_common_words:\n        words, counts = zip(*most_common_words)\n        plt.figure(figsize=(10, 5))\n        plt.bar(words[:10], counts[:10])\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            return plt.gca()\n    else:\n        print(\"The input file contains only stop words.\")\n        return None", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    # Load the data\n    df = pd.read_csv(file_path)\n    if df.empty or 'Text' not in df.columns:\n        print(\"The input file is empty or does not contain the 'Text' column.\")\n        return None\n\n    # Vectorize the text data\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    X = vectorizer.fit_transform(df['Text'])\n\n    # Get the most common words\n    word_counts = X.sum(axis=0)\n    words = vectorizer.get_feature_names_out()\n    most_common_words = [(words[i], word_counts[0, i]) for i in range(len(words))]\n\n    # Sort the words by frequency\n    most_common_words.sort(key=lambda x: x[1], reverse=True)\n\n    # Plot the most common words\n    if most_common_words:\n        words, counts = zip(*most_common_words)\n        plt.figure(figsize=(10, 5))\n        plt.bar(words[:10], counts[:10])\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            return plt.gca()\n    else:\n        print(\"The input file contains only stop words.\")\n        return None\n```\n"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Load the CSV file\n        df = pd.read_csv(file_path)\n        \n        # Check if 'Text' column exists, if not, assume the first column is text\n        if 'Text' not in df.columns:\n            df.columns = ['Text']\n        \n        # Initialize CountVectorizer with stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the data\n        X = vectorizer.fit_transform(df['Text'])\n        \n        # Sum the word counts\n        word_counts = X.sum(axis=0)\n        \n        # Get the words\n        words = vectorizer.get_feature_names_out()\n        \n        # Create a DataFrame for word counts\n        word_count_df = pd.DataFrame(word_counts, index=words, columns=['Count'])\n        \n        # Sort the DataFrame by count\n        word_count_df = word_count_df.sort_values(by='Count', ascending=False)\n        \n        # Plot the top 10 words\n        top_words = word_count_df.head(10)\n        ax = top_words.plot(kind='bar', legend=False)\n        ax.set_title('Top 10 Most Common Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Count')\n        \n        # Save the plot if a save path is provided\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        \n        # Return the axes object if no save path is provided\n        return ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Load the CSV file\n        df = pd.read_csv(file_path)\n        \n        # Check if 'Text' column exists, if not, assume the first column is text\n        if 'Text' not in df.columns:\n            df.columns = ['Text']\n        \n        # Initialize CountVectorizer with stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the data\n        X = vectorizer.fit_transform(df['Text'])\n        \n        # Sum the word counts\n        word_counts = X.sum(axis=0)\n        \n        # Get the words\n        words = vectorizer.get_feature_names_out()\n        \n        # Create a DataFrame for word counts\n        word_count_df = pd.DataFrame(word_counts, index=words, columns=['Count'])\n        \n        # Sort the DataFrame by count\n        word_count_df = word_count_df.sort_values(by='Count', ascending=False)\n        \n        # Plot the top 10 words\n        top_words = word_count_df.head(10)\n        ax = top_words.plot(kind='bar', legend=False)\n        ax.set_title('Top 10 Most Common Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Count')\n        \n        # Save the plot if a save path is provided\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        \n        # Return the axes object if no save path is provided\n        return ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n```\n"}
{"task_id": "BigCodeBench/1054", "solution": "import csv\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    # Read data from CSV file\n    data = []\n    with open(file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            data.append(float(row[0]))\n\n    # Generate a random sample of 30 values\n    sample = np.random.choice(data, 30, replace=False)\n\n    # Calculate mean and standard deviation\n    mean = np.mean(sample)\n    std_dev = np.std(sample, ddof=1)\n\n    # Plot histogram with normal distribution curve\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(sample, bins='auto', density=True, alpha=0.6, color='g')\n\n    # Overlay normal distribution curve\n    normal_dist = stats.norm.pdf(bins, mean, std_dev)\n    ax.plot(bins, normal_dist, linewidth=2, color='r')\n    ax.set_title('Histogram with Normal Distribution Curve')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return mean, std_dev, ax", "raw_solution": "import csv\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Read data from CSV file\n    data = []\n    with open(file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            data.append(float(row[0]))\n\n    # Generate a random sample of 30 values\n    sample = np.random.choice(data, 30, replace=False)\n\n    # Calculate mean and standard deviation\n    mean = np.mean(sample)\n    std_dev = np.std(sample, ddof=1)\n\n    # Plot histogram with normal distribution curve\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(sample, bins='auto', density=True, alpha=0.6, color='g')\n\n    # Overlay normal distribution curve\n    normal_dist = stats.norm.pdf(bins, mean, std_dev)\n    ax.plot(bins, normal_dist, linewidth=2, color='r')\n    ax.set_title('Histogram with Normal Distribution Curve')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return mean, std_dev, ax\n```\n"}
{"task_id": "BigCodeBench/1055", "solution": "import pandas as pd\nimport itertools\nimport random\ndef task_func(colors, states):\n    # Generate all combinations of colors and states\n    combinations = list(itertools.product(colors, states))\n    \n    # Shuffle the combinations\n    random.shuffle(combinations)\n    \n    # Determine the number of rows needed\n    num_rows = min(len(colors), len(states))\n    \n    # Create a DataFrame with the combinations\n    df = pd.DataFrame(combinations[:num_rows], columns=['Color', 'State'])\n    \n    # Format the DataFrame\n    df['Color:State'] = df.apply(lambda row: f\"{row['Color']}:{row['State']}\", axis=1)\n    \n    return df", "raw_solution": "import pandas as pd\nimport itertools\nimport random\n\ndef task_func(colors, states):\n    # Generate all combinations of colors and states\n    combinations = list(itertools.product(colors, states))\n    \n    # Shuffle the combinations\n    random.shuffle(combinations)\n    \n    # Determine the number of rows needed\n    num_rows = min(len(colors), len(states))\n    \n    # Create a DataFrame with the combinations\n    df = pd.DataFrame(combinations[:num_rows], columns=['Color', 'State'])\n    \n    # Format the DataFrame\n    df['Color:State'] = df.apply(lambda row: f\"{row['Color']}:{row['State']}\", axis=1)\n    \n    return df\n\n# Example usage"}
{"task_id": "BigCodeBench/1056", "solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\ndef task_func(n_pairs=26):\n    if not (1 <= n_pairs <= 26):\n        raise ValueError(\"n_pairs must be between 1 and 26 inclusive.\")\n\n    # Generate random pairs\n    pairs = random.sample(list(zip(LETTERS, NUMBERS)), n_pairs)\n\n    # Create a bar chart\n    counts = np.random.randint(1, 10, size=n_pairs)\n    fig, ax = plt.subplots()\n    bars = ax.bar([f\"{l}:{n}\" for l, n in pairs], counts)\n\n    # Add labels and title\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n\n    # Add labels to the bars\n    for bar, count in zip(bars, counts):\n        height = bar.get_height()\n        ax.annotate(f'{count}', xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3), textcoords='offset points', ha='center', va='bottom')\n\n    return bars", "raw_solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\n\n\ndef task_func(n_pairs=26):\n    if not (1 <= n_pairs <= 26):\n        raise ValueError(\"n_pairs must be between 1 and 26 inclusive.\")\n\n    # Generate random pairs\n    pairs = random.sample(list(zip(LETTERS, NUMBERS)), n_pairs)\n\n    # Create a bar chart\n    counts = np.random.randint(1, 10, size=n_pairs)\n    fig, ax = plt.subplots()\n    bars = ax.bar([f\"{l}:{n}\" for l, n in pairs], counts)\n\n    # Add labels and title\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n\n    # Add labels to the bars\n    for bar, count in zip(bars, counts):\n        height = bar.get_height()\n        ax.annotate(f'{count}', xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3), textcoords='offset points', ha='center', va='bottom')\n\n    return bars\n```\n"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Default values for animals and foods\n    default_animals = ['Dog', 'Cat', 'Elephant', 'Tiger', 'Lion', 'Zebra', 'Giraffe', 'Bear', 'Monkey', 'Kangaroo']\n    default_foods = ['Meat', 'Fish', 'Grass', 'Fruits', 'Insects', 'Seeds', 'Leaves']\n\n    # Use default values if no lists are provided\n    animals = animals or default_animals\n    foods = foods or default_foods\n\n    # Generate all combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n\n    # Shuffle combinations to ensure randomness\n    np.random.shuffle(combinations)\n\n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n\n    # Convert combinations to the desired format\n    df['Animal:Food'] = df.apply(lambda row: f\"{row['Animal']}:{row['Food']}\", axis=1)\n\n    # Drop the original columns\n    df = df.drop(columns=['Animal', 'Food'])\n\n    return df", "raw_solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Default values for animals and foods\n    default_animals = ['Dog', 'Cat', 'Elephant', 'Tiger', 'Lion', 'Zebra', 'Giraffe', 'Bear', 'Monkey', 'Kangaroo']\n    default_foods = ['Meat', 'Fish', 'Grass', 'Fruits', 'Insects', 'Seeds', 'Leaves']\n\n    # Use default values if no lists are provided\n    animals = animals or default_animals\n    foods = foods or default_foods\n\n    # Generate all combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n\n    # Shuffle combinations to ensure randomness\n    np.random.shuffle(combinations)\n\n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n\n    # Convert combinations to the desired format\n    df['Animal:Food'] = df.apply(lambda row: f\"{row['Animal']}:{row['Food']}\", axis=1)\n\n    # Drop the original columns\n    df = df.drop(columns=['Animal', 'Food'])\n\n    return df\n\n# Example usage\nanimal_food_pairs = task_func(['Dog', 'Cat'], ['Meat', 'Fish'])"}
{"task_id": "BigCodeBench/1058", "solution": "import itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nSHAPES = [\n    \"Circle\",\n    \"Square\",\n    \"Triangle\",\n    \"Rectangle\",\n    \"Pentagon\",\n    \"Hexagon\",\n    \"Heptagon\",\n    \"Octagon\",\n    \"Nonagon\",\n    \"Decagon\",\n]\nCOLORS = [\n    \"Red\",\n    \"Blue\",\n    \"Green\",\n    \"Yellow\",\n    \"Black\",\n    \"White\",\n    \"Purple\",\n    \"Orange\",\n    \"Pink\",\n    \"Brown\",\n]\ndef task_func(num_pairs=10):\n    \"\"\"\n    Generate a countplot of shape-color pairs.\n\n    Parameters:\n    - num_pairs (int): Number of unique shape-color pairs to display in the plot.\n    - Default is 10. If the input number is less than 1 or greater than 100,\n      it will be clamped to the range 1-100.\n\n    Returns:\n    - ax (matplotlib.axes._axes.Axes): The axes object of the countplot.\n\n    Requirements:\n    - itertools\n    - seaborn\n    - matplotlib\n\n    Example:\n    >>> ax = task_func(10)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    >>> ax = task_func(9)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    >>> ax = task_func(8)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    >>> ax = task_func(7)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    >>> ax = task_func(6)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    \"\"\"\n    num_pairs = max(1, min(num_pairs, 100))\n    pairs = list(itertools.product(SHAPES, COLORS))\n    selected_pairs = pairs[:num_pairs]\n\n    # Create a countplot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.countplot(x=selected_pairs, ax=ax)\n    ax.set_title(\"Shape-Color Pairs\")\n    ax.set_xlabel(\"Shape-Color Pair\")\n    ax.set_ylabel(\"Count\")\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nSHAPES = [\n    \"Circle\",\n    \"Square\",\n    \"Triangle\",\n    \"Rectangle\",\n    \"Pentagon\",\n    \"Hexagon\",\n    \"Heptagon\",\n    \"Octagon\",\n    \"Nonagon\",\n    \"Decagon\",\n]\nCOLORS = [\n    \"Red\",\n    \"Blue\",\n    \"Green\",\n    \"Yellow\",\n    \"Black\",\n    \"White\",\n    \"Purple\",\n    \"Orange\",\n    \"Pink\",\n    \"Brown\",\n]\n\n\ndef task_func(num_pairs=10):\n    \"\"\"\n    Generate a countplot of shape-color pairs.\n\n    Parameters:\n    - num_pairs (int): Number of unique shape-color pairs to display in the plot.\n    - Default is 10. If the input number is less than 1 or greater than 100,\n      it will be clamped to the range 1-100.\n\n    Returns:\n    - ax (matplotlib.axes._axes.Axes): The axes object of the countplot.\n\n    Requirements:\n    - itertools\n    - seaborn\n    - matplotlib\n\n    Example:\n    >>> ax = task_func(10)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    >>> ax = task_func(9)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    >>> ax = task_func(8)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    >>> ax = task_func(7)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    >>> ax = task_func(6)\n    >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    \"\"\"\n    num_pairs = max(1, min(num_pairs, 100))\n    pairs = list(itertools.product(SHAPES, COLORS))\n    selected_pairs = pairs[:num_pairs]\n\n    # Create a countplot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.countplot(x=selected_pairs, ax=ax)\n    ax.set_title(\"Shape-Color Pairs\")\n    ax.set_xlabel(\"Shape-Color Pair\")\n    ax.set_ylabel(\"Count\")\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1059", "solution": "import numpy as np\nimport random\nimport itertools\nimport pandas as pd\nPLANETS = [\n    \"Mercury\",\n    \"Venus\",\n    \"Earth\",\n    \"Mars\",\n    \"Jupiter\",\n    \"Saturn\",\n    \"Uranus\",\n    \"Neptune\",\n]\nELEMENTS = [\n    \"Hydrogen\",\n    \"Helium\",\n    \"Oxygen\",\n    \"Carbon\",\n    \"Nitrogen\",\n    \"Magnesium\",\n    \"Silicon\",\n    \"Iron\",\n    \"Nickel\",\n]\ndef task_func():\n    \"\"\"\n    ek **dataframe** banaen jahaan pratyek **row** mein random **planet**-**element** **pairs** hon.\n    pratyek **pair** 'Planet:Element' ke roop mein svaroopit hai. kee **number**\n    ke **planets** kee **number**, aur pratyek **row** mein utne hee **planet**-**element** **pairs** honge jitne **elements** hain.\n\n    Parameters:\n    - none\n\n    Returns:\n        **pandas**.**dataframe**: ek **dataframe** jahaan pratyek **cell** mein 'Planet:Element' ke **format** mein ek **string** hotee hai.\n        **dataframe** mein **planets** kee **number** ke equal **rows** kee **number** hotee hai aur\n        **elements** kee **number** ke equal **columns** kee **number** hotee hai.\n\n    Requirements:\n    - numpy\n    - random\n    - itertools\n    - pandas\n\n    Example:\n    >>> random.seed(0)\n    >>> planet_elements_table = task_func()\n    >>> planet_elements_table.head(2)\n    Hydrogen         Helium  ...          Iron         nickel\n    0   Uranus:Silicon  earth:Silicon  ...  earth:nickel  Uranus:Helium\n    1  Venus:Magnesium  Saturn:Helium  ...  Mercury:Iron   Venus:Helium\n    <BLANKLINE>\n    [2 rows x 9 columns]\n\n    \"\"\"\n    # Generate all possible combinations of planets and elements\n    planet_elements_pairs = list(itertools.product(PLANETS, ELEMENTS))\n\n    # Shuffle the pairs to ensure randomness\n    random.shuffle(planet_elements_pairs)\n\n    # Create a DataFrame with the shuffled pairs\n    df = pd.DataFrame(planet_elements_pairs, columns=[\"Planet\", \"Element\"])\n\n    # Convert the pairs into the desired format\n    df[\"Planet:Element\"] = df.apply(lambda row: f\"{row['Planet']}:{row['Element']}\", axis=1)\n\n    # Drop the original columns\n    df = df.drop(columns=[\"Planet\", \"Element\"])\n\n    return df", "raw_solution": "import numpy as np\nimport random\nimport itertools\nimport pandas as pd\n\n# Constants\nPLANETS = [\n    \"Mercury\",\n    \"Venus\",\n    \"Earth\",\n    \"Mars\",\n    \"Jupiter\",\n    \"Saturn\",\n    \"Uranus\",\n    \"Neptune\",\n]\nELEMENTS = [\n    \"Hydrogen\",\n    \"Helium\",\n    \"Oxygen\",\n    \"Carbon\",\n    \"Nitrogen\",\n    \"Magnesium\",\n    \"Silicon\",\n    \"Iron\",\n    \"Nickel\",\n]\n\n\ndef task_func():\n    \"\"\"\n    ek **dataframe** banaen jahaan pratyek **row** mein random **planet**-**element** **pairs** hon.\n    pratyek **pair** 'Planet:Element' ke roop mein svaroopit hai. kee **number**\n    ke **planets** kee **number**, aur pratyek **row** mein utne hee **planet**-**element** **pairs** honge jitne **elements** hain.\n\n    Parameters:\n    - none\n\n    Returns:\n        **pandas**.**dataframe**: ek **dataframe** jahaan pratyek **cell** mein 'Planet:Element' ke **format** mein ek **string** hotee hai.\n        **dataframe** mein **planets** kee **number** ke equal **rows** kee **number** hotee hai aur\n        **elements** kee **number** ke equal **columns** kee **number** hotee hai.\n\n    Requirements:\n    - numpy\n    - random\n    - itertools\n    - pandas\n\n    Example:\n    >>> random.seed(0)\n    >>> planet_elements_table = task_func()\n    >>> planet_elements_table.head(2)\n    Hydrogen         Helium  ...          Iron         nickel\n    0   Uranus:Silicon  earth:Silicon  ...  earth:nickel  Uranus:Helium\n    1  Venus:Magnesium  Saturn:Helium  ...  Mercury:Iron   Venus:Helium\n    <BLANKLINE>\n    [2 rows x 9 columns]\n\n    \"\"\"\n    # Generate all possible combinations of planets and elements\n    planet_elements_pairs = list(itertools.product(PLANETS, ELEMENTS))\n\n    # Shuffle the pairs to ensure randomness\n    random.shuffle(planet_elements_pairs)\n\n    # Create a DataFrame with the shuffled pairs\n    df = pd.DataFrame(planet_elements_pairs, columns=[\"Planet\", \"Element\"])\n\n    # Convert the pairs into the desired format\n    df[\"Planet:Element\"] = df.apply(lambda row: f\"{row['Planet']}:{row['Element']}\", axis=1)\n\n    # Drop the original columns\n    df = df.drop(columns=[\"Planet\", \"Element\"])\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/1060", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame, column_name: str) -> (str, plt.Axes):\n    if df.empty or column_name not in df.columns or df[column_name].isnull().all():\n        message = \"df khali hai or specified column mein koi data nahi hai.\"\n        fig, ax = plt.subplots()\n        ax.set_title(f\"{column_name} mein values ka distribution (koi data nahi)\")\n        ax.set_xlabel(\"value\")\n        ax.set_ylabel(\"frequency\")\n        return message, ax\n\n    value_counts = df[column_name].value_counts()\n    is_uniform = len(value_counts) == len(df[column_name])\n\n    message = \"values ka distribution uniform hai.\" if is_uniform else \"values ka distribution uniform nahi hai.\"\n    fig, ax = plt.subplots()\n    ax.hist(df[column_name], bins=len(value_counts), edgecolor='black', alpha=0.7)\n    ax.set_title(f\"{column_name} mein values ka distribution\")\n    ax.set_xlabel(\"value\")\n    ax.set_ylabel(\"frequency\")\n\n    return message, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame, column_name: str) -> (str, plt.Axes):\n    if df.empty or column_name not in df.columns or df[column_name].isnull().all():\n        message = \"df khali hai or specified column mein koi data nahi hai.\"\n        fig, ax = plt.subplots()\n        ax.set_title(f\"{column_name} mein values ka distribution (koi data nahi)\")\n        ax.set_xlabel(\"value\")\n        ax.set_ylabel(\"frequency\")\n        return message, ax\n\n    value_counts = df[column_name].value_counts()\n    is_uniform = len(value_counts) == len(df[column_name])\n\n    message = \"values ka distribution uniform hai.\" if is_uniform else \"values ka distribution uniform nahi hai.\"\n    fig, ax = plt.subplots()\n    ax.hist(df[column_name], bins=len(value_counts), edgecolor='black', alpha=0.7)\n    ax.set_title(f\"{column_name} mein values ka distribution\")\n    ax.set_xlabel(\"value\")\n    ax.set_ylabel(\"frequency\")\n\n    return message, ax\n```\n"}
{"task_id": "BigCodeBench/1061", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Normalize the array\n    row_sums = np.sum(arr, axis=1)\n    mean = np.mean(row_sums)\n    std_dev = np.std(row_sums)\n    \n    if std_dev == 0:\n        normalized_data = np.zeros_like(row_sums)\n    else:\n        normalized_data = (row_sums - mean) / std_dev\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set the title and labels\n    ax.set_title(\"Standardized data's histogram with standard normal PDF\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return ax, normalized_data", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Normalize the array\n    row_sums = np.sum(arr, axis=1)\n    mean = np.mean(row_sums)\n    std_dev = np.std(row_sums)\n    \n    if std_dev == 0:\n        normalized_data = np.zeros_like(row_sums)\n    else:\n        normalized_data = (row_sums - mean) / std_dev\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set the title and labels\n    ax.set_title(\"Standardized data's histogram with standard normal PDF\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return ax, normalized_data\n\n# Example usage:\n# ax, normalized_data = task_func(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/1062", "solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\ndef task_func(arr):\n    if arr.size == 0:\n        fig, ax = plt.subplots()\n        ax.set_title('Time Series of Row Sums')\n        return ax\n\n    sums = np.sum(arr, axis=1)\n    dates = pd.date_range(start='1/1/2020', periods=len(sums))\n    df = pd.DataFrame({'Date': dates, 'Sum': sums})\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sum'])\n    ax.set_title('Time Series of Row Sums')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sum')\n\n    return ax", "raw_solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef task_func(arr):\n    if arr.size == 0:\n        fig, ax = plt.subplots()\n        ax.set_title('Time Series of Row Sums')\n        return ax\n\n    sums = np.sum(arr, axis=1)\n    dates = pd.date_range(start='1/1/2020', periods=len(sums))\n    df = pd.DataFrame({'Date': dates, 'Sum': sums})\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sum'])\n    ax.set_title('Time Series of Row Sums')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sum')\n\n    return ax\n\n# Example usage:\n# arr = np.array([[i + j for i in range(3)] for j in range(5)])\n# ax = task_func(arr)\n# ax.get_title()\n```\n"}
{"task_id": "BigCodeBench/1063", "solution": "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(arr):\n    # Ensure input is a 2D numpy array\n    if not isinstance(arr, np.ndarray) or arr.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n\n    # Initialize PCA\n    pca = PCA(n_components=1)\n    pca.fit(arr)\n\n    # Plot variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(1), pca.explained_variance_ratio_)\n    ax.set_title(\"Explained Variance Ratio of Principal Components\")\n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Variance Ratio\")\n    ax.set_xticks([])  # Hide x-axis ticks\n\n    return ax", "raw_solution": "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(arr):\n    # Ensure input is a 2D numpy array\n    if not isinstance(arr, np.ndarray) or arr.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n\n    # Initialize PCA\n    pca = PCA(n_components=1)\n    pca.fit(arr)\n\n    # Plot variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(1), pca.explained_variance_ratio_)\n    ax.set_title(\"Explained Variance Ratio of Principal Components\")\n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Variance Ratio\")\n    ax.set_xticks([])  # Hide x-axis ticks\n\n    return ax\n\n# Example usage:\n# arr = np.array([[i+j for i in range(3)] for j in range(5)])\n# axes = task_func(arr)\n# print(axes.get_title())\n```\n"}
{"task_id": "BigCodeBench/1064", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    \"\"\"\n    Ek diye gaye 2D numerical array ka heatmap plot karta hai and pratyek row ke sum ko prints.\n    Heatmap ka color range array mein minimum and maximum values par aadhaarit hai.\n\n    Notes:\n        Function pratyek row ka sum ki ganana karta hai and in values ko prints.\n        Heatmap original array par aadhaarit plot kiya gaya hai, jiske color range ko array mein minimum se maximum value tak set kiya gaya hai.\n\n    Parameters:\n    - arr (numpy.array): Ek 2D numpy array numerical values ka.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Plot kiye gaye heatmap ke saath axes object.\n\n    Requirements:\n    - numpy\n    - seaborn\n\n    Example:\n    >>> arr = np.array([[i + j for i in range(3)] for j in range(5)])\n    >>> ax = task_func(arr)\n    >>> ax.get_title()\n    'Heatmap of the 2D Array'\n\n    \"\"\"\n    # Calculate the sum of each row\n    row_sums = np.sum(arr, axis=1)\n    print(\"Sum of each row:\", row_sums)\n\n    # Plot the heatmap\n    fig, ax = plt.subplots()\n    sns.heatmap(arr, ax=ax, vmin=np.min(arr), vmax=np.max(arr), cmap='viridis')\n    ax.set_title('Heatmap of the 2D Array')\n\n    return ax", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(arr):\n    \"\"\"\n    Ek diye gaye 2D numerical array ka heatmap plot karta hai and pratyek row ke sum ko prints.\n    Heatmap ka color range array mein minimum and maximum values par aadhaarit hai.\n\n    Notes:\n        Function pratyek row ka sum ki ganana karta hai and in values ko prints.\n        Heatmap original array par aadhaarit plot kiya gaya hai, jiske color range ko array mein minimum se maximum value tak set kiya gaya hai.\n\n    Parameters:\n    - arr (numpy.array): Ek 2D numpy array numerical values ka.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Plot kiye gaye heatmap ke saath axes object.\n\n    Requirements:\n    - numpy\n    - seaborn\n\n    Example:\n    >>> arr = np.array([[i + j for i in range(3)] for j in range(5)])\n    >>> ax = task_func(arr)\n    >>> ax.get_title()\n    'Heatmap of the 2D Array'\n\n    \"\"\"\n    # Calculate the sum of each row\n    row_sums = np.sum(arr, axis=1)\n    print(\"Sum of each row:\", row_sums)\n\n    # Plot the heatmap\n    fig, ax = plt.subplots()\n    sns.heatmap(arr, ax=ax, vmin=np.min(arr), vmax=np.max(arr), cmap='viridis')\n    ax.set_title('Heatmap of the 2D Array')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1065", "solution": "from scipy import fftpack\nfrom matplotlib import pyplot as plt\nimport numpy as np\ndef task_func(arr):\n    \"\"\"\n    Ek 2D numpy array mein pratyek row ke sum par ek fast Fourier transform (FFT) karta hai and\n    FFT coefficients ke absolute values ko plot karta hai.\n\n    Parameters:\n    - arr (numpy.ndarray): Ek 2D numpy array.\n\n    Returns:\n        matplotlib.axes.Axes: FFT coefficients ke absolute values ke plot ko pradarshit karne wala ek Axes axes.\n\n    Requirements:\n    - scipy.fftpack\n    - matplotlib.pyplot\n\n    Example:\n    >>> import numpy as np\n    >>> arr = np.array([[i + j for i in range(3)] for j in range(5)])\n    >>> ax = task_func(arr)\n    >>> ax.get_title()\n    'Absolute values of FFT coefficients'\n\n    \"\"\"\n    # Calculate the FFT for each row in the array\n    fft_coeffs = fftpack.fft(arr, axis=1)\n    \n    # Calculate the absolute values of the FFT coefficients\n    abs_fft_coeffs = np.abs(fft_coeffs)\n    \n    # Plot the absolute values of the FFT coefficients\n    fig, ax = plt.subplots()\n    ax.plot(abs_fft_coeffs)\n    ax.set_title('Absolute values of FFT coefficients')\n    ax.set_xlabel('Row index')\n    ax.set_ylabel('Absolute value')\n    \n    return ax", "raw_solution": "from scipy import fftpack\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef task_func(arr):\n    \"\"\"\n    Ek 2D numpy array mein pratyek row ke sum par ek fast Fourier transform (FFT) karta hai and\n    FFT coefficients ke absolute values ko plot karta hai.\n\n    Parameters:\n    - arr (numpy.ndarray): Ek 2D numpy array.\n\n    Returns:\n        matplotlib.axes.Axes: FFT coefficients ke absolute values ke plot ko pradarshit karne wala ek Axes axes.\n\n    Requirements:\n    - scipy.fftpack\n    - matplotlib.pyplot\n\n    Example:\n    >>> import numpy as np\n    >>> arr = np.array([[i + j for i in range(3)] for j in range(5)])\n    >>> ax = task_func(arr)\n    >>> ax.get_title()\n    'Absolute values of FFT coefficients'\n\n    \"\"\"\n    # Calculate the FFT for each row in the array\n    fft_coeffs = fftpack.fft(arr, axis=1)\n    \n    # Calculate the absolute values of the FFT coefficients\n    abs_fft_coeffs = np.abs(fft_coeffs)\n    \n    # Plot the absolute values of the FFT coefficients\n    fig, ax = plt.subplots()\n    ax.plot(abs_fft_coeffs)\n    ax.set_title('Absolute values of FFT coefficients')\n    ax.set_xlabel('Row index')\n    ax.set_ylabel('Absolute value')\n    \n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/1066", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\ndef task_func(num_samples=NUM_SAMPLES, num_outliers=NUM_OUTLIERS):\n    # Generate normal data\n    data = np.random.normal(0, 1, num_samples)\n\n    # Generate artificially roop se pesh kiye gae outliers\n    outliers = np.random.uniform(-10, 10, num_outliers)\n    data = np.concatenate([data, outliers])\n\n    # Sort data\n    data.sort()\n\n    # Calculate Q1 and Q3\n    Q1 = np.percentile(data, 25)\n    Q3 = np.percentile(data, 75)\n\n    # Calculate IQR\n    IQR = Q3 - Q1\n\n    # Define outlier range\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # Detect outliers\n    outliers_detected = data[(data < lower_bound) | (data > upper_bound)]\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=30, alpha=0.7, color='blue')\n    ax.set_title('Histogram of Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return data, outliers_detected, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\n\ndef task_func(num_samples=NUM_SAMPLES, num_outliers=NUM_OUTLIERS):\n    # Generate normal data\n    data = np.random.normal(0, 1, num_samples)\n\n    # Generate artificially roop se pesh kiye gae outliers\n    outliers = np.random.uniform(-10, 10, num_outliers)\n    data = np.concatenate([data, outliers])\n\n    # Sort data\n    data.sort()\n\n    # Calculate Q1 and Q3\n    Q1 = np.percentile(data, 25)\n    Q3 = np.percentile(data, 75)\n\n    # Calculate IQR\n    IQR = Q3 - Q1\n\n    # Define outlier range\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # Detect outliers\n    outliers_detected = data[(data < lower_bound) | (data > upper_bound)]\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=30, alpha=0.7, color='blue')\n    ax.set_title('Histogram of Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return data, outliers_detected, ax\n```\n"}
{"task_id": "BigCodeBench/1067", "solution": "import requests\nimport logging\ndef task_func(repo_url: str) -> dict:\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    try:\n        response = requests.get(repo_url)\n        response.raise_for_status()\n        repo_info = response.json()\n\n        # Check for rate limit exceeded\n        if 'rate' in response.headers and 'limit' in response.headers['rate']:\n            limit = int(response.headers['rate']['limit'])\n            remaining = int(response.headers['rate']['remaining'])\n            if remaining < limit / 10:\n                logger.warning(\"Rate limit is close to being exceeded.\")\n\n        return repo_info\n\n    except requests.exceptions.HTTPError as http_err:\n        logger.error(f\"HTTP error occurred: {http_err}\")\n        raise\n    except requests.exceptions.RequestException as err:\n        logger.error(f\"An error occurred: {err}\")\n        raise", "raw_solution": "import requests\nimport logging\n\ndef task_func(repo_url: str) -> dict:\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    try:\n        response = requests.get(repo_url)\n        response.raise_for_status()\n        repo_info = response.json()\n\n        # Check for rate limit exceeded\n        if 'rate' in response.headers and 'limit' in response.headers['rate']:\n            limit = int(response.headers['rate']['limit'])\n            remaining = int(response.headers['rate']['remaining'])\n            if remaining < limit / 10:\n                logger.warning(\"Rate limit is close to being exceeded.\")\n\n        return repo_info\n\n    except requests.exceptions.HTTPError as http_err:\n        logger.error(f\"HTTP error occurred: {http_err}\")\n        raise\n    except requests.exceptions.RequestException as err:\n        logger.error(f\"An error occurred: {err}\")\n        raise\n```\nThis script uses the requests library to make an HTTP GET request to the provided GitHub repository URL. It also uses the logging library to log information and warnings. The function checks for rate limit exceeded and logs a warning if the remaining requests are close to the limit. It also handles HTTP and general request exceptions.\n"}
{"task_id": "BigCodeBench/1068", "solution": "import warnings\nimport sqlite3\nimport pandas as pd\ndef task_func(db_path, query, warn_large_dataset=True):\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n        # Execute the SQL query\n        df = pd.read_sql_query(query, conn)\n        # Close the database connection\n        conn.close()\n\n        # Check if the dataset is large and warn if required\n        if warn_large_dataset and len(df) > 10000:\n            warnings.warn(\"Data mein 10000 se more rows. hain.\")\n\n        return df\n\n    except Exception as e:\n        raise Exception(f\"database se data prapt karne mein error: {str(e)}\") from e", "raw_solution": "import warnings\nimport sqlite3\nimport pandas as pd\n\ndef task_func(db_path, query, warn_large_dataset=True):\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n        # Execute the SQL query\n        df = pd.read_sql_query(query, conn)\n        # Close the database connection\n        conn.close()\n\n        # Check if the dataset is large and warn if required\n        if warn_large_dataset and len(df) > 10000:\n            warnings.warn(\"Data mein 10000 se more rows. hain.\")\n\n        return df\n\n    except Exception as e:\n        raise Exception(f\"database se data prapt karne mein error: {str(e)}\") from e\n\n# Example usage:\n# data = task_func('/path/to/sqlite.db', 'SELECT * FROM table_name')\n# print(data)\n```\n"}
{"task_id": "BigCodeBench/1069", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    df = pd.DataFrame(data_dict)\n    axes = []\n\n    for column in df.columns:\n        ax = df[column].value_counts().plot(kind='bar', title=column)\n        axes.append(ax)\n        plt.show()\n\n        if len(df[column].value_counts()) != len(df[column].unique()):\n            print(f\"The distribution of values in column '{column}' is not uniform.\")\n\n    return axes", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    df = pd.DataFrame(data_dict)\n    axes = []\n\n    for column in df.columns:\n        ax = df[column].value_counts().plot(kind='bar', title=column)\n        axes.append(ax)\n        plt.show()\n\n        if len(df[column].value_counts()) != len(df[column].unique()):\n            print(f\"The distribution of values in column '{column}' is not uniform.\")\n\n    return axes\n```\nThis script creates a bar plot for each column in the DataFrame, and checks if the distribution of values is uniform. If not, it prints a message. The function returns a list of matplotlib Axes objects, one for each column.\n"}
{"task_id": "BigCodeBench/1070", "solution": "import pandas as pd\nfrom random import shuffle\nPOSSIBLE_VALUES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\ndef task_func(list_of_lists):\n    dataframes = []\n    for column_names in list_of_lists:\n        if len(column_names) != len(POSSIBLE_VALUES):\n            raise ValueError(\"Each sublist must have the same length as POSSIBLE_VALUES.\")\n        \n        # Create a DataFrame with the given column names\n        df = pd.DataFrame(columns=column_names)\n        \n        # Shuffle the possible values\n        shuffled_values = POSSIBLE_VALUES[:]\n        shuffle(shuffled_values)\n        \n        # Assign the shuffled values to the DataFrame\n        for col in column_names:\n            df[col] = shuffled_values[:]\n            shuffled_values = shuffled_values[len(column_names):]\n        \n        dataframes.append(df)\n    \n    return dataframes", "raw_solution": "import pandas as pd\nfrom random import shuffle\n\n# Constants\nPOSSIBLE_VALUES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\n\ndef task_func(list_of_lists):\n    dataframes = []\n    for column_names in list_of_lists:\n        if len(column_names) != len(POSSIBLE_VALUES):\n            raise ValueError(\"Each sublist must have the same length as POSSIBLE_VALUES.\")\n        \n        # Create a DataFrame with the given column names\n        df = pd.DataFrame(columns=column_names)\n        \n        # Shuffle the possible values\n        shuffled_values = POSSIBLE_VALUES[:]\n        shuffle(shuffled_values)\n        \n        # Assign the shuffled values to the DataFrame\n        for col in column_names:\n            df[col] = shuffled_values[:]\n            shuffled_values = shuffled_values[len(column_names):]\n        \n        dataframes.append(df)\n    \n    return dataframes\n\n# Example usage:\n# random.seed(0)\n# dfs = task_func([['x', 'y', 'z'], ['a', 'b', 'c']])\n# print(dfs[0].head())\n```\n"}
{"task_id": "BigCodeBench/1071", "solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\nCOLORS = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\ndef task_func(list_of_lists):\n    color_cycle = cycle(COLORS)\n    x_values = np.arange(len(list_of_lists[0])) if list_of_lists else []\n\n    fig, ax = plt.subplots()\n\n    for i, inner_list in enumerate(list_of_lists):\n        if not inner_list:\n            continue\n        shuffle(inner_list)\n        ax.plot(x_values, inner_list, color=next(color_cycle), label=f\"Line {i+1}\")\n\n    ax.set_title(\"Shuffled Line Plot\")\n    ax.set_xlabel(\"X values\")\n    ax.set_ylabel(\"Y values\")\n    ax.legend()\n\n    return fig, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\n\nCOLORS = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\n\n\ndef task_func(list_of_lists):\n    color_cycle = cycle(COLORS)\n    x_values = np.arange(len(list_of_lists[0])) if list_of_lists else []\n\n    fig, ax = plt.subplots()\n\n    for i, inner_list in enumerate(list_of_lists):\n        if not inner_list:\n            continue\n        shuffle(inner_list)\n        ax.plot(x_values, inner_list, color=next(color_cycle), label=f\"Line {i+1}\")\n\n    ax.set_title(\"Shuffled Line Plot\")\n    ax.set_xlabel(\"X values\")\n    ax.set_ylabel(\"Y values\")\n    ax.legend()\n\n    return fig, ax\n```\n"}
{"task_id": "BigCodeBench/1072", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(list_of_lists):\n    series_list = []\n    for inner_list in list_of_lists:\n        series = pd.Series(inner_list)\n        series.index = np.arange(1, len(series) + 1)\n        np.random.seed(42)\n        series = series.sample(frac=1).reset_index(drop=True)\n        series_list.append(series)\n    return series_list", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(list_of_lists):\n    series_list = []\n    for inner_list in list_of_lists:\n        series = pd.Series(inner_list)\n        series.index = np.arange(1, len(series) + 1)\n        np.random.seed(42)\n        series = series.sample(frac=1).reset_index(drop=True)\n        series_list.append(series)\n    return series_list\n```\n"}
{"task_id": "BigCodeBench/1073", "solution": "import time\nimport matplotlib.pyplot as plt\ndef task_func(time_strings, time_format=\"%d/%m/%Y %H:%M:%S.%f\"):\n    try:\n        # Parse the time strings\n        parsed_times = [time.strptime(ts, time_format) for ts in time_strings]\n        \n        # Extract microseconds\n        microseconds = [t.tm_sec * 1e6 + t.tm_usec for t in parsed_times]\n        \n        # Plot the histogram\n        plt.hist(microseconds, bins=10, edgecolor='black')\n        plt.xlabel('Microseconds')\n        plt.ylabel('Frequency')\n        plt.title('Histogram of Microseconds')\n        \n        return plt.gca()\n    except ValueError as e:\n        print(f\"Error parsing time strings: {e}\")\n        return None\ntime_strings = ['30/03/2009 16:31:32.123', '15/04/2010 14:25:46.789', '20/12/2011 12:34:56.000']", "raw_solution": "import time\nimport matplotlib.pyplot as plt\n\ndef task_func(time_strings, time_format=\"%d/%m/%Y %H:%M:%S.%f\"):\n    try:\n        # Parse the time strings\n        parsed_times = [time.strptime(ts, time_format) for ts in time_strings]\n        \n        # Extract microseconds\n        microseconds = [t.tm_sec * 1e6 + t.tm_usec for t in parsed_times]\n        \n        # Plot the histogram\n        plt.hist(microseconds, bins=10, edgecolor='black')\n        plt.xlabel('Microseconds')\n        plt.ylabel('Frequency')\n        plt.title('Histogram of Microseconds')\n        \n        return plt.gca()\n    except ValueError as e:\n        print(f\"Error parsing time strings: {e}\")\n        return None\n\n# Example usage:\ntime_strings = ['30/03/2009 16:31:32.123', '15/04/2010 14:25:46.789', '20/12/2011 12:34:56.000']\nax = task_func(time_strings)\nif ax:\n    plt.show()\n```\nThis script first parses the time strings into a list of `time.struct_time` objects, then extracts the microseconds component from each time, and finally plots a histogram of the microseconds. If any time string cannot be parsed, the function prints an error message and returns `None`.\n"}
{"task_id": "BigCodeBench/1074", "solution": "import pytz\nfrom dateutil.parser import parse\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_string, from_tz, to_tz):\n    \"\"\"\n    Convert a time string from one timezone to another, taking into account daylight saving time.\n\n    Parameters:\n    - time_string (str): A time string in 'dd/mm/yy HH:MM:SS.fff' format. This string should represent a valid date and time.\n    - from_tz (str): The timezone of the time_string. Timezone should be a valid IANA name (e.g., 'UTC', 'America/New_York').\n    - to_tz (str): The timezone to convert the time_string to. This should also be a valid IANA name (e.g., 'Asia/Tokyo').\n\n    Returns:\n        str: The converted time string in 'dd/mm/yy HH:MM:SS.fff' format. Conversion takes into account any differences in daylight saving rules between the source and target timezones.\n\n    Requirements:\n    - pytz\n    - dateutil\n\n    Example:\n    >>> task_func('30/03/09 16:31:32.123', 'UTC', 'America/New_York')\n    '30/03/09 12:31:32.123000'\n    \"\"\"\n    # Parse the time string into a datetime object\n    naive_datetime = parse(time_string)\n    \n    # Get the source and target timezones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_datetime = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the target timezone\n    converted_datetime = aware_datetime.astimezone(to_timezone)\n    \n    # Return the converted datetime in the specified format\n    return converted_datetime.strftime(TIME_FORMAT)", "raw_solution": "import pytz\nfrom dateutil.parser import parse\n\n# Constants\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\n\ndef task_func(time_string, from_tz, to_tz):\n    \"\"\"\n    Convert a time string from one timezone to another, taking into account daylight saving time.\n\n    Parameters:\n    - time_string (str): A time string in 'dd/mm/yy HH:MM:SS.fff' format. This string should represent a valid date and time.\n    - from_tz (str): The timezone of the time_string. Timezone should be a valid IANA name (e.g., 'UTC', 'America/New_York').\n    - to_tz (str): The timezone to convert the time_string to. This should also be a valid IANA name (e.g., 'Asia/Tokyo').\n\n    Returns:\n        str: The converted time string in 'dd/mm/yy HH:MM:SS.fff' format. Conversion takes into account any differences in daylight saving rules between the source and target timezones.\n\n    Requirements:\n    - pytz\n    - dateutil\n\n    Example:\n    >>> task_func('30/03/09 16:31:32.123', 'UTC', 'America/New_York')\n    '30/03/09 12:31:32.123000'\n    \"\"\"\n    # Parse the time string into a datetime object\n    naive_datetime = parse(time_string)\n    \n    # Get the source and target timezones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_datetime = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the target timezone\n    converted_datetime = aware_datetime.astimezone(to_timezone)\n    \n    # Return the converted datetime in the specified format\n    return converted_datetime.strftime(TIME_FORMAT)\n```\n"}
{"task_id": "BigCodeBench/1075", "solution": "import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings):\n    # Convert strings to datetime objects\n    datetime_objects = [datetime.datetime.strptime(ts, TIME_FORMAT) for ts in time_strings]\n    \n    # Calculate differences in seconds\n    differences = np.diff([dt.timestamp() for dt in datetime_objects])\n    \n    # Plot differences\n    fig, ax = plt.subplots()\n    ax.bar(range(len(differences)), differences)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Seconds')\n    ax.set_title('Time Differences Between Consecutive Datetime Objects')\n    \n    return ax\ntime_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']", "raw_solution": "import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\n\ndef task_func(time_strings):\n    # Convert strings to datetime objects\n    datetime_objects = [datetime.datetime.strptime(ts, TIME_FORMAT) for ts in time_strings]\n    \n    # Calculate differences in seconds\n    differences = np.diff([dt.timestamp() for dt in datetime_objects])\n    \n    # Plot differences\n    fig, ax = plt.subplots()\n    ax.bar(range(len(differences)), differences)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Seconds')\n    ax.set_title('Time Differences Between Consecutive Datetime Objects')\n    \n    return ax\n\n# Example usage:\ntime_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']\nax = task_func(time_strings)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/1076", "solution": "from datetime import datetime\nimport pandas as pd\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings, target_tz):\n    utc_times = []\n    converted_times = []\n\n    for time_str in time_strings:\n        utc_time = datetime.strptime(time_str, TIME_FORMAT)\n        utc_time = utc_time.replace(tzinfo=ZoneInfo('UTC'))\n        target_time = utc_time.astimezone(ZoneInfo(target_tz))\n        utc_times.append(utc_time.strftime(TIME_FORMAT))\n        converted_times.append(target_time.strftime(TIME_FORMAT))\n\n    df = pd.DataFrame({\n        'Original Time': utc_times,\n        'Converted Time': converted_times\n    })\n\n    return df\ntime_strings = ['30/03/09 16:31:32.123', '15/04/10 14:25:46.789', '20/12/11 12:34:56.000']", "raw_solution": "from datetime import datetime\nimport pandas as pd\n\n# For Python versions lower than 3.9, use 'pytz' instead of 'zoneinfo'\ntry:\n    from zoneinfo import ZoneInfo\nexcept ImportError:\n    from pytz import timezone as ZoneInfo\n\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\ndef task_func(time_strings, target_tz):\n    utc_times = []\n    converted_times = []\n\n    for time_str in time_strings:\n        utc_time = datetime.strptime(time_str, TIME_FORMAT)\n        utc_time = utc_time.replace(tzinfo=ZoneInfo('UTC'))\n        target_time = utc_time.astimezone(ZoneInfo(target_tz))\n        utc_times.append(utc_time.strftime(TIME_FORMAT))\n        converted_times.append(target_time.strftime(TIME_FORMAT))\n\n    df = pd.DataFrame({\n        'Original Time': utc_times,\n        'Converted Time': converted_times\n    })\n\n    return df\n\n# Example usage:\ntime_strings = ['30/03/09 16:31:32.123', '15/04/10 14:25:46.789', '20/12/11 12:34:56.000']\ndf = task_func(time_strings, 'America/New_York')"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    # Convert time strings to datetime objects\n    tz = pytz.timezone(timezone)\n    datetime_objs = [datetime.strptime(ts, '%d/%m/%y %H:%M:%S.%f').replace(tzinfo=pytz.utc).astimezone(tz) for ts in time_strings]\n    \n    # Convert datetime objects to seconds since epoch\n    epoch_seconds = [(dt - datetime(1970, 1, 1, tzinfo=tz)).total_seconds() for dt in datetime_objs]\n    \n    # Calculate differences between consecutive timestamps\n    differences = np.diff(epoch_seconds)\n    \n    # Return mean difference or 0.0 if there are less than 2 timestamps\n    return np.mean(differences) if len(differences) > 0 else 0.0\ntime_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']", "raw_solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    # Convert time strings to datetime objects\n    tz = pytz.timezone(timezone)\n    datetime_objs = [datetime.strptime(ts, '%d/%m/%y %H:%M:%S.%f').replace(tzinfo=pytz.utc).astimezone(tz) for ts in time_strings]\n    \n    # Convert datetime objects to seconds since epoch\n    epoch_seconds = [(dt - datetime(1970, 1, 1, tzinfo=tz)).total_seconds() for dt in datetime_objs]\n    \n    # Calculate differences between consecutive timestamps\n    differences = np.diff(epoch_seconds)\n    \n    # Return mean difference or 0.0 if there are less than 2 timestamps\n    return np.mean(differences) if len(differences) > 0 else 0.0\n\n# Example usage:\ntime_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']\nmean_diff = task_func(time_strings, 'America/New_York')"}
{"task_id": "BigCodeBench/1078", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    bin_edges = np.arange(len(unique) + 1) - 0.5\n    plt.hist(arr, bins=bin_edges, edgecolor='black')\n    plt.xlabel('Unique Values')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Values')\n    plt.show()\n    \n    uniform_distribution = len(unique) == 1\n    return uniform_distribution, plt.gca()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    bin_edges = np.arange(len(unique) + 1) - 0.5\n    plt.hist(arr, bins=bin_edges, edgecolor='black')\n    plt.xlabel('Unique Values')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Values')\n    plt.show()\n    \n    uniform_distribution = len(unique) == 1\n    return uniform_distribution, plt.gca()\n```\n"}
{"task_id": "BigCodeBench/1079", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert price strings to floats\n    data['Price'] = data['Price_String'].apply(lambda x: float(x.replace(',', '')))\n    \n    # Calculate statistics\n    mean_price = np.mean(data['Price'])\n    median_price = np.median(data['Price'])\n    std_dev_price = np.std(data['Price'], ddof=1)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(data['Price'], bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n    ax.set_title('Histogram of Product Prices')\n    ax.set_xlabel('Price')\n    ax.set_ylabel('Frequency')\n    \n    # Return results and plot\n    return {\n        'mean': mean_price,\n        'median': median_price,\n        'std_dev': std_dev_price\n    }, ax\ndata = {\n    'Product': ['Apple', 'Banana'],\n    'Price_String': ['1,234.00', '567.89']\n}", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert price strings to floats\n    data['Price'] = data['Price_String'].apply(lambda x: float(x.replace(',', '')))\n    \n    # Calculate statistics\n    mean_price = np.mean(data['Price'])\n    median_price = np.median(data['Price'])\n    std_dev_price = np.std(data['Price'], ddof=1)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(data['Price'], bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n    ax.set_title('Histogram of Product Prices')\n    ax.set_xlabel('Price')\n    ax.set_ylabel('Frequency')\n    \n    # Return results and plot\n    return {\n        'mean': mean_price,\n        'median': median_price,\n        'std_dev': std_dev_price\n    }, ax\n\n# Example usage\ndata = {\n    'Product': ['Apple', 'Banana'],\n    'Price_String': ['1,234.00', '567.89']\n}\nresults, ax = task_func(data)"}
{"task_id": "BigCodeBench/1080", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nDATA = {\n    \"Area_String\": [\"1,000\", \"2,000\", \"3,000\", \"4,000\", \"5,000\"],\n    \"Price\": [100, 200, 300, 400, 500],\n}\ndef task_func(area_string, data=DATA):\n    # Convert the area string to an integer\n    area = int(area_string.replace(',', ''))\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n    \n    # Convert the 'Area_String' column to integers\n    df['Area'] = df['Area_String'].str.replace(',', '').astype(int)\n    \n    # Create a Linear Regression model\n    model = LinearRegression()\n    \n    # Train the model\n    model.fit(df[['Area']], df['Price'])\n    \n    # Predict the price for the given area\n    predicted_price = model.predict([[area]])[0]\n    \n    return predicted_price", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nDATA = {\n    \"Area_String\": [\"1,000\", \"2,000\", \"3,000\", \"4,000\", \"5,000\"],\n    \"Price\": [100, 200, 300, 400, 500],\n}\n\ndef task_func(area_string, data=DATA):\n    # Convert the area string to an integer\n    area = int(area_string.replace(',', ''))\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n    \n    # Convert the 'Area_String' column to integers\n    df['Area'] = df['Area_String'].str.replace(',', '').astype(int)\n    \n    # Create a Linear Regression model\n    model = LinearRegression()\n    \n    # Train the model\n    model.fit(df[['Area']], df['Price'])\n    \n    # Predict the price for the given area\n    predicted_price = model.predict([[area]])[0]\n    \n    return predicted_price\n\n# Example usage:\n# print(task_func('6,000'))  # Output: 600.0\n```\n"}
{"task_id": "BigCodeBench/1081", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data=None):\n    # Default data\n    default_data = {\n        'Weight_String': ['60.5', '65.7', '70.2', '75.9', '80.1'],\n        'Height': [160, 165, 170, 175, 180]\n    }\n\n    # Use provided data or default data\n    if data is None:\n        data = default_data\n\n    # Convert string weights to floats\n    try:\n        data['Weight_String'] = [float(weight) for weight in data['Weight_String']]\n    except ValueError:\n        raise ValueError(\"All weights must be in string format that can be converted to float.\")\n\n    # Create a DataFrame\n    df = pd.DataFrame(data)\n\n    # Create a scatter plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.scatterplot(x='Weight_String', y='Height', data=df)\n    ax.set_title('Weight vs Height')\n    ax.set_xlabel('Weight (in kg)')\n    ax.set_ylabel('Height (in cm)')\n\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data=None):\n    # Default data\n    default_data = {\n        'Weight_String': ['60.5', '65.7', '70.2', '75.9', '80.1'],\n        'Height': [160, 165, 170, 175, 180]\n    }\n\n    # Use provided data or default data\n    if data is None:\n        data = default_data\n\n    # Convert string weights to floats\n    try:\n        data['Weight_String'] = [float(weight) for weight in data['Weight_String']]\n    except ValueError:\n        raise ValueError(\"All weights must be in string format that can be converted to float.\")\n\n    # Create a DataFrame\n    df = pd.DataFrame(data)\n\n    # Create a scatter plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.scatterplot(x='Weight_String', y='Height', data=df)\n    ax.set_title('Weight vs Height')\n    ax.set_xlabel('Weight (in kg)')\n    ax.set_ylabel('Height (in cm)')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1082", "solution": "import pandas as pd\nfrom scipy.stats import pearsonr\ndef task_func(data):\n    # Convert string scores to float\n    data['Score'] = pd.to_numeric(data['Score_String'], errors='coerce')\n\n    # Encode categorical grades to numerical values\n    grade_mapping = {'A+': 5, 'A': 4, 'A-': 3, 'B+': 2, 'B': 1, 'B-': 0, 'C+': -1, 'C': -2, 'C-': -3, 'D+': -4, 'D': -5, 'D-': -6}\n    data['Encoded_Grade'] = data['Grade'].map(grade_mapping)\n\n    # Drop rows with NaN values\n    data = data.dropna()\n\n    # Calculate Pearson correlation coefficient\n    correlation, _ = pearsonr(data['Score'], data['Encoded_Grade'])\n\n    return correlation\ndata = {\n    'Score_String': ['80.5', '85.7', '90.2'],\n    'Grade': ['B', 'B+', 'A-']\n}", "raw_solution": "import pandas as pd\nfrom scipy.stats import pearsonr\n\ndef task_func(data):\n    # Convert string scores to float\n    data['Score'] = pd.to_numeric(data['Score_String'], errors='coerce')\n\n    # Encode categorical grades to numerical values\n    grade_mapping = {'A+': 5, 'A': 4, 'A-': 3, 'B+': 2, 'B': 1, 'B-': 0, 'C+': -1, 'C': -2, 'C-': -3, 'D+': -4, 'D': -5, 'D-': -6}\n    data['Encoded_Grade'] = data['Grade'].map(grade_mapping)\n\n    # Drop rows with NaN values\n    data = data.dropna()\n\n    # Calculate Pearson correlation coefficient\n    correlation, _ = pearsonr(data['Score'], data['Encoded_Grade'])\n\n    return correlation\n\n# Example usage\ndata = {\n    'Score_String': ['80.5', '85.7', '90.2'],\n    'Grade': ['B', 'B+', 'A-']\n}\n"}
{"task_id": "BigCodeBench/1083", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Validate input data\n    if not all(key in data for key in ['Salary_String', 'Experience']):\n        raise ValueError(\"Input data must contain 'Salary_String' and 'Experience' keys.\")\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Handle empty data\n    if df.empty:\n        fig, ax = plt.subplots()\n        ax.set_title(\"No Data Available\")\n        return ax\n\n    # Convert 'Salary_String' to float\n    try:\n        df['Salary'] = df['Salary_String'].str.replace(',', '').astype(float)\n    except ValueError as e:\n        raise ValueError(f\"Error converting 'Salary_String' to float: {e}\")\n\n    # Normalize 'Salary' using MinMaxScaler\n    scaler = MinMaxScaler()\n    df['Normalized_Salary'] = scaler.fit_transform(df[['Salary']])\n\n    # Plot data\n    fig, ax = plt.subplots()\n    ax.scatter(df['Experience'], df['Normalized_Salary'])\n    ax.set_xlabel('Experience')\n    ax.set_ylabel('Normalized Salary')\n    ax.set_title('Normalized Salary vs Experience')\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Validate input data\n    if not all(key in data for key in ['Salary_String', 'Experience']):\n        raise ValueError(\"Input data must contain 'Salary_String' and 'Experience' keys.\")\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Handle empty data\n    if df.empty:\n        fig, ax = plt.subplots()\n        ax.set_title(\"No Data Available\")\n        return ax\n\n    # Convert 'Salary_String' to float\n    try:\n        df['Salary'] = df['Salary_String'].str.replace(',', '').astype(float)\n    except ValueError as e:\n        raise ValueError(f\"Error converting 'Salary_String' to float: {e}\")\n\n    # Normalize 'Salary' using MinMaxScaler\n    scaler = MinMaxScaler()\n    df['Normalized_Salary'] = scaler.fit_transform(df[['Salary']])\n\n    # Plot data\n    fig, ax = plt.subplots()\n    ax.scatter(df['Experience'], df['Normalized_Salary'])\n    ax.set_xlabel('Experience')\n    ax.set_ylabel('Normalized Salary')\n    ax.set_title('Normalized Salary vs Experience')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1084", "solution": "import pandas as pd\nfrom sklearn.feature_selection import f_oneway\nimport matplotlib.pyplot as plt\ndef task_func(data_file_path: str):\n    # Load the data\n    df = pd.read_csv(data_file_path)\n\n    # Convert string representations of numbers to floats\n    df = df.apply(pd.to_numeric, errors='coerce')\n\n    # Calculate mean and standard deviation for each numerical column\n    means = df.mean()\n    std_devs = df.std()\n\n    # Plot histogram for each numerical column\n    axes = []\n    for column in df.select_dtypes(include='number').columns:\n        ax = df[column].plot(kind='hist', title=f'Histogram of {column}', bins=10)\n        axes.append(ax)\n\n    # Perform ANOVA test for each pair of numerical columns\n    anova_results = pd.DataFrame(columns=['F-value', 'P-value'])\n    for col1 in df.select_dtypes(include='number').columns:\n        for col2 in df.select_dtypes(include='number').columns:\n            if col1 != col2:\n                f_value, p_value = f_oneway(df[col1], df[col2])\n                anova_results.loc[f'{col1}-{col2}', ['F-value', 'P-value']] = [f_value, p_value]\n\n    return means, std_devs, axes, anova_results", "raw_solution": "import pandas as pd\nfrom sklearn.feature_selection import f_oneway\nimport matplotlib.pyplot as plt\n\ndef task_func(data_file_path: str):\n    # Load the data\n    df = pd.read_csv(data_file_path)\n\n    # Convert string representations of numbers to floats\n    df = df.apply(pd.to_numeric, errors='coerce')\n\n    # Calculate mean and standard deviation for each numerical column\n    means = df.mean()\n    std_devs = df.std()\n\n    # Plot histogram for each numerical column\n    axes = []\n    for column in df.select_dtypes(include='number').columns:\n        ax = df[column].plot(kind='hist', title=f'Histogram of {column}', bins=10)\n        axes.append(ax)\n\n    # Perform ANOVA test for each pair of numerical columns\n    anova_results = pd.DataFrame(columns=['F-value', 'P-value'])\n    for col1 in df.select_dtypes(include='number').columns:\n        for col2 in df.select_dtypes(include='number').columns:\n            if col1 != col2:\n                f_value, p_value = f_oneway(df[col1], df[col2])\n                anova_results.loc[f'{col1}-{col2}', ['F-value', 'P-value']] = [f_value, p_value]\n\n    return means, std_devs, axes, anova_results\n```\n"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    cleaned_text = re.sub(r'[^\\w\\s]', '', text.lower())\n    \n    # Split text into words\n    words = cleaned_text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n    \n    # Plot the bar chart\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    \n    return common_words, ax", "raw_solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    cleaned_text = re.sub(r'[^\\w\\s]', '', text.lower())\n    \n    # Split text into words\n    words = cleaned_text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n    \n    # Plot the bar chart\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    \n    return common_words, ax\n```\n"}
{"task_id": "BigCodeBench/1086", "solution": "import string\nimport random\nimport pandas as pd\nimport numpy as np\nNUM_SAMPLES = 1000\ndef task_func():\n    # Generate random strings\n    random_strings = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(NUM_SAMPLES)]\n\n    # Generate random floats with formatting\n    random_floats = [f\"{random.uniform(0, 10000):,.2f}\" for _ in range(NUM_SAMPLES)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'String Field': random_strings,\n        'Float Field': random_floats\n    })\n\n    return df", "raw_solution": "import string\nimport random\nimport pandas as pd\nimport numpy as np\n\n# Constants\nNUM_SAMPLES = 1000  # Number of samples\n\ndef task_func():\n    # Generate random strings\n    random_strings = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(NUM_SAMPLES)]\n\n    # Generate random floats with formatting\n    random_floats = [f\"{random.uniform(0, 10000):,.2f}\" for _ in range(NUM_SAMPLES)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'String Field': random_strings,\n        'Float Field': random_floats\n    })\n\n    return df\n\n# Example usage"}
{"task_id": "BigCodeBench/1087", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mean=123456.908, std_dev=1.2, save_plots=False):\n    # Generate random sample from a normal distribution\n    data = np.random.normal(mean, std_dev, 1000)\n\n    # Calculate skewness and kurtosis\n    skewness = stats.skew(data)\n    kurtosis = stats.kurtosis(data)\n\n    # Plot histogram\n    plt.figure(figsize=(10, 5))\n    plt.hist(data, bins=30, alpha=0.7, color='blue')\n    plt.title('Histogram of Random Sample')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    if save_plots:\n        histogram_path = 'histogram_plot.png'\n        plt.savefig(histogram_path)\n    else:\n        plt.show()\n\n    # Plot QQ-plot\n    plt.figure(figsize=(10, 5))\n    stats.probplot(data, dist=\"norm\", plot=plt)\n    plt.title('QQ-plot of Random Sample')\n    if save_plots:\n        qq_plot_path = 'qq_plot.png'\n        plt.savefig(qq_plot_path)\n    else:\n        plt.show()\n\n    # Return paths of saved plots if save_plots is True, otherwise return empty list\n    plot_paths = [histogram_path, qq_plot_path] if save_plots else []\n\n    return skewness, kurtosis, plot_paths", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mean=123456.908, std_dev=1.2, save_plots=False):\n    # Generate random sample from a normal distribution\n    data = np.random.normal(mean, std_dev, 1000)\n\n    # Calculate skewness and kurtosis\n    skewness = stats.skew(data)\n    kurtosis = stats.kurtosis(data)\n\n    # Plot histogram\n    plt.figure(figsize=(10, 5))\n    plt.hist(data, bins=30, alpha=0.7, color='blue')\n    plt.title('Histogram of Random Sample')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    if save_plots:\n        histogram_path = 'histogram_plot.png'\n        plt.savefig(histogram_path)\n    else:\n        plt.show()\n\n    # Plot QQ-plot\n    plt.figure(figsize=(10, 5))\n    stats.probplot(data, dist=\"norm\", plot=plt)\n    plt.title('QQ-plot of Random Sample')\n    if save_plots:\n        qq_plot_path = 'qq_plot.png'\n        plt.savefig(qq_plot_path)\n    else:\n        plt.show()\n\n    # Return paths of saved plots if save_plots is True, otherwise return empty list\n    plot_paths = [histogram_path, qq_plot_path] if save_plots else []\n\n    return skewness, kurtosis, plot_paths\n```\n"}
{"task_id": "BigCodeBench/1088", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data=None):\n    if data is None:\n        data = np.random.rand(100, 5)\n\n    df = pd.DataFrame(data)\n\n    # Replace values less than 0.5 with zeros\n    df[df < 0.5] = 0\n\n    # Standardize the data\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n\n    return pd.DataFrame(df_scaled, columns=df.columns)", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data=None):\n    if data is None:\n        data = np.random.rand(100, 5)\n\n    df = pd.DataFrame(data)\n\n    # Replace values less than 0.5 with zeros\n    df[df < 0.5] = 0\n\n    # Standardize the data\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n\n    return pd.DataFrame(df_scaled, columns=df.columns)\n```\n"}
{"task_id": "BigCodeBench/1089", "solution": "import numpy as np\nfrom collections import Counter\ndef task_func(list_of_tuples):\n    # Extracting numeric values and categories from the list of tuples\n    values = np.array([t[0] for t in list_of_tuples])\n    categories = [t[1] for t in list_of_tuples]\n    \n    # Calculating the sum of numeric values\n    sum_of_values = np.sum(values)\n    \n    # Counting occurrences of each category\n    category_counts = Counter(categories)\n    \n    return sum_of_values, category_counts\nlist_of_tuples = [(5, 'fruits'), (9, 'vegetables'), (-1, 'Dairy'), (-2, 'Bakery'), (4, 'Meat')]", "raw_solution": "import numpy as np\nfrom collections import Counter\n\ndef task_func(list_of_tuples):\n    # Extracting numeric values and categories from the list of tuples\n    values = np.array([t[0] for t in list_of_tuples])\n    categories = [t[1] for t in list_of_tuples]\n    \n    # Calculating the sum of numeric values\n    sum_of_values = np.sum(values)\n    \n    # Counting occurrences of each category\n    category_counts = Counter(categories)\n    \n    return sum_of_values, category_counts\n\n# Example usage\nlist_of_tuples = [(5, 'fruits'), (9, 'vegetables'), (-1, 'Dairy'), (-2, 'Bakery'), (4, 'Meat')]\nsum_of_values, category_counts = task_func(list_of_tuples)"}
{"task_id": "BigCodeBench/1090", "solution": "import ast\nimport json\nfrom collections import Counter\ndef task_func(file_pointer):\n    \"\"\"\n    Ek diye gaye file pointer se json file padhta hai, jo dictionaries ko actual dictionaries ke roop mein darshane wali strings ka moolyonkan karta hai, \n    and json data mein sabhi dictionaries entries mein pratyek key ki frequency ki ganana karta hai.\n\n    Notes:\n        Yeh function manta hai ki input json data, dictionaries or strings ki ek list hai jinhien dictionaries ke roop mein moolyonkan kiya ja sakta hai.\n\n    Parameters:\n    - file_pointer (file object): json data wali file object kholen. Yeh file\n    - already sahi mode mein khuli honi chahiye (udharan ke liye, r ke liye 'r').\n\n    Returns:\n        collections.Counter: Ek collections object jo dictionaries mein paayi gayi pratyek key ki frequency ka pratinidhitva karta hai.\n\n    Requirements:\n    - ast\n    - json\n    - collections.Counter\n\n    Example:\n    >>> with open(\"data.json\", \"r\") as file:\n    >>>    key_frequency = task_func(file)\n    >>>    print(key_frequency)\n    Counter({'name': 5, 'age': 5, 'city': 3})\n\n    \"\"\"\n    data = json.load(file_pointer)\n    counter = Counter()\n\n    for item in data:\n        if isinstance(item, dict):\n            counter.update(item.keys())\n\n    return counter", "raw_solution": "import ast\nimport json\nfrom collections import Counter\n\ndef task_func(file_pointer):\n    \"\"\"\n    Ek diye gaye file pointer se json file padhta hai, jo dictionaries ko actual dictionaries ke roop mein darshane wali strings ka moolyonkan karta hai, \n    and json data mein sabhi dictionaries entries mein pratyek key ki frequency ki ganana karta hai.\n\n    Notes:\n        Yeh function manta hai ki input json data, dictionaries or strings ki ek list hai jinhien dictionaries ke roop mein moolyonkan kiya ja sakta hai.\n\n    Parameters:\n    - file_pointer (file object): json data wali file object kholen. Yeh file\n    - already sahi mode mein khuli honi chahiye (udharan ke liye, r ke liye 'r').\n\n    Returns:\n        collections.Counter: Ek collections object jo dictionaries mein paayi gayi pratyek key ki frequency ka pratinidhitva karta hai.\n\n    Requirements:\n    - ast\n    - json\n    - collections.Counter\n\n    Example:\n    >>> with open(\"data.json\", \"r\") as file:\n    >>>    key_frequency = task_func(file)\n    >>>    print(key_frequency)\n    Counter({'name': 5, 'age': 5, 'city': 3})\n\n    \"\"\"\n    data = json.load(file_pointer)\n    counter = Counter()\n\n    for item in data:\n        if isinstance(item, dict):\n            counter.update(item.keys())\n\n    return counter\n```\n"}
{"task_id": "BigCodeBench/1091", "solution": "import ast\nimport os\nimport glob\ndef task_func(directory):\n    \"\"\"\n    Nirdeshit directory (directory) mein sabhi text files (text files) mein dictionaries (dictionaries) ke sabhi unicode string (string) representations (representations) ko Python dictionaries (Python dictionaries) mein badlen.\n\n    Notes:\n        Sunischit karen ki directory (directory) mein text filein (text files) dictionaries (dictionaries) ke valid unicode string (string) representations (representations) shaamil hon.\n\n    Parameters:\n    - directory (str): text files (text files) wali directory (directory) ka path.\n\n    Returns:\n        list: text files (text files) se nikaale gaye dictionaries (dictionaries) ki ek list (list).\n\n    Requirements:\n    - ast\n    - os\n    - glob\n\n    Raises:\n    - Yadi text file(len) mein invalid dictionary (dictionary) representation (representation) hai to function (function) ValueError dega.\n\n    Example:\n    >>> task_func(\"sample_directory/\")\n    [{'key1': 'value1'}, {'key2': 'value2'}]\n\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    text_files = glob.glob(os.path.join(directory, '*.txt'))\n    dictionaries = []\n\n    for file in text_files:\n        with open(file, 'r') as f:\n            try:\n                # Using ast.literal_eval to safely evaluate the string as a dictionary\n                dictionary = ast.literal_eval(f.read())\n                if not isinstance(dictionary, dict):\n                    raise ValueError(f\"The file {file} does not contain a valid dictionary.\")\n                dictionaries.append(dictionary)\n            except (ValueError, SyntaxError) as e:\n                raise ValueError(f\"Error parsing file {file}: {e}\")\n\n    return dictionaries", "raw_solution": "import ast\nimport os\nimport glob\n\n# Constants\nDIRECTORY = 'data'\n\ndef task_func(directory):\n    \"\"\"\n    Nirdeshit directory (directory) mein sabhi text files (text files) mein dictionaries (dictionaries) ke sabhi unicode string (string) representations (representations) ko Python dictionaries (Python dictionaries) mein badlen.\n\n    Notes:\n        Sunischit karen ki directory (directory) mein text filein (text files) dictionaries (dictionaries) ke valid unicode string (string) representations (representations) shaamil hon.\n\n    Parameters:\n    - directory (str): text files (text files) wali directory (directory) ka path.\n\n    Returns:\n        list: text files (text files) se nikaale gaye dictionaries (dictionaries) ki ek list (list).\n\n    Requirements:\n    - ast\n    - os\n    - glob\n\n    Raises:\n    - Yadi text file(len) mein invalid dictionary (dictionary) representation (representation) hai to function (function) ValueError dega.\n\n    Example:\n    >>> task_func(\"sample_directory/\")\n    [{'key1': 'value1'}, {'key2': 'value2'}]\n\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    text_files = glob.glob(os.path.join(directory, '*.txt'))\n    dictionaries = []\n\n    for file in text_files:\n        with open(file, 'r') as f:\n            try:\n                # Using ast.literal_eval to safely evaluate the string as a dictionary\n                dictionary = ast.literal_eval(f.read())\n                if not isinstance(dictionary, dict):\n                    raise ValueError(f\"The file {file} does not contain a valid dictionary.\")\n                dictionaries.append(dictionary)\n            except (ValueError, SyntaxError) as e:\n                raise ValueError(f\"Error parsing file {file}: {e}\")\n\n    return dictionaries\n```\n"}
{"task_id": "BigCodeBench/1092", "solution": "import ast\nimport requests\nfrom bs4 import BeautifulSoup\ndef task_func(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        return []\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    scripts = soup.find_all('script')\n\n    result = []\n    for script in scripts:\n        try:\n            parsed = ast.literal_eval(script.string)\n            if isinstance(parsed, dict):\n                result.append(parsed)\n        except (ValueError, SyntaxError):\n            continue\n\n    return result", "raw_solution": "import ast\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        return []\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    scripts = soup.find_all('script')\n\n    result = []\n    for script in scripts:\n        try:\n            parsed = ast.literal_eval(script.string)\n            if isinstance(parsed, dict):\n                result.append(parsed)\n        except (ValueError, SyntaxError):\n            continue\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/1093", "solution": "import ast\nimport re\ndef task_func(text_file: str) -> list:\n    \"\"\"\n    This function reads a text file, extracts all string representations of dictionaries using regular expressions,\n    and parses them into Python dictionaries.\n\n    Parameters:\n    - text_file (str): The path to the text file.\n\n    Returns:\n    - list: A list of dictionaries parsed from the text file.\n\n    Requirements:\n    - ast\n    - re\n\n    Example:\n    >>> task_func(\"sample.txt\")\n    [{'key1': 'value1'}, {'key2': 'value2'}]\n    \"\"\"\n    with open(text_file, 'r') as file:\n        content = file.read()\n\n    # Regular expression to find all dictionary string representations\n    dict_strings = re.findall(r'\\{.*?\\}', content)\n\n    dictionaries = []\n    for dict_str in dict_strings:\n        try:\n            # Parse the string into a Python dictionary using ast.literal_eval\n            dict_obj = ast.literal_eval(dict_str)\n            if isinstance(dict_obj, dict):\n                dictionaries.append(dict_obj)\n        except (ValueError, SyntaxError):\n            # Skip invalid dictionary strings\n            continue\n\n    return dictionaries", "raw_solution": "import ast\nimport re\n\ndef task_func(text_file: str) -> list:\n    \"\"\"\n    This function reads a text file, extracts all string representations of dictionaries using regular expressions,\n    and parses them into Python dictionaries.\n\n    Parameters:\n    - text_file (str): The path to the text file.\n\n    Returns:\n    - list: A list of dictionaries parsed from the text file.\n\n    Requirements:\n    - ast\n    - re\n\n    Example:\n    >>> task_func(\"sample.txt\")\n    [{'key1': 'value1'}, {'key2': 'value2'}]\n    \"\"\"\n    with open(text_file, 'r') as file:\n        content = file.read()\n\n    # Regular expression to find all dictionary string representations\n    dict_strings = re.findall(r'\\{.*?\\}', content)\n\n    dictionaries = []\n    for dict_str in dict_strings:\n        try:\n            # Parse the string into a Python dictionary using ast.literal_eval\n            dict_obj = ast.literal_eval(dict_str)\n            if isinstance(dict_obj, dict):\n                dictionaries.append(dict_obj)\n        except (ValueError, SyntaxError):\n            # Skip invalid dictionary strings\n            continue\n\n    return dictionaries\n```\n"}
{"task_id": "BigCodeBench/1094", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\ndef task_func(text):\n    # Initialize the tokenizer\n    tokenizer = RegexpTokenizer(r'\\$[a-zA-Z]+')\n    \n    # Tokenize the text\n    tokens = tokenizer.tokenize(text)\n    \n    # Remove the dollar symbol and convert to lowercase\n    words = [word[1:].lower() for word in tokens]\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the most common words\n    most_common_words = word_counts.most_common()\n    \n    return most_common_words\ntext = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\n\ndef task_func(text):\n    # Initialize the tokenizer\n    tokenizer = RegexpTokenizer(r'\\$[a-zA-Z]+')\n    \n    # Tokenize the text\n    tokens = tokenizer.tokenize(text)\n    \n    # Remove the dollar symbol and convert to lowercase\n    words = [word[1:].lower() for word in tokens]\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the most common words\n    most_common_words = word_counts.most_common()\n    \n    return most_common_words\n\n# Example usage\ntext = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\""}
{"task_id": "BigCodeBench/1095", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport os\ndef task_func(text, output_filename):\n    # Initialize the tokenizer\n    tokenizer = RegexpTokenizer(r'\\$[^\\s]+')\n    \n    # Tokenize the text\n    tokens = tokenizer.tokenize(text)\n    \n    # Remove punctuation from each token\n    cleaned_tokens = [token.strip(punctuation) for token in tokens]\n    \n    # Write the cleaned tokens to the output file\n    with open(output_filename, 'w') as file:\n        for token in cleaned_tokens:\n            file.write(token + '\\n')\n    \n    # Return the absolute path of the output file\n    return os.path.abspath(output_filename)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport os\n\ndef task_func(text, output_filename):\n    # Initialize the tokenizer\n    tokenizer = RegexpTokenizer(r'\\$[^\\s]+')\n    \n    # Tokenize the text\n    tokens = tokenizer.tokenize(text)\n    \n    # Remove punctuation from each token\n    cleaned_tokens = [token.strip(punctuation) for token in tokens]\n    \n    # Write the cleaned tokens to the output file\n    with open(output_filename, 'w') as file:\n        for token in cleaned_tokens:\n            file.write(token + '\\n')\n    \n    # Return the absolute path of the output file\n    return os.path.abspath(output_filename)\n\n# Example usage"}
{"task_id": "BigCodeBench/1096", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport csv\nimport os\ndef task_func(text, filename):\n    tokenizer = RegexpTokenizer(r'\\$[a-zA-Z]+')\n    dollar_words = tokenizer.tokenize(text)\n\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"Word\"])\n        for word in dollar_words:\n            writer.writerow([word[1:]])\n\n    return os.path.abspath(filename)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport csv\nimport os\n\nPUNCTUATION = set(punctuation)\n\ndef task_func(text, filename):\n    tokenizer = RegexpTokenizer(r'\\$[a-zA-Z]+')\n    dollar_words = tokenizer.tokenize(text)\n\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"Word\"])\n        for word in dollar_words:\n            writer.writerow([word[1:]])\n\n    return os.path.abspath(filename)\n\n# Example usage:\n# text = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"\n# print(task_func(text, 'dollar_words.csv'))\n```\n"}
{"task_id": "BigCodeBench/1097", "solution": "import re\nfrom string import punctuation\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    \"\"\"\n    specified text ko re karen, jismein urls, stopwords aur punctuation ko hata diya jae.\n\n    Parameters:\n    - text (str): re karne ke liye text.\n\n    Returns:\n        str: str text, jismein urls, purvanirdharit stopwords aur viram chihna hata specified hain.\n\n    Requirements:\n    - re\n    - string.punctuation\n\n    Example:\n    >>> task_func('Visit https://www.python.org for more info. I love to eat apples.')\n    'Visit info love eat apples'\n\n    \"\"\"\n    # Remove URLs\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', punctuation))\n\n    # Remove stopwords\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in PREDEFINED_STOPWORDS]\n\n    # Join the words back into a string\n    cleaned_text = ' '.join(filtered_words)\n\n    return cleaned_text", "raw_solution": "import re\nfrom string import punctuation\n\n# Predefined list of common stopwords\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\n\ndef task_func(text):\n    \"\"\"\n    specified text ko re karen, jismein urls, stopwords aur punctuation ko hata diya jae.\n\n    Parameters:\n    - text (str): re karne ke liye text.\n\n    Returns:\n        str: str text, jismein urls, purvanirdharit stopwords aur viram chihna hata specified hain.\n\n    Requirements:\n    - re\n    - string.punctuation\n\n    Example:\n    >>> task_func('Visit https://www.python.org for more info. I love to eat apples.')\n    'Visit info love eat apples'\n\n    \"\"\"\n    # Remove URLs\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', punctuation))\n\n    # Remove stopwords\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in PREDEFINED_STOPWORDS]\n\n    # Join the words back into a string\n    cleaned_text = ' '.join(filtered_words)\n\n    return cleaned_text\n```\n"}
{"task_id": "BigCodeBench/1098", "solution": "import re\nfrom collections import Counter\ndef task_func(text, top_n):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_count = Counter(words)\n    \n    # Get the top N most common words\n    most_common_words = word_count.most_common(top_n)\n    \n    return most_common_words", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(text, top_n):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_count = Counter(words)\n    \n    # Get the top N most common words\n    most_common_words = word_count.most_common(top_n)\n    \n    return most_common_words\n\n# Example usage:\n# result = task_func('Visit https://www.python.org for more info. Python is great. I love Python.', 2)\n# print(result)  # Output: [('Python', 2), ('Visit', 1)]\n```\n"}
{"task_id": "BigCodeBench/1099", "solution": "import re\nfrom collections import Counter\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http[s]?://\\S+', '', text)\n\n    # Tokenize the text into words\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Filter out the stopwords\n    filtered_words = [word for word in words if word not in PREDEFINED_STOPWORDS]\n\n    # Count the frequency of each word\n    word_count = Counter(filtered_words)\n\n    # Return the most common words\n    return word_count.most_common()", "raw_solution": "import re\nfrom collections import Counter\n\n# Predefined list of common stopwords\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\n\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http[s]?://\\S+', '', text)\n\n    # Tokenize the text into words\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Filter out the stopwords\n    filtered_words = [word for word in words if word not in PREDEFINED_STOPWORDS]\n\n    # Count the frequency of each word\n    word_count = Counter(filtered_words)\n\n    # Return the most common words\n    return word_count.most_common()\n```\n"}
{"task_id": "BigCodeBench/1100", "solution": "import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef task_func(texts):\n    # Remove URLs from the texts\n    cleaned_texts = [re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) for text in texts]\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the cleaned texts\n    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n\n    # Get feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract TF-IDF scores\n    tfidf_scores = tfidf_matrix.toarray()\n\n    # Round the scores to 8 decimal places\n    rounded_scores = [[round(score, 8) for score in row] for row in tfidf_scores]\n\n    return rounded_scores, feature_names.tolist()", "raw_solution": "import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef task_func(texts):\n    # Remove URLs from the texts\n    cleaned_texts = [re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) for text in texts]\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the cleaned texts\n    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n\n    # Get feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract TF-IDF scores\n    tfidf_scores = tfidf_matrix.toarray()\n\n    # Round the scores to 8 decimal places\n    rounded_scores = [[round(score, 8) for score in row] for row in tfidf_scores]\n\n    return rounded_scores, feature_names.tolist()\n```\n"}
{"task_id": "BigCodeBench/1101", "solution": "import subprocess\nimport os\nimport glob\nimport time\ndef task_func(test_dir):\n    # Get all python files in the directory\n    python_files = glob.glob(os.path.join(test_dir, '*.py'))\n    execution_times = {}\n\n    for file in python_files:\n        start_time = time.time()\n        # Run the python file using subprocess\n        subprocess.run(['python', file], check=True)\n        end_time = time.time()\n        # Calculate the execution time\n        execution_times[os.path.basename(file)] = end_time - start_time\n\n    return execution_times", "raw_solution": "import subprocess\nimport os\nimport glob\nimport time\n\ndef task_func(test_dir):\n    # Get all python files in the directory\n    python_files = glob.glob(os.path.join(test_dir, '*.py'))\n    execution_times = {}\n\n    for file in python_files:\n        start_time = time.time()\n        # Run the python file using subprocess\n        subprocess.run(['python', file], check=True)\n        end_time = time.time()\n        # Calculate the execution time\n        execution_times[os.path.basename(file)] = end_time - start_time\n\n    return execution_times\n```\nThis script uses the `subprocess` module to run Python scripts in the specified directory, and the `glob` module to find all Python files in the directory. The `time` module is used to measure the execution time of each script. The function returns a dictionary where the keys are the names of the Python scripts and the values are their execution times.\n"}
{"task_id": "BigCodeBench/1102", "solution": "import subprocess\nimport shlex\nfrom datetime import datetime\ndef task_func(script_path: str) -> dict:\n    '''\n    Run an R script and return the start time, end time, decoded stdout, and decoded stderr as a dictionary.\n    \n    Requirements:\n    - subprocess\n    - shlex\n    - datetime\n    \n    Parameters:\n    - script_path (str): Path to the R script to be executed.\n    \n    Returns:\n    - dict: A dictionary containing the start time, end time, stdout, and stderr of the script run.\n    \n    Example:\n    >>> task_func(\"/path/to/script.r\")\n    {\n        'Start Time': '2023-09-26 14:30:00',\n        'End Time': '2023-09-26 14:32:00',\n        'Stdout': 'Script output here...',\n        'Stderr': 'Any errors here...'\n    }\n    '''\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    command = f\"Rscript {script_path}\"\n    process = subprocess.Popen(shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    return {\n        'Start Time': start_time,\n        'End Time': end_time,\n        'Stdout': stdout.decode('utf-8'),\n        'Stderr': stderr.decode('utf-8')\n    }", "raw_solution": "import subprocess\nimport shlex\nfrom datetime import datetime\n\ndef task_func(script_path: str) -> dict:\n    '''\n    Run an R script and return the start time, end time, decoded stdout, and decoded stderr as a dictionary.\n    \n    Requirements:\n    - subprocess\n    - shlex\n    - datetime\n    \n    Parameters:\n    - script_path (str): Path to the R script to be executed.\n    \n    Returns:\n    - dict: A dictionary containing the start time, end time, stdout, and stderr of the script run.\n    \n    Example:\n    >>> task_func(\"/path/to/script.r\")\n    {\n        'Start Time': '2023-09-26 14:30:00',\n        'End Time': '2023-09-26 14:32:00',\n        'Stdout': 'Script output here...',\n        'Stderr': 'Any errors here...'\n    }\n    '''\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    command = f\"Rscript {script_path}\"\n    process = subprocess.Popen(shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    return {\n        'Start Time': start_time,\n        'End Time': end_time,\n        'Stdout': stdout.decode('utf-8'),\n        'Stderr': stderr.decode('utf-8')\n    }\n```\n"}
{"task_id": "BigCodeBench/1103", "solution": "import subprocess\nimport shutil\nimport os\ndef task_func(script_path: str, temp_dir: str) -> str:\n    # Create a temporary directory\n    os.makedirs(temp_dir, exist_ok=True)\n    \n    # Copy the script to the temporary directory\n    temp_script_path = os.path.join(temp_dir, os.path.basename(script_path))\n    shutil.copy(script_path, temp_script_path)\n    \n    # Execute the script using subprocess\n    try:\n        subprocess.run(['python', temp_script_path], check=True)\n        return \"Script executed successfully!\"\n    except subprocess.CalledProcessError:\n        return \"Script execution failed!\"", "raw_solution": "import subprocess\nimport shutil\nimport os\n\ndef task_func(script_path: str, temp_dir: str) -> str:\n    # Create a temporary directory\n    os.makedirs(temp_dir, exist_ok=True)\n    \n    # Copy the script to the temporary directory\n    temp_script_path = os.path.join(temp_dir, os.path.basename(script_path))\n    shutil.copy(script_path, temp_script_path)\n    \n    # Execute the script using subprocess\n    try:\n        subprocess.run(['python', temp_script_path], check=True)\n        return \"Script executed successfully!\"\n    except subprocess.CalledProcessError:\n        return \"Script execution failed!\"\n```\n"}
{"task_id": "BigCodeBench/1104", "solution": "import subprocess\nimport os\nimport threading\ndef task_func(script_path: str, timeout: int = 60) -> str:\n    if not os.path.isfile(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} was not found.\")\n\n    def run_script(script_path):\n        try:\n            subprocess.run(['python', script_path], check=True)\n            return \"Script executed successfully.\"\n        except subprocess.CalledProcessError:\n            return \"Terminating process due to error.\"\n\n    def timeout_handler():\n        if not script_process.is_alive():\n            return\n        script_process.terminate()\n        script_process.wait()\n\n    script_process = subprocess.Popen(['python', script_path])\n    timer = threading.Timer(timeout, timeout_handler)\n    timer.start()\n\n    try:\n        script_process.wait(timeout)\n    except subprocess.TimeoutExpired:\n        timer.cancel()\n        return \"Terminating process due to timeout.\"\n\n    timer.cancel()\n    return run_script(script_path)", "raw_solution": "import subprocess\nimport os\nimport threading\n\ndef task_func(script_path: str, timeout: int = 60) -> str:\n    if not os.path.isfile(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} was not found.\")\n\n    def run_script(script_path):\n        try:\n            subprocess.run(['python', script_path], check=True)\n            return \"Script executed successfully.\"\n        except subprocess.CalledProcessError:\n            return \"Terminating process due to error.\"\n\n    def timeout_handler():\n        if not script_process.is_alive():\n            return\n        script_process.terminate()\n        script_process.wait()\n\n    script_process = subprocess.Popen(['python', script_path])\n    timer = threading.Timer(timeout, timeout_handler)\n    timer.start()\n\n    try:\n        script_process.wait(timeout)\n    except subprocess.TimeoutExpired:\n        timer.cancel()\n        return \"Terminating process due to timeout.\"\n\n    timer.cancel()\n    return run_script(script_path)\n```\nThis script uses the `subprocess` module to run the script and the `os` module to check if the file exists. The `threading` module is used to handle the timeout. The `run_script` function is used to check if the script ran successfully or not. The `timeout_handler` function is used to terminate the script if it exceeds the timeout limit.\n"}
{"task_id": "BigCodeBench/1105", "solution": "import subprocess\nimport os\nimport time\nimport glob\ndef task_func(r_script_path: str, output_path: str, duration: int) -> (bool, str):\n    start_time = time.time()\n    try:\n        subprocess.run(['Rscript', r_script_path], check=True)\n    except subprocess.CalledProcessError:\n        return False, 'File not generated within the specified duration.'\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n\n    if elapsed_time <= duration:\n        output_files = glob.glob(os.path.join(output_path, '*.csv'))\n        if output_files:\n            return True, 'File generated successfully within the specified duration.'\n        else:\n            return False, 'File not generated within the specified duration.'\n    else:\n        return False, 'File not generated within the specified duration.'", "raw_solution": "import subprocess\nimport os\nimport time\nimport glob\n\ndef task_func(r_script_path: str, output_path: str, duration: int) -> (bool, str):\n    start_time = time.time()\n    try:\n        subprocess.run(['Rscript', r_script_path], check=True)\n    except subprocess.CalledProcessError:\n        return False, 'File not generated within the specified duration.'\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n\n    if elapsed_time <= duration:\n        output_files = glob.glob(os.path.join(output_path, '*.csv'))\n        if output_files:\n            return True, 'File generated successfully within the specified duration.'\n        else:\n            return False, 'File not generated within the specified duration.'\n    else:\n        return False, 'File not generated within the specified duration.'\n```\nThis script uses the subprocess module to run the R script and the glob module to check for the output file. It measures the time taken to run the script and compares it with the specified duration. If the script runs within the specified duration and an output file is generated, it returns True and a success message. Otherwise, it returns False and an error message.\n"}
{"task_id": "BigCodeBench/1106", "solution": "from datetime import datetime\nimport os\nfrom pathlib import Path\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(file_path):\n    \"\"\"\n    Ek file ke creation ka time nirdharit karen and use ek formatted string 'y-dh:% Y-y-dh:% m-y-dh:% d y-dh:% H:y-dh:% M:y-dh:% S' mein badlen.\n\n    Parameters:\n    - file_path (str): file ka path.\n\n    Returns:\n        str: file ke creation ka time 'y-dh:%Y-y-dh:%m-y-dh:%d y-dh:%H:y-dh:%M-y-dh:%S' format mein.\n\n    Requirements:\n    - datetime.datetime\n    - os\n    - pathlib.Path\n\n    Example:\n    >>> task_func('/path/to/file.txt')\n    '2023-09-28 12:30:45'\n\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Get file creation time\n    file_creation_time = datetime.fromtimestamp(Path(file_path).stat().st_ctime)\n\n    # Format the creation time\n    formatted_time = file_creation_time.strftime(DATE_FORMAT)\n\n    return formatted_time", "raw_solution": "from datetime import datetime\nimport os\nfrom pathlib import Path\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(file_path):\n    \"\"\"\n    Ek file ke creation ka time nirdharit karen and use ek formatted string 'y-dh:% Y-y-dh:% m-y-dh:% d y-dh:% H:y-dh:% M:y-dh:% S' mein badlen.\n\n    Parameters:\n    - file_path (str): file ka path.\n\n    Returns:\n        str: file ke creation ka time 'y-dh:%Y-y-dh:%m-y-dh:%d y-dh:%H:y-dh:%M-y-dh:%S' format mein.\n\n    Requirements:\n    - datetime.datetime\n    - os\n    - pathlib.Path\n\n    Example:\n    >>> task_func('/path/to/file.txt')\n    '2023-09-28 12:30:45'\n\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Get file creation time\n    file_creation_time = datetime.fromtimestamp(Path(file_path).stat().st_ctime)\n\n    # Format the creation time\n    formatted_time = file_creation_time.strftime(DATE_FORMAT)\n\n    return formatted_time\n```\n"}
{"task_id": "BigCodeBench/1107", "solution": "from datetime import datetime\nimport pytz\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(unix_timestamp, target_timezone):\n    \"\"\"\n    Convert a unix_timestamp to a specified timezone in a formatted date and time string.\n\n    Parameters:\n    - unix_timestamp (int): unix_timestamp timestamp, which is the number of seconds from the unix_timestamp (1 January, 1970, 00:00:00 UTC).\n    - target_timezone (str): target timezone as a string identifier (for example, 'America/New_York').\n\n    Returns:\n        str: A date and time string in the target timezone, formatted as 'y-%m-%d %H:%M:%S'.\n\n    Requirements:\n    - datetime.datetime\n    - pytz\n\n    Example:\n    >>> unix_timestamp = 1609459200\n    >>> target_timezone = 'America/New_York'\n    >>> task_func(unix_timestamp, target_timezone)\n    '2020-12-31 19:00:00'\n    \"\"\"\n    # Convert the unix_timestamp to a naive datetime object\n    naive_datetime = datetime.fromtimestamp(unix_timestamp)\n\n    # Get the target timezone\n    target_tz = pytz.timezone(target_timezone)\n\n    # Localize the naive datetime object to UTC\n    utc_tz = pytz.utc\n    aware_datetime = utc_tz.localize(naive_datetime)\n\n    # Convert to the target timezone\n    target_datetime = aware_datetime.astimezone(target_tz)\n\n    # Format the datetime object to a string\n    formatted_datetime = target_datetime.strftime(DATE_FORMAT)\n\n    return formatted_datetime", "raw_solution": "from datetime import datetime\nimport pytz\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\n\ndef task_func(unix_timestamp, target_timezone):\n    \"\"\"\n    Convert a unix_timestamp to a specified timezone in a formatted date and time string.\n\n    Parameters:\n    - unix_timestamp (int): unix_timestamp timestamp, which is the number of seconds from the unix_timestamp (1 January, 1970, 00:00:00 UTC).\n    - target_timezone (str): target timezone as a string identifier (for example, 'America/New_York').\n\n    Returns:\n        str: A date and time string in the target timezone, formatted as 'y-%m-%d %H:%M:%S'.\n\n    Requirements:\n    - datetime.datetime\n    - pytz\n\n    Example:\n    >>> unix_timestamp = 1609459200\n    >>> target_timezone = 'America/New_York'\n    >>> task_func(unix_timestamp, target_timezone)\n    '2020-12-31 19:00:00'\n    \"\"\"\n    # Convert the unix_timestamp to a naive datetime object\n    naive_datetime = datetime.fromtimestamp(unix_timestamp)\n\n    # Get the target timezone\n    target_tz = pytz.timezone(target_timezone)\n\n    # Localize the naive datetime object to UTC\n    utc_tz = pytz.utc\n    aware_datetime = utc_tz.localize(naive_datetime)\n\n    # Convert to the target timezone\n    target_datetime = aware_datetime.astimezone(target_tz)\n\n    # Format the datetime object to a string\n    formatted_datetime = target_datetime.strftime(DATE_FORMAT)\n\n    return formatted_datetime\n```\n"}
{"task_id": "BigCodeBench/1108", "solution": "from collections import Counter\nimport re\ndef task_func(result):\n    \"\"\"\n    Soochi \"result\" mein maujood dictionaries mein se \"url\" key se jude sabse common maan prapt karein.\n\n    Parameters:\n    - result (list): dictionaries ki ek soochi.\n\n    Returns:\n        dict: Sabse common maan aur unki counts ke saath ek dictionary.\n\n    Requirements:\n    - collections\n    - re\n\n    Example:\n    >>> result = [{\"hi\": 7, \"http://google.com\": 0}, {\"https://google.com\": 0}, {\"http://www.cwi.nl\": 1}]\n    >>> task_func(result)\n    {0: 2}\n\n    \"\"\"\n    # Extract URLs from each dictionary in the list\n    urls = []\n    for d in result:\n        for key, value in d.items():\n            if re.match(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', key):\n                urls.append((key, value))\n\n    # Count the occurrences of each URL\n    url_counts = Counter(urls)\n\n    # Find the most common URL\n    most_common_url = url_counts.most_common(1)\n\n    # Return the most common URL and its count\n    return {most_common_url[0][1]: most_common_url[0][0][1]}\nresult = [{\"hi\": 7, \"http://google.com\": 0}, {\"https://google.com\": 0}, {\"http://www.cwi.nl\": 1}]", "raw_solution": "from collections import Counter\nimport re\n\ndef task_func(result):\n    \"\"\"\n    Soochi \"result\" mein maujood dictionaries mein se \"url\" key se jude sabse common maan prapt karein.\n\n    Parameters:\n    - result (list): dictionaries ki ek soochi.\n\n    Returns:\n        dict: Sabse common maan aur unki counts ke saath ek dictionary.\n\n    Requirements:\n    - collections\n    - re\n\n    Example:\n    >>> result = [{\"hi\": 7, \"http://google.com\": 0}, {\"https://google.com\": 0}, {\"http://www.cwi.nl\": 1}]\n    >>> task_func(result)\n    {0: 2}\n\n    \"\"\"\n    # Extract URLs from each dictionary in the list\n    urls = []\n    for d in result:\n        for key, value in d.items():\n            if re.match(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', key):\n                urls.append((key, value))\n\n    # Count the occurrences of each URL\n    url_counts = Counter(urls)\n\n    # Find the most common URL\n    most_common_url = url_counts.most_common(1)\n\n    # Return the most common URL and its count\n    return {most_common_url[0][1]: most_common_url[0][0][1]}\n\n# Test cases\nresult = [{\"hi\": 7, \"http://google.com\": 0}, {\"https://google.com\": 0}, {\"http://www.cwi.nl\": 1}]"}
{"task_id": "BigCodeBench/1109", "solution": "import os\nfrom nltk import word_tokenize\ndef task_func(file_path='File.txt'):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n    \n    tokens = []\n    for line in lines:\n        tokens.extend(word_tokenize(line))\n    \n    return tokens", "raw_solution": "import os\nfrom nltk import word_tokenize\n\ndef task_func(file_path='File.txt'):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n    \n    tokens = []\n    for line in lines:\n        tokens.extend(word_tokenize(line))\n    \n    return tokens\n\n# Example usage:\n# Assuming 'sample.txt' and 'data.txt' exist and contain text.\n# print(task_func('sample.txt'))\n# print(task_func('data.txt'))\n```\n"}
{"task_id": "BigCodeBench/1110", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\ndef task_func(word_dict):\n    # Initialize a Counter object to count the frequency of each letter\n    letter_counter = Counter()\n\n    # Iterate over each word and its corresponding letter in the dictionary\n    for word, letter in word_dict.items():\n        # Increment the count for the corresponding letter\n        letter_counter[letter] += len(word)\n\n    # Convert the Counter object to a dictionary and return it\n    return dict(letter_counter)\nword_dict = {'apple': 'a', 'banana': 'b', 'cherry': 'c', 'date': 'd', 'elderberry': 'e', 'fig': 'f', 'grape': 'g', 'honeydew': 'h'}", "raw_solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\n\ndef task_func(word_dict):\n    # Initialize a Counter object to count the frequency of each letter\n    letter_counter = Counter()\n\n    # Iterate over each word and its corresponding letter in the dictionary\n    for word, letter in word_dict.items():\n        # Increment the count for the corresponding letter\n        letter_counter[letter] += len(word)\n\n    # Convert the Counter object to a dictionary and return it\n    return dict(letter_counter)\n\n# Example usage:\nword_dict = {'apple': 'a', 'banana': 'b', 'cherry': 'c', 'date': 'd', 'elderberry': 'e', 'fig': 'f', 'grape': 'g', 'honeydew': 'h'}\ncounts = task_func(word_dict)"}
{"task_id": "BigCodeBench/1111", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\nANIMAL = ['cat', 'camel', 'cow', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\ndef task_func(animal_dict):\n    # Filter out keys that are not in the ANIMAL list\n    filtered_dict = {key: value for key, value in animal_dict.items() if key in ANIMAL}\n\n    # Count the frequency of each letter in the values of the dictionary\n    letter_counts = Counter(itertools.chain.from_iterable(filtered_dict.values()))\n\n    # Sort the counts in lexicographical order\n    sorted_counts = dict(sorted(letter_counts.items()))\n\n    return sorted_counts\nanimal_dict = {'cat': 'c', 'dog': 'd', 'elephant': 'e', 'fox': 'f', 'giraffe': 'g', 'hippo': 'h', 'iguana': 'i', 'jaguar': 'j'}", "raw_solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\n\n#CONSTANT\nANIMAL = ['cat', 'camel', 'cow', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\n\ndef task_func(animal_dict):\n    # Filter out keys that are not in the ANIMAL list\n    filtered_dict = {key: value for key, value in animal_dict.items() if key in ANIMAL}\n\n    # Count the frequency of each letter in the values of the dictionary\n    letter_counts = Counter(itertools.chain.from_iterable(filtered_dict.values()))\n\n    # Sort the counts in lexicographical order\n    sorted_counts = dict(sorted(letter_counts.items()))\n\n    return sorted_counts\n\n# Example usage\nanimal_dict = {'cat': 'c', 'dog': 'd', 'elephant': 'e', 'fox': 'f', 'giraffe': 'g', 'hippo': 'h', 'iguana': 'i', 'jaguar': 'j'}\ncounts = task_func(animal_dict)"}
{"task_id": "BigCodeBench/1112", "solution": "import csv\nimport random\nimport os\nfrom datetime import datetime\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\ndef task_func(file_name=\"data.csv\"):\n    \"\"\"\n    This function generates a CSV file with random weather data.\n\n    Notes:\n        CSV file will have rows with names 'Temperature', 'Humidity', and 'Pressure'.\n        Temperature ranges from -50 to 50.\n        Humidity ranges from 0 to 100.\n        Pressure ranges from 980 to 1040.\n\n    Parameters:\n    - file_name (str): The path to the CSV file to be generated.\n\n    Returns:\n        str: The path to the generated CSV file.\n\n    Requirements:\n    - os\n    - datetime\n    - csv\n    - random\n\n    Example:\n    >>> task_func(\"data.csv\")\n    'path/to/data.csv'\n    \"\"\"\n    # Open the file in write mode\n    with open(file_name, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the header\n        writer.writerow(DATA)\n        \n        # Generate random data and write to the file\n        for _ in range(100):  # Generate 100 rows of data\n            row = [\n                random.uniform(*RANGE['Temperature']),\n                random.uniform(*RANGE['Humidity']),\n                random.uniform(*RANGE['Pressure'])\n            ]\n            writer.writerow(row)\n    \n    # Return the path to the generated file\n    return os.path.abspath(file_name)", "raw_solution": "import csv\nimport random\nimport os\nfrom datetime import datetime\n\n# Constants\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\n\ndef task_func(file_name=\"data.csv\"):\n    \"\"\"\n    This function generates a CSV file with random weather data.\n\n    Notes:\n        CSV file will have rows with names 'Temperature', 'Humidity', and 'Pressure'.\n        Temperature ranges from -50 to 50.\n        Humidity ranges from 0 to 100.\n        Pressure ranges from 980 to 1040.\n\n    Parameters:\n    - file_name (str): The path to the CSV file to be generated.\n\n    Returns:\n        str: The path to the generated CSV file.\n\n    Requirements:\n    - os\n    - datetime\n    - csv\n    - random\n\n    Example:\n    >>> task_func(\"data.csv\")\n    'path/to/data.csv'\n    \"\"\"\n    # Open the file in write mode\n    with open(file_name, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the header\n        writer.writerow(DATA)\n        \n        # Generate random data and write to the file\n        for _ in range(100):  # Generate 100 rows of data\n            row = [\n                random.uniform(*RANGE['Temperature']),\n                random.uniform(*RANGE['Humidity']),\n                random.uniform(*RANGE['Pressure'])\n            ]\n            writer.writerow(row)\n    \n    # Return the path to the generated file\n    return os.path.abspath(file_name)\n```\n"}
{"task_id": "BigCodeBench/1113", "solution": "import csv\nimport collections\ndef task_func(csv_file, emp_prefix='EMP$$'):\n    \"\"\"\n    csv file mein pratyek employee ke liye records ki number ginen.\n\n    Parameters:\n    - csv_file (str): csv file ka path. Yah parameter mandatory hai.\n    - emp_prefix (str): employee ids ka emp_prefix. Default 'EMP$$' hai.\n\n    Returns:\n        dict: pratyek employee ke liye records ki number wala ek dictionary.\n\n    Requirements:\n    - csv\n    - collections\n\n    Example:\n    >>> counts = task_func('/path/to/file.csv')\n    >>> print(counts)\n    {'emp$$001': 5, 'EMP$$002': 3}\n\n    \"\"\"\n    employee_counts = collections.defaultdict(int)\n\n    with open(csv_file, newline='') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            emp_id = row.get('employee_id', '')\n            if emp_id.startswith(emp_prefix):\n                employee_counts[emp_id] += 1\n\n    return dict(employee_counts)", "raw_solution": "import csv\nimport collections\n\ndef task_func(csv_file, emp_prefix='EMP$$'):\n    \"\"\"\n    csv file mein pratyek employee ke liye records ki number ginen.\n\n    Parameters:\n    - csv_file (str): csv file ka path. Yah parameter mandatory hai.\n    - emp_prefix (str): employee ids ka emp_prefix. Default 'EMP$$' hai.\n\n    Returns:\n        dict: pratyek employee ke liye records ki number wala ek dictionary.\n\n    Requirements:\n    - csv\n    - collections\n\n    Example:\n    >>> counts = task_func('/path/to/file.csv')\n    >>> print(counts)\n    {'emp$$001': 5, 'EMP$$002': 3}\n\n    \"\"\"\n    employee_counts = collections.defaultdict(int)\n\n    with open(csv_file, newline='') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            emp_id = row.get('employee_id', '')\n            if emp_id.startswith(emp_prefix):\n                employee_counts[emp_id] += 1\n\n    return dict(employee_counts)\n```\n"}
{"task_id": "BigCodeBench/1114", "solution": "from collections import defaultdict\nfrom random import randint\ndef task_func(dict1):\n    emp_data = defaultdict(list)\n\n    for dept_code, num_employees in dict1.items():\n        if dept_code.startswith('EMP$$'):\n            for _ in range(num_employees):\n                salary = randint(30000, 100000)  # Generating random salaries between 30,000 and 100,000\n                emp_data[dept_code].append(salary)\n\n    return dict(emp_data)", "raw_solution": "from collections import defaultdict\nfrom random import randint\n\ndef task_func(dict1):\n    emp_data = defaultdict(list)\n\n    for dept_code, num_employees in dict1.items():\n        if dept_code.startswith('EMP$$'):\n            for _ in range(num_employees):\n                salary = randint(30000, 100000)  # Generating random salaries between 30,000 and 100,000\n                emp_data[dept_code].append(salary)\n\n    return dict(emp_data)\n\n# Example usage:"}
{"task_id": "BigCodeBench/1115", "solution": "import random\nfrom string import ascii_uppercase\ndef task_func(dict1):\n    result = []\n    for dept_code, num_employees in dict1.items():\n        for _ in range(num_employees):\n            random_string = ''.join(random.choices(ascii_uppercase, k=5))\n            result.append(f\"{dept_code}{random_string}\")\n    return result", "raw_solution": "import random\nfrom string import ascii_uppercase\n\ndef task_func(dict1):\n    result = []\n    for dept_code, num_employees in dict1.items():\n        for _ in range(num_employees):\n            random_string = ''.join(random.choices(ascii_uppercase, k=5))\n            result.append(f\"{dept_code}{random_string}\")\n    return result\n\n# Example usage:\nrandom.seed(0)\nd = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\nemp_ids = task_func(d)"}
{"task_id": "BigCodeBench/1116", "solution": "import random\nimport statistics\nAGE_RANGE = (22, 60)\ndef task_func(dict1):\n    all_ages = []\n\n    for dept, num_employees in dict1.items():\n        if dept == 'EMP$$':\n            for _ in range(num_employees):\n                age = random.randint(*AGE_RANGE)\n                all_ages.append(age)\n\n    mean_age = statistics.mean(all_ages)\n    median_age = statistics.median(all_ages)\n    mode_age = statistics.multimode(all_ages)\n\n    return mean_age, median_age, mode_age", "raw_solution": "import random\nimport statistics\n\n# Constants\nAGE_RANGE = (22, 60)\n\ndef task_func(dict1):\n    all_ages = []\n\n    for dept, num_employees in dict1.items():\n        if dept == 'EMP$$':\n            for _ in range(num_employees):\n                age = random.randint(*AGE_RANGE)\n                all_ages.append(age)\n\n    mean_age = statistics.mean(all_ages)\n    median_age = statistics.median(all_ages)\n    mode_age = statistics.multimode(all_ages)\n\n    return mean_age, median_age, mode_age\n\n# Example usage:\nrandom.seed(0)\nd = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\nstats = task_func(d)"}
{"task_id": "BigCodeBench/1117", "solution": "import collections\nimport random\nimport json\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nLEVELS = ['Junior', 'Mid', 'Senior']\ndef task_func(department_data):\n    result = {}\n    for dept_code, num_employees in department_data.items():\n        if dept_code not in PREFICES:\n            raise ValueError(f\"Invalid department code: {dept_code}\")\n        \n        levels = random.choices(LEVELS, k=num_employees)\n        result[dept_code] = levels\n\n    return json.dumps(result, indent=4)", "raw_solution": "import collections\nimport random\nimport json\n\n# Constants\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nLEVELS = ['Junior', 'Mid', 'Senior']\n\ndef task_func(department_data):\n    result = {}\n    for dept_code, num_employees in department_data.items():\n        if dept_code not in PREFICES:\n            raise ValueError(f\"Invalid department code: {dept_code}\")\n        \n        levels = random.choices(LEVELS, k=num_employees)\n        result[dept_code] = levels\n\n    return json.dumps(result, indent=4)\n\n# Example usage:\nrandom.seed(0)\ndepartment_info = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\nlevel_data_json = task_func(department_info)"}
{"task_id": "BigCodeBench/1118", "solution": "import json\nimport csv\nimport requests\nfrom io import StringIO\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\ndef task_func(csv_url=CSV_URL, json_file_path=JSON_FILE):\n    # Download the CSV file\n    response = requests.get(csv_url)\n    response.raise_for_status()  # Raise an error for bad responses\n\n    # Read the CSV file\n    csv_data = StringIO(response.text)\n    reader = csv.DictReader(csv_data)\n\n    # Convert CSV data to JSON\n    json_data = [row for row in reader]\n\n    # Write the JSON data to a file\n    with open(json_file_path, 'w') as json_file:\n        json.dump(json_data, json_file, indent=4)\n\n    return json_file_path", "raw_solution": "import json\nimport csv\nimport requests\nfrom io import StringIO\n\n# Constants\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\n\ndef task_func(csv_url=CSV_URL, json_file_path=JSON_FILE):\n    # Download the CSV file\n    response = requests.get(csv_url)\n    response.raise_for_status()  # Raise an error for bad responses\n\n    # Read the CSV file\n    csv_data = StringIO(response.text)\n    reader = csv.DictReader(csv_data)\n\n    # Convert CSV data to JSON\n    json_data = [row for row in reader]\n\n    # Write the JSON data to a file\n    with open(json_file_path, 'w') as json_file:\n        json.dump(json_data, json_file, indent=4)\n\n    return json_file_path\n\n# Example usage\n# print(task_func(\"https://example.com/sample.csv\", \"sample.json\"))\n```\n"}
{"task_id": "BigCodeBench/1119", "solution": "import codecs\nimport random\nimport string\nimport hashlib\ndef task_func(password_length=10, salt=\"salty\"):\n    # Generate a random password\n    characters = string.ascii_letters + string.digits + string.punctuation\n    password = ''.join(random.choice(characters) for _ in range(password_length))\n\n    # Combine password and salt\n    combined = salt + password\n\n    # Encode the combined string using UTF-8\n    encoded_combined = combined.encode('utf-8')\n\n    # Create a SHA256 hash object\n    hash_object = hashlib.sha256()\n\n    # Update the hash object with the encoded combined string\n    hash_object.update(encoded_combined)\n\n    # Get the hexadecimal representation of the hash\n    hashed_password = hash_object.hexdigest()\n\n    return hashed_password", "raw_solution": "import codecs\nimport random\nimport string\nimport hashlib\n\ndef task_func(password_length=10, salt=\"salty\"):\n    # Generate a random password\n    characters = string.ascii_letters + string.digits + string.punctuation\n    password = ''.join(random.choice(characters) for _ in range(password_length))\n\n    # Combine password and salt\n    combined = salt + password\n\n    # Encode the combined string using UTF-8\n    encoded_combined = combined.encode('utf-8')\n\n    # Create a SHA256 hash object\n    hash_object = hashlib.sha256()\n\n    # Update the hash object with the encoded combined string\n    hash_object.update(encoded_combined)\n\n    # Get the hexadecimal representation of the hash\n    hashed_password = hash_object.hexdigest()\n\n    return hashed_password\n\n# Example usage"}
{"task_id": "BigCodeBench/1120", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    result = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f'http://ip-api.com/json/{domain}?fields=country,region,city,zip,lat,lon,timezone,isp,org,as&key={API_KEY}')\n        \n        if response.status_code == 200:\n            data = response.json()\n            result[domain] = data\n        else:\n            result[domain] = None\n\n    return result", "raw_solution": "import re\nimport urllib.parse\nimport requests\nimport json\n\ndef task_func(myString, API_KEY):\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    result = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f'http://ip-api.com/json/{domain}?fields=country,region,city,zip,lat,lon,timezone,isp,org,as&key={API_KEY}')\n        \n        if response.status_code == 200:\n            data = response.json()\n            result[domain] = data\n        else:\n            result[domain] = None\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/1121", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    result = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f\"http://ip-api.com/json/{domain}?fields=country,region,city,zip,lat,lon,timezone,isp,org,as&key={API_KEY}\")\n        \n        if response.status_code == 200:\n            data = response.json()\n            result[domain] = {\n                'status': data.get('status'),\n                'country': data.get('country'),\n                'region': data.get('region'),\n                'city': data.get('city'),\n                'zip': data.get('zip'),\n                'lat': data.get('lat'),\n                'lon': data.get('lon'),\n                'timezone': data.get('timezone'),\n                'isp': data.get('isp'),\n                'org': data.get('org'),\n                'as': data.get('as'),\n                'query': data.get('query')\n            }\n        else:\n            result[domain] = None\n\n    return result", "raw_solution": "import re\nimport urllib.parse\nimport requests\nimport json\n\ndef task_func(myString, API_KEY):\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    result = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f\"http://ip-api.com/json/{domain}?fields=country,region,city,zip,lat,lon,timezone,isp,org,as&key={API_KEY}\")\n        \n        if response.status_code == 200:\n            data = response.json()\n            result[domain] = {\n                'status': data.get('status'),\n                'country': data.get('country'),\n                'region': data.get('region'),\n                'city': data.get('city'),\n                'zip': data.get('zip'),\n                'lat': data.get('lat'),\n                'lon': data.get('lon'),\n                'timezone': data.get('timezone'),\n                'isp': data.get('isp'),\n                'org': data.get('org'),\n                'as': data.get('as'),\n                'query': data.get('query')\n            }\n        else:\n            result[domain] = None\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/1122", "solution": "import re\nimport socket\nimport urllib.parse\ndef task_func(myString):\n    # Extract all URLs from the input string\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    \n    # Dictionary to store domain and its corresponding IP\n    domain_ip = {}\n    \n    for url in urls:\n        # Parse the domain from the URL\n        domain = urllib.parse.urlparse(url).netloc\n        \n        try:\n            # Get the IP address of the domain\n            ip = socket.gethostbyname(domain)\n            domain_ip[domain] = ip\n        except socket.gaierror:\n            # If the domain cannot be resolved, set the IP to None\n            domain_ip[domain] = None\n    \n    return domain_ip", "raw_solution": "import re\nimport socket\nimport urllib.parse\n\ndef task_func(myString):\n    # Extract all URLs from the input string\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    \n    # Dictionary to store domain and its corresponding IP\n    domain_ip = {}\n    \n    for url in urls:\n        # Parse the domain from the URL\n        domain = urllib.parse.urlparse(url).netloc\n        \n        try:\n            # Get the IP address of the domain\n            ip = socket.gethostbyname(domain)\n            domain_ip[domain] = ip\n        except socket.gaierror:\n            # If the domain cannot be resolved, set the IP to None\n            domain_ip[domain] = None\n    \n    return domain_ip\n\n# Example usage:\n# print(task_func(\"Check these links: http://www.google.com, https://www.python.org\"))\n```\n"}
{"task_id": "BigCodeBench/1123", "solution": "import re\nimport urllib.parse\nimport ssl\nimport socket\ndef task_func(myString):\n    # Extract all URLs from the input string\n    urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    \n    # Dictionary to store SSL certificate expiry dates\n    ssl_dates = {}\n    \n    for url in urls:\n        try:\n            hostname = urllib.parse.urlparse(url).hostname\n            context = ssl.create_default_context()\n            with socket.create_connection((hostname, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n                    # Get the certificate\n                    cert = ssock.getpeercert()\n                    # Extract the expiry date\n                    expiry_date = cert['notAfter']\n                    ssl_dates[hostname] = expiry_date\n        except Exception as e:\n            print(f\"SSL error for {url}: {e}\")\n    \n    return ssl_dates", "raw_solution": "import re\nimport urllib.parse\nimport ssl\nimport socket\n\ndef task_func(myString):\n    # Extract all URLs from the input string\n    urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    \n    # Dictionary to store SSL certificate expiry dates\n    ssl_dates = {}\n    \n    for url in urls:\n        try:\n            hostname = urllib.parse.urlparse(url).hostname\n            context = ssl.create_default_context()\n            with socket.create_connection((hostname, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n                    # Get the certificate\n                    cert = ssock.getpeercert()\n                    # Extract the expiry date\n                    expiry_date = cert['notAfter']\n                    ssl_dates[hostname] = expiry_date\n        except Exception as e:\n            print(f\"SSL error for {url}: {e}\")\n    \n    return ssl_dates\n\n# Example usage:\n# print(task_func(\"Check these links: https://www.google.com, https://www.python.org\"))\n```\nThis script uses the `re` module to extract URLs from the input string, the `urllib.parse` module to parse the URLs, the `ssl` module to create a secure socket connection, and the `socket` module to create a network socket. It then attempts to retrieve the SSL certificate for each URL and extracts the expiry date, storing it in a dictionary. If an SSL error occurs, it is caught and printed, but the script continues with the next URL.\n"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Extract URL from the string\n    url_match = re.search(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    if not url_match:\n        return \"No valid URL found in the provided string.\"\n\n    url = url_match.group(0)\n    try:\n        # Fetch the webpage content\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the webpage content\n    soup = BeautifulSoup(response.content, 'html.parser')\n    title_tag = soup.find('title')\n\n    if not title_tag:\n        return \"No title tag found in the webpage.\"\n\n    return title_tag.text", "raw_solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Extract URL from the string\n    url_match = re.search(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    if not url_match:\n        return \"No valid URL found in the provided string.\"\n\n    url = url_match.group(0)\n    try:\n        # Fetch the webpage content\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the webpage content\n    soup = BeautifulSoup(response.content, 'html.parser')\n    title_tag = soup.find('title')\n\n    if not title_tag:\n        return \"No title tag found in the webpage.\"\n\n    return title_tag.text\n\n# Example usage:\n# print(task_func('Check this out: https://www.google.com'))\n# print(task_func('No URL here'))\n# print(task_func('Check this broken link: https://www.thisdoesnotexist12345.com'))\n```\n"}
{"task_id": "BigCodeBench/1125", "solution": "import re\nimport json\nimport requests\ndef task_func(myString, token):\n    # Extract URL from the string\n    url_match = re.search(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    if not url_match:\n        return {'message': 'No URL found in the string'}\n\n    url = url_match.group(0)\n\n    # Prepare headers and payload\n    headers = {\n        'Authorization': f'Bearer {token}',\n        'Content-Type': 'application/json'\n    }\n    payload = json.dumps({'url': url})\n\n    # Make POST request\n    response = requests.post(url, headers=headers, data=payload)\n\n    # Return response JSON\n    return response.json()", "raw_solution": "import re\nimport json\nimport requests\n\ndef task_func(myString, token):\n    # Extract URL from the string\n    url_match = re.search(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    if not url_match:\n        return {'message': 'No URL found in the string'}\n\n    url = url_match.group(0)\n\n    # Prepare headers and payload\n    headers = {\n        'Authorization': f'Bearer {token}',\n        'Content-Type': 'application/json'\n    }\n    payload = json.dumps({'url': url})\n\n    # Make POST request\n    response = requests.post(url, headers=headers, data=payload)\n\n    # Return response JSON\n    return response.json()\n\n# Example usage:\n# Assuming the API endpoint is set up to accept a POST request with a JSON payload containing a 'url' field.\n# The API returns a JSON response with a 'message' field.\n# >>> task_func('kripya check: https://www.google.com', 'aapka_token_yahaan')\n# {'message': 'URL processed successfully'}\n```\n"}
{"task_id": "BigCodeBench/1126", "solution": "import re\nimport hashlib\ndef task_func(input_str):\n    # Remove special characters, punctuation marks, and spaces using regular expressions\n    cleaned_str = re.sub(r'[^a-zA-Z0-9]', '', input_str)\n\n    # Generate SHA256 hash of the cleaned string\n    hash_object = hashlib.sha256(cleaned_str.encode())\n    hash_str = hash_object.hexdigest()\n\n    return hash_str", "raw_solution": "import re\nimport hashlib\n\ndef task_func(input_str):\n    # Remove special characters, punctuation marks, and spaces using regular expressions\n    cleaned_str = re.sub(r'[^a-zA-Z0-9]', '', input_str)\n\n    # Generate SHA256 hash of the cleaned string\n    hash_object = hashlib.sha256(cleaned_str.encode())\n    hash_str = hash_object.hexdigest()\n\n    return hash_str\n\n# Example usage:\n# print(task_func('Special $#! characters spaces 888323'))\n# Output: 'af30263c4d44d67917a4f0727191a4149e1ab615b772b2aeda859068178b146c'\n```\n"}
{"task_id": "BigCodeBench/1127", "solution": "import os\nimport re\nimport hashlib\ndef task_func(path, delimiter):\n    components = re.split(rf'{re.escape(delimiter)}+', path)\n    result = []\n\n    for component in components:\n        if component:\n            component_hash = None\n            if os.path.isfile(component):\n                with open(component, 'rb') as f:\n                    component_hash = hashlib.sha256(f.read()).hexdigest()\n            result.append((component, component_hash))\n    \n    return result", "raw_solution": "import os\nimport re\nimport hashlib\n\ndef task_func(path, delimiter):\n    components = re.split(rf'{re.escape(delimiter)}+', path)\n    result = []\n\n    for component in components:\n        if component:\n            component_hash = None\n            if os.path.isfile(component):\n                with open(component, 'rb') as f:\n                    component_hash = hashlib.sha256(f.read()).hexdigest()\n            result.append((component, component_hash))\n    \n    return result\n\n# Example usage:\n# print(task_func(\"Docs/src/file.txt\", \"/\"))\n```\nThis script uses the `os` module to check if a component is a file, the `re` module to split the path by the specified delimiter, and the `hashlib` module to compute the SHA-256 hash of a file. The function `task_func` returns a list of tuples, where each tuple contains a component of the path and its hash (if the component is a file). If the component is not a file, its hash is `None`.\n"}
{"task_id": "BigCodeBench/1128", "solution": "import json\nimport os\nimport hashlib\nimport base64\nimport time\ndef task_func(file_path, unknown_key):\n    # Load JSON data from the file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Extract the nested value using the unknown key\n    value = data['A'][unknown_key]['maindata'][0]['Info']\n\n    # Generate SHA256 hash of the value\n    hash_object = hashlib.sha256(value.encode())\n    hash_value = hash_object.hexdigest()\n\n    # Encode the hash value using base64\n    encoded_hash = base64.b64encode(hash_object.digest()).decode()\n\n    # Create a new file name with a timestamp\n    timestamp = str(int(time.time()))\n    new_file_name = f\"hashed_data_{timestamp}.txt\"\n    new_file_path = os.path.join(os.getcwd(), new_file_name)\n\n    # Write the encoded hash to the new file\n    with open(new_file_path, 'w') as new_file:\n        new_file.write(encoded_hash)\n\n    return new_file_path", "raw_solution": "import json\nimport os\nimport hashlib\nimport base64\nimport time\n\ndef task_func(file_path, unknown_key):\n    # Load JSON data from the file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Extract the nested value using the unknown key\n    value = data['A'][unknown_key]['maindata'][0]['Info']\n\n    # Generate SHA256 hash of the value\n    hash_object = hashlib.sha256(value.encode())\n    hash_value = hash_object.hexdigest()\n\n    # Encode the hash value using base64\n    encoded_hash = base64.b64encode(hash_object.digest()).decode()\n\n    # Create a new file name with a timestamp\n    timestamp = str(int(time.time()))\n    new_file_name = f\"hashed_data_{timestamp}.txt\"\n    new_file_path = os.path.join(os.getcwd(), new_file_name)\n\n    # Write the encoded hash to the new file\n    with open(new_file_path, 'w') as new_file:\n        new_file.write(encoded_hash)\n\n    return new_file_path\n```\n"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the URL using the provided key\n    url = data.get(unknown_key)\n    \n    # Check if the URL exists\n    if not url:\n        raise ValueError(f\"Key '{unknown_key}' not found in JSON data.\")\n    \n    # Make a request to download the file\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an error for bad responses\n    \n    # Generate a timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    \n    # Create a filename based on the unknown key and timestamp\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    save_dir = save_dir or os.getcwd()\n    \n    # Ensure the directory exists\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define the full file path\n    file_path = os.path.join(save_dir, filename)\n    \n    # Save the file\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n    \n    return file_path", "raw_solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the URL using the provided key\n    url = data.get(unknown_key)\n    \n    # Check if the URL exists\n    if not url:\n        raise ValueError(f\"Key '{unknown_key}' not found in JSON data.\")\n    \n    # Make a request to download the file\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an error for bad responses\n    \n    # Generate a timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    \n    # Create a filename based on the unknown key and timestamp\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    save_dir = save_dir or os.getcwd()\n    \n    # Ensure the directory exists\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define the full file path\n    file_path = os.path.join(save_dir, filename)\n    \n    # Save the file\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n    \n    return file_path\n\n# Example usage"}
{"task_id": "BigCodeBench/1130", "solution": "import os\nimport hashlib\nimport json\nfrom pathlib import Path\ndef task_func(directory: str) -> str:\n    \"\"\"\n    specified directory mein sabhi files ke liye SHA256 hash banaen, jismein up-directoryon mein maujood file bhi shaamil hain,\n    and in hash ko specified directory mein 'hashes.json' naamak ek json file mein sahejeyn.\n\n    Parameters:\n    - directory (str): directory ka path jismein hash ki jaane waali file shaamil hain.\n\n    Returns:\n        str: 'hashes.json' file ka absolute path jismein hash shaamil hain.\n\n    Requirements:\n    - os\n    - hashlib\n    - json\n    - pathlib.Path\n\n    Example:\n    >>> json_file = task_func(\"/path/to/directory\")\n    >>> print(f\"Hashes saved at: {json_file}\")\n\n    \"\"\"\n    # Create a Path object for the directory\n    dir_path = Path(directory)\n    \n    # Dictionary to store file hashes\n    file_hashes = {}\n    \n    # Iterate over all files in the directory and its subdirectories\n    for file_path in dir_path.rglob('*'):\n        if file_path.is_file():\n            # Calculate the SHA256 hash of the file\n            with open(file_path, 'rb') as file:\n                file_hash = hashlib.sha256(file.read()).hexdigest()\n                file_hashes[file_path.name] = file_hash\n    \n    # Save the hashes to a JSON file in the same directory\n    json_file_path = dir_path / 'hashes.json'\n    with open(json_file_path, 'w') as json_file:\n        json.dump(file_hashes, json_file, indent=4)\n    \n    return str(json_file_path)", "raw_solution": "import os\nimport hashlib\nimport json\nfrom pathlib import Path\n\ndef task_func(directory: str) -> str:\n    \"\"\"\n    specified directory mein sabhi files ke liye SHA256 hash banaen, jismein up-directoryon mein maujood file bhi shaamil hain,\n    and in hash ko specified directory mein 'hashes.json' naamak ek json file mein sahejeyn.\n\n    Parameters:\n    - directory (str): directory ka path jismein hash ki jaane waali file shaamil hain.\n\n    Returns:\n        str: 'hashes.json' file ka absolute path jismein hash shaamil hain.\n\n    Requirements:\n    - os\n    - hashlib\n    - json\n    - pathlib.Path\n\n    Example:\n    >>> json_file = task_func(\"/path/to/directory\")\n    >>> print(f\"Hashes saved at: {json_file}\")\n\n    \"\"\"\n    # Create a Path object for the directory\n    dir_path = Path(directory)\n    \n    # Dictionary to store file hashes\n    file_hashes = {}\n    \n    # Iterate over all files in the directory and its subdirectories\n    for file_path in dir_path.rglob('*'):\n        if file_path.is_file():\n            # Calculate the SHA256 hash of the file\n            with open(file_path, 'rb') as file:\n                file_hash = hashlib.sha256(file.read()).hexdigest()\n                file_hashes[file_path.name] = file_hash\n    \n    # Save the hashes to a JSON file in the same directory\n    json_file_path = dir_path / 'hashes.json'\n    with open(json_file_path, 'w') as json_file:\n        json.dump(file_hashes, json_file, indent=4)\n    \n    return str(json_file_path)\n```\n"}
{"task_id": "BigCodeBench/1131", "solution": "import hashlib\nimport binascii\nimport sqlite3\ndef task_func(salt, cursor):\n    if not isinstance(salt, str):\n        raise TypeError(\"Salt must be a string.\")\n\n    num_updated = 0\n    cursor.execute(\"SELECT id, password FROM users\")  # Assuming 'users' is the table name\n    rows = cursor.fetchall()\n\n    for row in rows:\n        user_id, password = row\n        salted_password = salt + password\n        hashed_password = hashlib.sha256(salted_password.encode()).hexdigest()\n\n        cursor.execute(\"UPDATE users SET password = ? WHERE id = ?\", (hashed_password, user_id))\n        num_updated += cursor.rowcount\n\n    return num_updated", "raw_solution": "import hashlib\nimport binascii\nimport sqlite3\n\ndef task_func(salt, cursor):\n    if not isinstance(salt, str):\n        raise TypeError(\"Salt must be a string.\")\n\n    num_updated = 0\n    cursor.execute(\"SELECT id, password FROM users\")  # Assuming 'users' is the table name\n    rows = cursor.fetchall()\n\n    for row in rows:\n        user_id, password = row\n        salted_password = salt + password\n        hashed_password = hashlib.sha256(salted_password.encode()).hexdigest()\n\n        cursor.execute(\"UPDATE users SET password = ? WHERE id = ?\", (hashed_password, user_id))\n        num_updated += cursor.rowcount\n\n    return num_updated\n\n# Example usage:\n# conn = sqlite3.connect('sample.db')\n# cursor = conn.cursor()\n# num_updated = task_func('mysalt', cursor)\n# print(num_updated)\n```\n"}
{"task_id": "BigCodeBench/1132", "solution": "import os\nimport hashlib\nimport base64\ndef task_func(password, PREFIX=\"ME\", SALT_LENGTH=16):\n    if SALT_LENGTH < 0:\n        raise ValueError(\"Salt length cannot be negative\")\n\n    # Generate random salt\n    salt = os.urandom(SALT_LENGTH)\n\n    # Concatenate prefix, password, and salt\n    combined = PREFIX.encode() + password.encode() + salt\n\n    # Create SHA256 hash of the combined string\n    hash_object = hashlib.sha256(combined)\n    hash_digest = hash_object.digest()\n\n    # Encode the hash in base64\n    base64_encoded = base64.b64encode(hash_digest).decode()\n\n    return base64_encoded", "raw_solution": "import os\nimport hashlib\nimport base64\n\ndef task_func(password, PREFIX=\"ME\", SALT_LENGTH=16):\n    if SALT_LENGTH < 0:\n        raise ValueError(\"Salt length cannot be negative\")\n\n    # Generate random salt\n    salt = os.urandom(SALT_LENGTH)\n\n    # Concatenate prefix, password, and salt\n    combined = PREFIX.encode() + password.encode() + salt\n\n    # Create SHA256 hash of the combined string\n    hash_object = hashlib.sha256(combined)\n    hash_digest = hash_object.digest()\n\n    # Encode the hash in base64\n    base64_encoded = base64.b64encode(hash_digest).decode()\n\n    return base64_encoded\n\n# Example usage:\n# hashed_password = task_func('password123', 'ME', 16)\n# print(hashed_password)\n```\n"}
{"task_id": "BigCodeBench/1133", "solution": "import json\nimport requests\ndef task_func(API_URL, endpoint, PREFIX):\n    try:\n        response = requests.get(f\"{API_URL}/{endpoint}\")\n        response.raise_for_status()\n        data = response.json()\n\n        filename = f\"{PREFIX}{endpoint}.json\"\n        with open(filename, 'w') as file:\n            json.dump(data, file, indent=4)\n\n        return filename\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {e}\")", "raw_solution": "import json\nimport requests\n\ndef task_func(API_URL, endpoint, PREFIX):\n    try:\n        response = requests.get(f\"{API_URL}/{endpoint}\")\n        response.raise_for_status()\n        data = response.json()\n\n        filename = f\"{PREFIX}{endpoint}.json\"\n        with open(filename, 'w') as file:\n            json.dump(data, file, indent=4)\n\n        return filename\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {e}\")\n\n# Example usage:\n# filename = task_func('https://api.github.com/', 'users', 'ME')\n# print(filename)\n```\n"}
{"task_id": "BigCodeBench/1134", "solution": "import os\nimport glob\nimport hashlib\ndef task_func(source_dir, target_dir, prefix='#Hash: '):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"The source directory '{source_dir}' does not exist.\")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    hashed_files = []\n\n    for file_path in glob.glob(os.path.join(source_dir, '*')):\n        with open(file_path, 'rb') as file:\n            content = file.read()\n            hash_object = hashlib.md5(content)\n            hash_str = hash_object.hexdigest()\n            new_content = f\"{prefix}{hash_str}\\n{content.decode('utf-8')}\"\n            new_file_path = os.path.join(target_dir, os.path.basename(file_path))\n            with open(new_file_path, 'w', encoding='utf-8') as new_file:\n                new_file.write(new_content)\n            hashed_files.append(new_file_path)\n\n    return hashed_files", "raw_solution": "import os\nimport glob\nimport hashlib\n\ndef task_func(source_dir, target_dir, prefix='#Hash: '):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"The source directory '{source_dir}' does not exist.\")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    hashed_files = []\n\n    for file_path in glob.glob(os.path.join(source_dir, '*')):\n        with open(file_path, 'rb') as file:\n            content = file.read()\n            hash_object = hashlib.md5(content)\n            hash_str = hash_object.hexdigest()\n            new_content = f\"{prefix}{hash_str}\\n{content.decode('utf-8')}\"\n            new_file_path = os.path.join(target_dir, os.path.basename(file_path))\n            with open(new_file_path, 'w', encoding='utf-8') as new_file:\n                new_file.write(new_content)\n            hashed_files.append(new_file_path)\n\n    return hashed_files\n```\n"}
{"task_id": "BigCodeBench/1135", "solution": "import collections\nimport json\nimport requests\ndef task_func(user, API_URL = 'https://api.github.com/users/'):\n    response = requests.get(API_URL + user + '/repos')\n    response.raise_for_status()  # Raise an error for bad responses\n\n    repos = response.json()\n    repo_names_with_dates = [(repo['name'], repo['created_at']) for repo in repos]\n\n    # Sort by creation date\n    sorted_repos = sorted(repo_names_with_dates, key=lambda x: x[1])\n\n    # Extract only the names\n    sorted_repo_names = [repo[0] for repo in sorted_repos]\n\n    return sorted_repo_names", "raw_solution": "import collections\nimport json\nimport requests\n\ndef task_func(user, API_URL = 'https://api.github.com/users/'):\n    response = requests.get(API_URL + user + '/repos')\n    response.raise_for_status()  # Raise an error for bad responses\n\n    repos = response.json()\n    repo_names_with_dates = [(repo['name'], repo['created_at']) for repo in repos]\n\n    # Sort by creation date\n    sorted_repos = sorted(repo_names_with_dates, key=lambda x: x[1])\n\n    # Extract only the names\n    sorted_repo_names = [repo[0] for repo in sorted_repos]\n\n    return sorted_repo_names\n\n# Example usage:\n# print(task_func('octocat'))\n```\nThis script uses the `requests` library to make an HTTP GET request to the GitHub API to retrieve a list of a user's repositories. It then parses the JSON response to extract the repository names and creation dates. The repositories are sorted by creation date, and the sorted list of repository names is returned. The example usage at the end demonstrates how to call the function with a specific user.\n"}
{"task_id": "BigCodeBench/1136", "solution": "import bs4\nimport requests\nimport re\nimport csv\ndef task_func(url=\"http://example.com\", csv_path=\"emails.csv\", \n          regex=r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \n          headers={'User-Agent': 'Mozilla/5.0'}):\n    response = requests.get(url, headers=headers)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    emails = re.findall(regex, soup.get_text())\n\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['email'])\n        for email in set(emails):  # remove duplicates\n            writer.writerow([email])\n\n    return csv_path", "raw_solution": "import bs4\nimport requests\nimport re\nimport csv\n\ndef task_func(url=\"http://example.com\", csv_path=\"emails.csv\", \n          regex=r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \n          headers={'User-Agent': 'Mozilla/5.0'}):\n    response = requests.get(url, headers=headers)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    emails = re.findall(regex, soup.get_text())\n\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['email'])\n        for email in set(emails):  # remove duplicates\n            writer.writerow([email])\n\n    return csv_path\n```\n"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    phone_numbers = []\n    if url.startswith('file://'):\n        with open(url[7:], 'r') as file:\n            content = file.read()\n    else:\n        response = requests.get(url)\n        content = response.text\n\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    text = soup.get_text()\n\n    phone_number_pattern = re.compile(r'\\+\\d{1,3}[-\\s]?\\(\\d{1,3}\\)[-\\s]?\\d{1,4}[-\\s]?\\d{1,4}')\n    phone_numbers = phone_number_pattern.findall(text)\n\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    return phone_numbers", "raw_solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    phone_numbers = []\n    if url.startswith('file://'):\n        with open(url[7:], 'r') as file:\n            content = file.read()\n    else:\n        response = requests.get(url)\n        content = response.text\n\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    text = soup.get_text()\n\n    phone_number_pattern = re.compile(r'\\+\\d{1,3}[-\\s]?\\(\\d{1,3}\\)[-\\s]?\\d{1,4}[-\\s]?\\d{1,4}')\n    phone_numbers = phone_number_pattern.findall(text)\n\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    return phone_numbers\n```\n"}
{"task_id": "BigCodeBench/1138", "solution": "import numpy as np\nimport itertools\ndef task_func(matrix):\n    # Flatten the matrix and sort the elements\n    flat_sorted = np.sort(matrix.flatten())\n\n    # Generate all unique combinations of two elements\n    combinations = list(itertools.combinations(flat_sorted, 2))\n\n    return flat_sorted, combinations", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(matrix):\n    # Flatten the matrix and sort the elements\n    flat_sorted = np.sort(matrix.flatten())\n\n    # Generate all unique combinations of two elements\n    combinations = list(itertools.combinations(flat_sorted, 2))\n\n    return flat_sorted, combinations\n\n# Example usage:\n# matrix = np.array([[1, 3], [2, 4]])\n# print(task_func(matrix))\n```\n"}
{"task_id": "BigCodeBench/1139", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data):\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df['Hours'].values.reshape(-1, 1), df['Scores'], test_size=0.2, random_state=42)\n\n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the scores for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean squared error\n    mse = np.mean((y_test - y_pred) ** 2)\n\n    return mse", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data):\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df['Hours'].values.reshape(-1, 1), df['Scores'], test_size=0.2, random_state=42)\n\n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the scores for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean squared error\n    mse = np.mean((y_test - y_pred) ** 2)\n\n    return mse\n\n# Example usage"}
