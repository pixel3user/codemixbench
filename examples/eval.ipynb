{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup and Installation\n",
    "\n",
    "This section installs the required dependencies for running the evaluation locally. It uses pip to install the necessary packages in an isolated environment.\n",
    "\n",
    "Optional Protobuf Installation: A commented-out line to force reinstall a specific version of protobuf if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-31T17:53:52.843715Z",
     "iopub.status.busy": "2025-03-31T17:53:52.843425Z",
     "iopub.status.idle": "2025-03-31T18:09:39.938702Z",
     "shell.execute_reply": "2025-03-31T18:09:39.937838Z",
     "shell.execute_reply.started": "2025-03-31T17:53:52.843694Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## If you want to use the evaluate locally, you need to install the requirements in an isolated environment\n",
    "!pip install -I -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt\n",
    "\n",
    "# You are strongly recommended to install the bigcodebench dependencies in another environment\n",
    "!pip install bigcodebench --upgrade\n",
    "\n",
    "# # Use legacy resolver if having resolution error with dependency tree (commented out)\n",
    "# !pip install bigcodebench --use-deprecated=legacy-resolver\n",
    "\n",
    "!pip install --upgrade huggingface_hub\n",
    "\n",
    "# Optional: Reinstall a specific version of protobuf if needed (commented out)\n",
    "!pip install 'protobuf<=3.20.1' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Authentication\n",
    "\n",
    "User your hugginface auth token to get access to open source models used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# paste your token as a string (you can also read it from an env var)\n",
    "login(token=\"api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variable Setup\n",
    "\n",
    "Sets the *VLLM_ALLOW_LONG_MAX_MODEL_LEN* environment variable to 1, enabling the use of longer model lengths. Usefull for some model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow overriding the model‐max‐length\n",
    "os.environ['VLLM_ALLOW_LONG_MAX_MODEL_LEN'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Names Definition\n",
    "\n",
    "A list of model names is defined for evaluation. The model list have all the required model being evaluated while the research. All of the models are hosted on huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    # \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    # \"bigcode/starcoder2-3b\",\n",
    "    # \"meta-llama/Llama-3.2-1B\",\n",
    "    # \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    # \"google/gemma-3-4b-it\",\n",
    "    # \"infly/OpenCoder-8B-Instruct\",\n",
    "    # \"microsoft/Phi-4-multimodal-instruct\",\n",
    "    # \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    # \"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    # \"meta-llama/CodeLlama-7b-Instruct-hf\",\n",
    "    # \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    # \"bigcode/starcoder2-7b\",\n",
    "    # \"microsoft/phi-4--main\",\n",
    "    # \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    # \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n",
    "    # \"bigcode/starcoder2-15b-instruct-v0.1\",\n",
    "    # # Additional models can be added here\n",
    "    \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    \"infly/OpenCoder-8B-Instruct\",\n",
    "    \"microsoft/Phi-4-multimodal-instruct\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Generation Loop\n",
    "\n",
    "- Defines paths to override default datasets and sets them as environment variables for evaluation.\n",
    "    1. Override Paths: Specifies paths to dataset files.\n",
    "    2. Environment Variable: Sets the BIGCODEBENCH_OVERRIDE_PATH environment variable for each dataset.\n",
    "\n",
    "    You can use the already generated dataset under the dataset folder for the model evalaution.\n",
    "\n",
    "- Runs the bigcodebench.generate command for each model and dataset.\n",
    "    1. The command is already with the default arguments for the required results. \n",
    "    2. max_new_tokens is set to 3700 to support the code generation of all the prompts which exceed the default length of the prompt length being 1280. \n",
    "    2. Some models requires to trust the remote code for a successfull generation otherwise the failed to load.\n",
    "\n",
    "- The default backend used is vllm.\n",
    "- The output of the code generation of each model is generated under the examples folder with each folder named bcb_results_cmd{\"cmd_value\"}. \n",
    "- The generated code samples will be stored in a file named [model_name]--bigcodebench-complete--vllm-0-1-sanitized_calibrated.jsonl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to override datasets and set them as environment variables\n",
    "override_paths = [\n",
    "    '../dataset/hindi-eng/MBigCodeBench-hini-end-cmd0.6.jsonl',\n",
    "    # '../dataset/hindi-eng/MBigCodeBench-hini-end-cmd0.9.jsonl',\n",
    "]\n",
    "\n",
    "# Key for the environment variable\n",
    "key = \"BIGCODEBENCH_OVERRIDE_PATH\"\n",
    "\n",
    "for path in override_paths:\n",
    "    # Set the environment variable for the override path\n",
    "    # This will override the dataset path for the generation command\n",
    "    os.environ[key] = path\n",
    "    print(f\"\\n▶️ Generating with BIGCODEBENCH_OVERRIDE_PATH={path}\\n\")\n",
    "\n",
    "    # Loop through each model and run the generation command\n",
    "    for model in model_names:\n",
    "        command = [\n",
    "            \"bigcodebench.generate\",\n",
    "            \"--model\", model,\n",
    "            \"--split\", \"complete\",\n",
    "            \"--subset\", \"full\",\n",
    "            \"--max_new_tokens=3700\",\n",
    "            \"--trust_remote_code=True\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"Running command for model: {model}\")\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(command, check=True, env=os.environ)\n",
    "            print(f\"✅ Completed: {model}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Failed: {model} with error: {e}\")\n",
    "\n",
    "    \n",
    "    # 3) after the inner loop, rename bcb_results → bcb_results_cmdX.Y\n",
    "    m = re.search(r'cmd(\\d+\\.\\d+)', path)\n",
    "    suffix = m.group(1) if m else \"unknown\"\n",
    "    src = \"bcb_results\"\n",
    "    dst = f\"bcb_results_cmd{suffix}\"\n",
    "\n",
    "    if os.path.isdir(src):\n",
    "        # if a previous dst exists, you can choose to remove or skip\n",
    "        if os.path.exists(dst):\n",
    "            print(f\"⚠️  Target folder {dst!r} already exists—skipping rename.\")\n",
    "        else:\n",
    "            os.rename(src, dst)\n",
    "            print(f\"🔄 Renamed folder: {src!r} → {dst!r}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Source folder {src!r} not found; nothing to rename.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "- The following commands runs the bigcodebench.evaluate command for each generated result.\n",
    "- All model generated files are listed under eval_model_names list.\n",
    "\n",
    "- Output:\n",
    "    1. All the resulted files will be stored in the same folder of model code example file.\n",
    "    2. The evaluation results will be stored in a file named [model_name]--bigcodebench-complete--vllm-0-1-sanitized_calibrated_eval_results.json.\n",
    "    3. The pass@k results will be stored in a file named [model_name]--bigcodebench-complete--vllm-0-1-sanitized_calibrated_pass_at_k.json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two result‐folders\n",
    "dirs = [\"bcb_results_cmd0.6\", \n",
    "        # \"bcb_results_cmd0.9\"\n",
    "    ]\n",
    "\n",
    "# List all model-generated files in the result folders\n",
    "eval_model_names = [\n",
    "    f\"{d}/{fn}\"\n",
    "    for d in dirs\n",
    "    for fn in os.listdir(d)\n",
    "    if os.path.isfile(os.path.join(d, fn))\n",
    "]\n",
    "\n",
    "# Loop through each model and run the evaluation command\n",
    "for model in eval_model_names:\n",
    "    command = [\n",
    "        \"bigcodebench.evaluate\",\n",
    "        \"--execution\", \"local\",\n",
    "        \"--split\", \"complete\",\n",
    "        \"--subset\", \"full\",\n",
    "        \"--samples\", model,\n",
    "        \"--save_pass_rate\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running evaluation for model: {model}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, env=os.environ)\n",
    "        print(f\"✅ Completed: {model}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed: {model} with error: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Plotting\n",
    "\n",
    "Loads evaluation results, calculates averages, and generates plots for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for data loading and visualization\n",
    "import os, glob, json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting parameters for generating visual figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for storing plots\n",
    "Path(\"figs\").mkdir(exist_ok=True)\n",
    "\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42      # embed TrueType (NeurIPS requirement)\n",
    "plt.rcParams[\"ps.fonttype\"]  = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define small, medium, and large model groups for visualization\n",
    "small_models = {\n",
    "    \"deepseek\": [\"deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B--main\"],\n",
    "    \"starcoder\": [\"bigcode--starcoder2-3b--main\"],\n",
    "    \"meta\": [\"meta-llama--Llama-3.2-1B--main\"],\n",
    "    \"qwen coder\": [\"Qwen--Qwen2.5-Coder-1.5B-Instruct--main\"],\n",
    "    \"google\": [\"google--gemma-3-4b-it--main\"]\n",
    "}\n",
    "\n",
    "medium_models = {\n",
    "    \"opencoder\": [\"infly--OpenCoder-8B-Instruct--main\"],\n",
    "    \"microsoft\": [\"microsoft--Phi-4-multimodal-instruct--main\"],\n",
    "    \"deepseek\": [\"deepseek-ai--DeepSeek-R1-Distill-Llama-8B--main\"],\n",
    "    \"hermes\": [\"NousResearch--Hermes-2-Theta-Llama-3-8B--main\"],\n",
    "    \"meta\": [\"meta-llama--CodeLlama-7b-Instruct-hf--main\", \"meta-llama--Llama-3.1-8B-Instruct--main\"],\n",
    "    \"qwen coder\": [\"Qwen--Qwen2.5-Coder-7B-Instruct--main\"],\n",
    "    \"starcoder\": [\"bigcode--starcoder2-7b--main\"]\n",
    "}\n",
    "\n",
    "large_models = {\n",
    "    \"microsoft\": [\"microsoft--phi-4--main\"],\n",
    "    \"deepseek\": [\n",
    "        \"deepseek-ai--DeepSeek-R1-Distill-Qwen-14B--main\",\n",
    "        \"deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct--main\"\n",
    "    ],\n",
    "    \"starcoder\": [\"bigcode--starcoder2-15b-instruct-v0.1--main\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base paths for the results of CMD 0.6 and CMD 0.9\n",
    "base_paths = {\n",
    "    \"0.6\": \"../eval/hindi-eng/bcb_results_cmd0.6\",\n",
    "    \"0.9\": \"../eval/hindi-eng/bcb_results_cmd0.9\"\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store data for each CMD version\n",
    "data = {\"0.6\": [], \"0.9\": []}\n",
    "\n",
    "# Loop through each CMD version and its corresponding result folder\n",
    "for cmd, root in base_paths.items():\n",
    "    for model_folder in os.listdir(root):   # List all model folders in the result folder\n",
    "        results_dir = os.path.join(root, model_folder, \"results\")   # Path to the results directory\n",
    "        pattern = os.path.join(results_dir, \"*sanitized_calibrated_pass_at_k.json\")\n",
    "        for fn in glob.glob(pattern):   # Find all matching files\n",
    "            with open(fn, 'r') as f:\n",
    "                obj = json.load(f)\n",
    "                data[cmd].append(obj)   # Append the data to the corresponding CMD version\n",
    "\n",
    "\n",
    "# Helper: average pass@1 for a given model_id over a flat list of records\n",
    "def avg_pass1(records, model_id):\n",
    "    # Extract all pass@1 values for the given model ID from the records\n",
    "    vals = [r[\"pass@1\"] for r in records if r[\"model\"] == model_id.strip()]\n",
    "    \n",
    "    if vals:\n",
    "      return sum(vals) / len(vals)\n",
    "    else:\n",
    "      # print(f'Found none for :{model_id}')\n",
    "      return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "- Generates plots comparing the performance of models across different datasets.\n",
    "    1. Subplots: Creates a grid of plots for small, medium, and large models.\n",
    "    2. Data Filtering: Filters models with valid data.\n",
    "    3. Plotting: Plots average pass@1 scores for CMD 0.6, CMD 0.9, and baseline CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the baseline results from a CSV file\n",
    "df = pd.read_csv(\"../dataset/bigcodebench_results.csv\")\n",
    "\n",
    "# Normalize model names by converting them to lowercase for easier matching\n",
    "df[\"model_lower\"] = df[\"model\"].str.lower()\n",
    "\n",
    "# Function to extract `complete` score from CSV for a model_id\n",
    "def get_csv_score(model_id):\n",
    "\n",
    "    core_name = model_id.split(\"--\", 1)[-1].replace(\"--main\", \"\").lower()\n",
    "\n",
    "    # Check if the model_id exists in the CSV file\n",
    "    match = df[df[\"model_lower\"].str.contains(core_name, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        return float(match.iloc[0][\"complete\"]) / 100.0\n",
    "    else:\n",
    "        print(f\"id:{core_name} not found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "size_to_col = {\"Small Models\": 0, \"Medium Models\": 1, \"Large Models\": 2}\n",
    "\n",
    "def shorten(name):\n",
    "    return name.replace(\"deepseek-ai--\", \"\") \\\n",
    "              .replace(\"bigcode--\", \"\") \\\n",
    "              .replace(\"meta-llama--\", \"\") \\\n",
    "              .replace(\"--main\", \"\") \\\n",
    "              .replace(\"Qwen--\",\"\") \\\n",
    "              .replace(\"google--\", \"\") \\\n",
    "              .replace(\"infly--\", \"\") \\\n",
    "              .replace(\"microsoft--\", \"\") \\\n",
    "              .replace(\"NousResearch--\", \"\") \\\n",
    "              .replace(\"infly--\",\"\")\n",
    "\n",
    "# 4) Create a 1x3 grid: Each model size gets one plot with 3 lines\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6), sharey=False)\n",
    "# fig.suptitle(\"CMD 0.6 vs CMD 0.9 vs BigCodeBench Baseline\", fontsize=16)\n",
    "\n",
    "size_to_col = {\"Small Models\": 0, \"Medium Models\": 1, \"Large Models\": 2}\n",
    "\n",
    "for size_label, grouping in [\n",
    "    (\"Small Models\",  small_models),\n",
    "    (\"Medium Models\", medium_models),\n",
    "    (\"Large Models\",  large_models)\n",
    "]:\n",
    "    ids = [mid for sub in grouping.values() for mid in sub]\n",
    "\n",
    "    # Calculate average pass@1 scores for CMD 0.6, CMD 0.9, and baseline CSV data\n",
    "    cmd06_vals = [avg_pass1(data[\"0.6\"], mid) for mid in ids]\n",
    "    cmd09_vals = [avg_pass1(data[\"0.9\"], mid) for mid in ids]\n",
    "    csv_vals   = [get_csv_score(mid) for mid in ids]\n",
    "\n",
    "    # Filter out models with no valid data at all\n",
    "    filtered = [(mid, a, b, c) for mid, a, b, c in zip(ids, cmd06_vals, cmd09_vals, csv_vals)\n",
    "                if a is not None or b is not None or c is not None]\n",
    "\n",
    "    if not filtered:\n",
    "        continue\n",
    "\n",
    "    # Unpack filtered data into separate lists\n",
    "    model_names, p06, p09, p_csv = zip(*filtered)\n",
    "    x_vals = list(range(len(model_names)))\n",
    "\n",
    "    col_idx = size_to_col[size_label]\n",
    "    ax = axes[col_idx]\n",
    "\n",
    "    # Plot all three lines\n",
    "    ax.plot(x_vals, p06, marker='o', linestyle='-', linewidth=2, label=\"CMD 0.6\", color=\"blue\")\n",
    "    ax.plot(x_vals, p09, marker='s', linestyle='-', linewidth=2, label=\"CMD 0.9\", color=\"green\")\n",
    "    ax.plot(x_vals, p_csv, marker='D', linestyle='--', linewidth=2, label=\"Original CSV\", color=\"crimson\")\n",
    "    \n",
    "    # Shorten model names for better readability\n",
    "    short_names = [shorten(name) for name in model_names]\n",
    "    \n",
    "    ax.set_title(size_label, fontsize=14)\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.set_xticklabels(short_names, rotation=45, ha=\"right\", fontsize=10)\n",
    "    ax.set_ylabel(\"Avg pass@1\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Adjust layout and save the plot as a PD\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.savefig(\"figs/cmd_results_grid.pdf\")\n",
    "\n",
    "\n",
    "# 5) Create a 1x3 grid with increased height for side-by-side comparison\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6.5), sharey=False)  # ⬅️ Increased height to 6.5\n",
    "# fig.suptitle(\"CMD 0.6 vs CMD 0.9 — Side-by-Side per Model Size\", fontsize=16)\n",
    "\n",
    "for size_label, grouping in [\n",
    "    (\"Small Models\",  small_models),\n",
    "    (\"Medium Models\", medium_models),\n",
    "    (\"Large Models\",  large_models)\n",
    "]:\n",
    "    all_ids = [mid for sub in grouping.values() for mid in sub]\n",
    "\n",
    "    # Calculate average pass@1 scores for CMD 0.6 and CMD 0.9\n",
    "    avg06 = [avg_pass1(data[\"0.6\"], mid) for mid in all_ids]\n",
    "    avg09 = [avg_pass1(data[\"0.9\"], mid) for mid in all_ids]\n",
    "\n",
    "    cats, p06, p09 = [], [], []\n",
    "    for mid, a6, a9 in zip(all_ids, avg06, avg09):\n",
    "        if a6 is not None or a9 is not None:\n",
    "            cats.append(mid)\n",
    "            p06.append(a6 or 0)\n",
    "            p09.append(a9 or 0)\n",
    "\n",
    "    cats = [shorten(name) for name in cats]\n",
    "\n",
    "    x = range(len(cats))\n",
    "    w = 0.35\n",
    "    col_idx = size_to_col[size_label]\n",
    "    ax = axes[col_idx]\n",
    "\n",
    "    # Plot CMD 0.6 and CMD 0.9 as side-by-side bars\n",
    "    ax.bar([i - w/2 for i in x], p06, width=w, label=\"CMD 0.6\")\n",
    "    ax.bar([i + w/2 for i in x], p09, width=w, label=\"CMD 0.9\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cats, rotation=45, ha=\"right\", fontsize=10)\n",
    "    ax.set_ylabel(\"Avg pass@1\")\n",
    "    ax.set_title(size_label, fontsize=14)\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout and save the plot as a PDF\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.savefig(\"figs/cmd_combined_results.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
