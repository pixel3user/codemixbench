{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup and Installation\n",
    "\n",
    "This section installs the required dependencies for running the evaluation locally. It uses pip to install the necessary packages in an isolated environment.\n",
    "\n",
    "Optional Protobuf Installation: A commented-out line to force reinstall a specific version of protobuf if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-31T17:53:52.843715Z",
     "iopub.status.busy": "2025-03-31T17:53:52.843425Z",
     "iopub.status.idle": "2025-03-31T18:09:39.938702Z",
     "shell.execute_reply": "2025-03-31T18:09:39.937838Z",
     "shell.execute_reply.started": "2025-03-31T17:53:52.843694Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4==4.8.2 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 1))\n",
      "  Using cached beautifulsoup4-4.8.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting blake3==0.4.1 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 2))\n",
      "  Using cached blake3-0.4.1-cp311-none-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting chardet==5.2.0 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 3))\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting cryptography==38.0.0 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 4))\n",
      "  Using cached cryptography-38.0.0-cp36-abi3-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting datetime==5.5 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 5))\n",
      "  Using cached DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting Django==4.2.7 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 6))\n",
      "  Using cached Django-4.2.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting dnspython==2.6.1 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 7))\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting docxtpl==0.11.5 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 8))\n",
      "  Using cached docxtpl-0.11.5-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting Faker==20.1.0 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 9))\n",
      "  Using cached Faker-20.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting flask_login==0.6.3 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 10))\n",
      "  Using cached Flask_Login-0.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting flask_restful==0.3.10 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 11))\n",
      "  Using cached Flask_RESTful-0.3.10-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting flask_wtf==1.2.1 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 12))\n",
      "  Using cached flask_wtf-1.2.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting Flask-Mail==0.9.1 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 13))\n",
      "  Using cached Flask-Mail-0.9.1.tar.gz (45 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting flask==3.0.3 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 14))\n",
      "  Using cached flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting folium==0.16.0 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 15))\n",
      "  Using cached folium-0.16.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting gensim==4.3.2 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 16))\n",
      "  Using cached gensim-4.3.2-cp311-cp311-win_amd64.whl.metadata (8.5 kB)\n",
      "Collecting geopandas==0.13.2 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 17))\n",
      "  Using cached geopandas-0.13.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting geopy==2.4.1 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 18))\n",
      "  Using cached geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting holidays==0.29 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 19))\n",
      "  Using cached holidays-0.29-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting keras==2.11.0 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 20))\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting Levenshtein==0.25.0 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 21))\n",
      "  Using cached Levenshtein-0.25.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting librosa==0.10.1 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 22))\n",
      "  Using cached librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting lxml==4.9.3 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 23))\n",
      "  Using cached lxml-4.9.3-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting matplotlib==3.7.0 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 24))\n",
      "  Using cached matplotlib-3.7.0-cp311-cp311-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting mechanize==0.4.9 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 25))\n",
      "  Using cached mechanize-0.4.9-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting natsort==7.1.1 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 26))\n",
      "  Using cached natsort-7.1.1-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting networkx==2.6.3 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 27))\n",
      "  Using cached networkx-2.6.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting nltk==3.8 (from -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt (line 28))\n",
      "  Using cached nltk-3.8-py3-none-any.whl.metadata (2.8 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 0.52.0 Requires-Python >=3.6,<3.9; 0.52.0rc3 Requires-Python >=3.6,<3.9; 0.53.0 Requires-Python >=3.6,<3.10; 0.53.0rc1.post1 Requires-Python >=3.6,<3.10; 0.53.0rc2 Requires-Python >=3.6,<3.10; 0.53.0rc3 Requires-Python >=3.6,<3.10; 0.53.1 Requires-Python >=3.6,<3.10; 0.54.0 Requires-Python >=3.7,<3.10; 0.54.0rc2 Requires-Python >=3.7,<3.10; 0.54.0rc3 Requires-Python >=3.7,<3.10; 0.54.1 Requires-Python >=3.7,<3.10; 0.55.0 Requires-Python >=3.7,<3.11; 0.55.0rc1 Requires-Python >=3.7,<3.11; 0.55.1 Requires-Python >=3.7,<3.11; 0.55.2 Requires-Python >=3.7,<3.11\n",
      "ERROR: Could not find a version that satisfies the requirement numba==0.55.0 (from versions: 0.1, 0.2, 0.3, 0.5.0, 0.6.0, 0.7.0, 0.7.1, 0.7.2, 0.8.0, 0.8.1, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.12.2, 0.13.0, 0.13.2, 0.13.3, 0.13.4, 0.14.0, 0.15.1, 0.16.0, 0.17.0, 0.18.1, 0.18.2, 0.19.1, 0.19.2, 0.20.0, 0.21.0, 0.22.0, 0.22.1, 0.23.0, 0.23.1, 0.24.0, 0.25.0, 0.26.0, 0.27.0, 0.28.1, 0.29.0, 0.30.0, 0.30.1, 0.31.0, 0.32.0, 0.33.0, 0.34.0, 0.35.0, 0.36.1, 0.36.2, 0.37.0, 0.38.0, 0.38.1, 0.39.0, 0.40.0, 0.40.1, 0.41.0, 0.42.0, 0.42.1, 0.43.0, 0.43.1, 0.44.0, 0.44.1, 0.45.0, 0.45.1, 0.46.0, 0.47.0, 0.48.0, 0.49.0, 0.49.1rc1, 0.49.1, 0.50.0rc1, 0.50.0, 0.50.1, 0.51.0rc1, 0.51.0, 0.51.1, 0.51.2, 0.52.0rc2, 0.56.0rc1, 0.56.0, 0.56.2, 0.56.3, 0.56.4, 0.57.0rc1, 0.57.0, 0.57.1rc1, 0.57.1, 0.58.0rc1, 0.58.0rc2, 0.58.0, 0.58.1, 0.59.0rc1, 0.59.0, 0.59.1, 0.60.0rc1, 0.60.0, 0.61.0rc1, 0.61.0rc2, 0.61.0, 0.61.1rc1, 0.61.2)\n",
      "ERROR: No matching distribution found for numba==0.55.0\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bigcodebench\n",
      "  Using cached bigcodebench-0.2.5-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\python311\\lib\\site-packages (from bigcodebench) (1.4.4)\n",
      "Collecting fire>=0.6.0 (from bigcodebench)\n",
      "  Using cached fire-0.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: multipledispatch>=0.6.0 in c:\\python311\\lib\\site-packages (from bigcodebench) (1.0.0)\n",
      "Collecting pqdm>=0.2.0 (from bigcodebench)\n",
      "  Using cached pqdm-0.2.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: tempdir>=0.7.1 in c:\\python311\\lib\\site-packages (from bigcodebench) (0.7.1)\n",
      "Collecting termcolor>=2.0.0 (from bigcodebench)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tqdm>=4.56.0 in c:\\python311\\lib\\site-packages (from bigcodebench) (4.67.1)\n",
      "Collecting tree_sitter>=0.22.0 (from bigcodebench)\n",
      "  Using cached tree_sitter-0.24.0-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Collecting tree-sitter-python>=0.21.0 (from bigcodebench)\n",
      "  Using cached tree_sitter_python-0.23.6-cp39-abi3-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: wget>=3.2 in c:\\python311\\lib\\site-packages (from bigcodebench) (3.2)\n",
      "Collecting transformers (from bigcodebench)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets (from bigcodebench)\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting gradio-client (from bigcodebench)\n",
      "  Using cached gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting vllm (from bigcodebench)\n",
      "  Using cached vllm-0.8.5.post1.tar.gz (7.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from bigcodebench) (1.24.1)\n",
      "Requirement already satisfied: rich in c:\\python311\\lib\\site-packages (from bigcodebench) (13.7.0)\n",
      "Collecting accelerate>=0.30.1 (from bigcodebench)\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting anthropic>=0.26.1 (from bigcodebench)\n",
      "  Using cached anthropic-0.51.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting google-genai (from bigcodebench)\n",
      "  Using cached google_genai-1.14.0-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting mistralai<1.0.0,>=0.2.0 (from bigcodebench)\n",
      "  Using cached mistralai-0.4.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting openai>=1.11.1 (from bigcodebench)\n",
      "  Downloading openai-1.78.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting e2b (from bigcodebench)\n",
      "  Using cached e2b-1.4.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from accelerate>=0.30.1->bigcodebench) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from accelerate>=0.30.1->bigcodebench) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\python311\\lib\\site-packages (from accelerate>=0.30.1->bigcodebench) (6.0.1)\n",
      "Collecting torch>=2.0.0 (from accelerate>=0.30.1->bigcodebench)\n",
      "  Using cached torch-2.7.0-cp311-cp311-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\python311\\lib\\site-packages (from accelerate>=0.30.1->bigcodebench) (0.31.1)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.30.1->bigcodebench)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python311\\lib\\site-packages (from anthropic>=0.26.1->bigcodebench) (4.2.0)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic>=0.26.1->bigcodebench)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.25.0 (from anthropic>=0.26.1->bigcodebench)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic>=0.26.1->bigcodebench)\n",
      "  Using cached jiter-0.9.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from anthropic>=0.26.1->bigcodebench)\n",
      "  Using cached pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Requirement already satisfied: sniffio in c:\\python311\\lib\\site-packages (from anthropic>=0.26.1->bigcodebench) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from anthropic>=0.26.1->bigcodebench) (4.13.2)\n",
      "Collecting orjson<3.11,>=3.9.10 (from mistralai<1.0.0,>=0.2.0->bigcodebench)\n",
      "  Using cached orjson-3.10.18-cp311-cp311-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: bounded-pool-executor in c:\\python311\\lib\\site-packages (from pqdm>=0.2.0->bigcodebench) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.56.0->bigcodebench) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from datasets->bigcodebench) (3.12.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->bigcodebench)\n",
      "  Using cached pyarrow-20.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->bigcodebench)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\python311\\lib\\site-packages (from datasets->bigcodebench) (2.1.4)\n",
      "Collecting requests>=2.32.2 (from datasets->bigcodebench)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: xxhash in c:\\python311\\lib\\site-packages (from datasets->bigcodebench) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets->bigcodebench)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->bigcodebench)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting attrs>=23.2.0 (from e2b->bigcodebench)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting httpcore<2.0.0,>=1.0.5 (from e2b->bigcodebench)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting packaging>=20.0 (from accelerate>=0.30.1->bigcodebench)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf<6.0.0,>=5.29.4 (from e2b->bigcodebench)\n",
      "  Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python311\\lib\\site-packages (from e2b->bigcodebench) (2.8.2)\n",
      "Collecting anyio<5,>=3.5.0 (from anthropic>=0.26.1->bigcodebench)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-genai->bigcodebench)\n",
      "  Using cached google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai->bigcodebench)\n",
      "  Using cached websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python311\\lib\\site-packages (from rich->bigcodebench) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python311\\lib\\site-packages (from rich->bigcodebench) (2.17.2)\n",
      "Collecting regex!=2019.12.17 (from transformers->bigcodebench)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->bigcodebench)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting cachetools (from vllm->bigcodebench)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: sentencepiece in c:\\python311\\lib\\site-packages (from vllm->bigcodebench) (0.2.0)\n",
      "Requirement already satisfied: blake3 in c:\\python311\\lib\\site-packages (from vllm->bigcodebench) (1.0.4)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\python311\\lib\\site-packages (from vllm->bigcodebench) (9.0.0)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting aiohttp (from vllm->bigcodebench)\n",
      "  Using cached aiohttp-3.11.18-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting prometheus_client>=0.18.0 (from vllm->bigcodebench)\n",
      "  Using cached prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from vllm->bigcodebench) (9.3.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm->bigcodebench)\n",
      "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm->bigcodebench)\n",
      "  Using cached tiktoken-0.9.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm->bigcodebench)\n",
      "  Using cached lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting outlines==0.1.11 (from vllm->bigcodebench)\n",
      "  Using cached outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lark==1.2.2 (from vllm->bigcodebench)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting filelock (from datasets->bigcodebench)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting partial-json-parser (from vllm->bigcodebench)\n",
      "  Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from vllm->bigcodebench) (26.4.0)\n",
      "Collecting msgspec (from vllm->bigcodebench)\n",
      "  Using cached msgspec-0.19.0-cp311-cp311-win_amd64.whl.metadata (7.1 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm->bigcodebench)\n",
      "  Using cached gguf-0.16.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting importlib_metadata (from vllm->bigcodebench)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm->bigcodebench)\n",
      "  Using cached mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm->bigcodebench)\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting einops (from vllm->bigcodebench)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.9.3 (from vllm->bigcodebench)\n",
      "  Using cached compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.18.0 (from vllm->bigcodebench)\n",
      "  Using cached depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting cloudpickle (from vllm->bigcodebench)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm->bigcodebench)\n",
      "  Using cached watchfiles-1.0.5-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting python-json-logger (from vllm->bigcodebench)\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting scipy (from vllm->bigcodebench)\n",
      "  Using cached scipy-1.15.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting ninja (from vllm->bigcodebench)\n",
      "  Using cached ninja-1.11.1.4-py3-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm->bigcodebench)\n",
      "  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm->bigcodebench)\n",
      "  Using cached opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm->bigcodebench)\n",
      "  Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm->bigcodebench)\n",
      "  Using cached opentelemetry_semantic_conventions_ai-0.4.8-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm->bigcodebench)\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting interegular (from outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from outlines==0.1.11->vllm->bigcodebench) (3.1.2)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from outlines==0.1.11->vllm->bigcodebench) (1.6.0)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting referencing (from outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting jsonschema (from outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached outlines_core-0.1.26-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from anyio<5,>=3.5.0->anthropic>=0.26.1->bigcodebench) (3.4)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting jinja2 (from outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->vllm->bigcodebench)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->vllm->bigcodebench)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->vllm->bigcodebench)\n",
      "  Using cached frozenlist-1.6.0-cp311-cp311-win_amd64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->vllm->bigcodebench)\n",
      "  Using cached multidict-6.4.3-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->vllm->bigcodebench)\n",
      "  Using cached propcache-0.3.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->vllm->bigcodebench)\n",
      "  Using cached yarl-1.20.0-cp311-cp311-win_amd64.whl.metadata (74 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-genai->bigcodebench)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-genai->bigcodebench)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from httpcore<2.0.0,>=1.0.5->e2b->bigcodebench) (2022.12.7)\n",
      "Collecting h11>=0.16 (from httpcore<2.0.0,>=1.0.5->e2b->bigcodebench)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub[hf_xet]>=0.30.0->vllm->bigcodebench)\n",
      "  Using cached hf_xet-1.1.0-cp37-abi3-win_amd64.whl.metadata (498 bytes)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->bigcodebench) (0.1.2)\n",
      "Collecting numpy (from bigcodebench)\n",
      "  Using cached numpy-2.2.5-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pillow (from vllm->bigcodebench)\n",
      "  Using cached pillow-11.2.1-cp311-cp311-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<1.27.0,>=1.26.0->vllm->bigcodebench)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib_metadata (from vllm->bigcodebench)\n",
      "  Using cached importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python311\\lib\\site-packages (from importlib_metadata->vllm->bigcodebench) (3.21.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm->bigcodebench)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm->bigcodebench)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm->bigcodebench)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.0.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm->bigcodebench)\n",
      "  Using cached grpcio-1.71.0-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm->bigcodebench)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm->bigcodebench)\n",
      "  Using cached opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting vllm (from bigcodebench)\n",
      "  Using cached vllm-0.8.5.tar.gz (7.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached vllm-0.8.4.tar.gz (6.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached vllm-0.8.3-py3-none-any.whl\n",
      "Collecting gguf==0.10.0 (from vllm->bigcodebench)\n",
      "  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting compressed-tensors==0.9.2 (from vllm->bigcodebench)\n",
      "  Using cached compressed_tensors-0.9.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->anthropic>=0.26.1->bigcodebench)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->anthropic>=0.26.1->bigcodebench)\n",
      "  Using cached pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->anthropic>=0.26.1->bigcodebench)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->e2b->bigcodebench) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.32.2->datasets->bigcodebench) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests>=2.32.2->datasets->bigcodebench) (1.26.13)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->accelerate>=0.30.1->bigcodebench)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.0->accelerate>=0.30.1->bigcodebench) (3.0)\n",
      "Collecting numpy (from bigcodebench)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas->datasets->bigcodebench) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python311\\lib\\site-packages (from pandas->datasets->bigcodebench) (2023.4)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\python311\\lib\\site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm->bigcodebench) (2.6.1)\n",
      "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached typer-0.15.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached rich_toolkit-0.14.5-py3-none-any.whl.metadata (999 bytes)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->outlines==0.1.11->vllm->bigcodebench) (2.1.3)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema->outlines==0.1.11->vllm->bigcodebench)\n",
      "  Using cached rpds_py-0.24.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai->bigcodebench)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python311\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.30.1->bigcodebench) (1.3.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\python311\\lib\\site-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm->bigcodebench) (8.1.7)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached httptools-0.6.4-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting rich (from bigcodebench)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm->bigcodebench)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Using cached bigcodebench-0.2.5-py3-none-any.whl (49 kB)\n",
      "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Using cached anthropic-0.51.0-py3-none-any.whl (263 kB)\n",
      "Using cached mistralai-0.4.2-py3-none-any.whl (20 kB)\n",
      "Downloading openai-1.78.1-py3-none-any.whl (680 kB)\n",
      "   ---------------------------------------- 0.0/680.9 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/680.9 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/680.9 kB 435.7 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 41.0/680.9 kB 393.8 kB/s eta 0:00:02\n",
      "   --- ----------------------------------- 61.4/680.9 kB 409.6 kB/s eta 0:00:02\n",
      "   --- ----------------------------------- 61.4/680.9 kB 409.6 kB/s eta 0:00:02\n",
      "   ---- ---------------------------------- 71.7/680.9 kB 328.6 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 92.2/680.9 kB 374.1 kB/s eta 0:00:02\n",
      "   ------ ------------------------------- 112.6/680.9 kB 385.0 kB/s eta 0:00:02\n",
      "   ------ ------------------------------- 112.6/680.9 kB 385.0 kB/s eta 0:00:02\n",
      "   ------ ------------------------------- 122.9/680.9 kB 361.0 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 143.4/680.9 kB 370.8 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 153.6/680.9 kB 366.6 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 174.1/680.9 kB 388.2 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 174.1/680.9 kB 388.2 kB/s eta 0:00:02\n",
      "   ---------- --------------------------- 194.6/680.9 kB 368.6 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 204.8/680.9 kB 355.7 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 204.8/680.9 kB 355.7 kB/s eta 0:00:02\n",
      "   ------------- ------------------------ 235.5/680.9 kB 351.4 kB/s eta 0:00:02\n",
      "   ------------- ------------------------ 235.5/680.9 kB 351.4 kB/s eta 0:00:02\n",
      "   -------------- ----------------------- 256.0/680.9 kB 365.7 kB/s eta 0:00:02\n",
      "   --------------- ---------------------- 276.5/680.9 kB 370.3 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 307.2/680.9 kB 380.2 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 307.2/680.9 kB 380.2 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 307.2/680.9 kB 380.2 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 307.2/680.9 kB 380.2 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 368.6/680.9 kB 395.6 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 389.1/680.9 kB 404.4 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 399.4/680.9 kB 389.1 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 409.6/680.9 kB 393.1 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 430.1/680.9 kB 395.3 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 450.6/680.9 kB 402.5 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 460.8/680.9 kB 400.5 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 460.8/680.9 kB 400.5 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 460.8/680.9 kB 400.5 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 481.3/680.9 kB 376.8 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 491.5/680.9 kB 375.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 512.0/680.9 kB 382.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 532.5/680.9 kB 388.6 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 542.7/680.9 kB 387.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 563.2/680.9 kB 388.9 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 573.4/680.9 kB 387.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 573.4/680.9 kB 387.6 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 593.9/680.9 kB 385.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 614.4/680.9 kB 390.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 624.6/680.9 kB 381.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 624.6/680.9 kB 381.9 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 645.1/680.9 kB 376.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 655.4/680.9 kB 378.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  675.8/680.9 kB 376.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 680.9/680.9 kB 376.6 kB/s eta 0:00:00\n",
      "Using cached pqdm-0.2.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tree_sitter-0.24.0-cp311-cp311-win_amd64.whl (120 kB)\n",
      "Using cached tree_sitter_python-0.23.6-cp39-abi3-win_amd64.whl (75 kB)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached e2b-1.4.0-py3-none-any.whl (97 kB)\n",
      "Using cached google_genai-1.14.0-py3-none-any.whl (168 kB)\n",
      "Using cached gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached compressed_tensors-0.9.2-py3-none-any.whl (97 kB)\n",
      "Using cached depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Using cached gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Using cached outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "Using cached outlines_core-0.1.26-cp311-cp311-win_amd64.whl (243 kB)\n",
      "Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached aiohttp-3.11.18-cp311-cp311-win_amd64.whl (443 kB)\n",
      "Using cached google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached jiter-0.9.0-cp311-cp311-win_amd64.whl (210 kB)\n",
      "Using cached lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "Using cached mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "Using cached orjson-3.10.18-cp311-cp311-win_amd64.whl (134 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pillow-11.2.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Using cached prometheus_client-0.21.1-py3-none-any.whl (54 kB)\n",
      "Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached pyarrow-20.0.0-cp311-cp311-win_amd64.whl (25.8 MB)\n",
      "Using cached pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tiktoken-0.9.0-cp311-cp311-win_amd64.whl (893 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached torch-2.7.0-cp311-cp311-win_amd64.whl (212.5 MB)\n",
      "Using cached websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached msgspec-0.19.0-cp311-cp311-win_amd64.whl (186 kB)\n",
      "Using cached ninja-1.11.1.4-py3-none-win_amd64.whl (296 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Using cached python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached scipy-1.15.3-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "Using cached watchfiles-1.0.5-cp311-cp311-win_amd64.whl (291 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Using cached fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Using cached frozenlist-1.6.0-cp311-cp311-win_amd64.whl (120 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached hf_xet-1.1.0-cp37-abi3-win_amd64.whl (4.2 MB)\n",
      "Using cached interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached multidict-6.4.3-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Using cached propcache-0.3.1-cp311-cp311-win_amd64.whl (45 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Using cached yarl-1.20.0-cp311-cp311-win_amd64.whl (93 kB)\n",
      "Using cached airportsdata-20250224-py3-none-any.whl (913 kB)\n",
      "Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "Using cached httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
      "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached rich_toolkit-0.14.5-py3-none-any.whl (24 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached rpds_py-0.24.0-cp311-cp311-win_amd64.whl (234 kB)\n",
      "Using cached typer-0.15.3-py3-none-any.whl (45 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: websockets, typing-inspection, tree-sitter-python, tree_sitter, termcolor, sympy, shellingham, safetensors, rpds-py, requests, regex, python-multipart, python-json-logger, python-dotenv, pydantic-core, pycountry, pyasn1, pyarrow, protobuf, propcache, prometheus_client, pillow, partial-json-parser, packaging, orjson, numpy, ninja, multidict, msgspec, lark, jiter, jinja2, interegular, importlib_metadata, httptools, hf-xet, h11, fsspec, frozenlist, filelock, email-validator, einops, distro, diskcache, dill, cloudpickle, cachetools, attrs, astor, anyio, annotated-types, airportsdata, aiohappyeyeballs, yarl, watchfiles, uvicorn, torch, tiktoken, starlette, scipy, rsa, rich, referencing, pydantic, pyasn1-modules, pqdm, opencv-python-headless, multiprocess, httpcore, gguf, fire, depyf, aiosignal, typer, tokenizers, rich-toolkit, prometheus-fastapi-instrumentator, lm-format-enforcer, jsonschema-specifications, httpx, google-auth, fastapi, aiohttp, accelerate, transformers, openai, mistralai, jsonschema, gradio-client, google-genai, fastapi-cli, e2b, anthropic, outlines_core, mistral_common, datasets, compressed-tensors, outlines, vllm, bigcodebench\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 11.0.3\n",
      "    Uninstalling websockets-11.0.3:\n",
      "      Successfully uninstalled websockets-11.0.3\n",
      "  Rolling back uninstall of websockets\n",
      "  Moving to c:\\python311\\lib\\site-packages\\websockets-11.0.3.dist-info\\\n",
      "   from C:\\Python311\\Lib\\site-packages\\~ebsockets-11.0.3.dist-info\n",
      "  Moving to c:\\python311\\lib\\site-packages\\websockets\\\n",
      "   from C:\\Python311\\Lib\\site-packages\\~ebsockets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python311\\\\Scripts\\\\websockets.exe' -> 'C:\\\\Python311\\\\Scripts\\\\websockets.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\python311\\lib\\site-packages (0.31.1)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from huggingface_hub) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python311\\lib\\site-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\python311\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\python311\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\slim\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface_hub) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "## If you want to use the evaluate locally, you need to install the requirements in an isolated environment\n",
    "!pip install -I -r https://raw.githubusercontent.com/bigcode-project/bigcodebench/main/Requirements/requirements-eval.txt\n",
    "\n",
    "# You are strongly recommended to install the bigcodebench dependencies in another environment\n",
    "!pip install bigcodebench --upgrade\n",
    "\n",
    "# # Use legacy resolver if having resolution error with dependency tree (commented out)\n",
    "# !pip install bigcodebench --use-deprecated=legacy-resolver\n",
    "\n",
    "!pip install --upgrade huggingface_hub\n",
    "\n",
    "# Optional: Reinstall a specific version of protobuf if needed (commented out)\n",
    "!pip install 'protobuf<=3.20.1' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Authentication\n",
    "\n",
    "User your hugginface auth token to get access to open source models used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# paste your token as a string (you can also read it from an env var)\n",
    "login(token=\"api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variable Setup\n",
    "\n",
    "Sets the *VLLM_ALLOW_LONG_MAX_MODEL_LEN* environment variable to 1, enabling the use of longer model lengths. Usefull for some model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow overriding the modelmaxlength\n",
    "os.environ['VLLM_ALLOW_LONG_MAX_MODEL_LEN'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Names Definition\n",
    "\n",
    "A list of model names is defined for evaluation. The model list have all the required model being evaluated while the research. All of the models are hosted on huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    # \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    # \"bigcode/starcoder2-3b\",\n",
    "    # \"meta-llama/Llama-3.2-1B\",\n",
    "    # \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    # \"google/gemma-3-4b-it\",\n",
    "    # \"infly/OpenCoder-8B-Instruct\",\n",
    "    # \"microsoft/Phi-4-multimodal-instruct\",\n",
    "    # \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    # \"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    # \"meta-llama/CodeLlama-7b-Instruct-hf\",\n",
    "    # \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    # \"bigcode/starcoder2-7b\",\n",
    "    # \"microsoft/phi-4--main\",\n",
    "    # \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    # \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n",
    "    # \"bigcode/starcoder2-15b-instruct-v0.1\",\n",
    "    # # Additional models can be added here\n",
    "    \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    \"infly/OpenCoder-8B-Instruct\",\n",
    "    \"microsoft/Phi-4-multimodal-instruct\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Generation Loop\n",
    "\n",
    "- Defines paths to override default datasets and sets them as environment variables for evaluation.\n",
    "    1. Override Paths: Specifies paths to dataset files.\n",
    "    2. Environment Variable: Sets the BIGCODEBENCH_OVERRIDE_PATH environment variable for each dataset.\n",
    "\n",
    "    You can use the already generated dataset under the dataset folder for the model evalaution.\n",
    "\n",
    "- Runs the bigcodebench.generate command for each model and dataset.\n",
    "    1. The command is already with the default arguments for the required results. \n",
    "    2. max_new_tokens is set to 3700 to support the code generation of all the prompts which exceed the default length of the prompt length being 1280. \n",
    "    2. Some models requires to trust the remote code for a successfull generation otherwise the failed to load.\n",
    "\n",
    "- The default backend used is vllm.\n",
    "- The output of the code generation of each model is generated under the examples folder with each folder named bcb_results_cmd{\"cmd_value\"}. \n",
    "- The generated code samples will be stored in a file named [model_name]--bigcodebench-complete--vllm-0-1-sanitized_calibrated.jsonl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to override datasets and set them as environment variables\n",
    "override_paths = [\n",
    "    '../dataset/hindi-eng/MBigCodeBench-hini-end-cmd0.6.jsonl',\n",
    "    # '../dataset/hindi-eng/MBigCodeBench-hini-end-cmd0.9.jsonl',\n",
    "]\n",
    "\n",
    "# Key for the environment variable\n",
    "key = \"BIGCODEBENCH_OVERRIDE_PATH\"\n",
    "\n",
    "for path in override_paths:\n",
    "    # Set the environment variable for the override path\n",
    "    # This will override the dataset path for the generation command\n",
    "    os.environ[key] = path\n",
    "    print(f\"\\n Generating with BIGCODEBENCH_OVERRIDE_PATH={path}\\n\")\n",
    "\n",
    "    # Loop through each model and run the generation command\n",
    "    for model in model_names:\n",
    "        command = [\n",
    "            \"bigcodebench.generate\",\n",
    "            \"--model\", model,\n",
    "            \"--split\", \"complete\",\n",
    "            \"--subset\", \"full\",\n",
    "            \"--max_new_tokens=3700\",\n",
    "            \"--trust_remote_code=True\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"Running command for model: {model}\")\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(command, check=True, env=os.environ)\n",
    "            print(f\" Completed: {model}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\" Failed: {model} with error: {e}\")\n",
    "\n",
    "    \n",
    "    # 3) after the inner loop, rename bcb_results  bcb_results_cmdX.Y\n",
    "    m = re.search(r'cmd(\\d+\\.\\d+)', path)\n",
    "    suffix = m.group(1) if m else \"unknown\"\n",
    "    src = \"bcb_results\"\n",
    "    dst = f\"bcb_results_cmd{suffix}\"\n",
    "\n",
    "    if os.path.isdir(src):\n",
    "        # if a previous dst exists, you can choose to remove or skip\n",
    "        if os.path.exists(dst):\n",
    "            print(f\"  Target folder {dst!r} already existsskipping rename.\")\n",
    "        else:\n",
    "            os.rename(src, dst)\n",
    "            print(f\" Renamed folder: {src!r}  {dst!r}\")\n",
    "    else:\n",
    "        print(f\"  Source folder {src!r} not found; nothing to rename.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "- The following commands runs the bigcodebench.evaluate command for each generated result.\n",
    "- All model generated files are listed under eval_model_names list.\n",
    "\n",
    "- Output:\n",
    "    1. All the resulted files will be stored in the same folder of model code example file.\n",
    "    2. The evaluation results will be stored in a file named [model_name]--bigcodebench-complete--vllm-0-1-sanitized_calibrated_eval_results.json.\n",
    "    3. The pass@k results will be stored in a file named [model_name]--bigcodebench-complete--vllm-0-1-sanitized_calibrated_pass_at_k.json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two resultfolders\n",
    "dirs = [\"bcb_results_cmd0.6\", \n",
    "        # \"bcb_results_cmd0.9\"\n",
    "    ]\n",
    "\n",
    "# List all model-generated files in the result folders\n",
    "eval_model_names = [\n",
    "    f\"{d}/{fn}\"\n",
    "    for d in dirs\n",
    "    for fn in os.listdir(d)\n",
    "    if os.path.isfile(os.path.join(d, fn))\n",
    "]\n",
    "\n",
    "# Loop through each model and run the evaluation command\n",
    "for model in eval_model_names:\n",
    "    command = [\n",
    "        \"bigcodebench.evaluate\",\n",
    "        \"--execution\", \"local\",\n",
    "        \"--split\", \"complete\",\n",
    "        \"--subset\", \"full\",\n",
    "        \"--samples\", model,\n",
    "        \"--save_pass_rate\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running evaluation for model: {model}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, env=os.environ)\n",
    "        print(f\" Completed: {model}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\" Failed: {model} with error: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Plotting\n",
    "\n",
    "Loads evaluation results, calculates averages, and generates plots for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for data loading and visualization\n",
    "import os, glob, json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting parameters for generating visual figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for storing plots\n",
    "Path(\"figs\").mkdir(exist_ok=True)\n",
    "\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42      # embed TrueType (NeurIPS requirement)\n",
    "plt.rcParams[\"ps.fonttype\"]  = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define small, medium, and large model groups for visualization\n",
    "small_models = {\n",
    "    \"deepseek\": [\"deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B--main\"],\n",
    "    \"starcoder\": [\"bigcode--starcoder2-3b--main\"],\n",
    "    \"meta\": [\"meta-llama--Llama-3.2-1B--main\"],\n",
    "    \"qwen coder\": [\"Qwen--Qwen2.5-Coder-1.5B-Instruct--main\"],\n",
    "    \"google\": [\"google--gemma-3-4b-it--main\"]\n",
    "}\n",
    "\n",
    "medium_models = {\n",
    "    \"opencoder\": [\"infly--OpenCoder-8B-Instruct--main\"],\n",
    "    \"microsoft\": [\"microsoft--Phi-4-multimodal-instruct--main\"],\n",
    "    \"deepseek\": [\"deepseek-ai--DeepSeek-R1-Distill-Llama-8B--main\"],\n",
    "    \"hermes\": [\"NousResearch--Hermes-2-Theta-Llama-3-8B--main\"],\n",
    "    \"meta\": [\"meta-llama--CodeLlama-7b-Instruct-hf--main\", \"meta-llama--Llama-3.1-8B-Instruct--main\"],\n",
    "    \"qwen coder\": [\"Qwen--Qwen2.5-Coder-7B-Instruct--main\"],\n",
    "    \"starcoder\": [\"bigcode--starcoder2-7b--main\"]\n",
    "}\n",
    "\n",
    "large_models = {\n",
    "    \"microsoft\": [\"microsoft--phi-4--main\"],\n",
    "    \"deepseek\": [\n",
    "        \"deepseek-ai--DeepSeek-R1-Distill-Qwen-14B--main\",\n",
    "        \"deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct--main\"\n",
    "    ],\n",
    "    \"starcoder\": [\"bigcode--starcoder2-15b-instruct-v0.1--main\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base paths for the results of CMD 0.6 and CMD 0.9\n",
    "base_paths = {\n",
    "    \"0.6\": \"../eval/hindi-eng/bcb_results_cmd0.6\",\n",
    "    \"0.9\": \"../eval/hindi-eng/bcb_results_cmd0.9\"\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store data for each CMD version\n",
    "data = {\"0.6\": [], \"0.9\": []}\n",
    "\n",
    "# Loop through each CMD version and its corresponding result folder\n",
    "for cmd, root in base_paths.items():\n",
    "    for model_folder in os.listdir(root):   # List all model folders in the result folder\n",
    "        results_dir = os.path.join(root, model_folder, \"results\")   # Path to the results directory\n",
    "        pattern = os.path.join(results_dir, \"*sanitized_calibrated_pass_at_k.json\")\n",
    "        for fn in glob.glob(pattern):   # Find all matching files\n",
    "            with open(fn, 'r') as f:\n",
    "                obj = json.load(f)\n",
    "                data[cmd].append(obj)   # Append the data to the corresponding CMD version\n",
    "\n",
    "\n",
    "# Helper: average pass@1 for a given model_id over a flat list of records\n",
    "def avg_pass1(records, model_id):\n",
    "    # Extract all pass@1 values for the given model ID from the records\n",
    "    vals = [r[\"pass@1\"] for r in records if r[\"model\"] == model_id.strip()]\n",
    "    \n",
    "    if vals:\n",
    "      return sum(vals) / len(vals)\n",
    "    else:\n",
    "      # print(f'Found none for :{model_id}')\n",
    "      return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "- Generates plots comparing the performance of models across different datasets.\n",
    "    1. Subplots: Creates a grid of plots for small, medium, and large models.\n",
    "    2. Data Filtering: Filters models with valid data.\n",
    "    3. Plotting: Plots average pass@1 scores for CMD 0.6, CMD 0.9, and baseline CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the baseline results from a CSV file\n",
    "df = pd.read_csv(\"../dataset/bigcodebench_results.csv\")\n",
    "\n",
    "# Normalize model names by converting them to lowercase for easier matching\n",
    "df[\"model_lower\"] = df[\"model\"].str.lower()\n",
    "\n",
    "# Function to extract `complete` score from CSV for a model_id\n",
    "def get_csv_score(model_id):\n",
    "\n",
    "    core_name = model_id.split(\"--\", 1)[-1].replace(\"--main\", \"\").lower()\n",
    "\n",
    "    # Check if the model_id exists in the CSV file\n",
    "    match = df[df[\"model_lower\"].str.contains(core_name, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        return float(match.iloc[0][\"complete\"]) / 100.0\n",
    "    else:\n",
    "        print(f\"id:{core_name} not found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "size_to_col = {\"Small Models\": 0, \"Medium Models\": 1, \"Large Models\": 2}\n",
    "\n",
    "def shorten(name):\n",
    "    return name.replace(\"deepseek-ai--\", \"\") \\\n",
    "              .replace(\"bigcode--\", \"\") \\\n",
    "              .replace(\"meta-llama--\", \"\") \\\n",
    "              .replace(\"--main\", \"\") \\\n",
    "              .replace(\"Qwen--\",\"\") \\\n",
    "              .replace(\"google--\", \"\") \\\n",
    "              .replace(\"infly--\", \"\") \\\n",
    "              .replace(\"microsoft--\", \"\") \\\n",
    "              .replace(\"NousResearch--\", \"\") \\\n",
    "              .replace(\"infly--\",\"\")\n",
    "\n",
    "# 4) Create a 1x3 grid: Each model size gets one plot with 3 lines\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6), sharey=False)\n",
    "# fig.suptitle(\"CMD 0.6 vs CMD 0.9 vs BigCodeBench Baseline\", fontsize=16)\n",
    "\n",
    "size_to_col = {\"Small Models\": 0, \"Medium Models\": 1, \"Large Models\": 2}\n",
    "\n",
    "for size_label, grouping in [\n",
    "    (\"Small Models\",  small_models),\n",
    "    (\"Medium Models\", medium_models),\n",
    "    (\"Large Models\",  large_models)\n",
    "]:\n",
    "    ids = [mid for sub in grouping.values() for mid in sub]\n",
    "\n",
    "    # Calculate average pass@1 scores for CMD 0.6, CMD 0.9, and baseline CSV data\n",
    "    cmd06_vals = [avg_pass1(data[\"0.6\"], mid) for mid in ids]\n",
    "    cmd09_vals = [avg_pass1(data[\"0.9\"], mid) for mid in ids]\n",
    "    csv_vals   = [get_csv_score(mid) for mid in ids]\n",
    "\n",
    "    # Filter out models with no valid data at all\n",
    "    filtered = [(mid, a, b, c) for mid, a, b, c in zip(ids, cmd06_vals, cmd09_vals, csv_vals)\n",
    "                if a is not None or b is not None or c is not None]\n",
    "\n",
    "    if not filtered:\n",
    "        continue\n",
    "\n",
    "    # Unpack filtered data into separate lists\n",
    "    model_names, p06, p09, p_csv = zip(*filtered)\n",
    "    x_vals = list(range(len(model_names)))\n",
    "\n",
    "    col_idx = size_to_col[size_label]\n",
    "    ax = axes[col_idx]\n",
    "\n",
    "    # Plot all three lines\n",
    "    ax.plot(x_vals, p06, marker='o', linestyle='-', linewidth=2, label=\"CMD 0.6\", color=\"blue\")\n",
    "    ax.plot(x_vals, p09, marker='s', linestyle='-', linewidth=2, label=\"CMD 0.9\", color=\"green\")\n",
    "    ax.plot(x_vals, p_csv, marker='D', linestyle='--', linewidth=2, label=\"Original CSV\", color=\"crimson\")\n",
    "    \n",
    "    # Shorten model names for better readability\n",
    "    short_names = [shorten(name) for name in model_names]\n",
    "    \n",
    "    ax.set_title(size_label, fontsize=14)\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.set_xticklabels(short_names, rotation=45, ha=\"right\", fontsize=10)\n",
    "    ax.set_ylabel(\"Avg pass@1\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Adjust layout and save the plot as a PD\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.savefig(\"figs/cmd_results_grid.pdf\")\n",
    "\n",
    "\n",
    "# 5) Create a 1x3 grid with increased height for side-by-side comparison\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6.5), sharey=False)  #  Increased height to 6.5\n",
    "# fig.suptitle(\"CMD 0.6 vs CMD 0.9  Side-by-Side per Model Size\", fontsize=16)\n",
    "\n",
    "for size_label, grouping in [\n",
    "    (\"Small Models\",  small_models),\n",
    "    (\"Medium Models\", medium_models),\n",
    "    (\"Large Models\",  large_models)\n",
    "]:\n",
    "    all_ids = [mid for sub in grouping.values() for mid in sub]\n",
    "\n",
    "    # Calculate average pass@1 scores for CMD 0.6 and CMD 0.9\n",
    "    avg06 = [avg_pass1(data[\"0.6\"], mid) for mid in all_ids]\n",
    "    avg09 = [avg_pass1(data[\"0.9\"], mid) for mid in all_ids]\n",
    "\n",
    "    cats, p06, p09 = [], [], []\n",
    "    for mid, a6, a9 in zip(all_ids, avg06, avg09):\n",
    "        if a6 is not None or a9 is not None:\n",
    "            cats.append(mid)\n",
    "            p06.append(a6 or 0)\n",
    "            p09.append(a9 or 0)\n",
    "\n",
    "    cats = [shorten(name) for name in cats]\n",
    "\n",
    "    x = range(len(cats))\n",
    "    w = 0.35\n",
    "    col_idx = size_to_col[size_label]\n",
    "    ax = axes[col_idx]\n",
    "\n",
    "    # Plot CMD 0.6 and CMD 0.9 as side-by-side bars\n",
    "    ax.bar([i - w/2 for i in x], p06, width=w, label=\"CMD 0.6\")\n",
    "    ax.bar([i + w/2 for i in x], p09, width=w, label=\"CMD 0.9\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cats, rotation=45, ha=\"right\", fontsize=10)\n",
    "    ax.set_ylabel(\"Avg pass@1\")\n",
    "    ax.set_title(size_label, fontsize=14)\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout and save the plot as a PDF\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.savefig(\"figs/cmd_combined_results.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
