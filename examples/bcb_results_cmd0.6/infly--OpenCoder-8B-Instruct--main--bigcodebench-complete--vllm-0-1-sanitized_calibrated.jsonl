{"task_id": "BigCodeBench/0", "solution": "import itertools\nfrom random import shuffle\ndef task_func(numbers=list(range(1, 11))):\n    \"\"\"\n    Dee gaee soochi ke sabhee permutations ke liye, pratyek consecutive sankhya pair ke poorn differences ke yog ka average gyaat karta hai\n    Pratyek kramaparivartan, differences kee ganana karane se pahle pherbadal kiya jaata hai.\n    \n    Args:\n    - numbers (list): numbers kee ek soochi. Default 1 se 10 tak kee sankhyaen hain.\n\n    Returns:\n        float: soochi ke pratyek pherbadal kramaparivartan ke poorn differences ke yog ka average.\n\n    Reqs:\n    - itertools\n    - random.shuffle\n\n    Examples:\n    - >>> result = task_func([1, 2, 3])\n    - >>> isinstance(result, float)\n    - True\n\n    \"\"\"\n    # Generate all permutations of the list\n    perms = list(itertools.permutations(numbers))\n    \n    # Shuffle the permutations to randomize the order\n    shuffle(perms)\n    \n    # Calculate the average of differences for each permutation\n    averages = []\n    for perm in perms:\n        differences = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n        averages.append(sum(differences) / len(differences))\n    \n    # Return the average of all permutation averages\n    return sum(averages) / len(averages)", "raw_solution": "import itertools\nfrom random import shuffle\n\ndef task_func(numbers=list(range(1, 11))):\n    \"\"\"\n    Dee gaee soochi ke sabhee permutations ke liye, pratyek consecutive sankhya pair ke poorn differences ke yog ka average gyaat karta hai\n    Pratyek kramaparivartan, differences kee ganana karane se pahle pherbadal kiya jaata hai.\n    \n    Args:\n    - numbers (list): numbers kee ek soochi. Default 1 se 10 tak kee sankhyaen hain.\n\n    Returns:\n        float: soochi ke pratyek pherbadal kramaparivartan ke poorn differences ke yog ka average.\n\n    Reqs:\n    - itertools\n    - random.shuffle\n\n    Examples:\n    - >>> result = task_func([1, 2, 3])\n    - >>> isinstance(result, float)\n    - True\n\n    \"\"\"\n    # Generate all permutations of the list\n    perms = list(itertools.permutations(numbers))\n    \n    # Shuffle the permutations to randomize the order\n    shuffle(perms)\n    \n    # Calculate the average of differences for each permutation\n    averages = []\n    for perm in perms:\n        differences = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n        averages.append(sum(differences) / len(differences))\n    \n    # Return the average of all permutation averages\n    return sum(averages) / len(averages)\n```\n"}
{"task_id": "BigCodeBench/1", "solution": "import collections\nimport random\nimport string\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative number\")\n\n    # Generate a random string of uppercase and lowercase letters\n    random_string = ''.join(random.choices(string.ascii_letters, k=length))\n\n    # Count the occurrences of each character in the string\n    char_count = collections.Counter(random_string)\n\n    return dict(char_count)", "raw_solution": "import collections\nimport random\nimport string\n\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative number\")\n\n    # Generate a random string of uppercase and lowercase letters\n    random_string = ''.join(random.choices(string.ascii_letters, k=length))\n\n    # Count the occurrences of each character in the string\n    char_count = collections.Counter(random_string)\n\n    return dict(char_count)\n\n# Example usage:"}
{"task_id": "BigCodeBench/2", "solution": "import random\nimport statistics\ndef task_func(LETTERS):\n    result = {}\n    for letter in LETTERS:\n        numbers = [random.randint(0, 100) for _ in range(10)]\n        result[letter] = numbers\n\n    sorted_dict = {k: v for k, v in sorted(result.items(), key=lambda item: statistics.mean(item[1]))}\n\n    return sorted_dict", "raw_solution": "import random\nimport statistics\n\ndef task_func(LETTERS):\n    result = {}\n    for letter in LETTERS:\n        numbers = [random.randint(0, 100) for _ in range(10)]\n        result[letter] = numbers\n\n    sorted_dict = {k: v for k, v in sorted(result.items(), key=lambda item: statistics.mean(item[1]))}\n\n    return sorted_dict\n\n# Example usage:\nrandom.seed(42)\nsorted_dict = task_func(['a', 'b', 'c'])"}
{"task_id": "BigCodeBench/3", "solution": "import random\nimport numpy as np\ndef task_func(LETTERS):\n    mean_dict = {}\n    for letter in LETTERS:\n        random_integers = [random.randint(0, 100) for _ in range(1, 11)]\n        mean_value = np.mean(random_integers)\n        mean_dict[letter] = mean_value\n    return mean_dict\nLETTERS = ['a', 'b', 'c']", "raw_solution": "import random\nimport numpy as np\n\ndef task_func(LETTERS):\n    mean_dict = {}\n    for letter in LETTERS:\n        random_integers = [random.randint(0, 100) for _ in range(1, 11)]\n        mean_value = np.mean(random_integers)\n        mean_dict[letter] = mean_value\n    return mean_dict\n\n# Test cases\nLETTERS = ['a', 'b', 'c']\nmean_dict = task_func(LETTERS)\n\n# Check if the output is a dictionary\nassert isinstance(mean_dict, dict), \"Output should be a dictionary.\"\n\n# Check if all keys are in the dictionary\nassert all(letter in mean_dict.keys() for letter in LETTERS), \"All letters should be keys in the dictionary.\"\n\n# Check if all values are floats\nassert all(isinstance(v, float) for v in mean_dict.values()), \"All values should be floats.\"\n```\n"}
{"task_id": "BigCodeBench/4", "solution": "from collections import Counter\nimport itertools\ndef task_func(d):\n    \"\"\"\n    input *dictionary* ke *values* mein pratyek *integer* ki *occurrence* ko ginen, jahan pratyek *value* *integers* ki ek *list* hai,\n    aur in *counts* ke saath ek *dictionary* lautayen. parinaami *dictionary* ki *keys* *integers* hain, aur *values* hain\n    input *dictionary* ki sabhi *lists* mein unke *respective* *counts*.\n\n    Params:\n    - d (dict): ek *dictionary* jahan pratyek *key* ek *string* hai aur *value* *integers* ki ek *list* hai.\n\n    Returns:\n        *dict*: ek *dictionary* jahan pratyek *key* input *lists* mein se kisi bhi *integer* se hai, aur *value* *count* hai\n        ki vah *integer* sabhi *lists* mein ek saath kitni *often* dikhayi deta hai.\n\n    Reqs:\n    - collections.Counter\n    - itertools\n\n    Examples:\n    - >>> d = {'a': [1, 2, 3, 1], 'b': [3, 4, 5], 'c': [1, 2]}\n    - >>> count_dict = task_func(d)\n    - >>> print(count_dict)\n    - {1: 3, 2: 2, 3: 2, 4: 1, 5: 1}\n\n    \"\"\"\n    # Combine all lists into a single iterable\n    all_items = itertools.chain.from_iterable(d.values())\n    \n    # Count occurrences of each integer\n    counts = Counter(all_items)\n    \n    return dict(counts)", "raw_solution": "from collections import Counter\nimport itertools\n\ndef task_func(d):\n    \"\"\"\n    input *dictionary* ke *values* mein pratyek *integer* ki *occurrence* ko ginen, jahan pratyek *value* *integers* ki ek *list* hai,\n    aur in *counts* ke saath ek *dictionary* lautayen. parinaami *dictionary* ki *keys* *integers* hain, aur *values* hain\n    input *dictionary* ki sabhi *lists* mein unke *respective* *counts*.\n\n    Params:\n    - d (dict): ek *dictionary* jahan pratyek *key* ek *string* hai aur *value* *integers* ki ek *list* hai.\n\n    Returns:\n        *dict*: ek *dictionary* jahan pratyek *key* input *lists* mein se kisi bhi *integer* se hai, aur *value* *count* hai\n        ki vah *integer* sabhi *lists* mein ek saath kitni *often* dikhayi deta hai.\n\n    Reqs:\n    - collections.Counter\n    - itertools\n\n    Examples:\n    - >>> d = {'a': [1, 2, 3, 1], 'b': [3, 4, 5], 'c': [1, 2]}\n    - >>> count_dict = task_func(d)\n    - >>> print(count_dict)\n    - {1: 3, 2: 2, 3: 2, 4: 1, 5: 1}\n\n    \"\"\"\n    # Combine all lists into a single iterable\n    all_items = itertools.chain.from_iterable(d.values())\n    \n    # Count occurrences of each integer\n    counts = Counter(all_items)\n    \n    return dict(counts)\n```\n"}
{"task_id": "BigCodeBench/5", "solution": "import random\nimport math\ndef task_func(LETTERS=[chr(i) for i in range(97, 123)]):\n    result = {}\n    for letter in LETTERS:\n        values = [random.randint(1, 10) for _ in range(1, 11)]\n        std_dev = math.sqrt(sum((x - sum(values) / len(values)) ** 2 for x in values) / len(values))\n        result[letter] = std_dev\n    return result", "raw_solution": "import random\nimport math\n\ndef task_func(LETTERS=[chr(i) for i in range(97, 123)]):\n    result = {}\n    for letter in LETTERS:\n        values = [random.randint(1, 10) for _ in range(1, 11)]\n        std_dev = math.sqrt(sum((x - sum(values) / len(values)) ** 2 for x in values) / len(values))\n        result[letter] = std_dev\n    return result\n\n# Example usage:\nrandom.seed(42)\nsd_dict = task_func()"}
{"task_id": "BigCodeBench/6", "solution": "import os\nimport re\ndef task_func(pattern, log_dir='/var/log/'):\n    # List all files in the directory\n    files = os.listdir(log_dir)\n    \n    # Filter files using the regex pattern\n    matching_files = [f for f in files if re.match(pattern, f)]\n    \n    # If no matching files, return None\n    if not matching_files:\n        return None\n    \n    # Sort files by modification time, most recent first\n    recent_file = max(matching_files, key=os.path.getmtime)\n    \n    # Return the path of the most recent file\n    return os.path.join(log_dir, recent_file)", "raw_solution": "import os\nimport re\n\ndef task_func(pattern, log_dir='/var/log/'):\n    # List all files in the directory\n    files = os.listdir(log_dir)\n    \n    # Filter files using the regex pattern\n    matching_files = [f for f in files if re.match(pattern, f)]\n    \n    # If no matching files, return None\n    if not matching_files:\n        return None\n    \n    # Sort files by modification time, most recent first\n    recent_file = max(matching_files, key=os.path.getmtime)\n    \n    # Return the path of the most recent file\n    return os.path.join(log_dir, recent_file)\n```\n"}
{"task_id": "BigCodeBench/7", "solution": "import csv\nimport collections\nimport operator\ndef task_func(csv_file_path):\n    product_sales = collections.defaultdict(int)\n\n    with open(csv_file_path, mode='r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            product = row['product']\n            sales = int(row['matra'])\n            product_sales[product] += sales\n\n    best_selling_product = max(product_sales.items(), key=operator.itemgetter(1))[0]\n\n    return best_selling_product", "raw_solution": "import csv\nimport collections\nimport operator\n\ndef task_func(csv_file_path):\n    product_sales = collections.defaultdict(int)\n\n    with open(csv_file_path, mode='r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            product = row['product']\n            sales = int(row['matra'])\n            product_sales[product] += sales\n\n    best_selling_product = max(product_sales.items(), key=operator.itemgetter(1))[0]\n\n    return best_selling_product\n```\n"}
{"task_id": "BigCodeBench/8", "solution": "from collections import Counter\nimport itertools\nfrom random import randint\ndef task_func(T1, RANGE=100):\n    # Convert the tuples into a single list of integers\n    numbers = [int(num) for tup in T1 for num in tup]\n\n    # Generate random numbers within the specified range\n    random_numbers = [randint(0, RANGE) for _ in numbers]\n\n    # Count occurrences of each number in the random numbers list\n    counts = Counter(random_numbers)\n\n    return counts", "raw_solution": "from collections import Counter\nimport itertools\nfrom random import randint\n\ndef task_func(T1, RANGE=100):\n    # Convert the tuples into a single list of integers\n    numbers = [int(num) for tup in T1 for num in tup]\n\n    # Generate random numbers within the specified range\n    random_numbers = [randint(0, RANGE) for _ in numbers]\n\n    # Count occurrences of each number in the random numbers list\n    counts = Counter(random_numbers)\n\n    return counts\n\n# Example usage:"}
{"task_id": "BigCodeBench/9", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(list_of_pairs):\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['category', 'value'])\n\n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='category', y='value', data=df)\n    ax.set_title('Category vs Value')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(list_of_pairs):\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['category', 'value'])\n\n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='category', y='value', data=df)\n    ax.set_title('Category vs Value')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/10", "solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\ndef task_func(T1, RANGE=100):\n    # Convert strings in T1 to integers\n    T1 = tuple(map(lambda x: tuple(map(int, x)), T1))\n    \n    # Calculate the sum of all integers in T1\n    total_sum = sum(itertools.chain.from_iterable(T1))\n    \n    # Generate a random list of integers with the size equal to the sum of T1\n    random_list = [random.randint(0, RANGE) for _ in range(total_sum)]\n    \n    # Calculate mean, median, and mode of the random list\n    mean = statistics.mean(random_list)\n    median = statistics.median(random_list)\n    try:\n        mode = statistics.mode(random_list)\n    except statistics.StatisticsError:\n        mode = None  # If there is no unique mode, set it to None\n\n    return mean, median, mode", "raw_solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\n\ndef task_func(T1, RANGE=100):\n    # Convert strings in T1 to integers\n    T1 = tuple(map(lambda x: tuple(map(int, x)), T1))\n    \n    # Calculate the sum of all integers in T1\n    total_sum = sum(itertools.chain.from_iterable(T1))\n    \n    # Generate a random list of integers with the size equal to the sum of T1\n    random_list = [random.randint(0, RANGE) for _ in range(total_sum)]\n    \n    # Calculate mean, median, and mode of the random list\n    mean = statistics.mean(random_list)\n    median = statistics.median(random_list)\n    try:\n        mode = statistics.mode(random_list)\n    except statistics.StatisticsError:\n        mode = None  # If there is no unique mode, set it to None\n\n    return mean, median, mode\n```\n"}
{"task_id": "BigCodeBench/11", "solution": "import numpy as np\nimport itertools\nimport random\ndef task_func(T1, max_value=100):\n    # Convert the string representations of integers to actual integers\n    T1 = tuple(tuple(map(int, sublist)) for sublist in T1)\n    \n    # Flatten the tuple and generate a random list of integers\n    flat_T1 = list(itertools.chain.from_iterable(T1))\n    random_list = [random.randint(0, max_value) for _ in flat_T1]\n    \n    # Calculate the percentiles\n    sorted_list = np.sort(random_list)\n    p25 = np.percentile(sorted_list, 25)\n    p50 = np.percentile(sorted_list, 50)\n    p75 = np.percentile(sorted_list, 75)\n    \n    return p25, p50, p75\nT1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))", "raw_solution": "import numpy as np\nimport itertools\nimport random\n\ndef task_func(T1, max_value=100):\n    # Convert the string representations of integers to actual integers\n    T1 = tuple(tuple(map(int, sublist)) for sublist in T1)\n    \n    # Flatten the tuple and generate a random list of integers\n    flat_T1 = list(itertools.chain.from_iterable(T1))\n    random_list = [random.randint(0, max_value) for _ in flat_T1]\n    \n    # Calculate the percentiles\n    sorted_list = np.sort(random_list)\n    p25 = np.percentile(sorted_list, 25)\n    p50 = np.percentile(sorted_list, 50)\n    p75 = np.percentile(sorted_list, 75)\n    \n    return p25, p50, p75\n\n# Example usage\nrandom.seed(42)\nT1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\npercentiles = task_func(T1)"}
{"task_id": "BigCodeBench/12", "solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    if not os.path.isfile(script_name):\n        raise FileNotFoundError(f\"The script file {script_name} does not exist.\")\n\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    try:\n        result = subprocess.run(['bash', script_name], check=True)\n        exit_status = result.returncode\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"An error occurred while running the script: {e}\")\n\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    log_entry = {\n        'start_time': start_time,\n        'end_time': end_time,\n        'exit_status': exit_status\n    }\n\n    with open(log_file, 'w') as f:\n        json.dump(log_entry, f)\n\n    return log_entry", "raw_solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\n\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    if not os.path.isfile(script_name):\n        raise FileNotFoundError(f\"The script file {script_name} does not exist.\")\n\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    try:\n        result = subprocess.run(['bash', script_name], check=True)\n        exit_status = result.returncode\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"An error occurred while running the script: {e}\")\n\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    log_entry = {\n        'start_time': start_time,\n        'end_time': end_time,\n        'exit_status': exit_status\n    }\n\n    with open(log_file, 'w') as f:\n        json.dump(log_entry, f)\n\n    return log_entry\n```\n"}
{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n\n        # List all files in the directory\n        files = ftp.nlst()\n\n        # Download each file using subprocess\n        for file in files:\n            try:\n                # Create a local file path\n                local_file_path = os.path.join(os.getcwd(), file)\n\n                # Use subprocess to download the file\n                subprocess.run(['wget', '-O', local_file_path, f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{file}'], check=True)\n\n                # Add the filename to the list\n                filenames.append(file)\n            except Exception as e:\n                print(f\"Error downloading file {file}: {str(e)}\")\n\n        # Close the FTP connection\n        ftp.quit()\n\n    except ftplib.all_errors as e:\n        print(f\"FTP error: {str(e)}\")\n\n    return filenames", "raw_solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n\n        # List all files in the directory\n        files = ftp.nlst()\n\n        # Download each file using subprocess\n        for file in files:\n            try:\n                # Create a local file path\n                local_file_path = os.path.join(os.getcwd(), file)\n\n                # Use subprocess to download the file\n                subprocess.run(['wget', '-O', local_file_path, f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{file}'], check=True)\n\n                # Add the filename to the list\n                filenames.append(file)\n            except Exception as e:\n                print(f\"Error downloading file {file}: {str(e)}\")\n\n        # Close the FTP connection\n        ftp.quit()\n\n    except ftplib.all_errors as e:\n        print(f\"FTP error: {str(e)}\")\n\n    return filenames\n```\nThis script connects to an FTP server, changes to the specified directory, and downloads all files in that directory. It uses the `subprocess` module to run the `wget` command to download each file. The filenames of the downloaded files are stored in a list, which is returned by the function.\n"}
{"task_id": "BigCodeBench/14", "solution": "import configparser\nimport os\nimport shutil\ndef task_func(config_file_path, archieve_dir ='/home/user/archive'):\n    if not os.path.isfile(config_file_path):\n        raise FileNotFoundError(f\"The configuration file {config_file_path} does not exist.\")\n    \n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n    \n    if 'Project' not in config or 'directory' not in config['Project']:\n        raise ValueError(\"The configuration file does not contain the required 'directory' setting.\")\n    \n    project_dir = config['Project']['directory']\n    if not os.path.isdir(project_dir):\n        raise FileNotFoundError(f\"The project directory {project_dir} does not exist.\")\n    \n    try:\n        shutil.make_archive(os.path.join(archieve_dir, os.path.basename(project_dir)), 'zip', project_dir)\n        return True\n    except Exception as e:\n        print(f\"An error occurred while creating the archive: {e}\")\n        return False", "raw_solution": "import configparser\nimport os\nimport shutil\n\ndef task_func(config_file_path, archieve_dir ='/home/user/archive'):\n    if not os.path.isfile(config_file_path):\n        raise FileNotFoundError(f\"The configuration file {config_file_path} does not exist.\")\n    \n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n    \n    if 'Project' not in config or 'directory' not in config['Project']:\n        raise ValueError(\"The configuration file does not contain the required 'directory' setting.\")\n    \n    project_dir = config['Project']['directory']\n    if not os.path.isdir(project_dir):\n        raise FileNotFoundError(f\"The project directory {project_dir} does not exist.\")\n    \n    try:\n        shutil.make_archive(os.path.join(archieve_dir, os.path.basename(project_dir)), 'zip', project_dir)\n        return True\n    except Exception as e:\n        print(f\"An error occurred while creating the archive: {e}\")\n        return False\n```\n"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_files = []\n    with open(commands_file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for index, row in enumerate(reader):\n            command = row[0]\n            try:\n                result = subprocess.run(command, shell=True, capture_output=True, text=True)\n                output_file_path = os.path.join(output_dir_path, f\"command_{index+1}_output.txt\")\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n                output_files.append(output_file_path)\n            except subprocess.CalledProcessError as e:\n                error_message = f\"Command '{command}' failed with error: {e.stderr}\"\n                output_file_path = os.path.join(output_dir_path, f\"command_{index+1}_output.txt\")\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(error_message)\n                output_files.append(output_file_path)\n\n    return output_files", "raw_solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_files = []\n    with open(commands_file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for index, row in enumerate(reader):\n            command = row[0]\n            try:\n                result = subprocess.run(command, shell=True, capture_output=True, text=True)\n                output_file_path = os.path.join(output_dir_path, f\"command_{index+1}_output.txt\")\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n                output_files.append(output_file_path)\n            except subprocess.CalledProcessError as e:\n                error_message = f\"Command '{command}' failed with error: {e.stderr}\"\n                output_file_path = os.path.join(output_dir_path, f\"command_{index+1}_output.txt\")\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(error_message)\n                output_files.append(output_file_path)\n\n    return output_files\n```\n"}
{"task_id": "BigCodeBench/16", "solution": "import os\nimport glob\nimport subprocess\ndef task_func(directory, backup_dir='/path/to/backup'):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n    if not log_files:\n        return \"No log files to backup.\"\n\n    backup_file = os.path.join(backup_dir, 'logs_backup.tar.gz')\n    os.makedirs(backup_dir, exist_ok=True)\n\n    subprocess.run(['tar', '-czf', backup_file] + log_files, check=True)\n\n    for log_file in log_files:\n        os.remove(log_file)\n\n    return backup_file", "raw_solution": "import os\nimport glob\nimport subprocess\n\ndef task_func(directory, backup_dir='/path/to/backup'):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n    if not log_files:\n        return \"No log files to backup.\"\n\n    backup_file = os.path.join(backup_dir, 'logs_backup.tar.gz')\n    os.makedirs(backup_dir, exist_ok=True)\n\n    subprocess.run(['tar', '-czf', backup_file] + log_files, check=True)\n\n    for log_file in log_files:\n        os.remove(log_file)\n\n    return backup_file\n```\n"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['name']):\n        if proc.info['name'] == process_name:\n            # If the process is running, terminate it\n            for p in psutil.process_iter(['pid', 'name']):\n                if p.info['name'] == process_name:\n                    p.terminate()\n                    time.sleep(1)  # Give some time for the process to terminate\n            return f\"Process found. Restarting {process_name}.\"\n\n    # If the process is not running, start it\n    subprocess.Popen(process_name)\n    return f\"Process not found. Starting {process_name}.\"", "raw_solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['name']):\n        if proc.info['name'] == process_name:\n            # If the process is running, terminate it\n            for p in psutil.process_iter(['pid', 'name']):\n                if p.info['name'] == process_name:\n                    p.terminate()\n                    time.sleep(1)  # Give some time for the process to terminate\n            return f\"Process found. Restarting {process_name}.\"\n\n    # If the process is not running, start it\n    subprocess.Popen(process_name)\n    return f\"Process not found. Starting {process_name}.\"\n```\n"}
{"task_id": "BigCodeBench/18", "solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\ndef task_func(file):\n    if not os.path.exists(file):\n        print(f\"File {file} does not exist.\")\n        return []\n\n    try:\n        # Split the CSV file into smaller files\n        subprocess.run(['split', '-l', '1000', file, 'split_'], check=True)\n\n        # Get list of all split files\n        split_files = glob.glob('split_*')\n\n        # Shuffle lines in each split file\n        for split_file in split_files:\n            with open(split_file, 'r') as f:\n                lines = list(csv.reader(f))\n            random.shuffle(lines)\n\n            with open(split_file, 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerows(lines)\n\n        return split_files\n\n    except subprocess.CalledProcessError as e:\n        print(f\"An error occurred while splitting the file: {e}\")\n        return []\n\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return []", "raw_solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\n\ndef task_func(file):\n    if not os.path.exists(file):\n        print(f\"File {file} does not exist.\")\n        return []\n\n    try:\n        # Split the CSV file into smaller files\n        subprocess.run(['split', '-l', '1000', file, 'split_'], check=True)\n\n        # Get list of all split files\n        split_files = glob.glob('split_*')\n\n        # Shuffle lines in each split file\n        for split_file in split_files:\n            with open(split_file, 'r') as f:\n                lines = list(csv.reader(f))\n            random.shuffle(lines)\n\n            with open(split_file, 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerows(lines)\n\n        return split_files\n\n    except subprocess.CalledProcessError as e:\n        print(f\"An error occurred while splitting the file: {e}\")\n        return []\n\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return []\n```\n"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Get all files in the directory and subdirectories\n    file_paths = glob.glob(os.path.join(directory, '**'), recursive=True)\n\n    # Filter out directories and keep only files\n    file_paths = [file for file in file_paths if os.path.isfile(file)]\n\n    if not file_paths:\n        return None\n\n    # Define the zip file path\n    zip_file_path = os.path.join(directory, 'files.zip')\n\n    # Create the zip file\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        for file in file_paths:\n            zip_file.write(file, os.path.relpath(file, directory))\n\n    return zip_file_path", "raw_solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Get all files in the directory and subdirectories\n    file_paths = glob.glob(os.path.join(directory, '**'), recursive=True)\n\n    # Filter out directories and keep only files\n    file_paths = [file for file in file_paths if os.path.isfile(file)]\n\n    if not file_paths:\n        return None\n\n    # Define the zip file path\n    zip_file_path = os.path.join(directory, 'files.zip')\n\n    # Create the zip file\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        for file in file_paths:\n            zip_file.write(file, os.path.relpath(file, directory))\n\n    return zip_file_path\n```\n"}
{"task_id": "BigCodeBench/20", "solution": "import ast\nimport pandas as pd\nimport seaborn as sns\ndef task_func(csv_file):\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert 'dict_column' from string to dictionary\n    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n\n    # Create a pairplot using Seaborn\n    ax = sns.pairplot(df)\n\n    return df, ax", "raw_solution": "import ast\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(csv_file):\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert 'dict_column' from string to dictionary\n    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n\n    # Create a pairplot using Seaborn\n    ax = sns.pairplot(df)\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/21", "solution": "import psutil\nimport platform\ndef task_func():\n    # Fetching system details\n    os_name = platform.system()\n    architecture = platform.architecture()[0]\n    memory_info = psutil.virtual_memory()\n\n    # Calculating memory usage percentage\n    memory_usage_percent = memory_info.percent\n\n    # Preparing the dictionary with system details\n    system_info = {\n        'OS': os_name,\n        'Architecture': architecture,\n        'Memory Usage': f\"{memory_usage_percent}%\"\n    }\n\n    return system_info", "raw_solution": "import psutil\nimport platform\n\ndef task_func():\n    # Fetching system details\n    os_name = platform.system()\n    architecture = platform.architecture()[0]\n    memory_info = psutil.virtual_memory()\n\n    # Calculating memory usage percentage\n    memory_usage_percent = memory_info.percent\n\n    # Preparing the dictionary with system details\n    system_info = {\n        'OS': os_name,\n        'Architecture': architecture,\n        'Memory Usage': f\"{memory_usage_percent}%\"\n    }\n\n    return system_info\n\n# Example usage"}
{"task_id": "BigCodeBench/22", "solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\ndef task_func(l1, l2, K=10):\n    # Zip the two lists together, filling in None for shorter lists\n    zipped_lists = zip_longest(l1, l2, fillvalue=None)\n    \n    # Filter out None values and create a list of pairs\n    pairs = [(x, y) for x, y in zipped_lists if x is not None and y is not None]\n    \n    # Select K random pairs from the list\n    random_pairs = choices(pairs, k=K)\n    \n    # Create a Counter object to count the frequency of each element in the pairs\n    counter = collections.Counter()\n    for x, y in random_pairs:\n        counter[x] += 1\n    \n    return counter", "raw_solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\n\ndef task_func(l1, l2, K=10):\n    # Zip the two lists together, filling in None for shorter lists\n    zipped_lists = zip_longest(l1, l2, fillvalue=None)\n    \n    # Filter out None values and create a list of pairs\n    pairs = [(x, y) for x, y in zipped_lists if x is not None and y is not None]\n    \n    # Select K random pairs from the list\n    random_pairs = choices(pairs, k=K)\n    \n    # Create a Counter object to count the frequency of each element in the pairs\n    counter = collections.Counter()\n    for x, y in random_pairs:\n        counter[x] += 1\n    \n    return counter\n\n# Example usage:"}
{"task_id": "BigCodeBench/23", "solution": "import numpy as np\nfrom itertools import zip_longest\ndef task_func(l1, l2, THRESHOLD=0.5):\n    \"\"\"\n    This function takes two lists of numbers, l1 and l2, and calculates the absolute difference between corresponding elements.\n    It then finds the element in the second list (l2) that has the smallest absolute difference with the threshold value (0.5 by default).\n    If the lists are of different lengths, it only compares elements up to the length of the shorter list.\n\n    Parameters:\n    - l1 (list): The first list of numbers.\n    - l2 (list): The second list of numbers.\n    - THRESHOLD (float): The threshold value for comparison. Default is 0.5.\n\n    Returns:\n    - float: The element in l2 that has the smallest absolute difference with the threshold value.\n\n    Requirements:\n    - numpy\n    - itertools.zip_longest\n\n    Examples:\n    - >>> l1 = [0.3, 1, 2, 3]\n    - >>> l2 = [0.7, 11, 12, 13]\n    - >>> closest = task_func(l1, l2)\n    - >>> print(closest)\n    - 0.7\n    \"\"\"\n    # Convert lists to numpy arrays for efficient computation\n    l1_array = np.array(l1)\n    l2_array = np.array(l2)\n\n    # Calculate absolute differences\n    differences = np.abs(l1_array - THRESHOLD)\n\n    # Find the index of the minimum difference\n    min_index = np.argmin(differences)\n\n    # Return the corresponding element from l2\n    return l2_array[min_index]\nl1 = [0.3, 1, 2, 3]\nl2 = [0.7, 11, 12, 13]", "raw_solution": "import numpy as np\nfrom itertools import zip_longest\n\ndef task_func(l1, l2, THRESHOLD=0.5):\n    \"\"\"\n    This function takes two lists of numbers, l1 and l2, and calculates the absolute difference between corresponding elements.\n    It then finds the element in the second list (l2) that has the smallest absolute difference with the threshold value (0.5 by default).\n    If the lists are of different lengths, it only compares elements up to the length of the shorter list.\n\n    Parameters:\n    - l1 (list): The first list of numbers.\n    - l2 (list): The second list of numbers.\n    - THRESHOLD (float): The threshold value for comparison. Default is 0.5.\n\n    Returns:\n    - float: The element in l2 that has the smallest absolute difference with the threshold value.\n\n    Requirements:\n    - numpy\n    - itertools.zip_longest\n\n    Examples:\n    - >>> l1 = [0.3, 1, 2, 3]\n    - >>> l2 = [0.7, 11, 12, 13]\n    - >>> closest = task_func(l1, l2)\n    - >>> print(closest)\n    - 0.7\n    \"\"\"\n    # Convert lists to numpy arrays for efficient computation\n    l1_array = np.array(l1)\n    l2_array = np.array(l2)\n\n    # Calculate absolute differences\n    differences = np.abs(l1_array - THRESHOLD)\n\n    # Find the index of the minimum difference\n    min_index = np.argmin(differences)\n\n    # Return the corresponding element from l2\n    return l2_array[min_index]\n\n# Test cases\nl1 = [0.3, 1, 2, 3]\nl2 = [0.7, 11, 12, 13]"}
{"task_id": "BigCodeBench/24", "solution": "import base64\nimport hashlib\nimport os\ndef task_func(password, SALT_LENGTH = 32):\n    if not password:\n        raise ValueError(\"Password must not be None or empty\")\n\n    # Generate a random salt\n    salt = os.urandom(SALT_LENGTH)\n    salt_b64 = base64.b64encode(salt)\n\n    # Hash the password using SHA-256 and PBKDF2 HMAC\n    dk = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    hashed_password_b64 = base64.b64encode(dk)\n\n    return salt_b64, hashed_password_b64", "raw_solution": "import base64\nimport hashlib\nimport os\n\ndef task_func(password, SALT_LENGTH = 32):\n    if not password:\n        raise ValueError(\"Password must not be None or empty\")\n\n    # Generate a random salt\n    salt = os.urandom(SALT_LENGTH)\n    salt_b64 = base64.b64encode(salt)\n\n    # Hash the password using SHA-256 and PBKDF2 HMAC\n    dk = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    hashed_password_b64 = base64.b64encode(dk)\n\n    return salt_b64, hashed_password_b64\n\n# Example usage:\n# salt, hashed_password = task_func('my_password')\n# print(isinstance(salt, bytes))  # Output: True\n# print(isinstance(hashed_password, bytes))  # Output: True\n```\n"}
{"task_id": "BigCodeBench/25", "solution": "import base64\nimport json\nimport zlib\ndef task_func(data_dict):\n    # Convert dictionary to JSON string\n    json_str = json.dumps(data_dict)\n    \n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode('utf-8'))\n    \n    # Encode the compressed data using base64\n    encoded_data = base64.b64encode(compressed_data).decode('utf-8')\n    \n    return encoded_data", "raw_solution": "import base64\nimport json\nimport zlib\n\ndef task_func(data_dict):\n    # Convert dictionary to JSON string\n    json_str = json.dumps(data_dict)\n    \n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode('utf-8'))\n    \n    # Encode the compressed data using base64\n    encoded_data = base64.b64encode(compressed_data).decode('utf-8')\n    \n    return encoded_data\n\n# Example usage"}
{"task_id": "BigCodeBench/26", "solution": "import base64\nfrom cryptography.fernet import Fernet\ndef task_func(message, encryption_key):\n    # Convert the encryption key from string to bytes\n    key_bytes = encryption_key.encode()\n\n    # Generate a Fernet object using the key\n    cipher = Fernet(key_bytes)\n\n    # Encrypt the message\n    encrypted_message = cipher.encrypt(message.encode())\n\n    # Encode the encrypted message with base64\n    encoded_message = base64.b64encode(encrypted_message)\n\n    # Convert the encoded message to a string\n    encoded_message_str = encoded_message.decode()\n\n    return encoded_message_str", "raw_solution": "import base64\nfrom cryptography.fernet import Fernet\n\ndef task_func(message, encryption_key):\n    # Convert the encryption key from string to bytes\n    key_bytes = encryption_key.encode()\n\n    # Generate a Fernet object using the key\n    cipher = Fernet(key_bytes)\n\n    # Encrypt the message\n    encrypted_message = cipher.encrypt(message.encode())\n\n    # Encode the encrypted message with base64\n    encoded_message = base64.b64encode(encrypted_message)\n\n    # Convert the encoded message to a string\n    encoded_message_str = encoded_message.decode()\n\n    return encoded_message_str\n\n# Example usage:\n# encrypted_message = task_func('Hello, World!', '01234567890123456789012345678901')\n# print(encrypted_message)\n```\n"}
{"task_id": "BigCodeBench/27", "solution": "import json\nimport base64\nfrom datetime import datetime\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    \"\"\"\n    Ek Python dictionary leta hai, usme vartaman timestamp jodta hai, sanshodhit dictionary ko JSON-formatted string mein serialize karta hai, aur phir is string ko ascii character encoding ke saath base64 encoding ka upyog karke encode karta hai.\n\n    Params:\n    - data (dict): Encode karne ke liye Python dictionary. dictionary mein 'timestamp' naamak koi name nahi honi chahiye, kyunki is name ka upyog function dwara vartaman timestamp ko sammilit karne ke liye kiya jata hai. Input dictionary ko 'timestamp' name jodkar place sanshodhit kiya jata hai.\n\n    Returns:\n        str: Ek base64 encoded string jo ek jode gaye timestamp ke saath input dictionary ka pratinidhitva karta hai, jo ascii mein encoded kiya gaya hai. Timestamp 'timestamp' name ke saath joda jata hai.\n        DATE_FORMAT: timestamp format. Default roop se 'YYYY-MM-DD HH:MM:SS'.\n\n    Reqs:\n    - json\n    - base64\n    - datetime.datetime\n\n    Examples:\n    - >>> data = {'name': 'John', 'age': 30, 'city': 'New York'}\n    - >>> encoded_data = task_func(data)\n    - >>> isinstance(encoded_data, str)\n    - True\n\n    \"\"\"\n    # Add current timestamp to the data\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n    \n    # Convert dictionary to JSON string\n    json_str = json.dumps(data)\n    \n    # Encode the JSON string to bytes\n    json_bytes = json_str.encode('ascii')\n    \n    # Encode the bytes to base64\n    base64_bytes = base64.b64encode(json_bytes)\n    \n    # Convert the base64 bytes to a string\n    base64_str = base64_bytes.decode('ascii')\n    \n    return base64_str", "raw_solution": "import json\nimport base64\nfrom datetime import datetime\n\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    \"\"\"\n    Ek Python dictionary leta hai, usme vartaman timestamp jodta hai, sanshodhit dictionary ko JSON-formatted string mein serialize karta hai, aur phir is string ko ascii character encoding ke saath base64 encoding ka upyog karke encode karta hai.\n\n    Params:\n    - data (dict): Encode karne ke liye Python dictionary. dictionary mein 'timestamp' naamak koi name nahi honi chahiye, kyunki is name ka upyog function dwara vartaman timestamp ko sammilit karne ke liye kiya jata hai. Input dictionary ko 'timestamp' name jodkar place sanshodhit kiya jata hai.\n\n    Returns:\n        str: Ek base64 encoded string jo ek jode gaye timestamp ke saath input dictionary ka pratinidhitva karta hai, jo ascii mein encoded kiya gaya hai. Timestamp 'timestamp' name ke saath joda jata hai.\n        DATE_FORMAT: timestamp format. Default roop se 'YYYY-MM-DD HH:MM:SS'.\n\n    Reqs:\n    - json\n    - base64\n    - datetime.datetime\n\n    Examples:\n    - >>> data = {'name': 'John', 'age': 30, 'city': 'New York'}\n    - >>> encoded_data = task_func(data)\n    - >>> isinstance(encoded_data, str)\n    - True\n\n    \"\"\"\n    # Add current timestamp to the data\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n    \n    # Convert dictionary to JSON string\n    json_str = json.dumps(data)\n    \n    # Encode the JSON string to bytes\n    json_bytes = json_str.encode('ascii')\n    \n    # Encode the bytes to base64\n    base64_bytes = base64.b64encode(json_bytes)\n    \n    # Convert the base64 bytes to a string\n    base64_str = base64_bytes.decode('ascii')\n    \n    return base64_str\n```\n"}
{"task_id": "BigCodeBench/28", "solution": "import requests\nimport json\nimport base64\ndef task_func(data, url=\"http://your-api-url.com\"):\n    # Convert dictionary to JSON string\n    json_data = json.dumps(data)\n    \n    # Encode JSON string to base64\n    base64_data = base64.b64encode(json_data.encode('utf-8')).decode('utf-8')\n    \n    # Prepare payload\n    payload = {'payload': base64_data}\n    \n    # Send POST request\n    response = requests.post(url, data=payload)\n    \n    return response", "raw_solution": "import requests\nimport json\nimport base64\n\ndef task_func(data, url=\"http://your-api-url.com\"):\n    # Convert dictionary to JSON string\n    json_data = json.dumps(data)\n    \n    # Encode JSON string to base64\n    base64_data = base64.b64encode(json_data.encode('utf-8')).decode('utf-8')\n    \n    # Prepare payload\n    payload = {'payload': base64_data}\n    \n    # Send POST request\n    response = requests.post(url, data=payload)\n    \n    return response\n\n# Example usage:\n# data = {'name': 'John', 'age': 30, 'city': 'New York'}\n# response = task_func(data, url=\"http://your-api-url.com\")\n# print(response.status_code)\n```\n"}
{"task_id": "BigCodeBench/29", "solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\ndef task_func(data):\n    \"\"\"\n    sklearn ke StandardScaler ka upayog kar ke ek sankhyaatmak **array** ko **standardized** karein aur **standardized data** ko base64 **format** mein ek ASCII **string** ke roop mein encode karein.\n\n    Params:\n    - data (numpy.ndarray): standardized aur encode karne ke liye numpy **array**.\n\n    Returns:\n        str: **Data** ka base64-encoded ASCII **string** **representation**.\n\n    Reqs:\n    - sklearn.preprocessing.StandardScaler\n    - numpy\n    - base64\n\n    Examples:\n    - >>> data = np.array([[0, 0], [0, 0], [1, 1], [1, 1]])\n    - >>> encoded_data = task_func(data)\n    - >>> print(encoded_data)\n    - W1stMS4gLTEuXQogWy0xLiAtMS5dCiBbIDEuICAxLl0KIFsgMS4gIDEuXV0=\n\n    \"\"\"\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Encoding the standardized data to base64\n    encoded_data = base64.b64encode(standardized_data.tobytes()).decode('utf-8')\n\n    return encoded_data", "raw_solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\n\ndef task_func(data):\n    \"\"\"\n    sklearn ke StandardScaler ka upayog kar ke ek sankhyaatmak **array** ko **standardized** karein aur **standardized data** ko base64 **format** mein ek ASCII **string** ke roop mein encode karein.\n\n    Params:\n    - data (numpy.ndarray): standardized aur encode karne ke liye numpy **array**.\n\n    Returns:\n        str: **Data** ka base64-encoded ASCII **string** **representation**.\n\n    Reqs:\n    - sklearn.preprocessing.StandardScaler\n    - numpy\n    - base64\n\n    Examples:\n    - >>> data = np.array([[0, 0], [0, 0], [1, 1], [1, 1]])\n    - >>> encoded_data = task_func(data)\n    - >>> print(encoded_data)\n    - W1stMS4gLTEuXQogWy0xLiAtMS5dCiBbIDEuICAxLl0KIFsgMS4gIDEuXV0=\n\n    \"\"\"\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Encoding the standardized data to base64\n    encoded_data = base64.b64encode(standardized_data.tobytes()).decode('utf-8')\n\n    return encoded_data\n```\n"}
{"task_id": "BigCodeBench/30", "solution": "import json\nimport os\nimport re\ndef task_func(\n    file_path,\n    attribute,\n    INPUT_JSON={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": str},  \n            \"age\": {\"type\": int},   \n            \"email\": {\"type\": str}  \n        },\n        \"required\": [\"name\", \"age\", \"email\"]\n    },\n    EMAIL_REGEX=r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"):\n    \"\"\"\n    Validate the structure and contents of a JSON file against pre-defined schema rules and retrieve a specified attribute from the JSON object. This ensures that all required fields are present, match their defined types, and checks the validity of the email format using a regular expression.\n    errors:\n    - If the file does not exist, required attributes are missing, type mismatches occur, or the email format is invalid.\n\n    Params:\n    - file_path (str): JSON file ka path.\n    - attribute (str): JSON object se prapt ki jaane waali attribute.\n    - INPUT_JSON (dict): Validate karne ke liye input JSON. default maan hai:\n    - '{'\n    - \"type\": \"object\",\n    - \"properties\": {\n    - \"name\": {\"type\": str},\n    - \"age\": {\"type\": int},\n    - \"email\": {\"type\": str}\n    - },\n    - \"required\": [\"name\", \"age\", \"email\"]\n    - }' / \n    - EMAIL_REGEX (str): Email validity ki jaanch karne ke liye upayog kiya jaane waala regex. default 'r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"' hai.\n\n    Returns:\n        Any: JSON schema mein defined type ke consistent, specified attribute ka maan.\n\n    Reqs:\n    - json\n    - os\n    - re\n\n    Examples:\n    - >>> task_func('/path/to/file.json', 'email')\n    - 'john.doe@example.com'\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    # Validate the JSON structure\n    for key, value in INPUT_JSON['properties'].items():\n        if key not in data:\n            raise ValueError(f\"Missing required attribute: {key}\")\n        if not isinstance(data[key], value['type']):\n            raise TypeError(f\"Attribute {key} is not of type {value['type']}\")\n    \n    # Check email format\n    if not re.match(EMAIL_REGEX, data['email']):\n        raise ValueError(\"Invalid email format.\")\n    \n    return data[attribute]", "raw_solution": "import json\nimport os\nimport re\n\ndef task_func(\n    file_path,\n    attribute,\n    INPUT_JSON={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": str},  \n            \"age\": {\"type\": int},   \n            \"email\": {\"type\": str}  \n        },\n        \"required\": [\"name\", \"age\", \"email\"]\n    },\n    EMAIL_REGEX=r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"):\n    \"\"\"\n    Validate the structure and contents of a JSON file against pre-defined schema rules and retrieve a specified attribute from the JSON object. This ensures that all required fields are present, match their defined types, and checks the validity of the email format using a regular expression.\n    errors:\n    - If the file does not exist, required attributes are missing, type mismatches occur, or the email format is invalid.\n\n    Params:\n    - file_path (str): JSON file ka path.\n    - attribute (str): JSON object se prapt ki jaane waali attribute.\n    - INPUT_JSON (dict): Validate karne ke liye input JSON. default maan hai:\n    - '{'\n    - \"type\": \"object\",\n    - \"properties\": {\n    - \"name\": {\"type\": str},\n    - \"age\": {\"type\": int},\n    - \"email\": {\"type\": str}\n    - },\n    - \"required\": [\"name\", \"age\", \"email\"]\n    - }' / \n    - EMAIL_REGEX (str): Email validity ki jaanch karne ke liye upayog kiya jaane waala regex. default 'r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"' hai.\n\n    Returns:\n        Any: JSON schema mein defined type ke consistent, specified attribute ka maan.\n\n    Reqs:\n    - json\n    - os\n    - re\n\n    Examples:\n    - >>> task_func('/path/to/file.json', 'email')\n    - 'john.doe@example.com'\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    # Validate the JSON structure\n    for key, value in INPUT_JSON['properties'].items():\n        if key not in data:\n            raise ValueError(f\"Missing required attribute: {key}\")\n        if not isinstance(data[key], value['type']):\n            raise TypeError(f\"Attribute {key} is not of type {value['type']}\")\n    \n    # Check email format\n    if not re.match(EMAIL_REGEX, data['email']):\n        raise ValueError(\"Invalid email format.\")\n    \n    return data[attribute]\n```\n"}
{"task_id": "BigCodeBench/31", "solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nPUNCTUATION = set(punctuation)\ndef task_func(text):\n    \"\"\"\n    Ek text mein \"$\" character se shuru hone wale words ki baar chart banao. Jin words ki shuruwat '$' character se hoti hai but unme only punctuation hote hain (jaise, '$!$' aur '$.$') ko frequency count mein shamil nahi kiya jaata hai.\n    - Yadi above conditions ka palan karne wala koi word nahi hai, to plot None hona chahiye.\n    - barplot x-axis par words aur y-axis par frequencyyon ko darshata hai.\n\n    Params:\n    - text (str): Input text.\n\n    Returns:\n        matplotlib.axes._axes.Axes: '$' character se shuru hone wale words ki frequency dikhane wala plot.\n\n    Reqs:\n    - nltk\n    - string\n    - seaborn\n    - matplotlib\n\n    Examples:\n    - >>> text = \"$child than resource indicate star $community station onto best green $exactly onto then age charge $friend than ready child really $let product coach decision professional $camera life off management factor $alone beat idea bit call $campaign fill stand Congress stuff $performance follow your resource road $data performance himself school here\"\n    - >>> ax = task_func(text)\n    - >>> print(ax)\n    - Axes(0.125,0.11;0.775x0.77)\n\n    \"\"\"\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Filter words starting with '$' and containing only punctuation\n    dollar_words = [word for word in words if word.startswith('$') and all(c in PUNCTUATION for c in word[1:])]\n\n    # Create a frequency distribution\n    freq_dist = nltk.FreqDist(dollar_words)\n\n    # Create a bar plot\n    if freq_dist:\n        plt.figure(figsize=(10, 6))\n        ax = sns.barplot(x=list(freq_dist.keys()), y=list(freq_dist.values()))\n        ax.set_title('Frequency of $-prefixed Words with Only Punctuation')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return ax\n    else:\n        return None", "raw_solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPUNCTUATION = set(punctuation)\n\n\ndef task_func(text):\n    \"\"\"\n    Ek text mein \"$\" character se shuru hone wale words ki baar chart banao. Jin words ki shuruwat '$' character se hoti hai but unme only punctuation hote hain (jaise, '$!$' aur '$.$') ko frequency count mein shamil nahi kiya jaata hai.\n    - Yadi above conditions ka palan karne wala koi word nahi hai, to plot None hona chahiye.\n    - barplot x-axis par words aur y-axis par frequencyyon ko darshata hai.\n\n    Params:\n    - text (str): Input text.\n\n    Returns:\n        matplotlib.axes._axes.Axes: '$' character se shuru hone wale words ki frequency dikhane wala plot.\n\n    Reqs:\n    - nltk\n    - string\n    - seaborn\n    - matplotlib\n\n    Examples:\n    - >>> text = \"$child than resource indicate star $community station onto best green $exactly onto then age charge $friend than ready child really $let product coach decision professional $camera life off management factor $alone beat idea bit call $campaign fill stand Congress stuff $performance follow your resource road $data performance himself school here\"\n    - >>> ax = task_func(text)\n    - >>> print(ax)\n    - Axes(0.125,0.11;0.775x0.77)\n\n    \"\"\"\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Filter words starting with '$' and containing only punctuation\n    dollar_words = [word for word in words if word.startswith('$') and all(c in PUNCTUATION for c in word[1:])]\n\n    # Create a frequency distribution\n    freq_dist = nltk.FreqDist(dollar_words)\n\n    # Create a bar plot\n    if freq_dist:\n        plt.figure(figsize=(10, 6))\n        ax = sns.barplot(x=list(freq_dist.keys()), y=list(freq_dist.values()))\n        ax.set_title('Frequency of $-prefixed Words with Only Punctuation')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return ax\n    else:\n        return None\n```\n"}
{"task_id": "BigCodeBench/32", "solution": "import requests\nfrom bs4 import BeautifulSoup\ndef task_func(url, tag):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n    except requests.RequestException as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    element = soup.find(tag)\n\n    if element:\n        return element.text\n    else:\n        return None", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, tag):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n    except requests.RequestException as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    element = soup.find(tag)\n\n    if element:\n        return element.text\n    else:\n        return None\n```\n"}
{"task_id": "BigCodeBench/33", "solution": "import numpy as np\nfrom functools import reduce\ndef task_func(list_of_pairs):\n    \"\"\"\n    Ek tuple ki list mein har tuple ke dusre 'values' ka 'product' calculate karein aur us 'product' ko ek single element wale 'numeric' 'array' ke roop mein return karein.\n\n    Params:\n    - list_of_pairs (list): Ek list tuples ki, jahan pehla element category hai aur doosra element numeric value hai.\n\n    Returns:\n        numpy.ndarray: Ek 1D numpy array jismein ek single element ho jo list mein tuples ke dusre values ka product ho.\n\n    Reqs:\n    - numpy\n    - functools.reduce\n\n    Examples:\n    - >>> list_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\n    - >>> product_array = task_func(list_of_pairs)\n    - >>> print(product_array)\n    - [360]\n\n    \"\"\"\n    # Extract the numeric values from the list of pairs\n    values = [pair[1] for pair in list_of_pairs]\n    \n    # Calculate the product of all numeric values\n    product = reduce(lambda x, y: x * y, values)\n    \n    # Convert the product to a numpy array\n    product_array = np.array([product])\n    \n    return product_array\nlist_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]", "raw_solution": "import numpy as np\nfrom functools import reduce\n\ndef task_func(list_of_pairs):\n    \"\"\"\n    Ek tuple ki list mein har tuple ke dusre 'values' ka 'product' calculate karein aur us 'product' ko ek single element wale 'numeric' 'array' ke roop mein return karein.\n\n    Params:\n    - list_of_pairs (list): Ek list tuples ki, jahan pehla element category hai aur doosra element numeric value hai.\n\n    Returns:\n        numpy.ndarray: Ek 1D numpy array jismein ek single element ho jo list mein tuples ke dusre values ka product ho.\n\n    Reqs:\n    - numpy\n    - functools.reduce\n\n    Examples:\n    - >>> list_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\n    - >>> product_array = task_func(list_of_pairs)\n    - >>> print(product_array)\n    - [360]\n\n    \"\"\"\n    # Extract the numeric values from the list of pairs\n    values = [pair[1] for pair in list_of_pairs]\n    \n    # Calculate the product of all numeric values\n    product = reduce(lambda x, y: x * y, values)\n    \n    # Convert the product to a numpy array\n    product_array = np.array([product])\n    \n    return product_array\n\n# Test cases\nlist_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\nproduct_array = task_func(list_of_pairs)"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Generate word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    \n    # Check if any words are available\n    if not wordcloud.words_:\n        raise ValueError(\"Urls hatane ke baad word cloud banaane ke lie koi words available nahin hai.\")\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud", "raw_solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Generate word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    \n    # Check if any words are available\n    if not wordcloud.words_:\n        raise ValueError(\"Urls hatane ke baad word cloud banaane ke lie koi words available nahin hai.\")\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud\n```\n"}
{"task_id": "BigCodeBench/35", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df, target_values=[1, 3, 4]):\n    # Replace values in the dataframe that are not in target_values with zeros\n    df[df.columns.difference(target_values)] = 0\n\n    # Plot the distribution of each column\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.boxplot(data=df, ax=ax)\n    ax.set_title('Distribution of Columns')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return df, ax\ndf = pd.DataFrame(np.random.randint(0,10,size=(100, 5)), columns=list('AbcDE'))", "raw_solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df, target_values=[1, 3, 4]):\n    # Replace values in the dataframe that are not in target_values with zeros\n    df[df.columns.difference(target_values)] = 0\n\n    # Plot the distribution of each column\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.boxplot(data=df, ax=ax)\n    ax.set_title('Distribution of Columns')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return df, ax\n\n# Example usage:\nnp.random.seed(42)\ndf = pd.DataFrame(np.random.randint(0,10,size=(100, 5)), columns=list('AbcDE'))"}
{"task_id": "BigCodeBench/36", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nTARGET_VALUES = np.array([1, 3, 4])\ndef task_func(df):\n    transformed_df = df.copy()\n    for col in df.columns:\n        if np.all(np.isin(df[col].unique(), TARGET_VALUES)):\n            transformed_df[col], fitted_lambda = stats.boxcox(df[col])\n        else:\n            transformed_df[col] = np.where(np.isin(df[col], TARGET_VALUES), \n                                             stats.boxcox(df[col], lmbda=fitted_lambda)[0], \n                                             0)\n    \n    fig, ax = plt.subplots()\n    for col in df.columns:\n        ax.hist(transformed_df[col], alpha=0.5, label=col)\n    ax.set_title('KDE Plot of Transformed Columns')\n    ax.legend()\n    \n    return transformed_df, fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nTARGET_VALUES = np.array([1, 3, 4])\n\ndef task_func(df):\n    transformed_df = df.copy()\n    for col in df.columns:\n        if np.all(np.isin(df[col].unique(), TARGET_VALUES)):\n            transformed_df[col], fitted_lambda = stats.boxcox(df[col])\n        else:\n            transformed_df[col] = np.where(np.isin(df[col], TARGET_VALUES), \n                                             stats.boxcox(df[col], lmbda=fitted_lambda)[0], \n                                             0)\n    \n    fig, ax = plt.subplots()\n    for col in df.columns:\n        ax.hist(transformed_df[col], alpha=0.5, label=col)\n    ax.set_title('KDE Plot of Transformed Columns')\n    ax.legend()\n    \n    return transformed_df, fig\n```\n"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n    # Initialize the RandomForestClassifier\n    model = RandomForestClassifier(random_state=42)\n    model.fit(df.drop(columns=[target_column]), df[target_column])\n\n    # Get feature importances\n    feature_importances = model.feature_importances_\n    feature_names = df.drop(columns=[target_column]).columns\n\n    # Create a DataFrame for feature importances\n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': feature_importances\n    })\n\n    # Sort the DataFrame by importance\n    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='Importance', y='Feature', data=importance_df)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Feature')\n    plt.title('Feature Importance Visualization')\n    plt.tight_layout()\n\n    return model, ax", "raw_solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_column):\n    # Initialize the RandomForestClassifier\n    model = RandomForestClassifier(random_state=42)\n    model.fit(df.drop(columns=[target_column]), df[target_column])\n\n    # Get feature importances\n    feature_importances = model.feature_importances_\n    feature_names = df.drop(columns=[target_column]).columns\n\n    # Create a DataFrame for feature importances\n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': feature_importances\n    })\n\n    # Sort the DataFrame by importance\n    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='Importance', y='Feature', data=importance_df)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Feature')\n    plt.title('Feature Importance Visualization')\n    plt.tight_layout()\n\n    return model, ax\n```\n"}
{"task_id": "BigCodeBench/38", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\ndef task_func(data_matrix):\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    df['Mean'] = df.mean(axis=1)\n\n    ax = df['Mean'].plot(kind='hist', bins=10, title='Mean ka Distribution')\n    plt.xlabel('Mean Value')\n    plt.ylabel('Frequency')\n\n    return df, ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\n\ndef task_func(data_matrix):\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    df['Mean'] = df.mean(axis=1)\n\n    ax = df['Mean'].plot(kind='hist', bins=10, title='Mean ka Distribution')\n    plt.xlabel('Mean Value')\n    plt.ylabel('Frequency')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/39", "solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\nALPHA = 0.05\ndef task_func(data_matrix):\n    means = np.mean(data_matrix, axis=1)\n    p_values = ttest_1samp(means, popmean=0).pvalue\n    significantly_significant_indices = np.where(p_values < ALPHA)[0]\n\n    fig, ax = plt.subplots()\n    ax.plot(means, label='means')\n    ax.plot(significantly_significant_indices, means[significantly_significant_indices], 'ro', label='significant means')\n    ax.axhline(np.mean(means), color='g', linestyle='--', label='population mean')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Mean Value')\n    ax.set_title('Mean Values and Significance')\n    ax.legend()\n\n    return significantly_significant_indices.tolist(), ax", "raw_solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHA = 0.05\n\ndef task_func(data_matrix):\n    means = np.mean(data_matrix, axis=1)\n    p_values = ttest_1samp(means, popmean=0).pvalue\n    significantly_significant_indices = np.where(p_values < ALPHA)[0]\n\n    fig, ax = plt.subplots()\n    ax.plot(means, label='means')\n    ax.plot(significantly_significant_indices, means[significantly_significant_indices], 'ro', label='significant means')\n    ax.axhline(np.mean(means), color='g', linestyle='--', label='population mean')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Mean Value')\n    ax.set_title('Mean Values and Significance')\n    ax.legend()\n\n    return significantly_significant_indices.tolist(), ax\n```\n"}
{"task_id": "BigCodeBench/40", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(data_matrix):\n    # Convert the 2D data_matrix to a pandas DataFrame\n    df = pd.DataFrame(data_matrix, columns=[f'Feature {i+1}' for i in range(data_matrix.shape[1])])\n    \n    # Calculate the Z-scores for each feature\n    z_scores = df.apply(zscore)\n    \n    # Calculate the mean of each feature\n    means = df.mean(axis=0)\n    \n    # Add the mean as a new column in the DataFrame\n    z_scores['Mean'] = means\n    \n    # Create a correlation matrix\n    corr_matrix = z_scores.corr()\n    \n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Matrix Heatmap')\n    \n    return z_scores, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef task_func(data_matrix):\n    # Convert the 2D data_matrix to a pandas DataFrame\n    df = pd.DataFrame(data_matrix, columns=[f'Feature {i+1}' for i in range(data_matrix.shape[1])])\n    \n    # Calculate the Z-scores for each feature\n    z_scores = df.apply(zscore)\n    \n    # Calculate the mean of each feature\n    means = df.mean(axis=0)\n    \n    # Add the mean as a new column in the DataFrame\n    z_scores['Mean'] = means\n    \n    # Create a correlation matrix\n    corr_matrix = z_scores.corr()\n    \n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Matrix Heatmap')\n    \n    return z_scores, ax\n```\n"}
{"task_id": "BigCodeBench/41", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\ndef task_func(data_matrix):\n    # Calculate skewness for each row\n    skewness_values = skew(data_matrix, axis=1)\n    \n    # Create a DataFrame with the results\n    df = pd.DataFrame(skewness_values, columns=['Skewness'])\n    \n    # Plot the distribution of skewness values\n    ax = df['Skewness'].plot(kind='hist', bins=10, title='Distribution of Skewness')\n    ax.set_xlabel('Skewness')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n\ndef task_func(data_matrix):\n    # Calculate skewness for each row\n    skewness_values = skew(data_matrix, axis=1)\n    \n    # Create a DataFrame with the results\n    df = pd.DataFrame(skewness_values, columns=['Skewness'])\n    \n    # Plot the distribution of skewness values\n    ax = df['Skewness'].plot(kind='hist', bins=10, title='Distribution of Skewness')\n    ax.set_xlabel('Skewness')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/42", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data_matrix, n_components=2):\n    pca = PCA(n_components=n_components)\n    components = pca.fit_transform(data_matrix)\n    explained_variance = pca.explained_variance_ratio_\n    cumulative_explained_variance = np.cumsum(explained_variance)\n\n    df = pd.DataFrame(components, columns=[f'Component {i+1}' for i in range(n_components)])\n    df['Mean'] = df.mean(axis=1)\n\n    fig, ax = plt.subplots()\n    ax.plot(range(1, n_components+1), cumulative_explained_variance, marker='o')\n    ax.set_xlabel('Number of Components')\n    ax.set_ylabel('Cumulative Explained Variance')\n    ax.set_title('Cumulative Explained Variance vs Number of Components')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data_matrix, n_components=2):\n    pca = PCA(n_components=n_components)\n    components = pca.fit_transform(data_matrix)\n    explained_variance = pca.explained_variance_ratio_\n    cumulative_explained_variance = np.cumsum(explained_variance)\n\n    df = pd.DataFrame(components, columns=[f'Component {i+1}' for i in range(n_components)])\n    df['Mean'] = df.mean(axis=1)\n\n    fig, ax = plt.subplots()\n    ax.plot(range(1, n_components+1), cumulative_explained_variance, marker='o')\n    ax.set_xlabel('Number of Components')\n    ax.set_ylabel('Cumulative Explained Variance')\n    ax.set_title('Cumulative Explained Variance vs Number of Components')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/43", "solution": "import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Initialize an empty list to store Axes objects\n    plots = []\n\n    # Calculate statistics for numeric columns\n    description = df.describe()\n\n    # Iterate over each numeric column\n    for column in df.select_dtypes(include=[np.number]).columns:\n        # Create a distribution plot for each numeric column\n        fig, ax = plt.subplots()\n        sns.histplot(df[column], bins=10, kde=True, ax=ax)\n        ax.set_title(f'Distribution of {column}')\n        plots.append(ax)\n\n    return description, plots", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Initialize an empty list to store Axes objects\n    plots = []\n\n    # Calculate statistics for numeric columns\n    description = df.describe()\n\n    # Iterate over each numeric column\n    for column in df.select_dtypes(include=[np.number]).columns:\n        # Create a distribution plot for each numeric column\n        fig, ax = plt.subplots()\n        sns.histplot(df[column], bins=10, kde=True, ax=ax)\n        ax.set_title(f'Distribution of {column}')\n        plots.append(ax)\n\n    return description, plots\n\n# Example usage:"}
{"task_id": "BigCodeBench/44", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if df is a pandas DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the scaler to the DataFrame\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Create a box plot for each column\n    ax = df_scaled.plot(kind='box', figsize=(10, 6))\n    plt.title('Box Plot of Scaled Data')\n    plt.xlabel('Columns')\n    plt.ylabel('Scaled Values')\n\n    return df_scaled, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Check if df is a pandas DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the scaler to the DataFrame\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Create a box plot for each column\n    ax = df_scaled.plot(kind='box', figsize=(10, 6))\n    plt.title('Box Plot of Scaled Data')\n    plt.xlabel('Columns')\n    plt.ylabel('Scaled Values')\n\n    return df_scaled, ax\n```\n"}
{"task_id": "BigCodeBench/45", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame):\n    # Fill missing values with the mean of the column\n    df_filled = df.apply(lambda x: x.fillna(x.mean()), axis=0)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df_filled)\n\n    # Create a DataFrame with the principal components\n    principalDf = pd.DataFrame(data=principalComponents, columns=['main ghatak 1', 'main ghatak 2'])\n\n    # Plot the principal components\n    fig, ax = plt.subplots()\n    sns.scatterplot(x='main ghatak 1', y='main ghatak 2', data=principalDf, ax=ax)\n    ax.set_xlabel('main ghatak 1')\n    ax.set_ylabel('main ghatak 2')\n\n    return principalDf, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame):\n    # Fill missing values with the mean of the column\n    df_filled = df.apply(lambda x: x.fillna(x.mean()), axis=0)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df_filled)\n\n    # Create a DataFrame with the principal components\n    principalDf = pd.DataFrame(data=principalComponents, columns=['main ghatak 1', 'main ghatak 2'])\n\n    # Plot the principal components\n    fig, ax = plt.subplots()\n    sns.scatterplot(x='main ghatak 1', y='main ghatak 2', data=principalDf, ax=ax)\n    ax.set_xlabel('main ghatak 1')\n    ax.set_ylabel('main ghatak 2')\n\n    return principalDf, ax\n```\n"}
{"task_id": "BigCodeBench/46", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Fill missing values with column mean\n    df_filled = df.apply(lambda x: x.fillna(x.mean()), axis=0)\n    \n    # Calculate Z-scores\n    df_zscore = df_filled.apply(zscore)\n    \n    # Plot histograms for each numeric column\n    plots = []\n    for column in df_zscore.select_dtypes(include=[np.number]).columns:\n        fig, ax = plt.subplots()\n        ax.hist(df_zscore[column], bins=10, color='blue', alpha=0.7)\n        ax.set_title(f'Histogram of {column}')\n        ax.set_xlabel('Z-score')\n        ax.set_ylabel('Frequency')\n        plots.append(ax)\n    \n    return df_zscore, plots", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Fill missing values with column mean\n    df_filled = df.apply(lambda x: x.fillna(x.mean()), axis=0)\n    \n    # Calculate Z-scores\n    df_zscore = df_filled.apply(zscore)\n    \n    # Plot histograms for each numeric column\n    plots = []\n    for column in df_zscore.select_dtypes(include=[np.number]).columns:\n        fig, ax = plt.subplots()\n        ax.hist(df_zscore[column], bins=10, color='blue', alpha=0.7)\n        ax.set_title(f'Histogram of {column}')\n        ax.set_xlabel('Z-score')\n        ax.set_ylabel('Frequency')\n        plots.append(ax)\n    \n    return df_zscore, plots\n```\n"}
{"task_id": "BigCodeBench/47", "solution": "from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    # Standardizing the DataFrame\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Creating a correlation matrix\n    corr_matrix = df_standardized.corr()\n\n    # Plotting the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df_standardized, ax", "raw_solution": "from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    # Standardizing the DataFrame\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Creating a correlation matrix\n    corr_matrix = df_standardized.corr()\n\n    # Plotting the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df_standardized, ax\n\n# Example usage"}
{"task_id": "BigCodeBench/48", "solution": "import time\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\ndef task_func(n, output_path=None):\n    \"\"\"\n    Generate n UNIX timestamps and format them in UTC DATE_FORMAT.\n    Generate a histogram plot of the timestamps. If an output_path is provided,\n    save the plot to the specified path. Otherwise, display the plot.\n\n    Params:\n    - n (int): The number of timestamps to generate.\n    - output_path (str, optional): Path to save the histogram plot. Defaults to None.\n\n    Returns:\n        list: The list of n formatted timestamps.\n\n    Reqs:\n    - time\n    - datetime\n    - random\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> timestamps = task_func(n=3, output_path=None)\n    - >>> print(timestamps)\n    - ['2013-07-06 20:56:46', '1977-07-29 23:34:23', '1971-09-14 11:29:44']\n    \"\"\"\n    timestamps = []\n    for _ in range(n):\n        unix_timestamp = random.randint(0, int(time.time()))\n        formatted_timestamp = datetime.utcfromtimestamp(unix_timestamp).strftime(DATE_FORMAT)\n        timestamps.append(formatted_timestamp)\n\n    plt.hist(timestamps, bins=10)\n    plt.xlabel('Timestamps')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Timestamps')\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n\n    return timestamps", "raw_solution": "import time\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\n\ndef task_func(n, output_path=None):\n    \"\"\"\n    Generate n UNIX timestamps and format them in UTC DATE_FORMAT.\n    Generate a histogram plot of the timestamps. If an output_path is provided,\n    save the plot to the specified path. Otherwise, display the plot.\n\n    Params:\n    - n (int): The number of timestamps to generate.\n    - output_path (str, optional): Path to save the histogram plot. Defaults to None.\n\n    Returns:\n        list: The list of n formatted timestamps.\n\n    Reqs:\n    - time\n    - datetime\n    - random\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> timestamps = task_func(n=3, output_path=None)\n    - >>> print(timestamps)\n    - ['2013-07-06 20:56:46', '1977-07-29 23:34:23', '1971-09-14 11:29:44']\n    \"\"\"\n    timestamps = []\n    for _ in range(n):\n        unix_timestamp = random.randint(0, int(time.time()))\n        formatted_timestamp = datetime.utcfromtimestamp(unix_timestamp).strftime(DATE_FORMAT)\n        timestamps.append(formatted_timestamp)\n\n    plt.hist(timestamps, bins=10)\n    plt.xlabel('Timestamps')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Timestamps')\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n\n    return timestamps\n```\n"}
{"task_id": "BigCodeBench/49", "solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"input list of timestamps is empty.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetime_objects = [datetime.fromtimestamp(ts) for ts in timestamps]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Timestamp': timestamps,\n        'Datetime': datetime_objects\n    })\n\n    # Plot a histogram\n    ax = df['Datetime'].plot(kind='hist', bins=10, title='Distribution of Datetime Objects')\n    plt.xlabel('Datetime')\n    plt.ylabel('Frequency')\n\n    return df, ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\n\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"input list of timestamps is empty.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetime_objects = [datetime.fromtimestamp(ts) for ts in timestamps]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Timestamp': timestamps,\n        'Datetime': datetime_objects\n    })\n\n    # Plot a histogram\n    ax = df['Datetime'].plot(kind='hist', bins=10, title='Distribution of Datetime Objects')\n    plt.xlabel('Datetime')\n    plt.ylabel('Frequency')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/50", "solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\ndef task_func(timestamp):\n    \"\"\"\n    Ek Unix timestamp ko different timezones mein date object mein badlen, ek Panda DataFrame banaen, aur ek bar chart banaen.\n    - Aapko constant timezones mein batae gae timezones ka upyog karna chahiye.\n    - Date ka format DATE_FORMAT ke anusaar hona chahiye.\n    - DataFrame mein 'timezone' aur 'datetime' column ke naam hone chahiye.\n    - Bar plot ka x-label 'timezone' par set kiya jaana chahiye jabki y-label 'datetime' par set kiya jaana chahiye.\n    - Plot ka title \"datetime = f(timezone)\" hona chahiye\n\n    Params:\n    - timestamp (int): The Unix timestamp.\n\n    Returns:\n        tuple: A tuple containing:\n        DataFrame: different timezone mein datetime waala ek Panda DataFrame.\n        Axes: Generate kiye gae bar chart ke liye ek matplotlib axis object.\n\n    Reqs:\n    - datetime\n    - pandas\n    - pytz\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> df, ax = task_func(1347517370)\n    - >>> print(df)\n    - Timezone            Datetime\n    - 0  America/New_York 2012-09-13 02:22:50\n    - 1     Europe/London 2012-09-13 07:22:50\n    - 2     Asia/Shanghai 2012-09-13 14:22:50\n    - 3        Asia/Tokyo 2012-09-13 15:22:50\n    - 4  Australia/Sydney 2012-09-13 16:22:50\n\n    \"\"\"\n    # Convert the timestamp to a datetime object\n    dt_object = datetime.fromtimestamp(timestamp, pytz.utc)\n\n    # Convert the datetime object to different timezones\n    data = []\n    for tz in TIMEZONES:\n        local_tz = pytz.timezone(tz)\n        local_dt = dt_object.astimezone(local_tz)\n        data.append([tz, local_dt.strftime(DATE_FORMAT)])\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=[\"timezone\", \"datetime\"])\n\n    # Create a bar chart\n    ax = df.plot(kind=\"bar\", x=\"timezone\", y=\"datetime\", legend=False)\n    ax.set_xlabel(\"Timezone\")\n    ax.set_ylabel(\"Datetime\")\n    ax.set_title(\"Datetime = f(Timezone)\")\n\n    return df, ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\n\n\ndef task_func(timestamp):\n    \"\"\"\n    Ek Unix timestamp ko different timezones mein date object mein badlen, ek Panda DataFrame banaen, aur ek bar chart banaen.\n    - Aapko constant timezones mein batae gae timezones ka upyog karna chahiye.\n    - Date ka format DATE_FORMAT ke anusaar hona chahiye.\n    - DataFrame mein 'timezone' aur 'datetime' column ke naam hone chahiye.\n    - Bar plot ka x-label 'timezone' par set kiya jaana chahiye jabki y-label 'datetime' par set kiya jaana chahiye.\n    - Plot ka title \"datetime = f(timezone)\" hona chahiye\n\n    Params:\n    - timestamp (int): The Unix timestamp.\n\n    Returns:\n        tuple: A tuple containing:\n        DataFrame: different timezone mein datetime waala ek Panda DataFrame.\n        Axes: Generate kiye gae bar chart ke liye ek matplotlib axis object.\n\n    Reqs:\n    - datetime\n    - pandas\n    - pytz\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> df, ax = task_func(1347517370)\n    - >>> print(df)\n    - Timezone            Datetime\n    - 0  America/New_York 2012-09-13 02:22:50\n    - 1     Europe/London 2012-09-13 07:22:50\n    - 2     Asia/Shanghai 2012-09-13 14:22:50\n    - 3        Asia/Tokyo 2012-09-13 15:22:50\n    - 4  Australia/Sydney 2012-09-13 16:22:50\n\n    \"\"\"\n    # Convert the timestamp to a datetime object\n    dt_object = datetime.fromtimestamp(timestamp, pytz.utc)\n\n    # Convert the datetime object to different timezones\n    data = []\n    for tz in TIMEZONES:\n        local_tz = pytz.timezone(tz)\n        local_dt = dt_object.astimezone(local_tz)\n        data.append([tz, local_dt.strftime(DATE_FORMAT)])\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=[\"timezone\", \"datetime\"])\n\n    # Create a bar chart\n    ax = df.plot(kind=\"bar\", x=\"timezone\", y=\"datetime\", legend=False)\n    ax.set_xlabel(\"Timezone\")\n    ax.set_ylabel(\"Datetime\")\n    ax.set_title(\"Datetime = f(Timezone)\")\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/51", "solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, age: int, height: int):\n    # Filter the dataframe based on age and height\n    filtered_df = df[(df['Age'] <= age) & (df['Height'] <= height)]\n    \n    # Check if the filtered dataframe has at least 3 columns\n    if len(filtered_df.columns) < 3:\n        filtered_df['Cluster'] = 0\n        ax = None\n    else:\n        # Perform KMeans clustering\n        kmeans = KMeans(n_clusters=3, random_state=0)\n        filtered_df['Cluster'] = kmeans.fit_predict(filtered_df[['Age', 'Height']])\n        \n        # Plot the clusters\n        ax = plt.scatter(filtered_df['Age'], filtered_df['Height'], c=filtered_df['Cluster'], cmap='viridis')\n        plt.xlabel('Age')\n        plt.ylabel('Height')\n        plt.title('KMeans Clustering based on Age and Height')\n    \n    return filtered_df, ax", "raw_solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, age: int, height: int):\n    # Filter the dataframe based on age and height\n    filtered_df = df[(df['Age'] <= age) & (df['Height'] <= height)]\n    \n    # Check if the filtered dataframe has at least 3 columns\n    if len(filtered_df.columns) < 3:\n        filtered_df['Cluster'] = 0\n        ax = None\n    else:\n        # Perform KMeans clustering\n        kmeans = KMeans(n_clusters=3, random_state=0)\n        filtered_df['Cluster'] = kmeans.fit_predict(filtered_df[['Age', 'Height']])\n        \n        # Plot the clusters\n        ax = plt.scatter(filtered_df['Age'], filtered_df['Height'], c=filtered_df['Cluster'], cmap='viridis')\n        plt.xlabel('Age')\n        plt.ylabel('Height')\n        plt.title('KMeans Clustering based on Age and Height')\n    \n    return filtered_df, ax\n\n# Example usage"}
{"task_id": "BigCodeBench/52", "solution": "import pandas as pd\nimport regex as re\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    cleaned_text = re.sub(r'[^\\w\\s]', '', text).lower()\n    \n    # Split the text into words\n    words = cleaned_text.split()\n    \n    # Filter out stopwords\n    filtered_words = [word for word in words if word not in STOPWORDS]\n    \n    # Create a pandas Series with word frequencies\n    word_counts = pd.Series(filtered_words).value_counts()\n    \n    return word_counts\ntext = \"This is a sample text. This text contains sample words.\"", "raw_solution": "import pandas as pd\nimport regex as re\n\n# Constants\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\n\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    cleaned_text = re.sub(r'[^\\w\\s]', '', text).lower()\n    \n    # Split the text into words\n    words = cleaned_text.split()\n    \n    # Filter out stopwords\n    filtered_words = [word for word in words if word not in STOPWORDS]\n    \n    # Create a pandas Series with word frequencies\n    word_counts = pd.Series(filtered_words).value_counts()\n    \n    return word_counts\n\n# Example usage\ntext = \"This is a sample text. This text contains sample words.\"\nword_counts = task_func(text)"}
{"task_id": "BigCodeBench/53", "solution": "import pandas as pd\nimport regex as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\ndef task_func(text):\n    pattern = r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)($|\\n)\"\n    matches = re.findall(pattern, text)\n    data = [match for match in matches if len(match) == 4]\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    \n    # Convert 'Age' column to integer\n    df['Age'] = df['Age'].astype(int)\n    \n    # Plot age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df['Age'], bins=10, kde=True)\n    plt.title('Age Distribution')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return df\ntext = 'Name: John Doe, Email: john.doe@example.com, Age: 30, Country: USA\\nName: Jane Doe, Email: jane.doe@example.com, Age: 25, Country: UK'", "raw_solution": "import pandas as pd\nimport regex as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\n\ndef task_func(text):\n    pattern = r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)($|\\n)\"\n    matches = re.findall(pattern, text)\n    data = [match for match in matches if len(match) == 4]\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    \n    # Convert 'Age' column to integer\n    df['Age'] = df['Age'].astype(int)\n    \n    # Plot age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df['Age'], bins=10, kde=True)\n    plt.title('Age Distribution')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return df\n\n# Example usage:\ntext = 'Name: John Doe, Email: john.doe@example.com, Age: 30, Country: USA\\nName: Jane Doe, Email: jane.doe@example.com, Age: 25, Country: UK'\ndf = task_func(text)"}
{"task_id": "BigCodeBench/54", "solution": "import pandas as pd\nimport regex as re\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(text):\n    # Split the text into sentences using regex\n    sentences = re.split(r'(?<=[.!?])\\s*', text)\n    \n    # Filter out empty sentences\n    sentences = [sentence for sentence in sentences if sentence.strip()]\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences to create a document-term matrix\n    dtm = vectorizer.fit_transform(sentences)\n\n    # Convert the document-term matrix to a pandas DataFrame\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\ntext = \"This is a sample sentence. This sentence contains sample words.\"", "raw_solution": "import pandas as pd\nimport regex as re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef task_func(text):\n    # Split the text into sentences using regex\n    sentences = re.split(r'(?<=[.!?])\\s*', text)\n    \n    # Filter out empty sentences\n    sentences = [sentence for sentence in sentences if sentence.strip()]\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences to create a document-term matrix\n    dtm = vectorizer.fit_transform(sentences)\n\n    # Convert the document-term matrix to a pandas DataFrame\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\n\n# Example usage:\ntext = \"This is a sample sentence. This sentence contains sample words.\"\ndtm = task_func(text)"}
{"task_id": "BigCodeBench/55", "solution": "import re\nimport pandas as pd\nSTOPWORDS = [\"Those\", \"are\", \"the\", \"words\", \"to\", \"ignore\"]\ndef task_func(text):\n    sentences = re.split(r'[.!?]', text)\n    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n\n    sentence_data = []\n    for i, sentence in enumerate(sentences, start=1):\n        words = re.findall(r'\\b\\w+\\b', sentence)\n        filtered_words = [word for word in words if word.upper() not in STOPWORDS]\n        sentence_data.append((f\"Sentence {i}\", len(filtered_words)))\n\n    df = pd.DataFrame(sentence_data, columns=[\"Sentence\", \"Word Count\"])\n    return df\ntext = \"This is a sample sentence. This sentence contains sample words.\"", "raw_solution": "import re\nimport pandas as pd\n\nSTOPWORDS = [\"Those\", \"are\", \"the\", \"words\", \"to\", \"ignore\"]\n\ndef task_func(text):\n    sentences = re.split(r'[.!?]', text)\n    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n\n    sentence_data = []\n    for i, sentence in enumerate(sentences, start=1):\n        words = re.findall(r'\\b\\w+\\b', sentence)\n        filtered_words = [word for word in words if word.upper() not in STOPWORDS]\n        sentence_data.append((f\"Sentence {i}\", len(filtered_words)))\n\n    df = pd.DataFrame(sentence_data, columns=[\"Sentence\", \"Word Count\"])\n    return df\n\n# Example usage:\ntext = \"This is a sample sentence. This sentence contains sample words.\"\ndf = task_func(text)"}
{"task_id": "BigCodeBench/56", "solution": "import pandas as pd\nimport regex as re\ndef task_func(text):\n    # Define a regex pattern to match the required format\n    pattern = r'Score: (\\d+), Category: (\\w+)'\n    \n    # Find all matches in the text\n    matches = re.findall(pattern, text)\n    \n    # Create a list of dictionaries from the matches\n    data = [{'Score': int(score), 'Category': category} for score, category in matches]\n    \n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n    \n    return df\ntext = \"Score: 85, Category: Math\\nScore: 90, Category: Science\\nScore: 80, Category: Math\"", "raw_solution": "import pandas as pd\nimport regex as re\n\ndef task_func(text):\n    # Define a regex pattern to match the required format\n    pattern = r'Score: (\\d+), Category: (\\w+)'\n    \n    # Find all matches in the text\n    matches = re.findall(pattern, text)\n    \n    # Create a list of dictionaries from the matches\n    data = [{'Score': int(score), 'Category': category} for score, category in matches]\n    \n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n    \n    return df\n\n# Example usage\ntext = \"Score: 85, Category: Math\\nScore: 90, Category: Science\\nScore: 80, Category: Math\"\ndf = task_func(text)"}
{"task_id": "BigCodeBench/57", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(csv_file_path: str, title: str):\n    # Load the data from the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Calculate the correlation matrix\n    corr_matrix = df.corr()\n\n    # Round each correlation to 2 decimal places\n    corr_matrix = corr_matrix.round(2)\n\n    # Create a heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar_kws={\"shrink\": 0.8})\n    plt.title(title)\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=45)\n    plt.tight_layout()\n\n    return corr_matrix, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(csv_file_path: str, title: str):\n    # Load the data from the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Calculate the correlation matrix\n    corr_matrix = df.corr()\n\n    # Round each correlation to 2 decimal places\n    corr_matrix = corr_matrix.round(2)\n\n    # Create a heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar_kws={\"shrink\": 0.8})\n    plt.title(title)\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=45)\n    plt.tight_layout()\n\n    return corr_matrix, ax\n```\n"}
{"task_id": "BigCodeBench/58", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, num_samples):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create a histogram of the samples\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(\"Normal Distribution\")\n\n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, num_samples):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create a histogram of the samples\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(\"Normal Distribution\")\n\n    return fig\n```\n"}
{"task_id": "BigCodeBench/59", "solution": "import wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(page_title):\n    try:\n        # Fetch the content of the Wikipedia page\n        page_content = wikipedia.page(page_title).content\n\n        # Generate a word cloud\n        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(page_content)\n\n        # Plot the word cloud\n        fig, ax = plt.subplots(figsize=(10, 5))\n        ax.imshow(wordcloud, interpolation='bilinear')\n        ax.axis('off')  # Turn off the axis grid and labels\n\n        return ax\n\n    except wikipedia.exceptions.PageError:\n        # Return None if the page does not exist\n        return None", "raw_solution": "import wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(page_title):\n    try:\n        # Fetch the content of the Wikipedia page\n        page_content = wikipedia.page(page_title).content\n\n        # Generate a word cloud\n        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(page_content)\n\n        # Plot the word cloud\n        fig, ax = plt.subplots(figsize=(10, 5))\n        ax.imshow(wordcloud, interpolation='bilinear')\n        ax.axis('off')  # Turn off the axis grid and labels\n\n        return ax\n\n    except wikipedia.exceptions.PageError:\n        # Return None if the page does not exist\n        return None\n```\n"}
{"task_id": "BigCodeBench/60", "solution": "import json\nimport pandas as pd\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    \"\"\"\n    Save the list of dictionaries given in the 'result' parameter to a CSV file (without index) and a JSON file.\n\n    Params:\n    - result (list): A list of dictionaries.\n    - csv_file_path (str): A path to a CSV file.\n    - json_file_path (str): A path to a JSON file.\n\n    Returns:\n        None\n\n    Reqs:\n    - pandas\n    - json\n\n    Examples:\n    - >>> result = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {1: 2, 3: 4, 5: 6}]\n    - >>> task_func(result, 'test.csv', 'test.json')\n\n    \"\"\"\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(result)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n\n    # Save the list of dictionaries to a JSON file\n    with open(json_file_path, 'w') as f:\n        json.dump(result, f, indent=4)\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {1: 2, 3: 4, 5: 6}]", "raw_solution": "import json\nimport pandas as pd\n\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    \"\"\"\n    Save the list of dictionaries given in the 'result' parameter to a CSV file (without index) and a JSON file.\n\n    Params:\n    - result (list): A list of dictionaries.\n    - csv_file_path (str): A path to a CSV file.\n    - json_file_path (str): A path to a JSON file.\n\n    Returns:\n        None\n\n    Reqs:\n    - pandas\n    - json\n\n    Examples:\n    - >>> result = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {1: 2, 3: 4, 5: 6}]\n    - >>> task_func(result, 'test.csv', 'test.json')\n\n    \"\"\"\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(result)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n\n    # Save the list of dictionaries to a JSON file\n    with open(json_file_path, 'w') as f:\n        json.dump(result, f, indent=4)\n\n# Example usage:\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {1: 2, 3: 4, 5: 6}]\ntask_func(result, 'test.csv', 'test.json')\n```\n"}
{"task_id": "BigCodeBench/61", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(result):\n    # Extracting 'from_user' values\n    from_user_values = [d['from_user'] for d in result]\n    \n    # Calculate square roots\n    square_roots = np.sqrt(from_user_values)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(from_user_values, square_roots, marker='o')\n    ax.set_title(PLOT_TITLE)\n    ax.set_xlabel(X_LABEL)\n    ax.set_ylabel(Y_LABEL)\n    \n    # Annotate with current time\n    current_time = datetime.now().strftime(TIME_FORMAT)\n    ax.annotate(current_time, xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10, color='gray')\n    \n    return square_roots, ax\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 16}, {\"some_key\": 2, \"another_key\": 4, \"from_user\": 9}]", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Constants\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(result):\n    # Extracting 'from_user' values\n    from_user_values = [d['from_user'] for d in result]\n    \n    # Calculate square roots\n    square_roots = np.sqrt(from_user_values)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(from_user_values, square_roots, marker='o')\n    ax.set_title(PLOT_TITLE)\n    ax.set_xlabel(X_LABEL)\n    ax.set_ylabel(Y_LABEL)\n    \n    # Annotate with current time\n    current_time = datetime.now().strftime(TIME_FORMAT)\n    ax.annotate(current_time, xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10, color='gray')\n    \n    return square_roots, ax\n\n# Example usage\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 16}, {\"some_key\": 2, \"another_key\": 4, \"from_user\": 9}]\nsquare_roots, ax = task_func(result)\nplt.show()\n```\nThis script will create a plot of the square roots of the 'from_user' values from the input list of dictionaries. It also annotates the plot with the current date and time.\n"}
{"task_id": "BigCodeBench/62", "solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    \"\"\"\n    Diye gaye result mein \"from_user\" maano ka ek histogram banata hai. Histogram bar ka rang diye gaye colors ki list se random dhang se chuna jaata hai.\n\n    Params:\n    - result (list): Dictionary ki ek list jismein \"from_user\" key shaamil hai.\n    - colors (list, optional): Histogram bar ke liye optional colors ki ek list. Defaults ['b', 'g', 'r', 'c', 'm', 'y', 'k'] hai.\n\n    Returns:\n        None: Function histogram pradarshit karta hai aur none maan vaapas nahi karta hai.\n\n    Reqs:\n    - random\n    - matplotlib\n    - seaborn\n\n    Examples:\n    - >>> result = [{\"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]\n    - >>> task_func(result)\n\n    \"\"\"\n    # Extracting the 'from_user' values from the result\n    from_user_values = [d['from_user'] for d in result]\n\n    # Randomly selecting a color from the colors list\n    selected_color = random.choice(colors)\n\n    # Plotting the histogram\n    sns.histplot(from_user_values, color=selected_color, bins=10, kde=True)\n    plt.title('Histogram of \"from_user\" values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()", "raw_solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    \"\"\"\n    Diye gaye result mein \"from_user\" maano ka ek histogram banata hai. Histogram bar ka rang diye gaye colors ki list se random dhang se chuna jaata hai.\n\n    Params:\n    - result (list): Dictionary ki ek list jismein \"from_user\" key shaamil hai.\n    - colors (list, optional): Histogram bar ke liye optional colors ki ek list. Defaults ['b', 'g', 'r', 'c', 'm', 'y', 'k'] hai.\n\n    Returns:\n        None: Function histogram pradarshit karta hai aur none maan vaapas nahi karta hai.\n\n    Reqs:\n    - random\n    - matplotlib\n    - seaborn\n\n    Examples:\n    - >>> result = [{\"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]\n    - >>> task_func(result)\n\n    \"\"\"\n    # Extracting the 'from_user' values from the result\n    from_user_values = [d['from_user'] for d in result]\n\n    # Randomly selecting a color from the colors list\n    selected_color = random.choice(colors)\n\n    # Plotting the histogram\n    sns.histplot(from_user_values, color=selected_color, bins=10, kde=True)\n    plt.title('Histogram of \"from_user\" values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n"}
{"task_id": "BigCodeBench/63", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(car_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n    \n    # Plot the distribution of vehicle colors\n    ax = df['Color'].value_counts().plot(kind='bar', color='skyblue')\n    ax.set_title('Vahan rango ka distribution')\n    ax.set_xlabel('Color')\n    ax.set_ylabel('Count')\n    \n    return df, ax\ncar_dict = {'Ford': 'Red', 'Toyota': 'Blue', 'Mercedes': 'Black', 'Tesla': 'White', 'BMW': 'Silver'}", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(car_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n    \n    # Plot the distribution of vehicle colors\n    ax = df['Color'].value_counts().plot(kind='bar', color='skyblue')\n    ax.set_title('Vahan rango ka distribution')\n    ax.set_xlabel('Color')\n    ax.set_ylabel('Count')\n    \n    return df, ax\n\n# Example usage\ncar_dict = {'Ford': 'Red', 'Toyota': 'Blue', 'Mercedes': 'Black', 'Tesla': 'White', 'BMW': 'Silver'}\ndf, ax = task_func(car_dict)\nplt.show()\n```\nThis script first creates a pandas DataFrame from the input dictionary, then uses matplotlib to create a bar chart of the color distribution. The function returns the DataFrame and the Axes object for further manipulation or testing.\n"}
{"task_id": "BigCodeBench/64", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Create a pivot table to count occurrences of each combination\n    pivot_df = df.pivot_table(index='col1', columns='col2', aggfunc='size', fill_value=0)\n    \n    # Plot the pivot table\n    plt.figure(figsize=(10, 6))\n    ax = sns.heatmap(pivot_df, annot=True, fmt='d', cmap='Blues')\n    ax.set_title('Distribution of col3 based on col1 and col2')\n    ax.set_xlabel('col2')\n    ax.set_ylabel('col1')\n    \n    return df, ax\ndata = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Create a pivot table to count occurrences of each combination\n    pivot_df = df.pivot_table(index='col1', columns='col2', aggfunc='size', fill_value=0)\n    \n    # Plot the pivot table\n    plt.figure(figsize=(10, 6))\n    ax = sns.heatmap(pivot_df, annot=True, fmt='d', cmap='Blues')\n    ax.set_title('Distribution of col3 based on col1 and col2')\n    ax.set_xlabel('col2')\n    ax.set_ylabel('col1')\n    \n    return df, ax\n\n# Example usage\ndata = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\ndf, ax = task_func(data)\nplt.show()\n```\nThis script first creates a pandas DataFrame from the input data, then creates a pivot table to count occurrences of each combination of 'col1' and 'col2'. It then plots this pivot table using seaborn to visualize the distribution of 'col3' based on 'col1' and 'col2'. The function returns the DataFrame and the Axes object for the plot.\n"}
{"task_id": "BigCodeBench/65", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    last_col = COLUMNS[-1]\n    unique_values = df[last_col].unique()\n    \n    grouped_df = df.groupby(last_col).size().reset_index(name='count')\n    \n    ax = grouped_df.plot(x=last_col, y='count', kind='line', marker='o')\n    ax.set_xlabel('-'.join(COLUMNS[:-1]))\n    ax.set_ylabel(last_col)\n    \n    return grouped_df, ax\ndata = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    last_col = COLUMNS[-1]\n    unique_values = df[last_col].unique()\n    \n    grouped_df = df.groupby(last_col).size().reset_index(name='count')\n    \n    ax = grouped_df.plot(x=last_col, y='count', kind='line', marker='o')\n    ax.set_xlabel('-'.join(COLUMNS[:-1]))\n    ax.set_ylabel(last_col)\n    \n    return grouped_df, ax\n\n# Example usage:\ndata = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\nanalyzed_df, ax = task_func(data)"}
{"task_id": "BigCodeBench/66", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Create a DataFrame from the given data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Create a new DataFrame by grouping by 'col1' and 'col2' and counting occurrences of 'col3'\n    new_df = df.groupby(['col1', 'col2'])['col3'].count().reset_index(name='count')\n    \n    # Create a seaborn plot of the distribution of 'col3' based on 'col1' and 'col2'\n    plt.figure(figsize=(10, 6))\n    ax = sns.countplot(data=df, x='col3', hue='col1', palette='Set2')\n    ax.set_xlabel('col3')\n    \n    return new_df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    # Create a DataFrame from the given data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Create a new DataFrame by grouping by 'col1' and 'col2' and counting occurrences of 'col3'\n    new_df = df.groupby(['col1', 'col2'])['col3'].count().reset_index(name='count')\n    \n    # Create a seaborn plot of the distribution of 'col3' based on 'col1' and 'col2'\n    plt.figure(figsize=(10, 6))\n    ax = sns.countplot(data=df, x='col3', hue='col1', palette='Set2')\n    ax.set_xlabel('col3')\n    \n    return new_df, ax\n```\n"}
{"task_id": "BigCodeBench/67", "solution": "import pandas as pd\nimport re\nimport os\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    \"\"\"\n    This function lists all files in a given directory that start with a specified pattern,\n    along with their sizes. It returns a pandas DataFrame containing the file names and sizes.\n\n    Params:\n    - dir_path (str): The path to the directory.\n    - pattern (str): The pattern to match file names. Default is '^EMP' (files starting with 'EMP').\n\n    Returns:\n        pandas.DataFrame: A DataFrame with columns 'File' and 'Size'.\n\n    Reqs:\n    - pandas\n    - re\n    - os\n\n    Examples:\n    - >>> report = task_func('/path/to/directory')\n    - >>> print(report)\n    \"\"\"\n    # List to store file names and sizes\n    files_data = []\n\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Iterate over all files in the directory\n    for file_name in os.listdir(dir_path):\n        if regex.match(file_name):\n            file_path = os.path.join(dir_path, file_name)\n            if os.path.isfile(file_path):\n                size = os.path.getsize(file_path)\n                files_data.append({'File': file_name, 'Size': size})\n\n    # Create a DataFrame from the collected data\n    df = pd.DataFrame(files_data, columns=['File', 'Size'])\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport os\n\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    \"\"\"\n    This function lists all files in a given directory that start with a specified pattern,\n    along with their sizes. It returns a pandas DataFrame containing the file names and sizes.\n\n    Params:\n    - dir_path (str): The path to the directory.\n    - pattern (str): The pattern to match file names. Default is '^EMP' (files starting with 'EMP').\n\n    Returns:\n        pandas.DataFrame: A DataFrame with columns 'File' and 'Size'.\n\n    Reqs:\n    - pandas\n    - re\n    - os\n\n    Examples:\n    - >>> report = task_func('/path/to/directory')\n    - >>> print(report)\n    \"\"\"\n    # List to store file names and sizes\n    files_data = []\n\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Iterate over all files in the directory\n    for file_name in os.listdir(dir_path):\n        if regex.match(file_name):\n            file_path = os.path.join(dir_path, file_name)\n            if os.path.isfile(file_path):\n                size = os.path.getsize(file_path)\n                files_data.append({'File': file_name, 'Size': size})\n\n    # Create a DataFrame from the collected data\n    df = pd.DataFrame(files_data, columns=['File', 'Size'])\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/68", "solution": "import pandas as pd\nimport seaborn as sns\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load the data into a DataFrame\n    df = pd.read_csv(data)\n\n    # Filter the DataFrame to include only rows where 'employee ID' starts with the specified prefix\n    filtered_df = df[df['employee ID'].str.startswith(emp_prefix)]\n\n    # Create a histogram of the 'age' column\n    ax = sns.histplot(filtered_df['age'], kde=True)\n\n    return filtered_df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\n\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load the data into a DataFrame\n    df = pd.read_csv(data)\n\n    # Filter the DataFrame to include only rows where 'employee ID' starts with the specified prefix\n    filtered_df = df[df['employee ID'].str.startswith(emp_prefix)]\n\n    # Create a histogram of the 'age' column\n    ax = sns.histplot(filtered_df['age'], kde=True)\n\n    return filtered_df, ax\n```\n"}
{"task_id": "BigCodeBench/69", "solution": "import random\nimport matplotlib.pyplot as plt\nSALARY_RANGE = (20000, 100000)\ndef task_func(dict1):\n    # Extract the number of employees for the 'EMPXX' department\n    num_employees = dict1.get('EMPXX', 0)\n    \n    # Generate random salaries for the employees\n    salaries = [random.randint(SALARY_RANGE[0], SALARY_RANGE[1]) for _ in range(num_employees)]\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(salaries, bins=10, color='skyblue', edgecolor='black')\n    ax.set_title('salary distribution in EMPXX department')\n    ax.set_xlabel('salary')\n    ax.set_ylabel('employees number')\n    \n    return ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\n# Constants\nSALARY_RANGE = (20000, 100000)\n\ndef task_func(dict1):\n    # Extract the number of employees for the 'EMPXX' department\n    num_employees = dict1.get('EMPXX', 0)\n    \n    # Generate random salaries for the employees\n    salaries = [random.randint(SALARY_RANGE[0], SALARY_RANGE[1]) for _ in range(num_employees)]\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(salaries, bins=10, color='skyblue', edgecolor='black')\n    ax.set_title('salary distribution in EMPXX department')\n    ax.set_xlabel('salary')\n    ax.set_ylabel('employees number')\n    \n    return ax\n\n# Example usage"}
{"task_id": "BigCodeBench/70", "solution": "import pandas as pd\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nCOLUMNS = ['email', 'list', 'sum', 'mean']\ndef task_func(json_file):\n    # Load data from json file\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Calculate sum and mean\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n\n    # Plotting\n    ax = None\n    if not df.empty:\n        ax = df.plot(x='email', y='mean', kind='bar', title='Mean Values')\n        plt.xlabel('Email')\n        plt.ylabel('Mean Value')\n        plt.show()\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['email', 'list', 'sum', 'mean']\n\ndef task_func(json_file):\n    # Load data from json file\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Calculate sum and mean\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n\n    # Plotting\n    ax = None\n    if not df.empty:\n        ax = df.plot(x='email', y='mean', kind='bar', title='Mean Values')\n        plt.xlabel('Email')\n        plt.ylabel('Mean Value')\n        plt.show()\n\n    return df, ax\n\n# Example usage:\n# df, ax = task_func('data/task_func/json_1.json')\n# print(df)\n```\nThis script reads a JSON file, converts it into a Pandas DataFrame, calculates the sum and mean of the lists in the 'list' column, and plots the mean values. If the DataFrame is empty, it returns None for the plot.\n"}
{"task_id": "BigCodeBench/71", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\ndef task_func(csv_file):\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(csv_file)\n\n    # Check if the required columns exist in the DataFrame\n    if not {'email', 'list'}.issubset(df.columns):\n        raise ValueError(\"CSV file must contain 'email' and 'list' columns.\")\n\n    # Calculate sum, mean, and standard deviation for each email\n    df['sum'] = df.groupby('email')['list'].transform(np.sum)\n    df['mean'] = df.groupby('email')['list'].transform(np.mean)\n    df['std'] = df.groupby('email')['list'].transform(np.std)\n\n    # Create a histogram for the mean values\n    plot = sns.histplot(df['mean'], bins=10, kde=True)\n\n    return df, plot", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\n\ndef task_func(csv_file):\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(csv_file)\n\n    # Check if the required columns exist in the DataFrame\n    if not {'email', 'list'}.issubset(df.columns):\n        raise ValueError(\"CSV file must contain 'email' and 'list' columns.\")\n\n    # Calculate sum, mean, and standard deviation for each email\n    df['sum'] = df.groupby('email')['list'].transform(np.sum)\n    df['mean'] = df.groupby('email')['list'].transform(np.mean)\n    df['std'] = df.groupby('email')['list'].transform(np.std)\n\n    # Create a histogram for the mean values\n    plot = sns.histplot(df['mean'], bins=10, kde=True)\n\n    return df, plot\n```\n"}
{"task_id": "BigCodeBench/72", "solution": "import pandas as pd\nimport os\nimport numpy as np\nimport ast\nimport matplotlib.pyplot as plt\ndef task_func(directory):\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # Get list of all CSV files in the directory\n    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n\n    if not csv_files:\n        return df, None\n\n    # Find the longest filename\n    longest_filename = max(csv_files, key=len)\n\n    # Load the longest filename CSV file into a DataFrame\n    df = pd.read_csv(os.path.join(directory, longest_filename))\n\n    # Check if the required columns exist\n    if 'email' not in df.columns or 'list' not in df.columns:\n        return df, None\n\n    # Convert 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and median for each list in the 'list' column\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['median'] = df['list'].apply(np.median)\n\n    # Plot the histogram of the 'median' column\n    ax = df['median'].plot(kind='hist', bins=10, title='Median Histogram')\n    plt.xlabel('Median Value')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport os\nimport numpy as np\nimport ast\nimport matplotlib.pyplot as plt\n\ndef task_func(directory):\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # Get list of all CSV files in the directory\n    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n\n    if not csv_files:\n        return df, None\n\n    # Find the longest filename\n    longest_filename = max(csv_files, key=len)\n\n    # Load the longest filename CSV file into a DataFrame\n    df = pd.read_csv(os.path.join(directory, longest_filename))\n\n    # Check if the required columns exist\n    if 'email' not in df.columns or 'list' not in df.columns:\n        return df, None\n\n    # Convert 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and median for each list in the 'list' column\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['median'] = df['list'].apply(np.median)\n\n    # Plot the histogram of the 'median' column\n    ax = df['median'].plot(kind='hist', bins=10, title='Median Histogram')\n    plt.xlabel('Median Value')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/73", "solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT email, list FROM emaildata\"\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    # Convert string representation of list to actual list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and variance for each list\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    # Plotting the results\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x='email', y=['sum', 'mean', 'var'], ax=ax)\n    ax.set_title('Sum, Mean, and Variance of Lists')\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Value')\n    ax.legend(title='Metric')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\n\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT email, list FROM emaildata\"\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    # Convert string representation of list to actual list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and variance for each list\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    # Plotting the results\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x='email', y=['sum', 'mean', 'var'], ax=ax)\n    ax.set_title('Sum, Mean, and Variance of Lists')\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Value')\n    ax.legend(title='Metric')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/74", "solution": "import socket\nimport requests\ndef task_func(host):\n    if not host or not isinstance(host, str):\n        raise ValueError(\"Host must be a non-empty string.\")\n\n    try:\n        # Get the IP address of the host\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        raise ConnectionError(f\"Could not resolve the host: {host}\")\n\n    try:\n        # Get geolocation information using ipinfo.io API\n        response = requests.get(f\"http://ipinfo.io/{ip_address}\")\n        response.raise_for_status()\n        geolocation_info = response.json()\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Could not connect to geolocation service: {e}\")\n\n    return {\n        \"ip_address\": ip_address,\n        \"geolocation\": geolocation_info\n    }", "raw_solution": "import socket\nimport requests\n\ndef task_func(host):\n    if not host or not isinstance(host, str):\n        raise ValueError(\"Host must be a non-empty string.\")\n\n    try:\n        # Get the IP address of the host\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        raise ConnectionError(f\"Could not resolve the host: {host}\")\n\n    try:\n        # Get geolocation information using ipinfo.io API\n        response = requests.get(f\"http://ipinfo.io/{ip_address}\")\n        response.raise_for_status()\n        geolocation_info = response.json()\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Could not connect to geolocation service: {e}\")\n\n    return {\n        \"ip_address\": ip_address,\n        \"geolocation\": geolocation_info\n    }\n\n# Example usage:\n# result = task_func('google.com')\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/75", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be a pandas DataFrame.\")\n    \n    if df.empty or sales_lower_bound >= sales_upper_bound:\n        raise ValueError(\"The DataFrame must be non-empty and sales_lower_bound must be less than sales_upper_bound.\")\n    \n    if fruits is None:\n        fruits = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n    \n    if days is None:\n        start_date = datetime(2024, 1, 1)\n        end_date = datetime(2024, 1, 7)\n        days = [start_date + timedelta(days=i) for i in range((end_date - start_date).days)]\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    data = []\n    for fruit, day in itertools.product(fruits, days):\n        sales = np.random.randint(sales_lower_bound, sales_upper_bound)\n        data.append({'Fruit': fruit, 'Day': day, 'Sales': sales})\n    \n    df = pd.DataFrame(data)\n    \n    plot = sns.boxplot(x='Fruit', y='Sales', data=df)\n    \n    return df, plot", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\n\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be a pandas DataFrame.\")\n    \n    if df.empty or sales_lower_bound >= sales_upper_bound:\n        raise ValueError(\"The DataFrame must be non-empty and sales_lower_bound must be less than sales_upper_bound.\")\n    \n    if fruits is None:\n        fruits = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n    \n    if days is None:\n        start_date = datetime(2024, 1, 1)\n        end_date = datetime(2024, 1, 7)\n        days = [start_date + timedelta(days=i) for i in range((end_date - start_date).days)]\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    data = []\n    for fruit, day in itertools.product(fruits, days):\n        sales = np.random.randint(sales_lower_bound, sales_upper_bound)\n        data.append({'Fruit': fruit, 'Day': day, 'Sales': sales})\n    \n    df = pd.DataFrame(data)\n    \n    plot = sns.boxplot(x='Fruit', y='Sales', data=df)\n    \n    return df, plot\n```\n"}
{"task_id": "BigCodeBench/76", "solution": "import random\nimport string\nfrom django.http import HttpResponse\ndef task_func(request, session_expire_time):\n    \"\"\"\n    This function generates a random age key of specific length (20) containing letters and digits,\n    and sets this key in a httpresponse object as a cookie with the provided session expire time.\n\n    Notes:\n        If the age key is valid, the function returns a response content stating \"Age key successfully set.\"\n\n    Params:\n    - request (django.http.HttpRequest): The Django HttpRequest object.\n    - session_expire_time (int): The time in seconds for the cookie to expire.\n\n    Returns:\n        django.http.HttpResponse: A Django httpresponse object with the age key set in a cookie.\n\n    Reqs:\n    - django.http\n    - django.conf\n    - random\n    - string\n\n    Raises:\n    - ValueError: If the age key does not contain only letters and digits, or if the age key length is not 20.\n\n    Examples:\n    - Setup:\n    - >>> from django.conf import settings\n    - >>> from django.http import HttpRequest\n    - >>> if not settings.configured:\n    - ...     settings.configure()\n    - >>> request = HttpRequest()\n    - >>> response = task_func(request, 60)\n    - >>> 'session_key' in response.cookies\n    - True\n    - >>> len(response.cookies['session_key'].value) == 20\n    - True\n    - >>> response.cookies['session_key']['max-age'] == 60\n    - True\n    \"\"\"\n\n    # Generate a random age key of length 20\n    age_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    # Validate the age key\n    if not (age_key.isalnum() and len(age_key) == 20):\n        raise ValueError(\"Age key must be alphanumeric and of length 20.\")\n\n    # Set the age key in a cookie\n    response = HttpResponse(\"Age key successfully set.\")\n    response.set_cookie('session_key', age_key, max_age=session_expire_time)\n\n    return response", "raw_solution": "import random\nimport string\nfrom django.http import HttpResponse\n\n\ndef task_func(request, session_expire_time):\n    \"\"\"\n    This function generates a random age key of specific length (20) containing letters and digits,\n    and sets this key in a httpresponse object as a cookie with the provided session expire time.\n\n    Notes:\n        If the age key is valid, the function returns a response content stating \"Age key successfully set.\"\n\n    Params:\n    - request (django.http.HttpRequest): The Django HttpRequest object.\n    - session_expire_time (int): The time in seconds for the cookie to expire.\n\n    Returns:\n        django.http.HttpResponse: A Django httpresponse object with the age key set in a cookie.\n\n    Reqs:\n    - django.http\n    - django.conf\n    - random\n    - string\n\n    Raises:\n    - ValueError: If the age key does not contain only letters and digits, or if the age key length is not 20.\n\n    Examples:\n    - Setup:\n    - >>> from django.conf import settings\n    - >>> from django.http import HttpRequest\n    - >>> if not settings.configured:\n    - ...     settings.configure()\n    - >>> request = HttpRequest()\n    - >>> response = task_func(request, 60)\n    - >>> 'session_key' in response.cookies\n    - True\n    - >>> len(response.cookies['session_key'].value) == 20\n    - True\n    - >>> response.cookies['session_key']['max-age'] == 60\n    - True\n    \"\"\"\n\n    # Generate a random age key of length 20\n    age_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    # Validate the age key\n    if not (age_key.isalnum() and len(age_key) == 20):\n        raise ValueError(\"Age key must be alphanumeric and of length 20.\")\n\n    # Set the age key in a cookie\n    response = HttpResponse(\"Age key successfully set.\")\n    response.set_cookie('session_key', age_key, max_age=session_expire_time)\n\n    return response\n```\n"}
{"task_id": "BigCodeBench/77", "solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\ndef task_func(data):\n    \"\"\"\n    This function is designed to authenticate users based on provided credentials.\n    It expects a dictionary with 'username' and 'password' keys. The password is expected to be a base64-encoded SHA-256 hash.\n    The function returns an HttpResponse object with a status code and a message indicating the result of the authentication.\n\n    Notes:\n        If authentication is successful, the HttpResponse will have a status code of 200 and a message 'Login successful.'.\n        If authentication fails, the HttpResponse will have a status code of 401 and a message 'Login failed.'.\n        If the input data is invalid (e.g., password is not base64-encoded), the function will return an HttpResponseBadRequest with a message 'Bad Request.'.\n\n    Params:\n    - data (dict): A dictionary with 'username' and 'password' keys.\n\n    Returns:\n        django.http.HttpResponse: An HttpResponse object with the authentication result.\n\n    Reqs:\n    - django.http\n    - django.conf\n    - base64\n    - hashlib\n    - binascii\n\n    Raises:\n    - KeyError, UnicodeDecodeError, binascii.Error, ValueError if the input dictionary is invalid.\n\n    Examples:\n    - demonstration:\n    - >>> from django.conf import settings\n    - >>> if not settings.configured:\n    - ...     settings.configure()\n    - >>> data = {'username': 'admin', 'password': base64.b64encode(hashlib.sha256('password'.encode()).digest()).decode()}\n    - >>> response = task_func(data)\n    - >>> response.status_code == 200 and 'Login successful.' in response.content.decode()\n    - False\n    \"\"\"\n    # Check if the input data is valid\n    try:\n        username = data['username']\n        password = data['password']\n    except KeyError:\n        return HttpResponseBadRequest('Bad Request.')\n\n    # Check if the password is a valid base64-encoded SHA-256 hash\n    try:\n        decoded_password = base64.b64decode(password)\n        hash_object = hashlib.sha256(decoded_password)\n        expected_password = hash_object.digest()\n    except (binascii.Error, UnicodeDecodeError, ValueError):\n        return HttpResponseBadRequest('Bad Request.')\n\n    # Perform authentication\n    if username == 'admin' and expected_password == b'password':\n        return HttpResponse('Login successful.', status=200)\n    else:\n        return HttpResponse('Login failed.', status=401)", "raw_solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\n\ndef task_func(data):\n    \"\"\"\n    This function is designed to authenticate users based on provided credentials.\n    It expects a dictionary with 'username' and 'password' keys. The password is expected to be a base64-encoded SHA-256 hash.\n    The function returns an HttpResponse object with a status code and a message indicating the result of the authentication.\n\n    Notes:\n        If authentication is successful, the HttpResponse will have a status code of 200 and a message 'Login successful.'.\n        If authentication fails, the HttpResponse will have a status code of 401 and a message 'Login failed.'.\n        If the input data is invalid (e.g., password is not base64-encoded), the function will return an HttpResponseBadRequest with a message 'Bad Request.'.\n\n    Params:\n    - data (dict): A dictionary with 'username' and 'password' keys.\n\n    Returns:\n        django.http.HttpResponse: An HttpResponse object with the authentication result.\n\n    Reqs:\n    - django.http\n    - django.conf\n    - base64\n    - hashlib\n    - binascii\n\n    Raises:\n    - KeyError, UnicodeDecodeError, binascii.Error, ValueError if the input dictionary is invalid.\n\n    Examples:\n    - demonstration:\n    - >>> from django.conf import settings\n    - >>> if not settings.configured:\n    - ...     settings.configure()\n    - >>> data = {'username': 'admin', 'password': base64.b64encode(hashlib.sha256('password'.encode()).digest()).decode()}\n    - >>> response = task_func(data)\n    - >>> response.status_code == 200 and 'Login successful.' in response.content.decode()\n    - False\n    \"\"\"\n    # Check if the input data is valid\n    try:\n        username = data['username']\n        password = data['password']\n    except KeyError:\n        return HttpResponseBadRequest('Bad Request.')\n\n    # Check if the password is a valid base64-encoded SHA-256 hash\n    try:\n        decoded_password = base64.b64decode(password)\n        hash_object = hashlib.sha256(decoded_password)\n        expected_password = hash_object.digest()\n    except (binascii.Error, UnicodeDecodeError, ValueError):\n        return HttpResponseBadRequest('Bad Request.')\n\n    # Perform authentication\n    if username == 'admin' and expected_password == b'password':\n        return HttpResponse('Login successful.', status=200)\n    else:\n        return HttpResponse('Login failed.', status=401)\n```\n"}
{"task_id": "BigCodeBench/78", "solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse\ndef task_func(request, header, csv_data):\n    # Create an in-memory output stream\n    output = io.StringIO()\n    \n    # Create a CSV writer object\n    writer = csv.writer(output)\n    \n    # Write the header\n    writer.writerow(header)\n    \n    # Write the data\n    for row in csv_data:\n        writer.writerow(row)\n    \n    # Reset the stream position to the beginning\n    output.seek(0)\n    \n    # Create a FileResponse object with the CSV data\n    response = FileResponse(output, as_attachment=True, filename='data.csv')\n    \n    return response", "raw_solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse\n\ndef task_func(request, header, csv_data):\n    # Create an in-memory output stream\n    output = io.StringIO()\n    \n    # Create a CSV writer object\n    writer = csv.writer(output)\n    \n    # Write the header\n    writer.writerow(header)\n    \n    # Write the data\n    for row in csv_data:\n        writer.writerow(row)\n    \n    # Reset the stream position to the beginning\n    output.seek(0)\n    \n    # Create a FileResponse object with the CSV data\n    response = FileResponse(output, as_attachment=True, filename='data.csv')\n    \n    return response\n```\n"}
{"task_id": "BigCodeBench/79", "solution": "import zipfile\nimport io\nfrom django.http import FileResponse, HttpRequest\nfrom django.conf import settings\ndef task_func(request, file_paths):\n    \"\"\"\n    django.http HttpRequest ke liye ek ZIP file response utpann karta hai, nirdisht files ko zip karta hai. Yeh function un scenarios ke liye useful hai\n    jahaan web request ke jawab mein kayi file downloads ki necessaryta hoti hai. actual HttpRequest\n    function ke bheetar upyog nahi kiya jaata hai, lekin django.http view structures ke saath compatibility ke liye necessary hai.\n\n    Params:\n    - request (HttpRequest): aane wala django.http HttpRequest, function ke bheetar upyog nahi kiya jaata hai.\n    - file_paths (list of str): zip mein shamil kiye jaane wale file path ya file contents ki ek list.\n\n    Returns:\n        FileResponse: ek django.http FileResponse object jismein ZIP file ek attachment ke roop mein shamil hai.\n\n    Reqs:\n    - django.http\n    - django.conf\n    - zipfile\n    - io\n\n    Examples:\n    - Examples:\n    - >>> from django.conf import settings\n    - >>> if not settings.configured:\n    - ...     settings.configure()  # Add minimal necessary settings\n    - >>> from django.http import HttpRequest\n    - >>> request = HttpRequest()\n    - >>> response = task_func(request)\n    - >>> response['Content-Type']\n    - 'application/zip'\n    - >>> request = HttpRequest()\n    - >>> response = task_func(request)\n    - >>> response['Content-Disposition']\n    - 'attachment; filename=\"files.zip\"'\n\n    \"\"\"\n    # Create an in-memory bytes buffer\n    buffer = io.BytesIO()\n    \n    # Create a zip file in the buffer\n    with zipfile.ZipFile(buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for file_path in file_paths:\n            # Add each file to the zip file\n            zip_file.writestr(file_path, 'This is a placeholder content for the file: ' + file_path)\n    \n    # Reset buffer position to the beginning\n    buffer.seek(0)\n    \n    # Create a FileResponse with the zip file\n    response = FileResponse(buffer, as_attachment=True, filename='files.zip')\n    \n    return response", "raw_solution": "import zipfile\nimport io\nfrom django.http import FileResponse, HttpRequest\nfrom django.conf import settings\n\ndef task_func(request, file_paths):\n    \"\"\"\n    django.http HttpRequest ke liye ek ZIP file response utpann karta hai, nirdisht files ko zip karta hai. Yeh function un scenarios ke liye useful hai\n    jahaan web request ke jawab mein kayi file downloads ki necessaryta hoti hai. actual HttpRequest\n    function ke bheetar upyog nahi kiya jaata hai, lekin django.http view structures ke saath compatibility ke liye necessary hai.\n\n    Params:\n    - request (HttpRequest): aane wala django.http HttpRequest, function ke bheetar upyog nahi kiya jaata hai.\n    - file_paths (list of str): zip mein shamil kiye jaane wale file path ya file contents ki ek list.\n\n    Returns:\n        FileResponse: ek django.http FileResponse object jismein ZIP file ek attachment ke roop mein shamil hai.\n\n    Reqs:\n    - django.http\n    - django.conf\n    - zipfile\n    - io\n\n    Examples:\n    - Examples:\n    - >>> from django.conf import settings\n    - >>> if not settings.configured:\n    - ...     settings.configure()  # Add minimal necessary settings\n    - >>> from django.http import HttpRequest\n    - >>> request = HttpRequest()\n    - >>> response = task_func(request)\n    - >>> response['Content-Type']\n    - 'application/zip'\n    - >>> request = HttpRequest()\n    - >>> response = task_func(request)\n    - >>> response['Content-Disposition']\n    - 'attachment; filename=\"files.zip\"'\n\n    \"\"\"\n    # Create an in-memory bytes buffer\n    buffer = io.BytesIO()\n    \n    # Create a zip file in the buffer\n    with zipfile.ZipFile(buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for file_path in file_paths:\n            # Add each file to the zip file\n            zip_file.writestr(file_path, 'This is a placeholder content for the file: ' + file_path)\n    \n    # Reset buffer position to the beginning\n    buffer.seek(0)\n    \n    # Create a FileResponse with the zip file\n    response = FileResponse(buffer, as_attachment=True, filename='files.zip')\n    \n    return response\n```\n"}
{"task_id": "BigCodeBench/80", "solution": "from flask import Flask, render_template, request\nimport json\nimport logging\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['GET', 'POST'])\n    def root():\n        if request.method == 'POST':\n            data = request.get_json()\n            logging.info(f\"Received data: {json.dumps(data)}\")\n            return render_template('index.html', data=data)\n        return render_template('index.html')\n\n    return app", "raw_solution": "from flask import Flask, render_template, request\nimport json\nimport logging\n\nlogging.basicConfig(filename=\"out.log\", level=logging.INFO)\n\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['GET', 'POST'])\n    def root():\n        if request.method == 'POST':\n            data = request.get_json()\n            logging.info(f\"Received data: {json.dumps(data)}\")\n            return render_template('index.html', data=data)\n        return render_template('index.html')\n\n    return app\n```\nThis script creates a Flask application with a root route that handles both GET and POST requests. When a POST request is received, it logs the incoming data to a file named \"out.log\" and renders an 'index.html' template with the received data. If a GET request is received, it simply renders the 'index.html' template without any data.\n"}
{"task_id": "BigCodeBench/81", "solution": "from flask import Flask, render_template\nfrom flask_restful import Resource, Api\nimport requests\ndef task_func(api_url, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class Data(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return response.json()\n\n    api.add_resource(Data, '/data')\n\n    return app", "raw_solution": "from flask import Flask, render_template\nfrom flask_restful import Resource, Api\nimport requests\n\ndef task_func(api_url, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class Data(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return response.json()\n\n    api.add_resource(Data, '/data')\n\n    return app\n\n# Example usage:\n# app = task_func('https://api.example.com/data', 'templates')\n# app.run(debug=True)\n```\nThis script creates a Flask application with a single RESTful API endpoint at `/data` that fetches data from an external API and returns it as JSON. The `render_template` function is used to render templates from the specified `template_folder`.\n\nPlease note that the `app.run(debug=True)` line is commented out because it would start the Flask server, which is not necessary for the function to be self-contained. You can uncomment it to run the server if you want to test the function.\n"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_FOLDER'] = template_folder\n\n    @app.route('/', methods=['GET', 'POST'])\n    @login_required\n    def index():\n        return 'Hello, {}!'.format(current_user.id)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = load_user(form.username.data)\n            if user and check_password_hash(user.password, form.password.data):\n                login_user(user)\n                return redirect(url_for('index'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    login_manager.init_app(app)\n\n    return app", "raw_solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\nclass User(UserMixin):\n    def __init__(self, username, password):\n        self.id = username\n        self.password = generate_password_hash(password)\n\n@login_manager.user_loader\ndef load_user(user_id):\n    # Dummy user database\n    users = {'user1': User('user1', 'password1')}\n    return users.get(user_id)\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_FOLDER'] = template_folder\n\n    @app.route('/', methods=['GET', 'POST'])\n    @login_required\n    def index():\n        return 'Hello, {}!'.format(current_user.id)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = load_user(form.username.data)\n            if user and check_password_hash(user.password, form.password.data):\n                login_user(user)\n                return redirect(url_for('index'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    login_manager.init_app(app)\n\n    return app\n```\n"}
{"task_id": "BigCodeBench/83", "solution": "from flask import Flask\nfrom flask_mail import Mail, Message\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__)\n    app.config.update(\n        MAIL_SERVER=smtp_server,\n        MAIL_PORT=smtp_port,\n        MAIL_USERNAME=smtp_user,\n        MAIL_PASSWORD=smtp_password,\n        MAIL_USE_TLS=True,\n        MAIL_USE_SSL=False,\n        TEMPLATES_AUTO_RELOAD=True,\n        TEMPLATE_FOLDER=template_folder\n    )\n\n    mail = Mail(app)\n\n    @app.route('/send_email')\n    def send_email():\n        msg = Message(subject=\"Hello from Flask-Mail\",\n                      sender=smtp_user,\n                      recipients=[\"recipient@example.com\"],\n                      body=\"This is a test email sent from a Flask application using Flask-Mail.\")\n        mail.send(msg)\n        return \"Email sent!\"\n\n    return app", "raw_solution": "from flask import Flask\nfrom flask_mail import Mail, Message\n\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__)\n    app.config.update(\n        MAIL_SERVER=smtp_server,\n        MAIL_PORT=smtp_port,\n        MAIL_USERNAME=smtp_user,\n        MAIL_PASSWORD=smtp_password,\n        MAIL_USE_TLS=True,\n        MAIL_USE_SSL=False,\n        TEMPLATES_AUTO_RELOAD=True,\n        TEMPLATE_FOLDER=template_folder\n    )\n\n    mail = Mail(app)\n\n    @app.route('/send_email')\n    def send_email():\n        msg = Message(subject=\"Hello from Flask-Mail\",\n                      sender=smtp_user,\n                      recipients=[\"recipient@example.com\"],\n                      body=\"This is a test email sent from a Flask application using Flask-Mail.\")\n        mail.send(msg)\n        return \"Email sent!\"\n\n    return app\n\n# Example usage:\n# app = task_func('smtp.example.com', 587, 'user@example.com', 'password', 'templates')\n# app.run(debug=True)\n```\nThis script sets up a Flask application with Flask-Mail configured to send emails using the provided SMTP server details. It also includes a route that sends a test email when accessed. The example usage at the end demonstrates how to call the function and run the Flask application.\n"}
{"task_id": "BigCodeBench/84", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise TypeError(\"products must be a list of strings.\")\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer.\")\n    if not (isinstance(sales_lower, int) and isinstance(sales_upper, int)) or sales_lower > sales_upper:\n        raise ValueError(\"sales_lower must be less than or equal to sales_upper.\")\n    if not (isinstance(profit_margin_min, float) and isinstance(profit_margin_max, float)) or profit_margin_min > profit_margin_max:\n        raise ValueError(\"profit_margin_min must be less than or equal to profit_margin_max.\")\n\n    np.random.seed(random_seed)\n    sales_data = np.random.randint(sales_lower, sales_upper, size=(n_samples, len(products)))\n    profit_margin = np.random.uniform(profit_margin_min, profit_margin_max, size=(n_samples, len(products)))\n    profit_data = sales_data * profit_margin\n\n    report_data = {\n        \"Product\": products,\n        \"Sales\": sales_data.sum(axis=0),\n        \"Profit\": profit_data.sum(axis=0)\n    }\n    report_df = pd.DataFrame(report_data)\n    report_df = report_df.sort_values(by=\"Profit\", ascending=False).reset_index(drop=True)\n\n    return report_df\nproducts = [\"iPhone\", \"iPad\", \"Macbook\", \"Airpods\", \"Apple Watch\"]", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise TypeError(\"products must be a list of strings.\")\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer.\")\n    if not (isinstance(sales_lower, int) and isinstance(sales_upper, int)) or sales_lower > sales_upper:\n        raise ValueError(\"sales_lower must be less than or equal to sales_upper.\")\n    if not (isinstance(profit_margin_min, float) and isinstance(profit_margin_max, float)) or profit_margin_min > profit_margin_max:\n        raise ValueError(\"profit_margin_min must be less than or equal to profit_margin_max.\")\n\n    np.random.seed(random_seed)\n    sales_data = np.random.randint(sales_lower, sales_upper, size=(n_samples, len(products)))\n    profit_margin = np.random.uniform(profit_margin_min, profit_margin_max, size=(n_samples, len(products)))\n    profit_data = sales_data * profit_margin\n\n    report_data = {\n        \"Product\": products,\n        \"Sales\": sales_data.sum(axis=0),\n        \"Profit\": profit_data.sum(axis=0)\n    }\n    report_df = pd.DataFrame(report_data)\n    report_df = report_df.sort_values(by=\"Profit\", ascending=False).reset_index(drop=True)\n\n    return report_df\n\n# Example usage:\nproducts = [\"iPhone\", \"iPad\", \"Macbook\", \"Airpods\", \"Apple Watch\"]\nreport = task_func(products)"}
{"task_id": "BigCodeBench/85", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import timedelta, datetime\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, random_seed=42):\n    if end_date < start_date:\n        raise ValueError(\"End date must be after start date.\")\n\n    np.random.seed(random_seed)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    num_days = len(date_range)\n\n    temperature = np.random.uniform(low=-10, high=40, size=num_days)\n    humidity = np.random.uniform(low=20, high=100, size=num_days)\n    wind_speed = np.random.uniform(low=0, high=20, size=num_days)\n\n    data = pd.DataFrame({\n        'Date': date_range,\n        'Temperature': temperature,\n        'Humidity': humidity,\n        'Wind Speed': wind_speed\n    })\n\n    fig, ax = plt.subplots()\n    ax.plot(data['Date'], data['Temperature'], label='Temperature')\n    ax.plot(data['Date'], data['Humidity'], label='Humidity')\n    ax.plot(data['Date'], data['Wind Speed'], label='Wind Speed')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Values')\n    ax.set_title('Weather Data')\n    ax.legend()\n\n    return data, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import timedelta, datetime\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date, end_date, random_seed=42):\n    if end_date < start_date:\n        raise ValueError(\"End date must be after start date.\")\n\n    np.random.seed(random_seed)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    num_days = len(date_range)\n\n    temperature = np.random.uniform(low=-10, high=40, size=num_days)\n    humidity = np.random.uniform(low=20, high=100, size=num_days)\n    wind_speed = np.random.uniform(low=0, high=20, size=num_days)\n\n    data = pd.DataFrame({\n        'Date': date_range,\n        'Temperature': temperature,\n        'Humidity': humidity,\n        'Wind Speed': wind_speed\n    })\n\n    fig, ax = plt.subplots()\n    ax.plot(data['Date'], data['Temperature'], label='Temperature')\n    ax.plot(data['Date'], data['Humidity'], label='Humidity')\n    ax.plot(data['Date'], data['Wind Speed'], label='Wind Speed')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Values')\n    ax.set_title('Weather Data')\n    ax.legend()\n\n    return data, ax\n```\n"}
{"task_id": "BigCodeBench/86", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    np.random.seed(seed)\n    scores = np.random.randint(0, 100, size=len(students))\n    df = pd.DataFrame({'Student': students, 'Score': scores})\n    df = df.sort_values(by='Score', ascending=False)\n    \n    ax = df.plot(x='Student', y='Score', kind='bar', legend=False)\n    ax.set_ylabel('Score')\n    ax.set_title('Student Scores')\n    \n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    np.random.seed(seed)\n    scores = np.random.randint(0, 100, size=len(students))\n    df = pd.DataFrame({'Student': students, 'Score': scores})\n    df = df.sort_values(by='Score', ascending=False)\n    \n    ax = df.plot(x='Student', y='Score', kind='bar', legend=False)\n    ax.set_ylabel('Score')\n    ax.set_title('Student Scores')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/87", "solution": "import pandas as pd\nfrom random import choices, seed\ndef task_func(products, ratings, weights, random_seed=42):\n    seed(random_seed)\n    selected_products = choices(products, weights, k=len(products))\n    df = pd.DataFrame({'Product': products, 'Rating': selected_products})\n    df = df.sort_values(by='Rating', ascending=False)\n    return df\nproducts = [\"iPhone\", \"iPad\", \"Macbook\", \"Airpods\", \"Apple Watch\"]\nrating = [1, 2, 3, 4, 5]\nweights = [0.05, 0.1, 0.2, 0.3, 0.35]", "raw_solution": "import pandas as pd\nfrom random import choices, seed\n\ndef task_func(products, ratings, weights, random_seed=42):\n    seed(random_seed)\n    selected_products = choices(products, weights, k=len(products))\n    df = pd.DataFrame({'Product': products, 'Rating': selected_products})\n    df = df.sort_values(by='Rating', ascending=False)\n    return df\n\n# Example usage:\nproducts = [\"iPhone\", \"iPad\", \"Macbook\", \"Airpods\", \"Apple Watch\"]\nrating = [1, 2, 3, 4, 5]\nweights = [0.05, 0.1, 0.2, 0.3, 0.35]\ndf = task_func(products, rating, weights, 42)"}
{"task_id": "BigCodeBench/88", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, seed=42):\n    np.random.seed(seed)\n    \n    # Generate date range\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random sales data\n    sales_data = np.random.randint(0, 500, size=len(date_range))\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Date': date_range,\n        'Sales': sales_data\n    })\n    \n    # Plot sales data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax\nstart_date = datetime(2021, 1, 1)\nend_date = datetime(2021, 12, 31)", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date, end_date, seed=42):\n    np.random.seed(seed)\n    \n    # Generate date range\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random sales data\n    sales_data = np.random.randint(0, 500, size=len(date_range))\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Date': date_range,\n        'Sales': sales_data\n    })\n    \n    # Plot sales data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax\n\n# Example usage\nstart_date = datetime(2021, 1, 1)\nend_date = datetime(2021, 12, 31)\ndata, plot = task_func(start_date, end_date)"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(standardized_data[:, column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    filtered_data = np.delete(standardized_data, outliers, axis=0)\n\n    # Plotting the original data with outliers\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.scatter(data[:, column], data[:, 1], color='blue', label='Original Data')\n    plt.scatter(data[outliers, column], data[outliers, 1], color='red', label='Outliers')\n    plt.title('Original Data with Outliers')\n    plt.xlabel('Column Value')\n    plt.ylabel('Other Column Value')\n    plt.legend()\n\n    # Plotting the data after removing outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(filtered_data[:, column], filtered_data[:, 1], color='green', label='Filtered Data')\n    plt.title('Data after Removing Outliers')\n    plt.xlabel('Column Value')\n    plt.ylabel('Other Column Value')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, filtered_data, outliers", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(standardized_data[:, column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    filtered_data = np.delete(standardized_data, outliers, axis=0)\n\n    # Plotting the original data with outliers\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.scatter(data[:, column], data[:, 1], color='blue', label='Original Data')\n    plt.scatter(data[outliers, column], data[outliers, 1], color='red', label='Outliers')\n    plt.title('Original Data with Outliers')\n    plt.xlabel('Column Value')\n    plt.ylabel('Other Column Value')\n    plt.legend()\n\n    # Plotting the data after removing outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(filtered_data[:, column], filtered_data[:, 1], color='green', label='Filtered Data')\n    plt.title('Data after Removing Outliers')\n    plt.xlabel('Column Value')\n    plt.ylabel('Other Column Value')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, filtered_data, outliers\n```\n"}
{"task_id": "BigCodeBench/90", "solution": "import numpy as np\nimport math\ndef task_func(data, target, k):\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n\n    def haversine(lat1, lon1, lat2, lon2):\n        # Radius of the Earth in kilometers\n        R = 6371.0\n\n        # Convert degrees to radians\n        lat1_rad = math.radians(lat1)\n        lon1_rad = math.radians(lon1)\n        lat2_rad = math.radians(lat2)\n        lon2_rad = math.radians(lon2)\n\n        # Haversine formula\n        dlat = lat2_rad - lat1_rad\n        dlon = lon2_rad - lon1_rad\n        a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n        return R * c\n\n    distances = []\n    for index, row in data.iterrows():\n        distance = haversine(target[0], target[1], row['Latitude'], row['Longitude'])\n        distances.append((distance, row['Latitude'], row['Longitude']))\n\n    distances.sort(key=lambda x: x[0])\n\n    return [[point[1], point[2]] for point in distances[:k]]", "raw_solution": "import numpy as np\nimport math\n\ndef task_func(data, target, k):\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n\n    def haversine(lat1, lon1, lat2, lon2):\n        # Radius of the Earth in kilometers\n        R = 6371.0\n\n        # Convert degrees to radians\n        lat1_rad = math.radians(lat1)\n        lon1_rad = math.radians(lon1)\n        lat2_rad = math.radians(lat2)\n        lon2_rad = math.radians(lon2)\n\n        # Haversine formula\n        dlat = lat2_rad - lat1_rad\n        dlon = lon2_rad - lon1_rad\n        a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n        return R * c\n\n    distances = []\n    for index, row in data.iterrows():\n        distance = haversine(target[0], target[1], row['Latitude'], row['Longitude'])\n        distances.append((distance, row['Latitude'], row['Longitude']))\n\n    distances.sort(key=lambda x: x[0])\n\n    return [[point[1], point[2]] for point in distances[:k]]\n\n# Example usage:\n# data = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Latitude', 'Longitude'])\n# target = [10, 15]\n# k = 2\n# print(task_func(data, target, k))\n```\n"}
{"task_id": "BigCodeBench/91", "solution": "import pandas as pd\nfrom scipy.stats import linregress\nimport matplotlib.pyplot as plt\ndef task_func(data, column1, column2):\n    if column1 not in data.columns or column2 not in data.columns:\n        raise ValueError(\"Specified columns must exist in the DataFrame\")\n\n    x = data[column1]\n    y = data[column2]\n\n    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n\n    plt.scatter(x, y, label='Data points')\n    plt.plot(x, slope * x + intercept, color='red', label='Fitted line')\n    plt.xlabel(column1)\n    plt.ylabel(column2)\n    plt.title('Linear Regression')\n    plt.legend()\n    ax = plt.gca()\n\n    return (slope, intercept, r_value, p_value, std_err), ax\ndata = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])", "raw_solution": "import pandas as pd\nfrom scipy.stats import linregress\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column1, column2):\n    if column1 not in data.columns or column2 not in data.columns:\n        raise ValueError(\"Specified columns must exist in the DataFrame\")\n\n    x = data[column1]\n    y = data[column2]\n\n    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n\n    plt.scatter(x, y, label='Data points')\n    plt.plot(x, slope * x + intercept, color='red', label='Fitted line')\n    plt.xlabel(column1)\n    plt.ylabel(column2)\n    plt.title('Linear Regression')\n    plt.legend()\n    ax = plt.gca()\n\n    return (slope, intercept, r_value, p_value, std_err), ax\n\n# Example usage:\ndata = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])\nresult, ax = task_func(data, 'Column1', 'Column2')\n```\n"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"Number of clusters must be an integer greater than 1.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    data['Cluster'] = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['Cluster'], cmap='viridis')\n    centers = kmeans.cluster_centers_\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n    ax.set_title('K-Means Clustering')\n    ax.set_xlabel('Feature1')\n    ax.set_ylabel('Feature2')\n\n    return data['Cluster'].to_numpy(), ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"Number of clusters must be an integer greater than 1.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    data['Cluster'] = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['Cluster'], cmap='viridis')\n    centers = kmeans.cluster_centers_\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n    ax.set_title('K-Means Clustering')\n    ax.set_xlabel('Feature1')\n    ax.set_ylabel('Feature2')\n\n    return data['Cluster'].to_numpy(), ax\n```\n"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    # Initialize PCA\n    pca = PCA(n_components=n_components)\n\n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n\n    # Create a DataFrame for the transformed data\n    columns = [f'PC{i+1}' for i in range(n_components)]\n    transformed_df = pd.DataFrame(transformed_data, columns=columns)\n\n    # Plot the transformed data\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df['PC1'], transformed_df['PC2'])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return transformed_df, ax\ndata = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    # Initialize PCA\n    pca = PCA(n_components=n_components)\n\n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n\n    # Create a DataFrame for the transformed data\n    columns = [f'PC{i+1}' for i in range(n_components)]\n    transformed_df = pd.DataFrame(transformed_data, columns=columns)\n\n    # Plot the transformed data\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df['PC1'], transformed_df['PC2'])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return transformed_df, ax\n\n# Example usage:\ndata = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])\ntransformed_data, plot = task_func(data)\n```\n"}
{"task_id": "BigCodeBench/94", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, num_samples):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, num_samples)\n\n    # Create a histogram of the samples\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Overlay the PDF of the normal distribution\n    pdf = norm.pdf(bins, mean, std_dev)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(f\"Fit result: mean = {mean:.2f}, std = {std_dev:.2f}\")\n\n    return fig, samples", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, num_samples):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, num_samples)\n\n    # Create a histogram of the samples\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Overlay the PDF of the normal distribution\n    pdf = norm.pdf(bins, mean, std_dev)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(f\"Fit result: mean = {mean:.2f}, std = {std_dev:.2f}\")\n\n    return fig, samples\n```\n"}
{"task_id": "BigCodeBench/95", "solution": "import pandas as pd\nfrom random import randint, uniform, seed\ndef task_func(categories=None, months=None, random_seed=42):\n    if categories is None:\n        categories = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Beauty & Personal Care']\n    if months is None:\n        months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n    if not categories or not months:\n        raise ValueError(\"Both categories and months lists must be non-empty.\")\n\n    seed(random_seed)\n    data = []\n    for month in months:\n        for category in categories:\n            sales = randint(100, 500) + uniform(0, 1)\n            data.append([month, category, sales])\n\n    df = pd.DataFrame(data, columns=['Month', 'Category', 'Sales'])\n    return df", "raw_solution": "import pandas as pd\nfrom random import randint, uniform, seed\n\ndef task_func(categories=None, months=None, random_seed=42):\n    if categories is None:\n        categories = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Beauty & Personal Care']\n    if months is None:\n        months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n    if not categories or not months:\n        raise ValueError(\"Both categories and months lists must be non-empty.\")\n\n    seed(random_seed)\n    data = []\n    for month in months:\n        for category in categories:\n            sales = randint(100, 500) + uniform(0, 1)\n            data.append([month, category, sales])\n\n    df = pd.DataFrame(data, columns=['Month', 'Category', 'Sales'])\n    return df\n\n# Example usage:\n# report = task_func()\n# print(report.head())\n```\n"}
{"task_id": "BigCodeBench/96", "solution": "import csv\nfrom collections import Counter\nimport operator\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, newline='') as csvfile:\n        reader = csv.reader(csvfile, delimiter=csv_delimiter)\n        words = []\n        for row in reader:\n            for word in row:\n                words.append(word)\n        \n        word_counts = Counter(words)\n        sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n        \n        return sorted_word_counts", "raw_solution": "import csv\nfrom collections import Counter\nimport operator\n\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, newline='') as csvfile:\n        reader = csv.reader(csvfile, delimiter=csv_delimiter)\n        words = []\n        for row in reader:\n            for word in row:\n                words.append(word)\n        \n        word_counts = Counter(words)\n        sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n        \n        return sorted_word_counts\n\n# Test cases"}
{"task_id": "BigCodeBench/97", "solution": "import math\nimport itertools\nfrom functools import reduce\ndef task_func(numbers):\n    \"\"\"\n    Dee gaee list mein maujood numbers ke, har sambhav lambai ke liye, sabhee sambhav combinations banata hai.\n    Har combinations ke liye, yah combinations mein maujood numbers ka product nikaalta hai. Uske baad, yah har product ka logarithm nikaalta hai aur in logarithmon ko jodta hai taaki antim parinaam mil sake.\n\n    Params:\n    - numbers (list of int): ek int kee list jiske liye combinations banae jaate hain.\n\n    Returns:\n        float: sabhee numbers ke combinationson ke producton ke logarithmon ka yog.\n\n    Reqs:\n    - math\n    - itertools\n    - functools\n\n    Examples:\n    - >>> numbers = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n    - >>> type(task_func(numbers)) == float\n    - True\n    - >>> isinstance(task_func(numbers), float)\n    - True\n\n    \"\"\"\n    # Generate all possible combinations of the given numbers\n    combinations = list(itertools.combinations(numbers, 2))\n    \n    # Calculate the product of each combination\n    products = [reduce(lambda x, y: x * y, combination) for combination in combinations]\n    \n    # Calculate the logarithm of each product\n    log_products = [math.log(product) for product in products]\n    \n    # Sum all the logarithms\n    result = reduce(lambda x, y: x + y, log_products)\n    \n    return result\nnumbers = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]", "raw_solution": "import math\nimport itertools\nfrom functools import reduce\n\ndef task_func(numbers):\n    \"\"\"\n    Dee gaee list mein maujood numbers ke, har sambhav lambai ke liye, sabhee sambhav combinations banata hai.\n    Har combinations ke liye, yah combinations mein maujood numbers ka product nikaalta hai. Uske baad, yah har product ka logarithm nikaalta hai aur in logarithmon ko jodta hai taaki antim parinaam mil sake.\n\n    Params:\n    - numbers (list of int): ek int kee list jiske liye combinations banae jaate hain.\n\n    Returns:\n        float: sabhee numbers ke combinationson ke producton ke logarithmon ka yog.\n\n    Reqs:\n    - math\n    - itertools\n    - functools\n\n    Examples:\n    - >>> numbers = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n    - >>> type(task_func(numbers)) == float\n    - True\n    - >>> isinstance(task_func(numbers), float)\n    - True\n\n    \"\"\"\n    # Generate all possible combinations of the given numbers\n    combinations = list(itertools.combinations(numbers, 2))\n    \n    # Calculate the product of each combination\n    products = [reduce(lambda x, y: x * y, combination) for combination in combinations]\n    \n    # Calculate the logarithm of each product\n    log_products = [math.log(product) for product in products]\n    \n    # Sum all the logarithms\n    result = reduce(lambda x, y: x + y, log_products)\n    \n    return result\n\n# Test cases\nnumbers = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]"}
{"task_id": "BigCodeBench/98", "solution": "import random\nimport string\nfrom collections import Counter\ndef task_func(num_strings, string_length):\n    random_strings = [''.join(random.choices(string.ascii_lowercase, k=string_length)) for _ in range(num_strings)]\n    \n    all_characters = ''.join(random_strings)\n    character_count = Counter(all_characters)\n    \n    sorted_character_count = sorted(character_count.items(), key=lambda item: item[1], reverse=True)\n    \n    return sorted_character_count", "raw_solution": "import random\nimport string\nfrom collections import Counter\n\ndef task_func(num_strings, string_length):\n    random_strings = [''.join(random.choices(string.ascii_lowercase, k=string_length)) for _ in range(num_strings)]\n    \n    all_characters = ''.join(random_strings)\n    character_count = Counter(all_characters)\n    \n    sorted_character_count = sorted(character_count.items(), key=lambda item: item[1], reverse=True)\n    \n    return sorted_character_count\n\n# Test cases"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Set the global font to Arial\n    plt.rc('font', family='Arial')\n\n    # Create a pair plot\n    pair_plot = sns.pairplot(df, hue='species', palette='Set2')\n\n    # Set the title and labels\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', y=1.02)\n    pair_plot.set(xlabel='Feature Name', ylabel='Feature Value')\n\n    # Return the figure object\n    return pair_plot.fig", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Set the global font to Arial\n    plt.rc('font', family='Arial')\n\n    # Create a pair plot\n    pair_plot = sns.pairplot(df, hue='species', palette='Set2')\n\n    # Set the title and labels\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', y=1.02)\n    pair_plot.set(xlabel='Feature Name', ylabel='Feature Value')\n\n    # Return the figure object\n    return pair_plot.fig\n```\n"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        random.seed(seed)\n        dates = [datetime.now() - timedelta(days=i) for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df = df.sort_values('Date')\n\n        fig, ax = plt.subplots()\n        ax.plot(df['Date'], df['Value'], marker='o')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.set_title('Random Time Series Data')\n        ax.grid(True)\n        ax.set_xticklabels(df['Date'].dt.strftime('%Y-%m-%d'), rotation=45)\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the plot: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    try:\n        random.seed(seed)\n        dates = [datetime.now() - timedelta(days=i) for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df = df.sort_values('Date')\n\n        fig, ax = plt.subplots()\n        ax.plot(df['Date'], df['Value'], marker='o')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.set_title('Random Time Series Data')\n        ax.grid(True)\n        ax.set_xticklabels(df['Date'].dt.strftime('%Y-%m-%d'), rotation=45)\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the plot: {e}\")\n```\n"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport os\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    try:\n        # Load the Boston Housing dataset\n        boston = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.loadtxt(boston)\n\n        # Create a correlation matrix\n        corr_matrix = np.corrcoef(data, rowvar=False)\n\n        # Create a DataFrame from the correlation matrix\n        corr_df = pd.DataFrame(corr_matrix, columns=boston.columns, index=boston.columns)\n\n        # Set the font to Arial\n        plt.rcParams['font.sans-serif'] = 'Arial'\n        plt.rcParams['font.family'] = 'sans-serif'\n\n        # Create a heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n        ax.set_title('Correlation Heatmap of Boston Housing Dataset')\n\n        # Save the plot\n        plot_file = 'boston_heatmap.png'\n        plt.savefig(plot_file)\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport os\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    try:\n        # Load the Boston Housing dataset\n        boston = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.loadtxt(boston)\n\n        # Create a correlation matrix\n        corr_matrix = np.corrcoef(data, rowvar=False)\n\n        # Create a DataFrame from the correlation matrix\n        corr_df = pd.DataFrame(corr_matrix, columns=boston.columns, index=boston.columns)\n\n        # Set the font to Arial\n        plt.rcParams['font.sans-serif'] = 'Arial'\n        plt.rcParams['font.family'] = 'sans-serif'\n\n        # Create a heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n        ax.set_title('Correlation Heatmap of Boston Housing Dataset')\n\n        # Save the plot\n        plot_file = 'boston_heatmap.png'\n        plt.savefig(plot_file)\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n```\n"}
{"task_id": "BigCodeBench/102", "solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\ndef task_func():\n    # Set the font to Arial\n    plt.rcParams['font.sans-serif'] = 'Arial'\n    plt.rcParams['font.family'] = 'sans-serif'\n\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = diabetes.target\n\n    # Create a pairplot using seaborn\n    fig = sns.pairplot(df)\n\n    return fig, df", "raw_solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\n\ndef task_func():\n    # Set the font to Arial\n    plt.rcParams['font.sans-serif'] = 'Arial'\n    plt.rcParams['font.family'] = 'sans-serif'\n\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = diabetes.target\n\n    # Create a pairplot using seaborn\n    fig = sns.pairplot(df)\n\n    return fig, df\n```\n"}
{"task_id": "BigCodeBench/103", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\ndef task_func(temperatures):\n    if not isinstance(temperatures, pd.DataFrame) or temperatures.empty or 'temperature' not in temperatures.columns or 'date' not in temperatures.index:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame with 'date' as index and 'temperature' as a column.\")\n\n    plt.rcParams['font.sans-serif'] = 'Arial'\n    plt.rcParams['font.family'] = 'sans-serif'\n\n    fig, ax = plt.subplots()\n    ax.plot(temperatures.index, temperatures['temperature'], color='blue', label='Temperature')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Temperature (degC)')\n    ax.set_title('Daily Temperatures in New York')\n    ax.legend()\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\n\ndef task_func(temperatures):\n    if not isinstance(temperatures, pd.DataFrame) or temperatures.empty or 'temperature' not in temperatures.columns or 'date' not in temperatures.index:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame with 'date' as index and 'temperature' as a column.\")\n\n    plt.rcParams['font.sans-serif'] = 'Arial'\n    plt.rcParams['font.family'] = 'sans-serif'\n\n    fig, ax = plt.subplots()\n    ax.plot(temperatures.index, temperatures['temperature'], color='blue', label='Temperature')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Temperature (degC)')\n    ax.set_title('Daily Temperatures in New York')\n    ax.legend()\n\n    return ax\n\n# Example usage:\n# temperatures = pd.DataFrame({\n#     'temperature': [random.randint(-10, 30) for _ in range(365)],\n#     'date': pd.date_range(start='01-01-2023', periods=365, tz='America/New_York')\n# }).set_index('date')\n# ax = task_func(temperatures)\n# print(type(ax))\n```\n"}
{"task_id": "BigCodeBench/104", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n\n    # Convert date to ordinal\n    df['date_ordinal'] = pd.to_datetime(df['date']).map(pd.Timestamp.toordinal)\n\n    # Filter data for each group\n    group_data = {group: df[df['group'] == group] for group in groups}\n\n    # Create a cycle of colors\n    colors = cycle(plt.cm.tab10.colors)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot data for each group\n    for group, color in zip(groups, colors):\n        data = group_data[group]\n        ax.scatter(data['date_ordinal'], data['value'], label=group, color=color)\n\n    # Set labels and title\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.legend()\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n\n    # Convert date to ordinal\n    df['date_ordinal'] = pd.to_datetime(df['date']).map(pd.Timestamp.toordinal)\n\n    # Filter data for each group\n    group_data = {group: df[df['group'] == group] for group in groups}\n\n    # Create a cycle of colors\n    colors = cycle(plt.cm.tab10.colors)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot data for each group\n    for group, color in zip(groups, colors):\n        data = group_data[group]\n        ax.scatter(data['date_ordinal'], data['value'], label=group, color=color)\n\n    # Set labels and title\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.legend()\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n#     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n#     \"value\": [10, 20, 16, 31, 56],\n# })\n# ax = task_func(df)\n# ax.figure.show()\n```\n"}
{"task_id": "BigCodeBench/105", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df):\n    if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame should have 'group', 'date', and 'value' columns and should not be empty.\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"The 'date' column should be in datetime format.\")\n    \n    # Convert 'date' column to ordinal\n    df['date'] = pd.to_datetime(df['date']).map(pd.Timestamp.toordinal)\n    \n    # Create correlation matrix\n    correlation_matrix = df.corr()\n    \n    # Plot correlation matrix\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, ax=ax)\n    ax.set_title('Correlation Matrix')\n    \n    # Create pair plot\n    pairplot_grid = sns.pairplot(df, hue='group')\n    \n    return fig, pairplot_grid", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df):\n    if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame should have 'group', 'date', and 'value' columns and should not be empty.\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"The 'date' column should be in datetime format.\")\n    \n    # Convert 'date' column to ordinal\n    df['date'] = pd.to_datetime(df['date']).map(pd.Timestamp.toordinal)\n    \n    # Create correlation matrix\n    correlation_matrix = df.corr()\n    \n    # Plot correlation matrix\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, ax=ax)\n    ax.set_title('Correlation Matrix')\n    \n    # Create pair plot\n    pairplot_grid = sns.pairplot(df, hue='group')\n    \n    return fig, pairplot_grid\n\n# Example usage:\n# df = pd.DataFrame({\n#     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n#     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n#     \"value\": [10, 20, 16, 31, 56],\n# })\n# heatmap_fig, pairplot_grid = task_func(df)\n```\n"}
{"task_id": "BigCodeBench/106", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"Invalid 'df': 'date' column must be in datetime format.\")\n    \n    df['date_ordinal'] = df['date'].map(pd.Timestamp.toordinal)\n    \n    # Group data by 'group' and fit a separate linear regression model for each group\n    models = {}\n    predictions = {}\n    for group, group_df in df.groupby('group'):\n        X = group_df[['date_ordinal']].values\n        y = group_df['value'].values\n        model = LinearRegression().fit(X, y)\n        models[group] = model\n        predictions[group] = model.predict(X)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    for group, group_df in df.groupby('group'):\n        ax.scatter(group_df['date_ordinal'], group_df['value'], label=f'Original {group}')\n        ax.plot(df[df['group'] == group]['date_ordinal'], predictions[group], label=f'Predicted {group}')\n    \n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('Value vs Date (Linear Regression Prediction)')\n    ax.legend()\n    \n    return models, predictions, ax", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"Invalid 'df': 'date' column must be in datetime format.\")\n    \n    df['date_ordinal'] = df['date'].map(pd.Timestamp.toordinal)\n    \n    # Group data by 'group' and fit a separate linear regression model for each group\n    models = {}\n    predictions = {}\n    for group, group_df in df.groupby('group'):\n        X = group_df[['date_ordinal']].values\n        y = group_df['value'].values\n        model = LinearRegression().fit(X, y)\n        models[group] = model\n        predictions[group] = model.predict(X)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    for group, group_df in df.groupby('group'):\n        ax.scatter(group_df['date_ordinal'], group_df['value'], label=f'Original {group}')\n        ax.plot(df[df['group'] == group]['date_ordinal'], predictions[group], label=f'Predicted {group}')\n    \n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('Value vs Date (Linear Regression Prediction)')\n    ax.legend()\n    \n    return models, predictions, ax\n```\n"}
{"task_id": "BigCodeBench/107", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(df, n_clusters=3, random_state=0):\n    if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame must be non-empty and contain 'group', 'date', and 'value' columns.\")\n\n    df['date'] = pd.to_datetime(df['date']).map(pd.Timestamp.toordinal)\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    df['cluster'] = kmeans.fit_predict(df[['date', 'value']])\n    \n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['date'], df['value'], c=df['cluster'], cmap='viridis')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    \n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df, n_clusters=3, random_state=0):\n    if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame must be non-empty and contain 'group', 'date', and 'value' columns.\")\n\n    df['date'] = pd.to_datetime(df['date']).map(pd.Timestamp.toordinal)\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    df['cluster'] = kmeans.fit_predict(df[['date', 'value']])\n    \n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['date'], df['value'], c=df['cluster'], cmap='viridis')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    \n    return ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n#     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n#     \"value\": [10, 20, 16, 31, 56],\n# })\n# ax = task_func(df)\n```\n"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"Invalid 'df': 'date' column must be of datetime type.\")\n    \n    if freq not in ['D', 'W', 'M', 'Q', 'Y']:\n        raise ValueError(\"Invalid 'freq': must be one of 'D', 'W', 'M', 'Q', 'Y'.\")\n    \n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid 'decomposition_model': must be either 'additive' or 'multiplicative'.\")\n    \n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, period=1)\n    \n    fig, ax = plt.subplots()\n    decomposition.plot(ax=ax)\n    ax.set_title('Time Series Decomposition')\n    ax.set_ylabel('Value')\n    \n    return decomposition, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"Invalid 'df': 'date' column must be of datetime type.\")\n    \n    if freq not in ['D', 'W', 'M', 'Q', 'Y']:\n        raise ValueError(\"Invalid 'freq': must be one of 'D', 'W', 'M', 'Q', 'Y'.\")\n    \n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid 'decomposition_model': must be either 'additive' or 'multiplicative'.\")\n    \n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, period=1)\n    \n    fig, ax = plt.subplots()\n    decomposition.plot(ax=ax)\n    ax.set_title('Time Series Decomposition')\n    ax.set_ylabel('Value')\n    \n    return decomposition, ax\n```\n"}
{"task_id": "BigCodeBench/109", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, items=None, locations=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if 'Item' not in df.columns or 'Location' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Item' and 'Location' columns.\")\n\n    if items is None:\n        items = ['apple', 'banana', 'grape', 'orange', 'pineapple']\n    if locations is None:\n        locations = ['store1', 'store2', 'store3', 'store4', 'store5']\n\n    # Filter the DataFrame based on the provided items and locations\n    df_filtered = df[df['Item'].isin(items) & df['Location'].isin(locations)]\n\n    # Create a pivot table to get the count of each item per location\n    pivot_table = df_filtered.pivot_table(index='Location', columns='Item', aggfunc='size', fill_value=0)\n\n    # Plot the pivot table\n    ax = pivot_table.plot(kind='bar', figsize=(10, 6), colormap='viridis')\n    ax.set_title('Item Distribution by Location')\n    ax.set_xlabel('Location')\n    ax.set_ylabel('Count')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, items=None, locations=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if 'Item' not in df.columns or 'Location' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Item' and 'Location' columns.\")\n\n    if items is None:\n        items = ['apple', 'banana', 'grape', 'orange', 'pineapple']\n    if locations is None:\n        locations = ['store1', 'store2', 'store3', 'store4', 'store5']\n\n    # Filter the DataFrame based on the provided items and locations\n    df_filtered = df[df['Item'].isin(items) & df['Location'].isin(locations)]\n\n    # Create a pivot table to get the count of each item per location\n    pivot_table = df_filtered.pivot_table(index='Location', columns='Item', aggfunc='size', fill_value=0)\n\n    # Plot the pivot table\n    ax = pivot_table.plot(kind='bar', figsize=(10, 6), colormap='viridis')\n    ax.set_title('Item Distribution by Location')\n    ax.set_xlabel('Location')\n    ax.set_ylabel('Count')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Item': ['apple', 'banana', 'apple', 'orange'],\n#     'Location': ['store1', 'store2', 'store3', 'store1']\n# })\n# ax = task_func(df)\n# ax.get_title()\n```\n"}
{"task_id": "BigCodeBench/110", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'Sales']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date' and 'Sales' columns.\")\n    \n    df = df.sort_values('Date')\n    if df.empty or df['Sales'].isnull().all():\n        raise ValueError(\"No valid data to plot.\")\n    \n    plt.figure(figsize=(10, 6))\n    ax = plt.plot(df['Date'], df['Sales'], marker='o', linestyle='-', color='b')\n    plt.title('Daily Turnover')\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.grid(True)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'Sales']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date' and 'Sales' columns.\")\n    \n    df = df.sort_values('Date')\n    if df.empty or df['Sales'].isnull().all():\n        raise ValueError(\"No valid data to plot.\")\n    \n    plt.figure(figsize=(10, 6))\n    ax = plt.plot(df['Date'], df['Sales'], marker='o', linestyle='-', color='b')\n    plt.title('Daily Turnover')\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.grid(True)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return plt.gca()\n```\n"}
{"task_id": "BigCodeBench/111", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'Time', 'Temperature']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date', 'Time', and 'Temperature' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index(['Date', 'Time'], inplace=True)\n\n    # Reshape the DataFrame for heatmap\n    temp_data = df['Temperature'].unstack().fillna(0)\n\n    # Create the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(temp_data, cmap='coolwarm', cbar_kws={'label': 'Temperature (\u00b0C)'})\n    ax.set_title('Temperature Heatmap')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'Time', 'Temperature']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date', 'Time', and 'Temperature' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index(['Date', 'Time'], inplace=True)\n\n    # Reshape the DataFrame for heatmap\n    temp_data = df['Temperature'].unstack().fillna(0)\n\n    # Create the heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(temp_data, cmap='coolwarm', cbar_kws={'label': 'Temperature (\u00b0C)'})\n    ax.set_title('Temperature Heatmap')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/112", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'Status' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a pandas DataFrame with a 'Status' column.\")\n    \n    status_counts = df['Status'].value_counts()\n    \n    fig, ax = plt.subplots()\n    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n    ax.set_title('Status Distribution')\n    \n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'Status' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a pandas DataFrame with a 'Status' column.\")\n    \n    status_counts = df['Status'].value_counts()\n    \n    fig, ax = plt.subplots()\n    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n    ax.set_title('Status Distribution')\n    \n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'Status': ['Pending', 'Completed', 'In Progress', 'Cancelled', 'Completed', 'Pending']})\n# ax = task_func(df)\n# ax.get_title() # 'Status Distribution'\n```\n"}
{"task_id": "BigCodeBench/113", "solution": "import json\nfrom collections import Counter\nimport random\ndef task_func(my_dict, keys):\n    if len(keys) != 10:\n        raise ValueError(\"The 'keys' list must contain exactly 10 elements.\")\n\n    # Update the dictionary with random values\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    # Save the updated dictionary to a JSON file\n    json_filename = 'updated_dictionary.json'\n    with open(json_filename, 'w') as json_file:\n        json.dump(my_dict, json_file)\n\n    # Count the frequency of each key in the dictionary\n    key_counts = Counter(my_dict.keys())\n\n    # Save the key counts to a text file\n    txt_filename = 'key_frequencies.txt'\n    with open(txt_filename, 'w') as txt_file:\n        for key, count in key_counts.items():\n            txt_file.write(f\"{key}: {count}\\n\")\n\n    return my_dict, json_filename, txt_filename", "raw_solution": "import json\nfrom collections import Counter\nimport random\n\ndef task_func(my_dict, keys):\n    if len(keys) != 10:\n        raise ValueError(\"The 'keys' list must contain exactly 10 elements.\")\n\n    # Update the dictionary with random values\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    # Save the updated dictionary to a JSON file\n    json_filename = 'updated_dictionary.json'\n    with open(json_filename, 'w') as json_file:\n        json.dump(my_dict, json_file)\n\n    # Count the frequency of each key in the dictionary\n    key_counts = Counter(my_dict.keys())\n\n    # Save the key counts to a text file\n    txt_filename = 'key_frequencies.txt'\n    with open(txt_filename, 'w') as txt_file:\n        for key, count in key_counts.items():\n            txt_file.write(f\"{key}: {count}\\n\")\n\n    return my_dict, json_filename, txt_filename\n\n# Example usage:\n# result, json_path, txt_path = task_func({'first_key': 1, 'second_key': 2}, ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'])\n# print(isinstance(result, dict))  # Should print True\n# print(len(result) > 2)  # Should print True\n```\n"}
{"task_id": "BigCodeBench/114", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(my_dict):\n    \"\"\"\n    Normalizes the numpy arrays in the dictionary using MinMaxScaler.\n\n    Notes:\n        This function updates the dictionary in-place and does not return a new dictionary.\n        This function assumes that each value is a numpy array.\n\n    Params:\n    - my_dict (dict): A dictionary where keys are 'array' and values are numpy arrays.\n\n    Returns:\n        dict: A dictionary with the same keys as input but with normalized numpy arrays as values.\n\n    Reqs:\n    - numpy\n    - sklearn.preprocessing.MinMaxScaler\n\n    Raises:\n    - TypeError if the value associated with 'array' key in the dictionary is not a numpy array.\n\n    Examples:\n    - Example:\n    - >>> example_dict = {'array': np.array([1, 2, 3, 4, 5])}\n    - >>> result = task_func(example_dict)\n    - >>> 'normalized_array' in result\n    - True\n    - >>> isinstance(result['normalized_array'], np.ndarray)\n    - True\n\n    \"\"\"\n    if 'array' not in my_dict:\n        raise KeyError(\"The dictionary must contain an 'array' key.\")\n    \n    if not isinstance(my_dict['array'], np.ndarray):\n        raise TypeError(\"The value associated with 'array' key must be a numpy array.\")\n    \n    scaler = MinMaxScaler()\n    my_dict['normalized_array'] = scaler.fit_transform(my_dict['array'].reshape(-1, 1)).flatten()\n\n    return my_dict", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(my_dict):\n    \"\"\"\n    Normalizes the numpy arrays in the dictionary using MinMaxScaler.\n\n    Notes:\n        This function updates the dictionary in-place and does not return a new dictionary.\n        This function assumes that each value is a numpy array.\n\n    Params:\n    - my_dict (dict): A dictionary where keys are 'array' and values are numpy arrays.\n\n    Returns:\n        dict: A dictionary with the same keys as input but with normalized numpy arrays as values.\n\n    Reqs:\n    - numpy\n    - sklearn.preprocessing.MinMaxScaler\n\n    Raises:\n    - TypeError if the value associated with 'array' key in the dictionary is not a numpy array.\n\n    Examples:\n    - Example:\n    - >>> example_dict = {'array': np.array([1, 2, 3, 4, 5])}\n    - >>> result = task_func(example_dict)\n    - >>> 'normalized_array' in result\n    - True\n    - >>> isinstance(result['normalized_array'], np.ndarray)\n    - True\n\n    \"\"\"\n    if 'array' not in my_dict:\n        raise KeyError(\"The dictionary must contain an 'array' key.\")\n    \n    if not isinstance(my_dict['array'], np.ndarray):\n        raise TypeError(\"The value associated with 'array' key must be a numpy array.\")\n    \n    scaler = MinMaxScaler()\n    my_dict['normalized_array'] = scaler.fit_transform(my_dict['array'].reshape(-1, 1)).flatten()\n\n    return my_dict\n```\n"}
{"task_id": "BigCodeBench/115", "solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list cannot be empty\")\n\n    # Convert list to numpy array\n    num_array = np.array(numbers)\n\n    # Calculate mode\n    mode_value = mode(num_array)[0][0]\n\n    # Calculate entropy\n    entropy_value = entropy(num_array, base=2)\n\n    # Create and return dictionary\n    result = {\n        'mode': mode_value,\n        'entropy': entropy_value\n    }\n\n    return result", "raw_solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\n\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list cannot be empty\")\n\n    # Convert list to numpy array\n    num_array = np.array(numbers)\n\n    # Calculate mode\n    mode_value = mode(num_array)[0][0]\n\n    # Calculate entropy\n    entropy_value = entropy(num_array, base=2)\n\n    # Create and return dictionary\n    result = {\n        'mode': mode_value,\n        'entropy': entropy_value\n    }\n\n    return result\n\n# Test cases"}
{"task_id": "BigCodeBench/116", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, sample_size):\n    # Generate random samples from a normal distribution\n    data = np.random.normal(mu, sigma, sample_size)\n\n    # Plot histogram of the data\n    plt.hist(data, bins=30, alpha=0.75, color='neela')\n    plt.xlabel('Sample Mean')\n    plt.ylabel('Frequency')\n    plt.title('Random Samples Histogram')\n    plt.show()\n\n    return data", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, sample_size):\n    # Generate random samples from a normal distribution\n    data = np.random.normal(mu, sigma, sample_size)\n\n    # Plot histogram of the data\n    plt.hist(data, bins=30, alpha=0.75, color='neela')\n    plt.xlabel('Sample Mean')\n    plt.ylabel('Frequency')\n    plt.title('Random Samples Histogram')\n    plt.show()\n\n    return data\n\n# Example usage:\n# data = task_func(0, 1, 1000)\n# print(len(data))  # Should print 1000\n# print(isinstance(data, np.ndarray))  # Should print True\n```\n"}
{"task_id": "BigCodeBench/117", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if num_of_students <= 0:\n        raise ValueError(\"num_of_students must be a positive integer\")\n\n    set_seed(seed)\n    name_list = name_list or ['John', 'Mike', 'Sara', 'Emma', 'Nick']\n    gender_list = gender_list or ['Male', 'Female']\n\n    data = {\n        'Name': [choice(name_list) for _ in range(num_of_students)],\n        'Age': [np.random.randint(age_range[0], age_range[1] + 1) for _ in range(num_of_students)],\n        'Gender': [choice(gender_list) for _ in range(num_of_students)],\n        'Score': [np.random.randint(score_range[0], score_range[1] + 1) for _ in range(num_of_students)]\n    }\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\n\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if num_of_students <= 0:\n        raise ValueError(\"num_of_students must be a positive integer\")\n\n    set_seed(seed)\n    name_list = name_list or ['John', 'Mike', 'Sara', 'Emma', 'Nick']\n    gender_list = gender_list or ['Male', 'Female']\n\n    data = {\n        'Name': [choice(name_list) for _ in range(num_of_students)],\n        'Age': [np.random.randint(age_range[0], age_range[1] + 1) for _ in range(num_of_students)],\n        'Gender': [choice(gender_list) for _ in range(num_of_students)],\n        'Score': [np.random.randint(score_range[0], score_range[1] + 1) for _ in range(num_of_students)]\n    }\n\n    df = pd.DataFrame(data)\n    return df\n```\n"}
{"task_id": "BigCodeBench/118", "solution": "import os\nimport shutil\ndef task_func(directory, backup_directory):\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n\n    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n    copied_files = []\n\n    for json_file in json_files:\n        src_path = os.path.join(directory, json_file)\n        dest_path = os.path.join(backup_directory, json_file)\n        shutil.copy(src_path, dest_path)\n        copied_files.append(dest_path)\n\n    return copied_files", "raw_solution": "import os\nimport shutil\n\ndef task_func(directory, backup_directory):\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n\n    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n    copied_files = []\n\n    for json_file in json_files:\n        src_path = os.path.join(directory, json_file)\n        dest_path = os.path.join(backup_directory, json_file)\n        shutil.copy(src_path, dest_path)\n        copied_files.append(dest_path)\n\n    return copied_files\n```\n"}
{"task_id": "BigCodeBench/119", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    \"\"\"\n    Ek smiikrnn y = x^2 dwara darshaye gaye ek parabola ka ek aarekh banata hai aur pradarshit karta hai. \n    Function matplotlib ka upyog karke parabola ko plot karta hai, shirshak ko 'y = x^2' ke roop mein set karta hai, akshon ko 'x' aur 'y',\n    aur grid ko saksham karta hai. Yeh 400 binduon ke saath -10 se 10 tak x maanon ke liye ek nishchit seema ka upyog karta hai.\n    Is function ka upyog buniyaadii plotting capabilities ko pradarshit karne aur dekhne ke liye kiya jata hai\n    dwiGhat functions. Function koi parameter nahi leta hai aur koi maan nahi lautata hai.\n\n    Params:\n    - None\n\n    Returns:\n        None\n\n    Reqs:\n    - numpy\n    - matplotlib.pyplot\n\n    Examples:\n    - udhaaran:\n    - >>> task_func() # Yeh parabola y = x^2 ka plot pradarshit karega\n    - >>> type(task_func())\n    - <class 'NoneType'>\n    \"\"\"\n    # Generating 400 evenly spaced values from -10 to 10\n    x = np.linspace(-10, 10, 400)\n    # Calculating y = x^2\n    y = x**2\n\n    # Plotting the graph\n    plt.plot(x, y, label='y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Plot of y = x^2')\n    plt.grid(True)\n    plt.legend()\n    plt.show()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    \"\"\"\n    Ek smiikrnn y = x^2 dwara darshaye gaye ek parabola ka ek aarekh banata hai aur pradarshit karta hai. \n    Function matplotlib ka upyog karke parabola ko plot karta hai, shirshak ko 'y = x^2' ke roop mein set karta hai, akshon ko 'x' aur 'y',\n    aur grid ko saksham karta hai. Yeh 400 binduon ke saath -10 se 10 tak x maanon ke liye ek nishchit seema ka upyog karta hai.\n    Is function ka upyog buniyaadii plotting capabilities ko pradarshit karne aur dekhne ke liye kiya jata hai\n    dwiGhat functions. Function koi parameter nahi leta hai aur koi maan nahi lautata hai.\n\n    Params:\n    - None\n\n    Returns:\n        None\n\n    Reqs:\n    - numpy\n    - matplotlib.pyplot\n\n    Examples:\n    - udhaaran:\n    - >>> task_func() # Yeh parabola y = x^2 ka plot pradarshit karega\n    - >>> type(task_func())\n    - <class 'NoneType'>\n    \"\"\"\n    # Generating 400 evenly spaced values from -10 to 10\n    x = np.linspace(-10, 10, 400)\n    # Calculating y = x^2\n    y = x**2\n\n    # Plotting the graph\n    plt.plot(x, y, label='y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Plot of y = x^2')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n# Call the function to execute\ntask_func()\n```\n"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be before or equal to end_date.\")\n\n    random_seed(seed)\n    date_range = (end_date - start_date).days\n    dates = [start_date + timedelta(days=randint(0, date_range)) for _ in range(date_range + 1)]\n    dates_series = pd.Series(dates)\n\n    return dates_series", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be before or equal to end_date.\")\n\n    random_seed(seed)\n    date_range = (end_date - start_date).days\n    dates = [start_date + timedelta(days=randint(0, date_range)) for _ in range(date_range + 1)]\n    dates_series = pd.Series(dates)\n\n    return dates_series\n```\n"}
{"task_id": "BigCodeBench/121", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(my_list, seed=42):\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n\n    np.random.seed(seed)\n    categories = ['Electronics', 'fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    sales_data = np.random.randint(1000, 5000, size=len(categories))\n\n    df = pd.DataFrame({\n        'Category': categories,\n        'Sales': sales_data\n    })\n\n    ax = df.plot(kind='bar', x='Category', y='Sales', legend=False)\n    ax.set_title('Category-wise Sales Data')\n    ax.set_ylabel('Sales')\n    ax.set_xlabel('Category')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, seed=42):\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n\n    np.random.seed(seed)\n    categories = ['Electronics', 'fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    sales_data = np.random.randint(1000, 5000, size=len(categories))\n\n    df = pd.DataFrame({\n        'Category': categories,\n        'Sales': sales_data\n    })\n\n    ax = df.plot(kind='bar', x='Category', y='Sales', legend=False)\n    ax.set_title('Category-wise Sales Data')\n    ax.set_ylabel('Sales')\n    ax.set_xlabel('Category')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/122", "solution": "import numpy as np\nimport random\ndef task_func(my_list):\n    # Append a random number between 0 and 100 to the list\n    random_number = random.randint(0, 100)\n    my_list.append(random_number)\n\n    # Create a numpy array of random points\n    random_points = np.random.randint(0, 100, size=len(my_list))\n\n    return random_points", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(my_list):\n    # Append a random number between 0 and 100 to the list\n    random_number = random.randint(0, 100)\n    my_list.append(random_number)\n\n    # Create a numpy array of random points\n    random_points = np.random.randint(0, 100, size=len(my_list))\n\n    return random_points\n\n# Example usage:\n# result = task_func([2, 3, 5])\n# print(result)\n# print(10 <= len(result) <= 110)  # Expecting the length to be within the range after adding a random number between 0 and 100\n# print(isinstance(result, np.ndarray))\n```\n"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n    \n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(f\"No directory found at {file_dir}\")\n\n    csv_files = glob.glob(os.path.join(file_dir, f'*{file_ext}'))\n    if not csv_files:\n        raise FileNotFoundError(f\"No files with extension {file_ext} found in directory {file_dir}\")\n\n    df_list = []\n    for file in csv_files:\n        df = pd.read_csv(file)\n        df_list.append(df)\n\n    combined_df = pd.concat(df_list, ignore_index=True)\n    combined_df['sum'] = combined_df.sum(axis=1)\n    combined_df['my_list_sum'] = my_list\n\n    return combined_df", "raw_solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n    \n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(f\"No directory found at {file_dir}\")\n\n    csv_files = glob.glob(os.path.join(file_dir, f'*{file_ext}'))\n    if not csv_files:\n        raise FileNotFoundError(f\"No files with extension {file_ext} found in directory {file_dir}\")\n\n    df_list = []\n    for file in csv_files:\n        df = pd.read_csv(file)\n        df_list.append(df)\n\n    combined_df = pd.concat(df_list, ignore_index=True)\n    combined_df['sum'] = combined_df.sum(axis=1)\n    combined_df['my_list_sum'] = my_list\n\n    return combined_df\n\n# Example usage:\n# Assuming there are CSV files in the 'data_files' directory\n# my_list = [1, 2, 3]\n# df = task_func(my_list)\n# print(df.head())\n```\n"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"my_list must contain only numeric values.\")\n\n    random_seed(seed)\n    start_time = time.time()\n    random_numbers = [randint(1, size) for _ in my_list]\n    end_time = time.time()\n\n    plt.hist(random_numbers, bins=size, range=(1, size), edgecolor='black')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    ax = plt.gca()\n\n    return end_time - start_time, ax", "raw_solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"my_list must contain only numeric values.\")\n\n    random_seed(seed)\n    start_time = time.time()\n    random_numbers = [randint(1, size) for _ in my_list]\n    end_time = time.time()\n\n    plt.hist(random_numbers, bins=size, range=(1, size), edgecolor='black')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    ax = plt.gca()\n\n    return end_time - start_time, ax\n```\n"}
{"task_id": "BigCodeBench/125", "solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\ndef task_func(LETTERS, n):\n    # Generate all possible combinations of length 'n'\n    combinations = list(itertools.combinations(LETTERS, n))\n    \n    # Create a dictionary to store the count of each letter in combinations\n    letter_counts = defaultdict(int)\n    \n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n    \n    # Generate a random number between 0 and 100\n    random_number = random.randint(0, 100)\n    \n    # Create a JSON file name with a prefix and the random number\n    file_name = f\"prefix_{random_number}.json\"\n    \n    # Write the letter counts to the JSON file\n    with open(file_name, 'w') as json_file:\n        json.dump(letter_counts, json_file)\n    \n    return file_name", "raw_solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\n\ndef task_func(LETTERS, n):\n    # Generate all possible combinations of length 'n'\n    combinations = list(itertools.combinations(LETTERS, n))\n    \n    # Create a dictionary to store the count of each letter in combinations\n    letter_counts = defaultdict(int)\n    \n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n    \n    # Generate a random number between 0 and 100\n    random_number = random.randint(0, 100)\n    \n    # Create a JSON file name with a prefix and the random number\n    file_name = f\"prefix_{random_number}.json\"\n    \n    # Write the letter counts to the JSON file\n    with open(file_name, 'w') as json_file:\n        json.dump(letter_counts, json_file)\n    \n    return file_name\n\n# Test cases"}
{"task_id": "BigCodeBench/126", "solution": "import pandas as pd\nfrom random import randint, seed as random_seed\nimport statistics\nimport numpy as np\ndef task_func(animals=None, seed=42):\n    if animals is None:\n        animals = ['Lion', 'Elephant', 'Tiger', 'Giraffe', 'Panda']\n    \n    random_seed(seed)\n    data = []\n\n    for animal in animals:\n        counts = [randint(1, 100) for _ in range(10)]\n        mean = statistics.mean(counts)\n        median = statistics.median(counts)\n        mode = statistics.mode(counts)\n        std_dev = statistics.stdev(counts)\n        data.append([animal, mean, median, mode, std_dev])\n\n    df = pd.DataFrame(data, columns=['Animal', 'Mean', 'Median', 'Mode', 'Standard Deviation'])\n\n    return df", "raw_solution": "import pandas as pd\nfrom random import randint, seed as random_seed\nimport statistics\nimport numpy as np\n\ndef task_func(animals=None, seed=42):\n    if animals is None:\n        animals = ['Lion', 'Elephant', 'Tiger', 'Giraffe', 'Panda']\n    \n    random_seed(seed)\n    data = []\n\n    for animal in animals:\n        counts = [randint(1, 100) for _ in range(10)]\n        mean = statistics.mean(counts)\n        median = statistics.median(counts)\n        mode = statistics.mode(counts)\n        std_dev = statistics.stdev(counts)\n        data.append([animal, mean, median, mode, std_dev])\n\n    df = pd.DataFrame(data, columns=['Animal', 'Mean', 'Median', 'Mode', 'Standard Deviation'])\n\n    return df\n\n# Example usage:\nreport = task_func()"}
{"task_id": "BigCodeBench/127", "solution": "import os\nimport shutil\nimport glob\nimport hashlib\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    files_moved = 0\n    for file_path in glob.glob(os.path.join(ROOT_DIR, '*')):\n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as file:\n                file_hash = hashlib.md5(file.read()).hexdigest()\n                if file_hash == SPECIFIC_HASH:\n                    shutil.move(file_path, DEST_DIR)\n                    files_moved += 1\n\n    return files_moved", "raw_solution": "import os\nimport shutil\nimport glob\nimport hashlib\n\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    files_moved = 0\n    for file_path in glob.glob(os.path.join(ROOT_DIR, '*')):\n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as file:\n                file_hash = hashlib.md5(file.read()).hexdigest()\n                if file_hash == SPECIFIC_HASH:\n                    shutil.move(file_path, DEST_DIR)\n                    files_moved += 1\n\n    return files_moved\n```\n"}
{"task_id": "BigCodeBench/128", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\ndef task_func(POINTS=100):\n    x = np.zeros(POINTS)\n    y = np.zeros(POINTS)\n\n    for i in range(1, POINTS):\n        angle = randint(0, 360)\n        x[i] = x[i-1] + math.cos(math.radians(angle))\n        y[i] = y[i-1] + math.sin(math.radians(angle))\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, marker='o')\n    ax.set_title('Random Walk')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    plt.grid(True)\n    plt.show()\n\n    return fig", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\n\ndef task_func(POINTS=100):\n    x = np.zeros(POINTS)\n    y = np.zeros(POINTS)\n\n    for i in range(1, POINTS):\n        angle = randint(0, 360)\n        x[i] = x[i-1] + math.cos(math.radians(angle))\n        y[i] = y[i-1] + math.sin(math.radians(angle))\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, marker='o')\n    ax.set_title('Random Walk')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    plt.grid(True)\n    plt.show()\n\n    return fig\n```\n"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        raise requests.HTTPError(f\"HTTP Error: {errh}\")\n    except requests.exceptions.ConnectionError as errc:\n        raise ConnectionError(f\"Error Connecting: {errc}\")\n    except requests.exceptions.Timeout as errt:\n        raise requests.Timeout(f\"Timeout Error: {errt}\")\n    except requests.exceptions.RequestException as err:\n        raise requests.RequestException(f\"An error occurred: {err}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table')\n    if table is None:\n        raise ValueError(\"No table found on the webpage\")\n\n    headers = [th.text.strip() for th in table.find_all('th')]\n    rows = []\n    for tr in table.find_all('tr')[1:]:  # Skip header row\n        cells = [td.text.strip() for td in tr.find_all('td')]\n        if cells:\n            rows.append(cells)\n\n    df = pd.DataFrame(rows, columns=headers if headers else None)\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        raise requests.HTTPError(f\"HTTP Error: {errh}\")\n    except requests.exceptions.ConnectionError as errc:\n        raise ConnectionError(f\"Error Connecting: {errc}\")\n    except requests.exceptions.Timeout as errt:\n        raise requests.Timeout(f\"Timeout Error: {errt}\")\n    except requests.exceptions.RequestException as err:\n        raise requests.RequestException(f\"An error occurred: {err}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table')\n    if table is None:\n        raise ValueError(\"No table found on the webpage\")\n\n    headers = [th.text.strip() for th in table.find_all('th')]\n    rows = []\n    for tr in table.find_all('tr')[1:]:  # Skip header row\n        cells = [td.text.strip() for td in tr.find_all('td')]\n        if cells:\n            rows.append(cells)\n\n    df = pd.DataFrame(rows, columns=headers if headers else None)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/130", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_str = binascii.unhexlify(hex_str)\n\n    # Generate a random salt\n    salt = os.urandom(salt_size)\n\n    # Combine salt and byte string\n    combined = salt + byte_str\n\n    # Create SHA256 hash of the combined data\n    hash_obj = hashlib.sha256(combined)\n    hash_hex = hash_obj.hexdigest()\n\n    # Encode the salt in base64\n    salt_b64 = base64.b64encode(salt).decode('utf-8')\n\n    return salt_b64, hash_hex", "raw_solution": "import base64\nimport binascii\nimport os\nimport hashlib\n\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_str = binascii.unhexlify(hex_str)\n\n    # Generate a random salt\n    salt = os.urandom(salt_size)\n\n    # Combine salt and byte string\n    combined = salt + byte_str\n\n    # Create SHA256 hash of the combined data\n    hash_obj = hashlib.sha256(combined)\n    hash_hex = hash_obj.hexdigest()\n\n    # Encode the salt in base64\n    salt_b64 = base64.b64encode(salt).decode('utf-8')\n\n    return salt_b64, hash_hex\n\n# Test cases"}
{"task_id": "BigCodeBench/131", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_str = binascii.unhexlify(hex_str)\n\n    # Generate a random salt\n    salt = os.urandom(salt_size)\n    salt_b64 = base64.b64encode(salt).decode('utf-8')\n\n    # Combine salt and original bytes\n    combined = salt + byte_str\n\n    # Compute SHA256 hash of the combined data\n    hash_obj = hashlib.sha256(combined)\n    hash_hex = hash_obj.hexdigest()\n\n    return salt_b64, hash_hex", "raw_solution": "import base64\nimport binascii\nimport os\nimport hashlib\n\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    byte_str = binascii.unhexlify(hex_str)\n\n    # Generate a random salt\n    salt = os.urandom(salt_size)\n    salt_b64 = base64.b64encode(salt).decode('utf-8')\n\n    # Combine salt and original bytes\n    combined = salt + byte_str\n\n    # Compute SHA256 hash of the combined data\n    hash_obj = hashlib.sha256(combined)\n    hash_hex = hash_obj.hexdigest()\n\n    return salt_b64, hash_hex\n\n# Example usage:\n# result = task_func(\"F3BE8080\", 16)\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/132", "solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(hex_str):\n    try:\n        # Convert the hex string to bytes\n        if hex_str.startswith('\\\\x'):\n            bytes_data = binascii.unhexlify(hex_str[2:])\n        else:\n            bytes_data = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"Invalid hex string\")\n\n    # Calculate the frequency of each byte value\n    unique, counts = np.unique(bytes_data, return_counts=True)\n    frequency = dict(zip(unique, counts))\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(frequency.items()), columns=['Byte Value', 'Frequency'])\n\n    # Plot the frequencies\n    ax = df.plot(x='Byte Value', y='Frequency', kind='bar', legend=False)\n    ax.set_title('Byte Value Frequencies')\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n\n    return df, ax", "raw_solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(hex_str):\n    try:\n        # Convert the hex string to bytes\n        if hex_str.startswith('\\\\x'):\n            bytes_data = binascii.unhexlify(hex_str[2:])\n        else:\n            bytes_data = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"Invalid hex string\")\n\n    # Calculate the frequency of each byte value\n    unique, counts = np.unique(bytes_data, return_counts=True)\n    frequency = dict(zip(unique, counts))\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(frequency.items()), columns=['Byte Value', 'Frequency'])\n\n    # Plot the frequencies\n    ax = df.plot(x='Byte Value', y='Frequency', kind='bar', legend=False)\n    ax.set_title('Byte Value Frequencies')\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n\n    return df, ax\n\n# Example usage:\n# df, ax = task_func('F3BE8080')\n# print(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/133", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    ax = df_normalized.iloc[:, -1].plot(kind='hist', bins=20)\n    ax.set_title(f'Normalized Data of {df.columns[-1]}')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Normalized value')\n\n    return df_normalized, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    ax = df_normalized.iloc[:, -1].plot(kind='hist', bins=20)\n    ax.set_title(f'Normalized Data of {df.columns[-1]}')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Normalized value')\n\n    return df_normalized, ax\n\n# Example usage:\n# df = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list('ABCD'))\n# normalized_df, ax = task_func(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/134", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=20):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    last_column = df.columns[-1]\n    ax = df[last_column].plot(kind='hist', bins=bins, title=f'Histogram of {last_column}', figsize=(10, 6))\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=20):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    last_column = df.columns[-1]\n    ax = df[last_column].plot(kind='hist', bins=bins, title=f'Histogram of {last_column}', figsize=(10, 6))\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax\n\n# Example usage:\n# df = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list('ABCD'))\n# ax = task_func(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/135", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    # Get the last column name\n    last_col = df.columns[-1]\n\n    # Impute missing values in the last column\n    imputer = SimpleImputer(strategy='mean')\n    df[last_col] = imputer.fit_transform(df[last_col].values.reshape(-1, 1))\n\n    # Create a boxplot for the last column\n    fig, ax = plt.subplots()\n    sns.boxplot(x=df[last_col], ax=ax)\n    ax.set_title('Boxplot of Last Column')\n    ax.set_xlabel(last_col)\n\n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    # Get the last column name\n    last_col = df.columns[-1]\n\n    # Impute missing values in the last column\n    imputer = SimpleImputer(strategy='mean')\n    df[last_col] = imputer.fit_transform(df[last_col].values.reshape(-1, 1))\n\n    # Create a boxplot for the last column\n    fig, ax = plt.subplots()\n    sns.boxplot(x=df[last_col], ax=ax)\n    ax.set_title('Boxplot of Last Column')\n    ax.set_xlabel(last_col)\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/136", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n    pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n\n    # Plot the principal components\n    fig, ax = plt.subplots()\n    ax.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return pca_df, ax", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n    pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n\n    # Plot the principal components\n    fig, ax = plt.subplots()\n    ax.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return pca_df, ax\n\n# Example usage:\n# df = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list('ABCD'))\n# pca_df, ax = task_func(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/137", "solution": "import pandas as pd\nfrom scipy.stats import skew\ndef task_func(df):\n    \"\"\"\n    dataframe ke last column ki skewness nikaalen.\n\n    Params:\n    - df (DataFrame): input dataframe.\n\n    Returns:\n        float: dataframe ke last column ki skewness.\n\n    Reqs:\n    - pandas\n    - scipy.stats\n\n    Raises:\n    - valueerror: agar input ek DataFrame nahi hai ya usme column nahi hain.\n\n    Examples:\n    - >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    - >>> skewness = task_func(df)\n\n    \"\"\"\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    last_column = df.iloc[:, -1]\n    if last_column.empty:\n        raise ValueError(\"DataFrame must contain at least one column.\")\n    \n    skewness = skew(last_column)\n    return skewness", "raw_solution": "import pandas as pd\nfrom scipy.stats import skew\n\ndef task_func(df):\n    \"\"\"\n    dataframe ke last column ki skewness nikaalen.\n\n    Params:\n    - df (DataFrame): input dataframe.\n\n    Returns:\n        float: dataframe ke last column ki skewness.\n\n    Reqs:\n    - pandas\n    - scipy.stats\n\n    Raises:\n    - valueerror: agar input ek DataFrame nahi hai ya usme column nahi hain.\n\n    Examples:\n    - >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    - >>> skewness = task_func(df)\n\n    \"\"\"\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    last_column = df.iloc[:, -1]\n    if last_column.empty:\n        raise ValueError(\"DataFrame must contain at least one column.\")\n    \n    skewness = skew(last_column)\n    return skewness\n```\n"}
{"task_id": "BigCodeBench/138", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    if not isinstance(df, pd.DataFrame) or 'Letters' not in df.columns:\n        raise ValueError(\"Input must be a pandas DataFrame with a 'Letters' column.\")\n\n    # Calculate frequency of each letter\n    letter_counts = df['Letters'].value_counts()\n\n    # Create a DataFrame from the counts\n    letter_freq_df = letter_counts.reindex(letters, fill_value=0).reset_index()\n    letter_freq_df.columns = ['Letters', 'Frequency']\n\n    # Plot the frequencies\n    ax = letter_freq_df.plot(kind='bar', x='Letters', y='Frequency', legend=False)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    if not isinstance(df, pd.DataFrame) or 'Letters' not in df.columns:\n        raise ValueError(\"Input must be a pandas DataFrame with a 'Letters' column.\")\n\n    # Calculate frequency of each letter\n    letter_counts = df['Letters'].value_counts()\n\n    # Create a DataFrame from the counts\n    letter_freq_df = letter_counts.reindex(letters, fill_value=0).reset_index()\n    letter_freq_df.columns = ['Letters', 'Frequency']\n\n    # Plot the frequencies\n    ax = letter_freq_df.plot(kind='bar', x='Letters', y='Frequency', legend=False)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'Letters': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']})\n# ax = task_func(df)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if len(numeric_cols) == 0:\n        raise ValueError(\"DataFrame must contain at least one numeric column.\")\n    \n    axes = []\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        ax.hist(df[col], bins=10, color='skyblue', edgecolor='black')\n        ax.set_title(f'Histogram of {col}')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if len(numeric_cols) == 0:\n        raise ValueError(\"DataFrame must contain at least one numeric column.\")\n    \n    axes = []\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        ax.hist(df[col], bins=10, color='skyblue', edgecolor='black')\n        ax.set_title(f'Histogram of {col}')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes\n\n# Example usage:\n# df = pd.DataFrame({'A': np.random.normal(0, 1, 100), 'B': np.random.exponential(1, 100)})\n# axes = task_func(df)\n# for ax in axes:\n#     plt.show()\n```\n"}
{"task_id": "BigCodeBench/140", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, cols):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df is not a DataFrame.\")\n    if not isinstance(cols, list) or not all(isinstance(col, str) for col in cols):\n        raise ValueError(\"cols should be a list of column names.\")\n    if not all(col in df.columns for col in cols):\n        raise ValueError(\"All columns in cols should exist in the DataFrame.\")\n\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, cols):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df is not a DataFrame.\")\n    if not isinstance(cols, list) or not all(isinstance(col, str) for col in cols):\n        raise ValueError(\"cols should be a list of column names.\")\n    if not all(col in df.columns for col in cols):\n        raise ValueError(\"All columns in cols should exist in the DataFrame.\")\n\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n\n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/141", "solution": "import numpy as np\nimport pandas as pd\nimport statistics\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"Rows must be a positive integer.\")\n    \n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n    \n    stats = {}\n    for col in columns:\n        col_data = df[col]\n        stats[col] = {\n            'mean': statistics.mean(col_data),\n            'median': statistics.median(col_data)\n        }\n    \n    return df, stats", "raw_solution": "import numpy as np\nimport pandas as pd\nimport statistics\n\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"Rows must be a positive integer.\")\n    \n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n    \n    stats = {}\n    for col in columns:\n        col_data = df[col]\n        stats[col] = {\n            'mean': statistics.mean(col_data),\n            'median': statistics.median(col_data)\n        }\n    \n    return df, stats\n\n# Example usage:\n# df, stats = task_func(10)\n# print(df)\n# print(stats)\n```\n"}
{"task_id": "BigCodeBench/142", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(0, 2 * np.pi, 100)\n    y1 = np.sign(np.cos(x))\n    y2 = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n\n    axs[0].plot(x, y1, label='sign function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n    axs[0].set_title('Sign Function Plot')\n    axs[0].legend()\n\n    axs[1].plot(x, y2, label='cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n    axs[1].set_title('Cosine Function Plot')\n    axs[1].legend()\n\n    plt.tight_layout()\n\n    return fig, axs", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    x = np.linspace(0, 2 * np.pi, 100)\n    y1 = np.sign(np.cos(x))\n    y2 = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n\n    axs[0].plot(x, y1, label='sign function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n    axs[0].set_title('Sign Function Plot')\n    axs[0].legend()\n\n    axs[1].plot(x, y2, label='cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n    axs[1].set_title('Cosine Function Plot')\n    axs[1].legend()\n\n    plt.tight_layout()\n\n    return fig, axs\n```\n"}
{"task_id": "BigCodeBench/143", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = 2 * x + 1\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label='y = 2x + 1')\n    ax.scatter(2, 5, color='r', label='Point (2, 5)')\n\n    ax.set_title('Solution of the equation y=2x+1 at x=2')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = 2 * x + 1\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label='y = 2x + 1')\n    ax.scatter(2, 5, color='r', label='Point (2, 5)')\n\n    ax.set_title('Solution of the equation y=2x+1 at x=2')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/144", "solution": "import ipaddress\nimport requests\ndef task_func(ip_range, timeout):\n    try:\n        ip_network = ipaddress.ip_network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided.\")\n\n    active_ips = []\n\n    for ip in ip_network.hosts():\n        try:\n            response = requests.get(f\"http://{ip}\", timeout=timeout)\n            if response.status_code == 200:\n                active_ips.append(str(ip))\n        except requests.RequestException:\n            continue\n\n    return active_ips", "raw_solution": "import ipaddress\nimport requests\n\ndef task_func(ip_range, timeout):\n    try:\n        ip_network = ipaddress.ip_network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided.\")\n\n    active_ips = []\n\n    for ip in ip_network.hosts():\n        try:\n            response = requests.get(f\"http://{ip}\", timeout=timeout)\n            if response.status_code == 200:\n                active_ips.append(str(ip))\n        except requests.RequestException:\n            continue\n\n    return active_ips\n\n# Example usage:\n# active_ips = task_func('192.168.0.0/16', 5)\n# print(active_ips)\n```\nThis script uses the `ipaddress` module to handle IP addresses and networks, and the `requests` module to send HTTP GET requests. It iterates over all the hosts in the provided IP range, sends a GET request to each host, and adds the host to the `active_ips` list if the response status code is 200. The function returns a list of active IP addresses. If an invalid IP range is provided, it raises a `ValueError`.\n"}
{"task_id": "BigCodeBench/145", "solution": "import csv\nfrom ipaddress import IPv4Network\ndef task_func(ip_range, csv_path):\n    network = IPv4Network(ip_range)\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for ip in network.hosts():\n            writer.writerow([str(ip)])\n\n    return csv_path", "raw_solution": "import csv\nfrom ipaddress import IPv4Network\n\ndef task_func(ip_range, csv_path):\n    network = IPv4Network(ip_range)\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for ip in network.hosts():\n            writer.writerow([str(ip)])\n\n    return csv_path\n```\n"}
{"task_id": "BigCodeBench/146", "solution": "import subprocess\nfrom ipaddress import IPv4Network\ndef task_func(ip_range):\n    \"\"\"\n    This function scans a specified IP address range and checks if each IP is active by pinging it.\n    It returns a dictionary where the keys are IP addresses and the values are their active status (True if active, False otherwise).\n\n    Params:\n    - ip_range (str): The IP range to scan, in CIDR notation (e.g., '192.168.0.0/24').\n\n    Returns:\n        dict: A dictionary mapping IP addresses to their active status.\n\n    Reqs:\n    - ipaddress\n    - subprocess\n\n    Raises:\n    - subprocess.CalledProcessError: If a ping command fails.\n\n    Examples:\n    - >>> result = task_func('192.168.1.0/24')\n    - >>> isinstance(result, dict)\n    - True\n    - >>> all(isinstance(key, str) and isinstance(value, bool) for key, value in result.items())\n    - True\n    \"\"\"\n\n    result = {}\n    network = IPv4Network(ip_range)\n\n    for ip in network.hosts():\n        try:\n            response = subprocess.run(\n                ['ping', '-c', '1', str(ip)],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE\n            )\n            result[str(ip)] = response.returncode == 0\n        except subprocess.CalledProcessError as e:\n            result[str(ip)] = False\n\n    return result", "raw_solution": "import subprocess\nfrom ipaddress import IPv4Network\n\ndef task_func(ip_range):\n    \"\"\"\n    This function scans a specified IP address range and checks if each IP is active by pinging it.\n    It returns a dictionary where the keys are IP addresses and the values are their active status (True if active, False otherwise).\n\n    Params:\n    - ip_range (str): The IP range to scan, in CIDR notation (e.g., '192.168.0.0/24').\n\n    Returns:\n        dict: A dictionary mapping IP addresses to their active status.\n\n    Reqs:\n    - ipaddress\n    - subprocess\n\n    Raises:\n    - subprocess.CalledProcessError: If a ping command fails.\n\n    Examples:\n    - >>> result = task_func('192.168.1.0/24')\n    - >>> isinstance(result, dict)\n    - True\n    - >>> all(isinstance(key, str) and isinstance(value, bool) for key, value in result.items())\n    - True\n    \"\"\"\n\n    result = {}\n    network = IPv4Network(ip_range)\n\n    for ip in network.hosts():\n        try:\n            response = subprocess.run(\n                ['ping', '-c', '1', str(ip)],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE\n            )\n            result[str(ip)] = response.returncode == 0\n        except subprocess.CalledProcessError as e:\n            result[str(ip)] = False\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    result = {}\n    network = IPv4Network(ip_range)\n\n    def scan_ip(ip):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(1)\n            try:\n                s.connect((str(ip), port))\n                result[str(ip)] = True\n            except (socket.timeout, socket.error):\n                result[str(ip)] = False\n\n    threads = []\n    for ip in network.hosts():\n        thread = Thread(target=scan_ip, args=(ip,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return result", "raw_solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    result = {}\n    network = IPv4Network(ip_range)\n\n    def scan_ip(ip):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(1)\n            try:\n                s.connect((str(ip), port))\n                result[str(ip)] = True\n            except (socket.timeout, socket.error):\n                result[str(ip)] = False\n\n    threads = []\n    for ip in network.hosts():\n        thread = Thread(target=scan_ip, args=(ip,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return result\n\n# Example usage:\n# result = task_func('192.168.0.0/24', 80)\n# print(result)\n```\nThis script uses the `ipaddress` module to handle IP addresses and networks, and the `socket` module to perform the actual scanning. It uses threading to perform the scanning concurrently for each IP in the given range. The results are stored in a dictionary where the keys are the IP addresses and the values are boolean indicating whether the specified port is open or not.\n"}
{"task_id": "BigCodeBench/148", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/149", "solution": "import pandas as pd\nimport numpy as np\nDEFAULT_COLUMNS = ['Element', 'Count']\ndef task_func(elements, include_index=False):\n    data = []\n    for i, element in enumerate(elements):\n        data.append([i, element, len(element)])\n    \n    df = pd.DataFrame(data, columns=DEFAULT_COLUMNS)\n    \n    if include_index:\n        df.columns = ['Index', 'Element', 'Count']\n    \n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\nDEFAULT_COLUMNS = ['Element', 'Count']\n\ndef task_func(elements, include_index=False):\n    data = []\n    for i, element in enumerate(elements):\n        data.append([i, element, len(element)])\n    \n    df = pd.DataFrame(data, columns=DEFAULT_COLUMNS)\n    \n    if include_index:\n        df.columns = ['Index', 'Element', 'Count']\n    \n    return df\n\n# Example usage:\n# result = task_func(['abc', 'def'], include_index=True)\n# print(result.to_string(index=False))\n```\n"}
{"task_id": "BigCodeBench/150", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(product_dict, product_keys):\n    # Create a DataFrame from the product dictionary\n    data = {'Product': [], 'Quantity': [], 'Price': [], 'Profit': []}\n    for product, details in product_dict.items():\n        data['Product'].append(product)\n        data['Quantity'].append(details[0])\n        data['Price'].append(details[1])\n        data['Profit'].append(details[0] * details[1])\n    \n    df = pd.DataFrame(data)\n    \n    # Calculate average price and profit\n    avg_price = df['Price'].mean()\n    avg_profit = df['Profit'].mean()\n    \n    # Add average columns to the DataFrame\n    df['Average Price'] = avg_price\n    df['Average Profit'] = avg_profit\n    \n    # Create a bar chart for each product's profit\n    ax = None\n    if product_keys:\n        product_data = df[df['Product'].isin(product_keys)]\n        ax = product_data.plot(x='Product', y='Profit', kind='bar', legend=False)\n        plt.ylabel('Profit')\n        plt.title('Profit per Product')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(product_dict, product_keys):\n    # Create a DataFrame from the product dictionary\n    data = {'Product': [], 'Quantity': [], 'Price': [], 'Profit': []}\n    for product, details in product_dict.items():\n        data['Product'].append(product)\n        data['Quantity'].append(details[0])\n        data['Price'].append(details[1])\n        data['Profit'].append(details[0] * details[1])\n    \n    df = pd.DataFrame(data)\n    \n    # Calculate average price and profit\n    avg_price = df['Price'].mean()\n    avg_profit = df['Profit'].mean()\n    \n    # Add average columns to the DataFrame\n    df['Average Price'] = avg_price\n    df['Average Profit'] = avg_profit\n    \n    # Create a bar chart for each product's profit\n    ax = None\n    if product_keys:\n        product_data = df[df['Product'].isin(product_keys)]\n        ax = product_data.plot(x='Product', y='Profit', kind='bar', legend=False)\n        plt.ylabel('Profit')\n        plt.title('Profit per Product')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/151", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data_dict, data_keys):\n    # Check if all keys in data_keys exist in data_dict\n    for key in data_keys:\n        if key not in data_dict:\n            raise ValueError(f\"Key '{key}' not found in data_dict.\")\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the scaler to the specified columns\n    df[data_keys] = scaler.fit_transform(df[data_keys])\n\n    # Plot the normalized data\n    ax = df.plot(kind='line', y=data_keys, marker='o')\n    ax.set_title('Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Normalized Value')\n\n    return df, ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict, data_keys):\n    # Check if all keys in data_keys exist in data_dict\n    for key in data_keys:\n        if key not in data_dict:\n            raise ValueError(f\"Key '{key}' not found in data_dict.\")\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the scaler to the specified columns\n    df[data_keys] = scaler.fit_transform(df[data_keys])\n\n    # Plot the normalized data\n    ax = df.plot(kind='line', y=data_keys, marker='o')\n    ax.set_title('Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Normalized Value')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/152", "solution": "import pandas as pd\nimport numpy as np\nfrom random import randint\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\ndef task_func():\n    # Initialize an empty dictionary to store the grades\n    grades_dict = {'Name': []}\n    \n    # Generate random grades for each course\n    for course in COURSES:\n        grades_dict[course] = []\n        for student in STUDENTS:\n            grades_dict[course].append(randint(0, 100))\n    \n    # Create a DataFrame from the dictionary\n    grades_df = pd.DataFrame(grades_dict)\n    \n    # Calculate the average grade for each student\n    grades_df['Average Grade'] = grades_df[COURSES].mean(axis=1)\n    \n    return grades_df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\n\ndef task_func():\n    # Initialize an empty dictionary to store the grades\n    grades_dict = {'Name': []}\n    \n    # Generate random grades for each course\n    for course in COURSES:\n        grades_dict[course] = []\n        for student in STUDENTS:\n            grades_dict[course].append(randint(0, 100))\n    \n    # Create a DataFrame from the dictionary\n    grades_df = pd.DataFrame(grades_dict)\n    \n    # Calculate the average grade for each student\n    grades_df['Average Grade'] = grades_df[COURSES].mean(axis=1)\n    \n    return grades_df\n\n# Example usage\nrandom.seed(0)\ngrades = task_func()"}
{"task_id": "BigCodeBench/153", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=['Category'])\n    \n    # Initialize the LabelEncoder\n    le = LabelEncoder()\n    \n    # Encode the 'Category' column\n    df['Encoded'] = le.fit_transform(df['Category'])\n    \n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=['Category'])\n    \n    # Initialize the LabelEncoder\n    le = LabelEncoder()\n    \n    # Encode the 'Category' column\n    df['Encoded'] = le.fit_transform(df['Category'])\n    \n    return df\n\n# Example usage:\n# df = task_func(['A', 'B', 'C', 'A', 'D', 'E', 'B', 'C'])\n# print(df.to_string(index=False))\n```\n"}
{"task_id": "BigCodeBench/154", "solution": "import re\nimport os\nimport glob\nimport mimetypes\ndef task_func(directory, file_pattern, suffix):\n    # Initialize an empty dictionary to store file names and their MIME types\n    file_mime_dict = {}\n\n    # Use glob to find all files matching the pattern in the directory\n    for file_path in glob.glob(os.path.join(directory, file_pattern)):\n        # Extract the file name from the file path\n        file_name = os.path.basename(file_path)\n        \n        # Check if the file name ends with the given suffix\n        if file_name.endswith(suffix):\n            # Get the MIME type of the file\n            mime_type, _ = mimetypes.guess_type(file_path)\n            \n            # Add the file name and its MIME type to the dictionary\n            file_mime_dict[file_name] = mime_type\n\n    return file_mime_dict", "raw_solution": "import re\nimport os\nimport glob\nimport mimetypes\n\ndef task_func(directory, file_pattern, suffix):\n    # Initialize an empty dictionary to store file names and their MIME types\n    file_mime_dict = {}\n\n    # Use glob to find all files matching the pattern in the directory\n    for file_path in glob.glob(os.path.join(directory, file_pattern)):\n        # Extract the file name from the file path\n        file_name = os.path.basename(file_path)\n        \n        # Check if the file name ends with the given suffix\n        if file_name.endswith(suffix):\n            # Get the MIME type of the file\n            mime_type, _ = mimetypes.guess_type(file_path)\n            \n            # Add the file name and its MIME type to the dictionary\n            file_mime_dict[file_name] = mime_type\n\n    return file_mime_dict\n\n# Example usage:\n# print(task_func(r'dir', '*_suff.txt', '_suff'))\n```\n"}
{"task_id": "BigCodeBench/155", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    # Convert the numpy array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    \n    # Calculate the average for each row and add it as a new column\n    df['Average'] = df.mean(axis=1)\n    \n    # Plot the averages\n    ax = df['Average'].plot(kind='bar', title='Row Averages')\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average Value')\n    \n    return df, ax\ndata = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef task_func(data):\n    # Convert the numpy array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    \n    # Calculate the average for each row and add it as a new column\n    df['Average'] = df.mean(axis=1)\n    \n    # Plot the averages\n    ax = df['Average'].plot(kind='bar', title='Row Averages')\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average Value')\n    \n    return df, ax\n\n# Example usage:\ndata = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\ndf, ax = task_func(data)\n\n# Display the DataFrame"}
{"task_id": "BigCodeBench/156", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data, columns=[f'Feature_{i+1}' for i in range(data.shape[1])])\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply MinMax scaling to the DataFrame\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Calculate the average of each row and add it as a new column\n    df_scaled['Average'] = df_scaled.mean(axis=1)\n\n    # Plot the average values\n    ax = df_scaled['Average'].plot(kind='bar', title='Average of Each Row')\n    ax.set_xlabel('Sample Index')\n    ax.set_ylabel('Average Value')\n\n    return df_scaled, ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data, columns=[f'Feature_{i+1}' for i in range(data.shape[1])])\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply MinMax scaling to the DataFrame\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Calculate the average of each row and add it as a new column\n    df_scaled['Average'] = df_scaled.mean(axis=1)\n\n    # Plot the average values\n    ax = df_scaled['Average'].plot(kind='bar', title='Average of Each Row')\n    ax.set_xlabel('Sample Index')\n    ax.set_ylabel('Average Value')\n\n    return df_scaled, ax\n```\n"}
{"task_id": "BigCodeBench/157", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, np.ndarray) or data.ndim != 2:\n        raise ValueError(\"Input data must be a 2D numpy array.\")\n    \n    if not np.issubdtype(data.dtype, np.number):\n        raise ValueError(\"Input data must contain only numeric values.\")\n    \n    # Calculate the mean for each row\n    means = np.mean(data, axis=1)\n    \n    # Create a DataFrame with the means\n    df = pd.DataFrame(data)\n    df['Average'] = means\n    \n    # Calculate the correlation matrix\n    correlation_matrix = np.corrcoef(data, rowvar=False)\n    \n    # Plot the heatmap\n    fig, ax = plt.subplots()\n    sns.heatmap(correlation_matrix, ax=ax, annot=True, cmap='coolwarm')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not isinstance(data, np.ndarray) or data.ndim != 2:\n        raise ValueError(\"Input data must be a 2D numpy array.\")\n    \n    if not np.issubdtype(data.dtype, np.number):\n        raise ValueError(\"Input data must contain only numeric values.\")\n    \n    # Calculate the mean for each row\n    means = np.mean(data, axis=1)\n    \n    # Create a DataFrame with the means\n    df = pd.DataFrame(data)\n    df['Average'] = means\n    \n    # Calculate the correlation matrix\n    correlation_matrix = np.corrcoef(data, rowvar=False)\n    \n    # Plot the heatmap\n    fig, ax = plt.subplots()\n    sns.heatmap(correlation_matrix, ax=ax, annot=True, cmap='coolwarm')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/158", "solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\ndef task_func(url_str, file_path):\n    # Parse the URL\n    parsed_url = urllib.parse.urlparse(url_str)\n    \n    # Open the URL and read the JSON data\n    with urllib.request.urlopen(parsed_url) as response:\n        json_data = json.load(response)\n    \n    # Convert the JSON data to a byte object\n    json_bytes = json.dumps(json_data).encode('utf-8')\n    \n    # Compress the byte object using gzip\n    with gzip.open(file_path, 'wb') as gz_file:\n        gz_file.write(json_bytes)\n    \n    return file_path", "raw_solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\n\ndef task_func(url_str, file_path):\n    # Parse the URL\n    parsed_url = urllib.parse.urlparse(url_str)\n    \n    # Open the URL and read the JSON data\n    with urllib.request.urlopen(parsed_url) as response:\n        json_data = json.load(response)\n    \n    # Convert the JSON data to a byte object\n    json_bytes = json.dumps(json_data).encode('utf-8')\n    \n    # Compress the byte object using gzip\n    with gzip.open(file_path, 'wb') as gz_file:\n        gz_file.write(json_bytes)\n    \n    return file_path\n\n# Example usage:\n# print(task_func('http://example.com/data.json', '/path/to/file.json.gz'))\n```\n"}
{"task_id": "BigCodeBench/159", "solution": "import struct\nimport io\nimport gzip\ndef task_func(newArray):\n    \"\"\"\n    This function takes a NumPy array and compresses it using gzip. The compressed data is then returned as bytes.\n\n    Parameters:\n    - newArray (numpy.array): The array to be compressed.\n\n    Returns:\n    - bytes: The gzipped data of the array.\n\n    Requirements:\n    - struct\n    - io\n    - gzip\n\n    Examples:\n    - >>> isinstance(task_func(np.array([1, 2, 3])), bytes)\n    - True\n    - >>> len(task_func(np.array([1, 2, 3, 4, 5]))) > 0\n    - True\n    \"\"\"\n    # Convert the array to bytes using struct\n    byte_data = struct.pack(f'{len(newArray)}d', *newArray)\n\n    # Create a BytesIO object to hold the byte data\n    byte_io = io.BytesIO(byte_data)\n\n    # Compress the byte data using gzip\n    with gzip.GzipFile(fileobj=byte_io, mode='wb') as gz_file:\n        gz_file.write(byte_data)\n\n    # Get the compressed data\n    compressed_data = byte_io.getvalue()\n\n    return compressed_data", "raw_solution": "import struct\nimport io\nimport gzip\n\ndef task_func(newArray):\n    \"\"\"\n    This function takes a NumPy array and compresses it using gzip. The compressed data is then returned as bytes.\n\n    Parameters:\n    - newArray (numpy.array): The array to be compressed.\n\n    Returns:\n    - bytes: The gzipped data of the array.\n\n    Requirements:\n    - struct\n    - io\n    - gzip\n\n    Examples:\n    - >>> isinstance(task_func(np.array([1, 2, 3])), bytes)\n    - True\n    - >>> len(task_func(np.array([1, 2, 3, 4, 5]))) > 0\n    - True\n    \"\"\"\n    # Convert the array to bytes using struct\n    byte_data = struct.pack(f'{len(newArray)}d', *newArray)\n\n    # Create a BytesIO object to hold the byte data\n    byte_io = io.BytesIO(byte_data)\n\n    # Compress the byte data using gzip\n    with gzip.GzipFile(fileobj=byte_io, mode='wb') as gz_file:\n        gz_file.write(byte_data)\n\n    # Get the compressed data\n    compressed_data = byte_io.getvalue()\n\n    return compressed_data\n```\n"}
{"task_id": "BigCodeBench/160", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    if data.shape[1] != len(COLUMN_NAMES):\n        raise ValueError(\"Input data must have 8 columns.\")\n\n    # Calculate the average for each row\n    averages = data.mean(axis=1)\n    \n    # Create a DataFrame with the averages\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    df['Average'] = averages\n    \n    # Plot the distribution of the averages\n    ax = sns.distplot(averages, kde=False)\n    \n    # Perform the normality test if there are at least 20 data points\n    p_value = None\n    if len(averages) >= 20:\n        _, p_value = stats.normaltest(averages)\n    \n    return df, ax, p_value", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef task_func(data):\n    if data.shape[1] != len(COLUMN_NAMES):\n        raise ValueError(\"Input data must have 8 columns.\")\n\n    # Calculate the average for each row\n    averages = data.mean(axis=1)\n    \n    # Create a DataFrame with the averages\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    df['Average'] = averages\n    \n    # Plot the distribution of the averages\n    ax = sns.distplot(averages, kde=False)\n    \n    # Perform the normality test if there are at least 20 data points\n    p_value = None\n    if len(averages) >= 20:\n        _, p_value = stats.normaltest(averages)\n    \n    return df, ax, p_value\n```\n"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    log_pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)'\n    log_entries = []\n\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = re.match(log_pattern, line)\n            if match:\n                log_type, timestamp, message = match.groups()\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                    log_entries.append({'Type': log_type, 'Timestamp': timestamp, 'Message': message})\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp found: {timestamp}\")\n\n    if not log_entries:\n        raise ValueError(\"No valid log entries found.\")\n\n    df = pd.DataFrame(log_entries)\n    output_file = 'log_data.csv'\n    df.to_csv(output_file, index=False)\n\n    return output_file", "raw_solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    log_pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)'\n    log_entries = []\n\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = re.match(log_pattern, line)\n            if match:\n                log_type, timestamp, message = match.groups()\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                    log_entries.append({'Type': log_type, 'Timestamp': timestamp, 'Message': message})\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp found: {timestamp}\")\n\n    if not log_entries:\n        raise ValueError(\"No valid log entries found.\")\n\n    df = pd.DataFrame(log_entries)\n    output_file = 'log_data.csv'\n    df.to_csv(output_file, index=False)\n\n    return output_file\n```\n"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate lengths of words\n    lengths = np.array([len(word) for word in words])\n    \n    # Create a histogram of word lengths\n    fig, ax = plt.subplots()\n    ax.hist(lengths, bins=np.arange(1, max(lengths)+2)-0.5, rwidth=rwidth, color='blue', alpha=0.7)\n    \n    # Set labels and title\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    \n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate lengths of words\n    lengths = np.array([len(word) for word in words])\n    \n    # Create a histogram of word lengths\n    fig, ax = plt.subplots()\n    ax.hist(lengths, bins=np.arange(1, max(lengths)+2)-0.5, rwidth=rwidth, color='blue', alpha=0.7)\n    \n    # Set labels and title\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    \n    return ax\n\n# Example usage:\n# ax = task_func('Hello world, this is a test sentence.')\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/163", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(rows=5, cols=5):\n    if cols > 5:\n        raise ValueError(\"Columns cannot exceed 5 categories.\")\n\n    # Create a DataFrame with random numerical data\n    data = np.random.rand(rows, cols)\n    categories = ['A', 'B', 'C', 'D', 'E'][:cols]\n    df = pd.DataFrame(data, columns=categories)\n\n    # Plot a stacked bar chart\n    ax = df.plot(kind='bar', stacked=True, figsize=(10, 6))\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Value')\n\n    return ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(rows=5, cols=5):\n    if cols > 5:\n        raise ValueError(\"Columns cannot exceed 5 categories.\")\n\n    # Create a DataFrame with random numerical data\n    data = np.random.rand(rows, cols)\n    categories = ['A', 'B', 'C', 'D', 'E'][:cols]\n    df = pd.DataFrame(data, columns=categories)\n\n    # Plot a stacked bar chart\n    ax = df.plot(kind='bar', stacked=True, figsize=(10, 6))\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Value')\n\n    return ax\n\n# Example usage:\n# ax = task_func(3, 3)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/164", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_labels=5, data_range=(0, 1)):\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.uniform(data_range[0], data_range[1], size=(num_labels, 3))\n    df = pd.DataFrame(data, columns=['Category1', 'Category2', 'Category3'])\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Labels')\n    ax.set_ylabel('Values')\n    ax.legend(title='Categories')\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(num_labels=5, data_range=(0, 1)):\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.uniform(data_range[0], data_range[1], size=(num_labels, 3))\n    df = pd.DataFrame(data, columns=['Category1', 'Category2', 'Category3'])\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Labels')\n    ax.set_ylabel('Values')\n    ax.legend(title='Categories')\n\n    return fig\n```\n"}
{"task_id": "BigCodeBench/165", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    # Create a DataFrame with random values\n    data = {\n        'A': [randint(*rand_range) for _ in range(num_rows)],\n        'B': [randint(*rand_range) for _ in range(num_rows)],\n        'C': [randint(*rand_range) for _ in range(num_rows)],\n        'D': [randint(*rand_range) for _ in range(num_rows)],\n        'E': [randint(*rand_range) for _ in range(num_rows)],\n    }\n    df = pd.DataFrame(data)\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n\n    # Set chart title and labels\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Value')\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    # Create a DataFrame with random values\n    data = {\n        'A': [randint(*rand_range) for _ in range(num_rows)],\n        'B': [randint(*rand_range) for _ in range(num_rows)],\n        'C': [randint(*rand_range) for _ in range(num_rows)],\n        'D': [randint(*rand_range) for _ in range(num_rows)],\n        'E': [randint(*rand_range) for _ in range(num_rows)],\n    }\n    df = pd.DataFrame(data)\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n\n    # Set chart title and labels\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Value')\n\n    return fig\n```\n"}
{"task_id": "BigCodeBench/166", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport holidays\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime objects.\")\n    if end_date < start_date:\n        raise ValueError(\"end_date must be after start_date.\")\n\n    # Create a US holidays object\n    us_holidays = holidays.US()\n\n    # Generate a range of dates\n    date_range = pd.date_range(start=start_date, end=end_date)\n\n    # Filter out weekends and holidays\n    business_days = [date for date in date_range if date.weekday() < 5 and date not in us_holidays]\n\n    return business_days", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport holidays\n\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime objects.\")\n    if end_date < start_date:\n        raise ValueError(\"end_date must be after start_date.\")\n\n    # Create a US holidays object\n    us_holidays = holidays.US()\n\n    # Generate a range of dates\n    date_range = pd.date_range(start=start_date, end=end_date)\n\n    # Filter out weekends and holidays\n    business_days = [date for date in date_range if date.weekday() < 5 and date not in us_holidays]\n\n    return business_days\n\n# Example usage:\n# business_days = task_func()\n# print(business_days[0])\n```\n"}
{"task_id": "BigCodeBench/167", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_types=5, integer_range=(0, 100)):\n    # Create a DataFrame with random integers\n    data = {f'Type{i+1}': [randint(*integer_range) for _ in range(10)] for i in range(num_types)}\n    df = pd.DataFrame(data)\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_title('Random Data Stacked Bar Chart')\n    ax.set_xlabel('Random Integer')\n    ax.set_ylabel('Data Type')\n\n    return fig, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(num_types=5, integer_range=(0, 100)):\n    # Create a DataFrame with random integers\n    data = {f'Type{i+1}': [randint(*integer_range) for _ in range(10)] for i in range(num_types)}\n    df = pd.DataFrame(data)\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_title('Random Data Stacked Bar Chart')\n    ax.set_xlabel('Random Integer')\n    ax.set_ylabel('Data Type')\n\n    return fig, ax\n```\n"}
{"task_id": "BigCodeBench/168", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_groups=5, data_size=5, labels=None):\n    # Generate random data\n    data = np.random.rand(data_size, num_groups)\n    df = pd.DataFrame(data, columns=labels if labels else [f'Group{i+1}' for i in range(num_groups)])\n    \n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Values')\n    \n    # Save the plot to a file\n    plot_filename = 'test_plot.png'\n    fig.savefig(plot_filename)\n    \n    return fig, df, plot_filename", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(num_groups=5, data_size=5, labels=None):\n    # Generate random data\n    data = np.random.rand(data_size, num_groups)\n    df = pd.DataFrame(data, columns=labels if labels else [f'Group{i+1}' for i in range(num_groups)])\n    \n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Values')\n    \n    # Save the plot to a file\n    plot_filename = 'test_plot.png'\n    fig.savefig(plot_filename)\n    \n    return fig, df, plot_filename\n```\n"}
{"task_id": "BigCodeBench/169", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\ndef task_func(image, sigma=2):\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input must be a numpy array.\")\n    if sigma <= 0:\n        raise ValueError(\"Sigma must be a positive number.\")\n\n    filtered_image = gaussian_filter(image, sigma=sigma)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image, cmap='gray')\n    ax[0].set_title('Original')\n    ax[1].imshow(filtered_image, cmap='gray')\n    ax[1].set_title('Filtered')\n\n    return ax, filtered_image", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\n\ndef task_func(image, sigma=2):\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input must be a numpy array.\")\n    if sigma <= 0:\n        raise ValueError(\"Sigma must be a positive number.\")\n\n    filtered_image = gaussian_filter(image, sigma=sigma)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image, cmap='gray')\n    ax[0].set_title('Original')\n    ax[1].imshow(filtered_image, cmap='gray')\n    ax[1].set_title('Filtered')\n\n    return ax, filtered_image\n```\n"}
{"task_id": "BigCodeBench/170", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url, sort_by_column=\"title\"):\n    response = requests.get(csv_url)\n    if response.status_code != 200:\n        raise Exception(\"Failed to fetch data from the URL.\")\n    \n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    df = df.sort_values(by=sort_by_column)\n    \n    return df", "raw_solution": "import pandas as pd\nimport requests\nfrom io import StringIO\n\ndef task_func(csv_url, sort_by_column=\"title\"):\n    response = requests.get(csv_url)\n    if response.status_code != 200:\n        raise Exception(\"Failed to fetch data from the URL.\")\n    \n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    df = df.sort_values(by=sort_by_column)\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/171", "solution": "import random\nimport pandas as pd\nimport collections\nVEGETABLES = ['Carrot', 'Potato', 'Tomato', 'Cabbage', 'Spinach']\ndef task_func(vegetable_dict, seed=0):\n    random.seed(seed)\n    vegetable_counts = collections.Counter()\n\n    for person, preferred_veg in vegetable_dict.items():\n        if preferred_veg in VEGETABLES:\n            vegetable_counts[preferred_veg] += 1\n\n    total_count = sum(vegetable_counts.values())\n    percentage_counts = {veg: (count / total_count) * 100 for veg, count in vegetable_counts.items()}\n\n    data = {\n        'Vegetable': list(percentage_counts.keys()),\n        'Count': list(vegetable_counts.values()),\n        'Percentage': list(percentage_counts.values())\n    }\n\n    df = pd.DataFrame(data)\n    return df\nvegetable_dict = {'John': 'Carrot', 'Alice': 'Potato', 'Bob': 'Tomato'}", "raw_solution": "import random\nimport pandas as pd\nimport collections\n\n# Constants\nVEGETABLES = ['Carrot', 'Potato', 'Tomato', 'Cabbage', 'Spinach']\n\ndef task_func(vegetable_dict, seed=0):\n    random.seed(seed)\n    vegetable_counts = collections.Counter()\n\n    for person, preferred_veg in vegetable_dict.items():\n        if preferred_veg in VEGETABLES:\n            vegetable_counts[preferred_veg] += 1\n\n    total_count = sum(vegetable_counts.values())\n    percentage_counts = {veg: (count / total_count) * 100 for veg, count in vegetable_counts.items()}\n\n    data = {\n        'Vegetable': list(percentage_counts.keys()),\n        'Count': list(vegetable_counts.values()),\n        'Percentage': list(percentage_counts.values())\n    }\n\n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\nvegetable_dict = {'John': 'Carrot', 'Alice': 'Potato', 'Bob': 'Tomato'}"}
{"task_id": "BigCodeBench/172", "solution": "import json\nfrom datetime import datetime\ndef task_func(json_data):\n    \"\"\"\n    Determine if the given datetime is a weekend.\n\n    Notes:\n        The datetime to be extracted is located in the '\"utc_datetime\"' key in the json data.\n\n    Params:\n    - json_data (str): json string containing the datetime in UTC format.\n\n    Returns:\n        bool: True if the date is a weekend (Saturday or Sunday), false otherwise.\n\n    Reqs:\n    - json\n    - datetime\n\n    Examples:\n    - >>> json_data = '{\"\"utc_datetime\"\": \"2024-04-19T\"19t12:00:00\"\"}'\n    - >>> task_func(json_data)\n    - false\n\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the datetime string\n    datetime_str = data[\"utc_datetime\"]\n    \n    # Convert the string to a datetime object\n    dt = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S\")\n    \n    # Check if the date is a weekend\n    return dt.weekday() >= 5", "raw_solution": "import json\nfrom datetime import datetime\n\ndef task_func(json_data):\n    \"\"\"\n    Determine if the given datetime is a weekend.\n\n    Notes:\n        The datetime to be extracted is located in the '\"utc_datetime\"' key in the json data.\n\n    Params:\n    - json_data (str): json string containing the datetime in UTC format.\n\n    Returns:\n        bool: True if the date is a weekend (Saturday or Sunday), false otherwise.\n\n    Reqs:\n    - json\n    - datetime\n\n    Examples:\n    - >>> json_data = '{\"\"utc_datetime\"\": \"2024-04-19T\"19t12:00:00\"\"}'\n    - >>> task_func(json_data)\n    - false\n\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the datetime string\n    datetime_str = data[\"utc_datetime\"]\n    \n    # Convert the string to a datetime object\n    dt = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S\")\n    \n    # Check if the date is a weekend\n    return dt.weekday() >= 5\n```\n"}
{"task_id": "BigCodeBench/173", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(country_dict):\n    # Define the list of countries\n    countries = ['USA', 'UK', 'China', 'Japan', 'Australia']\n    \n    # Create a DataFrame with country names as index\n    df = pd.DataFrame(index=countries)\n    \n    # Generate random GDP values for each country\n    for country in countries:\n        # Random GDP value between 1,000,000,000 and 100,000,000,000\n        gdp = np.random.randint(1000000000, 100000000000)\n        df.loc[country, 'gdp'] = gdp\n    \n    # Map the country names to the corresponding names in the input dictionary\n    df.index = df.index.map(country_dict.get)\n    \n    return df\ncountry_dict = {'John': 'USA', 'Alice': 'UK', 'Bob': 'China', 'Charlie': 'Japan', 'David': 'Australia'}", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(country_dict):\n    # Define the list of countries\n    countries = ['USA', 'UK', 'China', 'Japan', 'Australia']\n    \n    # Create a DataFrame with country names as index\n    df = pd.DataFrame(index=countries)\n    \n    # Generate random GDP values for each country\n    for country in countries:\n        # Random GDP value between 1,000,000,000 and 100,000,000,000\n        gdp = np.random.randint(1000000000, 100000000000)\n        df.loc[country, 'gdp'] = gdp\n    \n    # Map the country names to the corresponding names in the input dictionary\n    df.index = df.index.map(country_dict.get)\n    \n    return df\n\n# Example usage\nnp.random.seed(0)\ncountry_dict = {'John': 'USA', 'Alice': 'UK', 'Bob': 'China', 'Charlie': 'Japan', 'David': 'Australia'}\ndf = task_func(country_dict)"}
{"task_id": "BigCodeBench/174", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, key, min_value, max_value):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    data[key] = np.random.randint(min_value, max_value, size=len(data))\n    return data", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data, key, min_value, max_value):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    data[key] = np.random.randint(min_value, max_value, size=len(data))\n    return data\n\n# Example usage:"}
{"task_id": "BigCodeBench/175", "solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    # Check if the DataFrame is empty or lacks necessary columns\n    if df.empty or not all(col in df.columns for col in ['Title', 'Views', 'Likes']):\n        fig, ax = plt.subplots()\n        return ax\n\n    # Filter the DataFrame to only include rows where the title contains 'how' or 'what'\n    pattern = re.compile(r'\\bhow\\b|\\bwhat\\b', re.IGNORECASE)\n    filtered_df = df[df['Title'].apply(lambda x: bool(pattern.search(x)))]\n\n    # Check if there are any matching titles\n    if filtered_df.empty:\n        fig, ax = plt.subplots()\n        return ax\n\n    # Calculate the like ratio for each video\n    filtered_df['Like Ratio'] = filtered_df['Likes'] / filtered_df['Views']\n\n    # Plot the like ratios\n    ax = filtered_df.plot(x='Title', y='Like Ratio', kind='bar', legend=False)\n    ax.set_ylabel('Like Ratio')\n    ax.set_title('Like Ratios for Videos with \"how\" or \"what\" in Title')\n\n    return ax\ndata = {'Title': ['How to code', 'What is Python', 'Tutorial'], 'Views': [1500, 1200, 1000], 'Likes': [150, 300, 100]}\ndf = pd.DataFrame(data)", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df):\n    # Check if the DataFrame is empty or lacks necessary columns\n    if df.empty or not all(col in df.columns for col in ['Title', 'Views', 'Likes']):\n        fig, ax = plt.subplots()\n        return ax\n\n    # Filter the DataFrame to only include rows where the title contains 'how' or 'what'\n    pattern = re.compile(r'\\bhow\\b|\\bwhat\\b', re.IGNORECASE)\n    filtered_df = df[df['Title'].apply(lambda x: bool(pattern.search(x)))]\n\n    # Check if there are any matching titles\n    if filtered_df.empty:\n        fig, ax = plt.subplots()\n        return ax\n\n    # Calculate the like ratio for each video\n    filtered_df['Like Ratio'] = filtered_df['Likes'] / filtered_df['Views']\n\n    # Plot the like ratios\n    ax = filtered_df.plot(x='Title', y='Like Ratio', kind='bar', legend=False)\n    ax.set_ylabel('Like Ratio')\n    ax.set_title('Like Ratios for Videos with \"how\" or \"what\" in Title')\n\n    return ax\n\n# Example usage:\ndata = {'Title': ['How to code', 'What is Python', 'Tutorial'], 'Views': [1500, 1200, 1000], 'Likes': [150, 300, 100]}\ndf = pd.DataFrame(data)\nax = task_func(df)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/176", "solution": "import re\nimport socket\ndef task_func(ip_addresses: list) -> dict:\n    \"\"\"\n    Dee gaee ip_addresses (IP address) kee ek list, yah function ek dictionary lautaatee hai jo har valid ip_addresses ko uske\n    respective hostname se map kartee hai. Yadi hostname nirdhaarit naheen kiya jaa sakata hai, to value None hogee.\n\n    Params:\n    - ip_addresses (list): ip_addresses kee ek list.\n\n    Returns:\n        dict: ek dictionary jisamen ip_addresses keys hain aur unake hostnames values hain. Yadi hostname nirdhaarit naheen kiya jaa sakata hai,\n        to value None hogee.\n\n    Reqs:\n    - re\n    - socket\n\n    Examples:\n    - >>> task_func(['8.8.8.8', '8.8.4.4'])\n    - {'8.8.8.8': 'dns.google', '8.8.4.4': 'dns.google'}\n\n    \"\"\"\n    hostname_dict = {}\n    for ip in ip_addresses:\n        try:\n            hostname = socket.gethostbyaddr(ip)[0]\n            hostname_dict[ip] = hostname\n        except socket.herror:\n            hostname_dict[ip] = None\n    return hostname_dict", "raw_solution": "import re\nimport socket\n\ndef task_func(ip_addresses: list) -> dict:\n    \"\"\"\n    Dee gaee ip_addresses (IP address) kee ek list, yah function ek dictionary lautaatee hai jo har valid ip_addresses ko uske\n    respective hostname se map kartee hai. Yadi hostname nirdhaarit naheen kiya jaa sakata hai, to value None hogee.\n\n    Params:\n    - ip_addresses (list): ip_addresses kee ek list.\n\n    Returns:\n        dict: ek dictionary jisamen ip_addresses keys hain aur unake hostnames values hain. Yadi hostname nirdhaarit naheen kiya jaa sakata hai,\n        to value None hogee.\n\n    Reqs:\n    - re\n    - socket\n\n    Examples:\n    - >>> task_func(['8.8.8.8', '8.8.4.4'])\n    - {'8.8.8.8': 'dns.google', '8.8.4.4': 'dns.google'}\n\n    \"\"\"\n    hostname_dict = {}\n    for ip in ip_addresses:\n        try:\n            hostname = socket.gethostbyaddr(ip)[0]\n            hostname_dict[ip] = hostname\n        except socket.herror:\n            hostname_dict[ip] = None\n    return hostname_dict\n```\n"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import defaultdict\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Title' and 'Content' columns.\")\n    \n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    keyword_pattern = re.compile(r'\\b(like|what)\\b', re.IGNORECASE)\n    word_freq = defaultdict(int)\n    \n    for index, row in df.iterrows():\n        title_match = keyword_pattern.search(row['Title'])\n        content_match = keyword_pattern.search(row['Content'])\n        \n        if title_match or content_match:\n            content_words = nltk.word_tokenize(row['Content'])\n            for word in content_words:\n                if word.isalpha() and word not in punctuation:\n                    word_freq[word.lower()] += 1\n    \n    return dict(word_freq)", "raw_solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import defaultdict\n\nnltk.download('punkt')\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Title' and 'Content' columns.\")\n    \n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    keyword_pattern = re.compile(r'\\b(like|what)\\b', re.IGNORECASE)\n    word_freq = defaultdict(int)\n    \n    for index, row in df.iterrows():\n        title_match = keyword_pattern.search(row['Title'])\n        content_match = keyword_pattern.search(row['Content'])\n        \n        if title_match or content_match:\n            content_words = nltk.word_tokenize(row['Content'])\n            for word in content_words:\n                if word.isalpha() and word not in punctuation:\n                    word_freq[word.lower()] += 1\n    \n    return dict(word_freq)\n```\n"}
{"task_id": "BigCodeBench/178", "solution": "import re\nimport json\nimport urllib.request\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(ip_address):\n    \"\"\"\n    JSON response se public IP pata prapt karein jismein IP pata shaamil hai.\n\n    Notes:\n        function ko jaanchana hoga ki diya gaya IP pata valid hai ya nahi.\n        Yadi IP pata valid nahi hai, to function 'amnya IP pata prapt hua' lautaega.\n\n    Params:\n    - ip_address (str): IP pate wali JSON-swaroopit string.\n\n    Returns:\n        str: public IP pata.\n\n    Reqs:\n    - re\n    - urllib.request\n    - json\n\n    Examples:\n    - >>> ip_address = '{\"ip\": \"192.168.1.1\"}'\n    - >>> task_func(ip_address)\n    - '192.168.1.1'\n\n    \"\"\"\n    # Check if the input is a valid IP address\n    if re.match(IP_REGEX, ip_address):\n        return ip_address\n\n    # Fetch the public IP address from an API\n    try:\n        with urllib.request.urlopen('https://api.ipify.org?format=json') as response:\n            data = response.read()\n            json_data = json.loads(data)\n            return json_data.get('ip', 'No IP found')\n    except Exception as e:\n        return f\"Error: {e}\"", "raw_solution": "import re\nimport json\nimport urllib.request\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef task_func(ip_address):\n    \"\"\"\n    JSON response se public IP pata prapt karein jismein IP pata shaamil hai.\n\n    Notes:\n        function ko jaanchana hoga ki diya gaya IP pata valid hai ya nahi.\n        Yadi IP pata valid nahi hai, to function 'amnya IP pata prapt hua' lautaega.\n\n    Params:\n    - ip_address (str): IP pate wali JSON-swaroopit string.\n\n    Returns:\n        str: public IP pata.\n\n    Reqs:\n    - re\n    - urllib.request\n    - json\n\n    Examples:\n    - >>> ip_address = '{\"ip\": \"192.168.1.1\"}'\n    - >>> task_func(ip_address)\n    - '192.168.1.1'\n\n    \"\"\"\n    # Check if the input is a valid IP address\n    if re.match(IP_REGEX, ip_address):\n        return ip_address\n\n    # Fetch the public IP address from an API\n    try:\n        with urllib.request.urlopen('https://api.ipify.org?format=json') as response:\n            data = response.read()\n            json_data = json.loads(data)\n            return json_data.get('ip', 'No IP found')\n    except Exception as e:\n        return f\"Error: {e}\"\n\n# Test cases"}
{"task_id": "BigCodeBench/179", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ndef task_func(df):\n    # Check if required columns exist\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        fig, ax = plt.subplots()\n        return ax\n\n    # Filter rows where 'Title' contains 'kaise' or 'kya'\n    filtered_df = df[df['Title'].apply(lambda x: bool(re.search(r'\\bkaise\\b|\\bkya\\b', x)))]\n\n    # Check if there are any matching rows\n    if filtered_df.empty:\n        fig, ax = plt.subplots()\n        return ax\n\n    # Combine 'Title' and 'Content' into a single text column\n    combined_text = filtered_df['Title'] + ' ' + filtered_df['Content']\n\n    # Calculate TF-IDF scores\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(combined_text)\n\n    # Calculate mean TF-IDF scores for each row\n    mean_tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=1)\n\n    # Plot the TF-IDF scores\n    fig, ax = plt.subplots()\n    ax.plot(mean_tfidf_scores, marker='o')\n    ax.set_ylabel('TF-IDF Score')\n    ax.set_xticks(range(len(filtered_df)))\n    ax.set_xticklabels(filtered_df['Title'], rotation=90, ha='center')\n    ax.set_title('TF-IDF Scores of Articles with \"kaise\" or \"kya\" in Title')\n\n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ndef task_func(df):\n    # Check if required columns exist\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        fig, ax = plt.subplots()\n        return ax\n\n    # Filter rows where 'Title' contains 'kaise' or 'kya'\n    filtered_df = df[df['Title'].apply(lambda x: bool(re.search(r'\\bkaise\\b|\\bkya\\b', x)))]\n\n    # Check if there are any matching rows\n    if filtered_df.empty:\n        fig, ax = plt.subplots()\n        return ax\n\n    # Combine 'Title' and 'Content' into a single text column\n    combined_text = filtered_df['Title'] + ' ' + filtered_df['Content']\n\n    # Calculate TF-IDF scores\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(combined_text)\n\n    # Calculate mean TF-IDF scores for each row\n    mean_tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=1)\n\n    # Plot the TF-IDF scores\n    fig, ax = plt.subplots()\n    ax.plot(mean_tfidf_scores, marker='o')\n    ax.set_ylabel('TF-IDF Score')\n    ax.set_xticks(range(len(filtered_df)))\n    ax.set_xticklabels(filtered_df['Title'], rotation=90, ha='center')\n    ax.set_title('TF-IDF Scores of Articles with \"kaise\" or \"kya\" in Title')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/180", "solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The file {img_path} does not exist.\")\n    \n    # Load the image\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    results = []\n    for scale_factor in scale_factors:\n        # Resize the image\n        scaled_img = resize(img_array, (int(img_array.shape[0] * scale_factor), int(img_array.shape[1] * scale_factor)), anti_aliasing=True)\n        \n        # Plot the image\n        fig, ax = plt.subplots()\n        ax.imshow(scaled_img)\n        ax.set_title(f'Scale factor: {scale_factor} ({scaled_img.shape[0]}, {scaled_img.shape[1]}, {scaled_img.shape[2]})')\n        results.append((ax, scaled_img))\n    \n    return results", "raw_solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The file {img_path} does not exist.\")\n    \n    # Load the image\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    results = []\n    for scale_factor in scale_factors:\n        # Resize the image\n        scaled_img = resize(img_array, (int(img_array.shape[0] * scale_factor), int(img_array.shape[1] * scale_factor)), anti_aliasing=True)\n        \n        # Plot the image\n        fig, ax = plt.subplots()\n        ax.imshow(scaled_img)\n        ax.set_title(f'Scale factor: {scale_factor} ({scaled_img.shape[0]}, {scaled_img.shape[1]}, {scaled_img.shape[2]})')\n        results.append((ax, scaled_img))\n    \n    return results\n```\n"}
{"task_id": "BigCodeBench/181", "solution": "from django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\ndef task_func(data, min_delay, max_delay):\n    \"\"\"\n    Ek random delay ke baad, network ki latency ka anukaran karne ke liye json data ke saath ek Django HttpResponse banaen.\n\n    Params:\n    - data (str): response body mein shaamil kiya jaane vaala data.\n    - min_delay (int): second mein min_delay.\n    - max_delay (int): second mein maximum delay.\n\n    Returns:\n        HttpResponse: json data ke saath ek Django HttpResponse.\n\n    Reqs:\n    - django\n    - random\n    - time\n\n    Examples:\n    - >>> import json\n    - >>> random.seed(0)\n    - >>> response = task_func(json.dumps({\"Sample-Key\": \"Sample-Value\"}), 1, 5)\n    - >>> response.status_code\n    - 200\n    - >>> json.loads(response.content)\n    - {\"Sample-Key\": \"Sample-Value\"}\n\n    \"\"\"\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n    return HttpResponse(data, content_type='application/json')", "raw_solution": "from django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\n\ndef task_func(data, min_delay, max_delay):\n    \"\"\"\n    Ek random delay ke baad, network ki latency ka anukaran karne ke liye json data ke saath ek Django HttpResponse banaen.\n\n    Params:\n    - data (str): response body mein shaamil kiya jaane vaala data.\n    - min_delay (int): second mein min_delay.\n    - max_delay (int): second mein maximum delay.\n\n    Returns:\n        HttpResponse: json data ke saath ek Django HttpResponse.\n\n    Reqs:\n    - django\n    - random\n    - time\n\n    Examples:\n    - >>> import json\n    - >>> random.seed(0)\n    - >>> response = task_func(json.dumps({\"Sample-Key\": \"Sample-Value\"}), 1, 5)\n    - >>> response.status_code\n    - 200\n    - >>> json.loads(response.content)\n    - {\"Sample-Key\": \"Sample-Value\"}\n\n    \"\"\"\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n    return HttpResponse(data, content_type='application/json')\n```\n"}
{"task_id": "BigCodeBench/182", "solution": "import re\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\ndef task_func(df):\n    # Define the keywords to look for\n    keywords = ['kaise', 'kya']\n\n    # Create a new column in the DataFrame to indicate if the title or content contains the keywords\n    df['Contains Keywords'] = df['Title'].apply(lambda x: any(re.search(keyword, x, re.IGNORECASE) for keyword in keywords)) | \\\n                             df['Content'].apply(lambda x: any(re.search(keyword, x, re.IGNORECASE) for keyword in keywords))\n\n    # Filter the DataFrame to only include rows that contain the keywords\n    filtered_df = df[df['Contains Keywords']]\n\n    # Extract the content from the filtered DataFrame\n    content = filtered_df['Content'].tolist()\n\n    # Convert the content into a matrix of token counts\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(content)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n\n    # Return the cluster labels for the filtered content\n    return kmeans.labels_.tolist()", "raw_solution": "import re\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ndef task_func(df):\n    # Define the keywords to look for\n    keywords = ['kaise', 'kya']\n\n    # Create a new column in the DataFrame to indicate if the title or content contains the keywords\n    df['Contains Keywords'] = df['Title'].apply(lambda x: any(re.search(keyword, x, re.IGNORECASE) for keyword in keywords)) | \\\n                             df['Content'].apply(lambda x: any(re.search(keyword, x, re.IGNORECASE) for keyword in keywords))\n\n    # Filter the DataFrame to only include rows that contain the keywords\n    filtered_df = df[df['Contains Keywords']]\n\n    # Extract the content from the filtered DataFrame\n    content = filtered_df['Content'].tolist()\n\n    # Convert the content into a matrix of token counts\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(content)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n\n    # Return the cluster labels for the filtered content\n    return kmeans.labels_.tolist()\n\n# Example usage:\ndf_sample = pd.DataFrame({\n    'Title': ['How to code?', 'What is Python?', 'The art of programming', 'How to cook?', 'What is life?'],\n    'Content': ['This is a tutorial about coding...', 'Python is a programming language...',\n                'Programming is an art...', 'This is a cooking tutorial...', 'Life is complicated...']\n})\n"}
{"task_id": "BigCodeBench/183", "solution": "from django.http import HttpResponse\nimport uuid\ndef task_func(data):\n    \"\"\"\n    json data ke saath ek django HttpResponse banaen, aur requests ko track karne ke lie HTTP headers mein ek uuid shaamil karen.\n\n    Params:\n    - data (str): response body mein shaamil kie jaane vaala json-swaroopit data.\n\n    Returns:\n        HttpResponse: json data aur uuid ke saath ek django HttpResponse.\n\n    Reqs:\n    - django\n    - uuid\n\n    Examples:\n    - >>> import json\n    - >>> response = task_func(json.dumps({\"Sample-Key\": \"Sample-Value\"}))\n    - >>> response.has_key('uuid')\n    - True\n\n    \"\"\"\n    # Generate a unique UUID\n    request_id = str(uuid.uuid4())\n\n    # Create an HttpResponse with the given data\n    response = HttpResponse(data, content_type='application/json')\n\n    # Add the UUID to the response headers\n    response['uuid'] = request_id\n\n    return response", "raw_solution": "from django.http import HttpResponse\nimport uuid\n\ndef task_func(data):\n    \"\"\"\n    json data ke saath ek django HttpResponse banaen, aur requests ko track karne ke lie HTTP headers mein ek uuid shaamil karen.\n\n    Params:\n    - data (str): response body mein shaamil kie jaane vaala json-swaroopit data.\n\n    Returns:\n        HttpResponse: json data aur uuid ke saath ek django HttpResponse.\n\n    Reqs:\n    - django\n    - uuid\n\n    Examples:\n    - >>> import json\n    - >>> response = task_func(json.dumps({\"Sample-Key\": \"Sample-Value\"}))\n    - >>> response.has_key('uuid')\n    - True\n\n    \"\"\"\n    # Generate a unique UUID\n    request_id = str(uuid.uuid4())\n\n    # Create an HttpResponse with the given data\n    response = HttpResponse(data, content_type='application/json')\n\n    # Add the UUID to the response headers\n    response['uuid'] = request_id\n\n    return response\n```\n"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    def preprocess_text(text):\n        # Remove punctuation and numbers\n        text = re.sub(r'[^\\w\\s]', '', text)\n        text = re.sub(r'\\d+', '', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n\n    # Apply preprocessing to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n\n    # Vectorize the text data\n    vectorizer = CountVectorizer()\n    vectorized_data = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert the result to a DataFrame\n    vectorized_df = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return vectorized_df", "raw_solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    def preprocess_text(text):\n        # Remove punctuation and numbers\n        text = re.sub(r'[^\\w\\s]', '', text)\n        text = re.sub(r'\\d+', '', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n\n    # Apply preprocessing to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n\n    # Vectorize the text data\n    vectorizer = CountVectorizer()\n    vectorized_data = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert the result to a DataFrame\n    vectorized_df = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return vectorized_df\n```\n"}
{"task_id": "BigCodeBench/185", "solution": "import pandas as pd\nimport numpy as np\nimport folium\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Ek city kee soochee ke lie marker ke saath ek map banaen, jahaan coordinates diye gae paraas ke bheetar randomly utpann hote hain.\n\n    Params:\n    - dic (dict): 'Lon' aur 'Lat' dic ke saath shabdakosh, pratyek ek tapal (min, max) coordinate seema ke lie.\n    - Default: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - cities (list): city ke names kee soochee. Default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n    Returns:\n        tuple: Ek tapal jisamen (folium.Map, pandas.DataFrame) shaamil hai.\n        DataFrame mein 'shahar', 'longitude', aur 'latitude' column shaamil hain.\n\n    Reqs:\n    - pandas\n    - numpy\n    - folium\n\n    Raises:\n    - valueerror: Yadi 'Lon' ya 'Lat' kunjiyaan shabdakosh mein gum hain, ya yadi unake maan tuples nahin hain.\n\n    Examples:\n    - >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - >>> map_obj, city_data = task_func(dic)\n\n    \"\"\"\n    # Check if the dictionary contains valid Lon and Lat ranges\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys.\")\n    if not all(isinstance(dic[key], tuple) and len(dic[key]) == 2 for key in ['Lon', 'Lat']):\n        raise ValueError(\"Lon and Lat must be tuples with two elements each.\")\n\n    # Create a map centered at the mean of the Lon and Lat ranges\n    lon_range = (dic['Lon'][0] + dic['Lon'][1]) / 2\n    lat_range = (dic['Lat'][0] + dic['Lat'][1]) / 2\n    map_obj = folium.Map(location=[lat_range, lon_range], zoom_start=2)\n\n    # Generate random coordinates for each city\n    city_data = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        folium.Marker([lat, lon], tooltip=city).add_to(map_obj)\n        city_data.append({'shahar': city, 'longitude': lon, 'latitude': lat})\n\n    # Create a DataFrame from the city data\n    city_df = pd.DataFrame(city_data)\n\n    return map_obj, city_df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport folium\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Ek city kee soochee ke lie marker ke saath ek map banaen, jahaan coordinates diye gae paraas ke bheetar randomly utpann hote hain.\n\n    Params:\n    - dic (dict): 'Lon' aur 'Lat' dic ke saath shabdakosh, pratyek ek tapal (min, max) coordinate seema ke lie.\n    - Default: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - cities (list): city ke names kee soochee. Default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n    Returns:\n        tuple: Ek tapal jisamen (folium.Map, pandas.DataFrame) shaamil hai.\n        DataFrame mein 'shahar', 'longitude', aur 'latitude' column shaamil hain.\n\n    Reqs:\n    - pandas\n    - numpy\n    - folium\n\n    Raises:\n    - valueerror: Yadi 'Lon' ya 'Lat' kunjiyaan shabdakosh mein gum hain, ya yadi unake maan tuples nahin hain.\n\n    Examples:\n    - >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - >>> map_obj, city_data = task_func(dic)\n\n    \"\"\"\n    # Check if the dictionary contains valid Lon and Lat ranges\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys.\")\n    if not all(isinstance(dic[key], tuple) and len(dic[key]) == 2 for key in ['Lon', 'Lat']):\n        raise ValueError(\"Lon and Lat must be tuples with two elements each.\")\n\n    # Create a map centered at the mean of the Lon and Lat ranges\n    lon_range = (dic['Lon'][0] + dic['Lon'][1]) / 2\n    lat_range = (dic['Lat'][0] + dic['Lat'][1]) / 2\n    map_obj = folium.Map(location=[lat_range, lon_range], zoom_start=2)\n\n    # Generate random coordinates for each city\n    city_data = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        folium.Marker([lat, lon], tooltip=city).add_to(map_obj)\n        city_data.append({'shahar': city, 'longitude': lon, 'latitude': lat})\n\n    # Create a DataFrame from the city data\n    city_df = pd.DataFrame(city_data)\n\n    return map_obj, city_df\n```\n"}
{"task_id": "BigCodeBench/186", "solution": "from geopy.distance import geodesic\nimport folium\ndef task_func(dic):\n    if not dic:\n        raise ValueError(\"Input dictionary is empty.\")\n    \n    # Initialize the map\n    folium_map = folium.Map(location=[0, 0], zoom_start=2)\n\n    # Add markers to the map\n    for name, coords in dic.items():\n        folium.Marker([coords['Lat'], coords['Lon']], popup=name).add_to(folium_map)\n    \n    # Calculate distances\n    distances = {}\n    for name1, coords1 in dic.items():\n        for name2, coords2 in dic.items():\n            if name1 != name2:\n                dist = geodesic((coords1['Lat'], coords1['Lon']), (coords2['Lat'], coords2['Lon'])).kilometers\n                distances[(name1, name2)] = dist\n\n    return folium_map, distances", "raw_solution": "from geopy.distance import geodesic\nimport folium\n\ndef task_func(dic):\n    if not dic:\n        raise ValueError(\"Input dictionary is empty.\")\n    \n    # Initialize the map\n    folium_map = folium.Map(location=[0, 0], zoom_start=2)\n\n    # Add markers to the map\n    for name, coords in dic.items():\n        folium.Marker([coords['Lat'], coords['Lon']], popup=name).add_to(folium_map)\n    \n    # Calculate distances\n    distances = {}\n    for name1, coords1 in dic.items():\n        for name2, coords2 in dic.items():\n            if name1 != name2:\n                dist = geodesic((coords1['Lat'], coords1['Lon']), (coords2['Lat'], coords2['Lon'])).kilometers\n                distances[(name1, name2)] = dist\n\n    return folium_map, distances\n\n# Example usage:\n# result = task_func({'Place1': {'Lat': 0, 'Lon': 0}, 'Place2': {'Lat': 0, 'Lon': 1}})\n# print(isinstance(result, tuple) and len(result) == 2)\n# print(isinstance(result[0], folium.Map) and isinstance(result[1], dict))\n```\n"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Create a GeoPandas DataFrame for a list of cities with randomly generated coordinates based on specified ranges.\n\n    Params:\n    - dic (dict): Dictionary with 'Lon' and 'Lat' keys, each a tuple (min, max) for coordinate range.\n    - default: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - cities (list): List of city names. default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n    Returns:\n        GeoDataFrame: A GeoPandas DataFrame containing 'City' and 'Coordinates' (Point objects).\n\n    Reqs:\n    - numpy\n    - geopandas\n    - shapely.geometry\n\n    Raises:\n    - ValueError: If 'Lon' or 'Lat' keys are missing in the dictionary, or if their values are not tuples.\n\n    Examples:\n    - >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - >>> gdf = task_func(dic)\n\n    \"\"\"\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"Values for 'Lon' and 'Lat' must be tuples.\")\n\n    data = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        point = Point(lon, lat)\n        data.append({'City': city, 'Coordinates': point})\n\n    gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n\n    return gdf", "raw_solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Create a GeoPandas DataFrame for a list of cities with randomly generated coordinates based on specified ranges.\n\n    Params:\n    - dic (dict): Dictionary with 'Lon' and 'Lat' keys, each a tuple (min, max) for coordinate range.\n    - default: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - cities (list): List of city names. default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n    Returns:\n        GeoDataFrame: A GeoPandas DataFrame containing 'City' and 'Coordinates' (Point objects).\n\n    Reqs:\n    - numpy\n    - geopandas\n    - shapely.geometry\n\n    Raises:\n    - ValueError: If 'Lon' or 'Lat' keys are missing in the dictionary, or if their values are not tuples.\n\n    Examples:\n    - >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    - >>> gdf = task_func(dic)\n\n    \"\"\"\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"Values for 'Lon' and 'Lat' must be tuples.\")\n\n    data = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        point = Point(lon, lat)\n        data.append({'City': city, 'Coordinates': point})\n\n    gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n\n    return gdf\n```\n"}
{"task_id": "BigCodeBench/188", "solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\ndef task_func(dic):\n    geolocator = Photon(user_agent=\"geoapiExercises\")\n    map_ = folium.Map(location=[0, 0], zoom_start=2)\n\n    for place, location in dic.items():\n        if isinstance(location, dict) and 'Lat' in location and 'Lon' in location:\n            folium.Marker([location['Lat'], location['Lon']], tooltip=place).add_to(map_)\n        elif isinstance(location, str):\n            location_data = geolocator.geocode(location)\n            if location_data:\n                folium.Marker([location_data.latitude, location_data.longitude], tooltip=place).add_to(map_)\n    \n    return map_", "raw_solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\n\ndef task_func(dic):\n    geolocator = Photon(user_agent=\"geoapiExercises\")\n    map_ = folium.Map(location=[0, 0], zoom_start=2)\n\n    for place, location in dic.items():\n        if isinstance(location, dict) and 'Lat' in location and 'Lon' in location:\n            folium.Marker([location['Lat'], location['Lon']], tooltip=place).add_to(map_)\n        elif isinstance(location, str):\n            location_data = geolocator.geocode(location)\n            if location_data:\n                folium.Marker([location_data.latitude, location_data.longitude], tooltip=place).add_to(map_)\n    \n    return map_\n\n# Example usage:\nlocations = {'Place1': {'Lat': 0, 'Lon': 0}, 'Place2': 'New York, USA'}\nresult = task_func(locations)"}
{"task_id": "BigCodeBench/189", "solution": "import re\nimport json\nimport requests\ndef task_func(data_url: str) -> list:\n    try:\n        response = requests.get(data_url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(\"Invalid URL input\") from e\n\n    data = response.json()\n    names = data.get(\"names\", [])\n    \n    # Using regex to filter out names enclosed in square brackets\n    pattern = re.compile(r'\\[.*?\\]')\n    filtered_names = [name for name in names if not pattern.search(name)]\n\n    return filtered_names", "raw_solution": "import re\nimport json\nimport requests\n\ndef task_func(data_url: str) -> list:\n    try:\n        response = requests.get(data_url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(\"Invalid URL input\") from e\n\n    data = response.json()\n    names = data.get(\"names\", [])\n    \n    # Using regex to filter out names enclosed in square brackets\n    pattern = re.compile(r'\\[.*?\\]')\n    filtered_names = [name for name in names if not pattern.search(name)]\n\n    return filtered_names\n\n# Example usage:\n# Assuming the URL returns a JSON response with the structure {\"names\": [\"John\", \"[Adam]\", \"Eve\"]}\n# print(task_func(\"https://api.example.com/other_data\"))\n# Output: ['John', 'Eve']\n```\n"}
{"task_id": "BigCodeBench/190", "solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\ndef task_func(csv_input):\n    # Connect to SQLite database\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    # Create table\n    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {TABLE_NAME} (id INTEGER PRIMARY KEY, name TEXT)\")\n\n    # Read CSV data\n    if isinstance(csv_input, StringIO):\n        df = pd.read_csv(csv_input)\n    else:\n        df = pd.read_csv(csv_input)\n\n    # Insert data into table\n    for _, row in df.iterrows():\n        cursor.execute(f\"INSERT INTO {TABLE_NAME} (id, name) VALUES (?, ?)\", (row['id'], row['name']))\n\n    # Commit changes\n    conn.commit()\n\n    # Query data and load into DataFrame\n    query = f\"SELECT * FROM {TABLE_NAME}\"\n    df_loaded = pd.read_sql_query(query, conn)\n\n    # Close connection\n    conn.close()\n\n    return df_loaded", "raw_solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\n\n# Constants\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\n\n\ndef task_func(csv_input):\n    # Connect to SQLite database\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    # Create table\n    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {TABLE_NAME} (id INTEGER PRIMARY KEY, name TEXT)\")\n\n    # Read CSV data\n    if isinstance(csv_input, StringIO):\n        df = pd.read_csv(csv_input)\n    else:\n        df = pd.read_csv(csv_input)\n\n    # Insert data into table\n    for _, row in df.iterrows():\n        cursor.execute(f\"INSERT INTO {TABLE_NAME} (id, name) VALUES (?, ?)\", (row['id'], row['name']))\n\n    # Commit changes\n    conn.commit()\n\n    # Query data and load into DataFrame\n    query = f\"SELECT * FROM {TABLE_NAME}\"\n    df_loaded = pd.read_sql_query(query, conn)\n\n    # Close connection\n    conn.close()\n\n    return df_loaded\n```\n"}
{"task_id": "BigCodeBench/191", "solution": "import random\nfrom scipy import stats\ndef task_func(animals, mean):\n    sales = {animal: 0 for animal in animals}\n    for _ in range(stats.poisson.rvs(mean)):\n        chosen_animal = random.choice(animals)\n        sales[chosen_animal] += 1\n    return sales\nANIMALS = ['Dog', 'Cat', 'Bird', 'Fish', 'Hamster']", "raw_solution": "import random\nfrom scipy import stats\n\ndef task_func(animals, mean):\n    sales = {animal: 0 for animal in animals}\n    for _ in range(stats.poisson.rvs(mean)):\n        chosen_animal = random.choice(animals)\n        sales[chosen_animal] += 1\n    return sales\n\n# Test cases\nANIMALS = ['Dog', 'Cat', 'Bird', 'Fish', 'Hamster']\nsales = task_func(ANIMALS, 120)"}
{"task_id": "BigCodeBench/192", "solution": "import re\nimport smtplib\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    \"\"\"\n    Ek string se sabhi names ko nikalen jo square brackets mein band nahi hain aur names ko ek email mein bhejen.\n\n    Notes:\n        Email mein message \"Subject: Extracted Names\n\n\" ke praroop mein hai, jismein nikala gaya naam \"\nJosie Smith\nMugsy Dog Smith\" hai.\n\n    Params:\n    - text (str): Wah text jisse naam nikalne hain.\n    - smtp_server (str): Email bhejne ke liye upyog kiya jaane wala SMTP server.\n    - smtp_port (int): SMTP server ke liye upyog kiya jaane wala port.\n    - email_address (str): Wah email pata jisse email bhejna hai.\n    - email_password (str): Email pate ka password.\n    - recepient_address (str): Recepient email pata.\n\n    Returns:\n        list: Nikale gaye names ki ek list.\n\n    Reqs:\n    - re\n    - smtplib\n\n    Examples:\n    - >>> from unittest.mock import MagicMock\n    - >>> mock_smtp_instance = MagicMock()\n    - >>> mock_smtp = MagicMock(return_value=mock_smtp_instance)\n    - >>> task_func(text=\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\", smtp=mock_smtp)\n    - ['Josie Smith', 'Mugsy Dog Smith']\n\n    \"\"\"\n    # Extract names using regex\n    names = re.findall(r'\\b\\w+\\s\\w+\\b', text)\n\n    # Create email content\n    email_content = \"Subject: Extracted Names\\n\\n\" + \"\\n\".join(names)\n\n    # Send email\n    if smtp:\n        smtp_instance = smtp(smtp_server, smtp_port)\n        smtp_instance.starttls()\n        smtp_instance.login(email_address, email_password)\n        smtp_instance.sendmail(email_address, recepient_address, email_content)\n        smtp_instance.quit()\n    else:\n        with smtplib.SMTP(smtp_server, smtp_port) as server:\n            server.starttls()\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recepient_address, email_content)\n\n    return names", "raw_solution": "import re\nimport smtplib\n\n# Constants\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\n\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    \"\"\"\n    Ek string se sabhi names ko nikalen jo square brackets mein band nahi hain aur names ko ek email mein bhejen.\n\n    Notes:\n        Email mein message \"Subject: Extracted Names\n\n\" ke praroop mein hai, jismein nikala gaya naam \"\nJosie Smith\nMugsy Dog Smith\" hai.\n\n    Params:\n    - text (str): Wah text jisse naam nikalne hain.\n    - smtp_server (str): Email bhejne ke liye upyog kiya jaane wala SMTP server.\n    - smtp_port (int): SMTP server ke liye upyog kiya jaane wala port.\n    - email_address (str): Wah email pata jisse email bhejna hai.\n    - email_password (str): Email pate ka password.\n    - recepient_address (str): Recepient email pata.\n\n    Returns:\n        list: Nikale gaye names ki ek list.\n\n    Reqs:\n    - re\n    - smtplib\n\n    Examples:\n    - >>> from unittest.mock import MagicMock\n    - >>> mock_smtp_instance = MagicMock()\n    - >>> mock_smtp = MagicMock(return_value=mock_smtp_instance)\n    - >>> task_func(text=\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\", smtp=mock_smtp)\n    - ['Josie Smith', 'Mugsy Dog Smith']\n\n    \"\"\"\n    # Extract names using regex\n    names = re.findall(r'\\b\\w+\\s\\w+\\b', text)\n\n    # Create email content\n    email_content = \"Subject: Extracted Names\\n\\n\" + \"\\n\".join(names)\n\n    # Send email\n    if smtp:\n        smtp_instance = smtp(smtp_server, smtp_port)\n        smtp_instance.starttls()\n        smtp_instance.login(email_address, email_password)\n        smtp_instance.sendmail(email_address, recepient_address, email_content)\n        smtp_instance.quit()\n    else:\n        with smtplib.SMTP(smtp_server, smtp_port) as server:\n            server.starttls()\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recepient_address, email_content)\n\n    return names\n```\n"}
{"task_id": "BigCodeBench/193", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\ndef task_func(rows, columns):\n    data = {\n        f'col{i}': [\n            choice(DATA_TYPES)(\n                *[\n                    choice([\n                        ''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), 5)),\n                        np.random.randint(0, 10),\n                        np.random.rand() * 9,\n                        [np.random.randint(0, 10) for _ in range(np.random.randint(1, 6))],\n                        tuple(np.random.randint(0, 10, np.random.randint(1, 6))),\n                        {f'key{i}': np.random.randint(0, 10) for i in range(np.random.randint(1, 6))},\n                        set(np.random.choice(np.random.randint(0, 10, np.random.randint(1, 6)), np.random.randint(1, 6), replace=False))\n                    ])\n                    for _ in range(rows)\n                ]\n            )\n            for _ in range(columns)\n        ]\n        for i in range(columns)\n    }\n\n    df = pd.DataFrame(data)\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import choice\n\n# Constants\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\n\n\ndef task_func(rows, columns):\n    data = {\n        f'col{i}': [\n            choice(DATA_TYPES)(\n                *[\n                    choice([\n                        ''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), 5)),\n                        np.random.randint(0, 10),\n                        np.random.rand() * 9,\n                        [np.random.randint(0, 10) for _ in range(np.random.randint(1, 6))],\n                        tuple(np.random.randint(0, 10, np.random.randint(1, 6))),\n                        {f'key{i}': np.random.randint(0, 10) for i in range(np.random.randint(1, 6))},\n                        set(np.random.choice(np.random.randint(0, 10, np.random.randint(1, 6)), np.random.randint(1, 6), replace=False))\n                    ])\n                    for _ in range(rows)\n                ]\n            )\n            for _ in range(columns)\n        ]\n        for i in range(columns)\n    }\n\n    df = pd.DataFrame(data)\n\n    return df\n\n# Example usage:\n# df = task_func(2, 3)\n# print(df.shape)  # Output: (2, 3)\n# print(isinstance(df, pd.DataFrame))  # Output: True\n```\n"}
{"task_id": "BigCodeBench/194", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\ndef task_func(data_size):\n    # Generate random data\n    data = np.random.rand(data_size)\n    \n    # Generate a random color from the list of colors\n    color = np.random.choice(BAR_COLOR)\n    \n    # Plot the histogram\n    plt.hist(data, bins=10, color=color, alpha=0.7)\n    plt.title('Histogram')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return data, color", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\n\ndef task_func(data_size):\n    # Generate random data\n    data = np.random.rand(data_size)\n    \n    # Generate a random color from the list of colors\n    color = np.random.choice(BAR_COLOR)\n    \n    # Plot the histogram\n    plt.hist(data, bins=10, color=color, alpha=0.7)\n    plt.title('Histogram')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return data, color\n```\n"}
{"task_id": "BigCodeBench/195", "solution": "import subprocess\nimport platform\nimport time\ndef task_func(url):\n    \"\"\"\n    default web browser mein ek web page ko background process mein kholen.\n\n    Params:\n    - url (str): webpage ka URL jise khola jaana hai.\n\n    Returns:\n        int: subprocess ka return code.\n\n    Reqs:\n    - subprocess\n    - platform\n    - time\n\n    Examples:\n    - >>> task_func('https://www.google.com')\n    - 0\n\n    \"\"\"\n    # Check the operating system\n    if platform.system() == \"Windows\":\n        # For Windows, use start command\n        command = [\"start\", \"chrome\", url]\n    else:\n        # For Unix-based systems, use open command\n        command = [\"open\", url]\n\n    # Start the process\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # Wait for the process to complete\n    process.wait()\n\n    # Return the return code of the process\n    return process.returncode", "raw_solution": "import subprocess\nimport platform\nimport time\n\ndef task_func(url):\n    \"\"\"\n    default web browser mein ek web page ko background process mein kholen.\n\n    Params:\n    - url (str): webpage ka URL jise khola jaana hai.\n\n    Returns:\n        int: subprocess ka return code.\n\n    Reqs:\n    - subprocess\n    - platform\n    - time\n\n    Examples:\n    - >>> task_func('https://www.google.com')\n    - 0\n\n    \"\"\"\n    # Check the operating system\n    if platform.system() == \"Windows\":\n        # For Windows, use start command\n        command = [\"start\", \"chrome\", url]\n    else:\n        # For Unix-based systems, use open command\n        command = [\"open\", url]\n\n    # Start the process\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # Wait for the process to complete\n    process.wait()\n\n    # Return the return code of the process\n    return process.returncode\n```\nThis script checks the operating system and uses the appropriate command to open the URL in the default web browser. It then waits for the process to complete and returns its return code.\n"}
{"task_id": "BigCodeBench/196", "solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef task_func(length, range_limit=100, seed=0):\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random.seed(seed)\n    data = [random.randint(1, range_limit) for _ in range(length)]\n    data.sort()\n\n    fig, ax = plt.subplots()\n    sns.histplot(data, bins=30, kde=True, ax=ax)\n    ax.set_title('Histogram of Random Numbers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax, data", "raw_solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef task_func(length, range_limit=100, seed=0):\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random.seed(seed)\n    data = [random.randint(1, range_limit) for _ in range(length)]\n    data.sort()\n\n    fig, ax = plt.subplots()\n    sns.histplot(data, bins=30, kde=True, ax=ax)\n    ax.set_title('Histogram of Random Numbers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax, data\n```\n"}
{"task_id": "BigCodeBench/197", "solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(l1, l2, N=10):\n    # Get the N largest elements from both lists\n    largest_l1 = heapq.nlargest(N, l1)\n    largest_l2 = heapq.nlargest(N, l2)\n\n    # Calculate square root and square for both lists\n    sqrt_l1 = [math.sqrt(x) for x in largest_l1]\n    sqrt_l2 = [math.sqrt(x) for x in largest_l2]\n\n    # Plotting the results\n    fig, ax = plt.subplots()\n    ax.plot(sqrt_l1, label='sqrt_l1', marker='o')\n    ax.plot(sqrt_l2, label='sqrt_l2', marker='x')\n    ax.set_title('Sqrt of Largest Elements')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return ax\nl1 = [99, 86, 90, 70, 86, 95, 56, 98, 80, 81]\nl2 = [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]", "raw_solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(l1, l2, N=10):\n    # Get the N largest elements from both lists\n    largest_l1 = heapq.nlargest(N, l1)\n    largest_l2 = heapq.nlargest(N, l2)\n\n    # Calculate square root and square for both lists\n    sqrt_l1 = [math.sqrt(x) for x in largest_l1]\n    sqrt_l2 = [math.sqrt(x) for x in largest_l2]\n\n    # Plotting the results\n    fig, ax = plt.subplots()\n    ax.plot(sqrt_l1, label='sqrt_l1', marker='o')\n    ax.plot(sqrt_l2, label='sqrt_l2', marker='x')\n    ax.set_title('Sqrt of Largest Elements')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return ax\n\n# Example usage:\nl1 = [99, 86, 90, 70, 86, 95, 56, 98, 80, 81]\nl2 = [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\nax = task_func(l1, l2)\nplt.show()\n```\nThis script uses the `heapq` module to find the N largest elements in each list, the `math` module to calculate the square root, and `matplotlib.pyplot` to plot the results. The function returns a matplotlib Axes object, which can be used to further customize the plot if needed.\n"}
{"task_id": "BigCodeBench/198", "solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n\n    # Sort the data\n    sorted_data = np.sort(data)\n\n    # Calculate average\n    avg = statistics.mean(sorted_data)\n\n    # Find elements greater than the average\n    greater_avg = sorted_data[sorted_data > avg]\n\n    # Count elements greater than the specified value\n    count = np.sum(greater_avg > value)\n\n    # Plot histogram\n    plt.hist(sorted_data, bins='auto', color='skyblue', alpha=0.7)\n    plt.title('Histogram of Sorted Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return greater_avg, count", "raw_solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n\n    # Sort the data\n    sorted_data = np.sort(data)\n\n    # Calculate average\n    avg = statistics.mean(sorted_data)\n\n    # Find elements greater than the average\n    greater_avg = sorted_data[sorted_data > avg]\n\n    # Count elements greater than the specified value\n    count = np.sum(greater_avg > value)\n\n    # Plot histogram\n    plt.hist(sorted_data, bins='auto', color='skyblue', alpha=0.7)\n    plt.title('Histogram of Sorted Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return greater_avg, count\n\n# Example usage:\ngreater_avg, count = task_func([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 5)"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime) or utc_datetime.tzinfo is None or utc_datetime.tzinfo.utcoffset(utc_datetime) is None:\n        raise ValueError(\"utc_datetime must be a datetime object with timezone information.\")\n\n    set_seed(seed)\n\n    report_data = []\n\n    for city in cities:\n        local_tz = pytz.timezone(timezones[city])\n        local_time = utc_datetime.astimezone(local_tz)\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        report_data.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_condition\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df", "raw_solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime) or utc_datetime.tzinfo is None or utc_datetime.tzinfo.utcoffset(utc_datetime) is None:\n        raise ValueError(\"utc_datetime must be a datetime object with timezone information.\")\n\n    set_seed(seed)\n\n    report_data = []\n\n    for city in cities:\n        local_tz = pytz.timezone(timezones[city])\n        local_time = utc_datetime.astimezone(local_tz)\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        report_data.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_condition\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df\n```\n"}
{"task_id": "BigCodeBench/200", "solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    random_numbers = [random.random() for _ in range(n)]\n    \n    # Calculate the average of the random numbers\n    average = statistics.mean(random_numbers)\n    \n    # Find numbers greater than their average\n    greater_than_avg = [num for num in random_numbers if num > average]\n    \n    # Count how many numbers are greater than or equal to the specified value\n    count = bisect.bisect_left(sorted(random_numbers), value)\n    \n    # Plot the sorted numbers\n    plt.hist(random_numbers, bins=10, alpha=0.7, color='blue')\n    plt.title('Histogram of Random Numbers')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return greater_than_avg, count", "raw_solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    random_numbers = [random.random() for _ in range(n)]\n    \n    # Calculate the average of the random numbers\n    average = statistics.mean(random_numbers)\n    \n    # Find numbers greater than their average\n    greater_than_avg = [num for num in random_numbers if num > average]\n    \n    # Count how many numbers are greater than or equal to the specified value\n    count = bisect.bisect_left(sorted(random_numbers), value)\n    \n    # Plot the sorted numbers\n    plt.hist(random_numbers, bins=10, alpha=0.7, color='blue')\n    plt.title('Histogram of Random Numbers')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return greater_than_avg, count\n\n# Example usage:\n# greater_avg, count = task_func(10, 0.5)\n# print(isinstance(greater_avg, list) and isinstance(count, int))\n# print(len(greater_avg) <= 10)\n```\n"}
{"task_id": "BigCodeBench/201", "solution": "import bisect\nimport statistics\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, column, value):\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in DataFrame.\")\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number.\")\n\n    # Extract the column data\n    data = df[column].to_list()\n\n    # Calculate the average\n    avg = statistics.mean(data)\n\n    # Filter values greater than the average\n    greater_avg = [x for x in data if x > avg]\n\n    # Count values greater than the given value\n    num_greater_value = len([x for x in data if x > value])\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=10, color='blue', alpha=0.7)\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return greater_avg, num_greater_value, ax", "raw_solution": "import bisect\nimport statistics\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, column, value):\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in DataFrame.\")\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number.\")\n\n    # Extract the column data\n    data = df[column].to_list()\n\n    # Calculate the average\n    avg = statistics.mean(data)\n\n    # Filter values greater than the average\n    greater_avg = [x for x in data if x > avg]\n\n    # Count values greater than the given value\n    num_greater_value = len([x for x in data if x > value])\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=10, color='blue', alpha=0.7)\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return greater_avg, num_greater_value, ax\n```\n"}
{"task_id": "BigCodeBench/202", "solution": "import re\nimport json\nfrom collections import Counter\ndef task_func(json_str, top_n=10):\n    # Extract URLs from the JSON string\n    urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', json_str)\n    \n    # Count occurrences of each URL\n    url_counter = Counter(urls)\n    \n    # Get the top N URLs\n    top_urls = dict(url_counter.most_common(top_n))\n    \n    return top_urls\njson_str = '{\"name\": \"John\", \"website\": \"https://www.example.com\", \"url\": \"https://www.example.com\"}'", "raw_solution": "import re\nimport json\nfrom collections import Counter\n\ndef task_func(json_str, top_n=10):\n    # Extract URLs from the JSON string\n    urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', json_str)\n    \n    # Count occurrences of each URL\n    url_counter = Counter(urls)\n    \n    # Get the top N URLs\n    top_urls = dict(url_counter.most_common(top_n))\n    \n    return top_urls\n\n# Example usage:\njson_str = '{\"name\": \"John\", \"website\": \"https://www.example.com\", \"url\": \"https://www.example.com\"}'"}
{"task_id": "BigCodeBench/203", "solution": "import json\nimport smtplib\nimport re\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    \"\"\"\n    JSON-formatted string se recepient ke email address aur names ko extract aur names ko ek email mein bhejen. Bheje gaye message ka format 'Subject: Extracted Names\n\nName1\nName2\n...'.\n\n    Params:\n    - input_data (str): JSON-formatted string jismein recepient ka email pata aur names ki list shaamil hai.\n    - smtp_server (str): email bhejne ke liye upyog karne ke liye SMTP server.\n    - smtp_port (int): SMTP server ke liye upyog karne ke liye port.\n    - email_address (str): vah email pata jisse email bhejna hai.\n    - email_password (str): email address ka password.\n\n    Returns:\n        list: nikaale gaye names ki ek list.\n\n    Reqs:\n    - re\n    - smtplib\n\n    Examples:\n    - >>> from unittest.mock import MagicMock\n    - >>> mock_smtp_instance = MagicMock()\n    - >>> mock_smtp = MagicMock(return_value=mock_smtp_instance)\n    - >>> task_func('{\"recipient\": \"recipient@example.com\", \"names\": [\"Josie Smith\", \"Mugsy Dog Smith\"]}', smtp=mock_smtp)\n    - ['Josie Smith', 'Mugsy Dog Smith']\n\n    \"\"\"\n    if not input_data:\n        raise ValueError(\"Input data is required\")\n\n    data = json.loads(input_data)\n    recipient = data.get('recipient')\n    names = data.get('names')\n\n    if not recipient or not names:\n        raise ValueError(\"Recipient and names are required\")\n\n    email_content = \"Subject: Extracted Names\\n\\n\" + \"\\n\".join(names)\n\n    if smtp:\n        smtp_instance = smtp(smtp_server, smtp_port)\n        smtp_instance.login(email_address, email_password)\n        smtp_instance.sendmail(email_address, recipient, email_content)\n        smtp_instance.quit()\n    else:\n        with smtplib.SMTP(smtp_server, smtp_port) as server:\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recipient, email_content)\n\n    return names", "raw_solution": "import json\nimport smtplib\nimport re\n\n# Constants\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\n\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    \"\"\"\n    JSON-formatted string se recepient ke email address aur names ko extract aur names ko ek email mein bhejen. Bheje gaye message ka format 'Subject: Extracted Names\n\nName1\nName2\n...'.\n\n    Params:\n    - input_data (str): JSON-formatted string jismein recepient ka email pata aur names ki list shaamil hai.\n    - smtp_server (str): email bhejne ke liye upyog karne ke liye SMTP server.\n    - smtp_port (int): SMTP server ke liye upyog karne ke liye port.\n    - email_address (str): vah email pata jisse email bhejna hai.\n    - email_password (str): email address ka password.\n\n    Returns:\n        list: nikaale gaye names ki ek list.\n\n    Reqs:\n    - re\n    - smtplib\n\n    Examples:\n    - >>> from unittest.mock import MagicMock\n    - >>> mock_smtp_instance = MagicMock()\n    - >>> mock_smtp = MagicMock(return_value=mock_smtp_instance)\n    - >>> task_func('{\"recipient\": \"recipient@example.com\", \"names\": [\"Josie Smith\", \"Mugsy Dog Smith\"]}', smtp=mock_smtp)\n    - ['Josie Smith', 'Mugsy Dog Smith']\n\n    \"\"\"\n    if not input_data:\n        raise ValueError(\"Input data is required\")\n\n    data = json.loads(input_data)\n    recipient = data.get('recipient')\n    names = data.get('names')\n\n    if not recipient or not names:\n        raise ValueError(\"Recipient and names are required\")\n\n    email_content = \"Subject: Extracted Names\\n\\n\" + \"\\n\".join(names)\n\n    if smtp:\n        smtp_instance = smtp(smtp_server, smtp_port)\n        smtp_instance.login(email_address, email_password)\n        smtp_instance.sendmail(email_address, recipient, email_content)\n        smtp_instance.quit()\n    else:\n        with smtplib.SMTP(smtp_server, smtp_port) as server:\n            server.login(email_address, email_password)\n            server.sendmail(email_address, recipient, email_content)\n\n    return names\n```\n"}
{"task_id": "BigCodeBench/204", "solution": "import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Calculate mean\n    mean = np.mean(L)\n\n    # Calculate median\n    median = np.median(L)\n\n    # Calculate mode\n    mode_data = Counter(L)\n    mode = mode_data.most_common(1)[0][0]\n\n    # Calculate standard deviation\n    std_dev = np.std(L)\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(L, bins='auto', alpha=0.7, color='blue')\n    ax.set_title('Histogram of L')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"mode\": mode,\n        \"std_dev\": std_dev,\n        \"plot\": ax\n    }\nL = [1, 2, 3, 4, 5, 6, 7, 8, 9]", "raw_solution": "import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    # Calculate mean\n    mean = np.mean(L)\n\n    # Calculate median\n    median = np.median(L)\n\n    # Calculate mode\n    mode_data = Counter(L)\n    mode = mode_data.most_common(1)[0][0]\n\n    # Calculate standard deviation\n    std_dev = np.std(L)\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(L, bins='auto', alpha=0.7, color='blue')\n    ax.set_title('Histogram of L')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"mode\": mode,\n        \"std_dev\": std_dev,\n        \"plot\": ax\n    }\n\n# Example usage\nL = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nstats = task_func(L)"}
{"task_id": "BigCodeBench/205", "solution": "import subprocess\nfrom multiprocessing import Pool\ndef execute_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\ndef task_func(commands):\n    if not commands:\n        return []\n\n    with Pool() as pool:\n        results = pool.map(execute_command, commands)\n\n    return results", "raw_solution": "import subprocess\nfrom multiprocessing import Pool\n\ndef execute_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\n\ndef task_func(commands):\n    if not commands:\n        return []\n\n    with Pool() as pool:\n        results = pool.map(execute_command, commands)\n\n    return results\n\n# Test cases"}
{"task_id": "BigCodeBench/206", "solution": "import csv\nimport json\nimport os\ndef task_func(file_name):\n    if not os.path.exists(file_name):\n        raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n\n    json_file_name = os.path.splitext(file_name)[0] + '.json'\n\n    with open(file_name, mode='r', newline='', encoding='utf-8') as csvfile:\n        reader = csv.DictReader(csvfile)\n        data = [row for row in reader]\n\n    with open(json_file_name, mode='w', encoding='utf-8') as jsonfile:\n        json.dump(data, jsonfile, indent=4)\n\n    return json_file_name", "raw_solution": "import csv\nimport json\nimport os\n\ndef task_func(file_name):\n    if not os.path.exists(file_name):\n        raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n\n    json_file_name = os.path.splitext(file_name)[0] + '.json'\n\n    with open(file_name, mode='r', newline='', encoding='utf-8') as csvfile:\n        reader = csv.DictReader(csvfile)\n        data = [row for row in reader]\n\n    with open(json_file_name, mode='w', encoding='utf-8') as jsonfile:\n        json.dump(data, jsonfile, indent=4)\n\n    return json_file_name\n\n# Example usage:\n# Assuming we have a CSV file at '/tmp/report_example.csv'\n# json_file = task_func('/tmp/report_example.csv')\n# print(json_file)  # Output: '/tmp/report_example.json'\n```\n"}
{"task_id": "BigCodeBench/207", "solution": "import re\nimport requests\ndef task_func(input):\n    \"\"\"\n    input string se ek api endpoint nikaalen, endpoint par ek GET request bhejen, aur json format mein response data lautaen.\n\n    Params:\n    - input (str): input string jismein ek api endpoint shaamil hai.\n\n    Returns:\n        dict: response data.\n\n    Reqs:\n    - re\n    - json\n    - requests\n\n    Examples:\n    - >>> task_func('Fetch data from https://api.example.com/data')\n    - {'key': 'value'}\n\n    \"\"\"\n    # Extract the API endpoint from the input string\n    match = re.search(r'https?://[^\\s]+', input)\n    if not match:\n        raise ValueError(\"No valid API endpoint found in the input string.\")\n    \n    api_endpoint = match.group(0)\n    \n    # Make a GET request to the API endpoint\n    response = requests.get(api_endpoint)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch data from {api_endpoint}. Status code: {response.status_code}\")\n    \n    # Return the response data in JSON format\n    return response.json()", "raw_solution": "import re\nimport requests\n\ndef task_func(input):\n    \"\"\"\n    input string se ek api endpoint nikaalen, endpoint par ek GET request bhejen, aur json format mein response data lautaen.\n\n    Params:\n    - input (str): input string jismein ek api endpoint shaamil hai.\n\n    Returns:\n        dict: response data.\n\n    Reqs:\n    - re\n    - json\n    - requests\n\n    Examples:\n    - >>> task_func('Fetch data from https://api.example.com/data')\n    - {'key': 'value'}\n\n    \"\"\"\n    # Extract the API endpoint from the input string\n    match = re.search(r'https?://[^\\s]+', input)\n    if not match:\n        raise ValueError(\"No valid API endpoint found in the input string.\")\n    \n    api_endpoint = match.group(0)\n    \n    # Make a GET request to the API endpoint\n    response = requests.get(api_endpoint)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch data from {api_endpoint}. Status code: {response.status_code}\")\n    \n    # Return the response data in JSON format\n    return response.json()\n```\n"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n    \n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    walk = np.cumsum(steps)\n    \n    stats = {\n        'count': elements,\n        'mean': np.mean(walk),\n        'std': np.std(walk),\n        'min': np.min(walk),\n        '5%': np.percentile(walk, 5),\n        '25%': np.percentile(walk, 25),\n        '50%': np.percentile(walk, 50),\n        '75%': np.percentile(walk, 75),\n        '95%': np.percentile(walk, 95),\n        'max': np.max(walk)\n    }\n    \n    fig, ax = plt.subplots()\n    ax.plot(walk, label='Random Walk')\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    ax.legend()\n    \n    return stats, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n    \n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    walk = np.cumsum(steps)\n    \n    stats = {\n        'count': elements,\n        'mean': np.mean(walk),\n        'std': np.std(walk),\n        'min': np.min(walk),\n        '5%': np.percentile(walk, 5),\n        '25%': np.percentile(walk, 25),\n        '50%': np.percentile(walk, 50),\n        '75%': np.percentile(walk, 75),\n        '95%': np.percentile(walk, 95),\n        'max': np.max(walk)\n    }\n    \n    fig, ax = plt.subplots()\n    ax.plot(walk, label='Random Walk')\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    ax.legend()\n    \n    return stats, ax\n```\n"}
{"task_id": "BigCodeBench/209", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert list of tuples to numpy array\n    data_array = np.array(data)\n\n    # Plotting the scatter graph\n    fig, ax = plt.subplots()\n    ax.scatter(data_array[:, 0], data_array[:, 1], label='Data Points')\n\n    # Finding the maximum tuple\n    max_tuple = max(data, key=itemgetter(1))\n\n    # Highlighting the maximum tuple\n    ax.scatter(max_tuple[0], max_tuple[1], color='red', label='Max Tuple Highlighted')\n\n    # Adding labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot with Max Tuple Highlighted')\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert list of tuples to numpy array\n    data_array = np.array(data)\n\n    # Plotting the scatter graph\n    fig, ax = plt.subplots()\n    ax.scatter(data_array[:, 0], data_array[:, 1], label='Data Points')\n\n    # Finding the maximum tuple\n    max_tuple = max(data, key=itemgetter(1))\n\n    # Highlighting the maximum tuple\n    ax.scatter(max_tuple[0], max_tuple[1], color='red', label='Max Tuple Highlighted')\n\n    # Adding labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot with Max Tuple Highlighted')\n    ax.legend()\n\n    return ax\n\n# Example usage\n# ax = task_func([(10, 20), (30, 40), (25, 50)])\n# print(type(ax))\n```\n"}
{"task_id": "BigCodeBench/210", "solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Count the frequency of each letter\n    letter_counts = collections.Counter(itemgetter(0)(x) for x in data)\n    \n    # Find the letter with the maximum count\n    max_letter = max(letter_counts.items(), key=itemgetter(1))[0]\n    \n    # Prepare data for plotting\n    letters = list(letter_counts.keys())\n    counts = list(letter_counts.values())\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.bar(letters, counts, label='Letter Counts')\n    \n    # Highlight the letter with the maximum count\n    ax.bar(max_letter, letter_counts[max_letter], color='red', label='Max Value Letter')\n    \n    # Set labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n    \n    # Add legend\n    ax.legend()\n    \n    return ax\ndataset = [('a', 10), ('b', 15), ('a', 5), ('c', 20)]", "raw_solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Count the frequency of each letter\n    letter_counts = collections.Counter(itemgetter(0)(x) for x in data)\n    \n    # Find the letter with the maximum count\n    max_letter = max(letter_counts.items(), key=itemgetter(1))[0]\n    \n    # Prepare data for plotting\n    letters = list(letter_counts.keys())\n    counts = list(letter_counts.values())\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.bar(letters, counts, label='Letter Counts')\n    \n    # Highlight the letter with the maximum count\n    ax.bar(max_letter, letter_counts[max_letter], color='red', label='Max Value Letter')\n    \n    # Set labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n    \n    # Add legend\n    ax.legend()\n    \n    return ax\n\n# Example usage\ndataset = [('a', 10), ('b', 15), ('a', 5), ('c', 20)]\nax = task_func(dataset)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    if headers is None:\n        headers = {'accept': 'application/octet-stream'}\n    \n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n    \n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    zip_file_path = os.path.join(destination_directory, \"downloaded.zip\")\n    \n    with open(zip_file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n    \n    # List the extracted files\n    extracted_files = os.listdir(destination_directory)\n    \n    # Clean up the zip file\n    os.remove(zip_file_path)\n    \n    return extracted_files", "raw_solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    if headers is None:\n        headers = {'accept': 'application/octet-stream'}\n    \n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n    \n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    zip_file_path = os.path.join(destination_directory, \"downloaded.zip\")\n    \n    with open(zip_file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n    \n    # List the extracted files\n    extracted_files = os.listdir(destination_directory)\n    \n    # Clean up the zip file\n    os.remove(zip_file_path)\n    \n    return extracted_files\n```\n"}
{"task_id": "BigCodeBench/212", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert list of tuples to numpy array\n    data_array = np.array(data)\n\n    # Find the maximum y-value and its corresponding x-value\n    max_y_point = max(data_array, key=itemgetter(1))\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(data_array[:, 0], data_array[:, 1], label='Points')\n    ax.scatter(max_y_point[0], max_y_point[1], color='red', label='Max Y Point')\n\n    # Set labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Max Y Point Highlighted')\n    ax.legend()\n\n    return ax, max_y_point", "raw_solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert list of tuples to numpy array\n    data_array = np.array(data)\n\n    # Find the maximum y-value and its corresponding x-value\n    max_y_point = max(data_array, key=itemgetter(1))\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(data_array[:, 0], data_array[:, 1], label='Points')\n    ax.scatter(max_y_point[0], max_y_point[1], color='red', label='Max Y Point')\n\n    # Set labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Max Y Point Highlighted')\n    ax.legend()\n\n    return ax, max_y_point\n```\n"}
{"task_id": "BigCodeBench/213", "solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\ndef task_func(intervals=100, seed=0):\n    random.seed(seed)\n    times = []\n    values = []\n\n    for _ in range(intervals):\n        start_time = time.time()\n        random_value = random.random()\n        end_time = time.time()\n        times.append(end_time - start_time)\n        values.append(random_value)\n\n    fig, ax = plt.subplots()\n    ax.plot(times, values, 'o')\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Random Value')\n    ax.set_title('Random Values vs Time')\n\n    kurt = kurtosis(values)\n    \n    return ax, kurt", "raw_solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\n\ndef task_func(intervals=100, seed=0):\n    random.seed(seed)\n    times = []\n    values = []\n\n    for _ in range(intervals):\n        start_time = time.time()\n        random_value = random.random()\n        end_time = time.time()\n        times.append(end_time - start_time)\n        values.append(random_value)\n\n    fig, ax = plt.subplots()\n    ax.plot(times, values, 'o')\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Random Value')\n    ax.set_title('Random Values vs Time')\n\n    kurt = kurtosis(values)\n    \n    return ax, kurt\n\n# Example usage:\n# ax, kurt = task_func(5)\n# print(type(ax))\n# print(kurt)\n```\n"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate a random RGB image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Convert the image from BGR to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Plot the image\n    fig, ax = plt.subplots()\n    ax.imshow(image_rgb)\n    ax.set_title('Random RGB Image')\n    ax.axis('off')\n\n    return ax, image", "raw_solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate a random RGB image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Convert the image from BGR to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Plot the image\n    fig, ax = plt.subplots()\n    ax.imshow(image_rgb)\n    ax.set_title('Random RGB Image')\n    ax.axis('off')\n\n    return ax, image\n```\n"}
{"task_id": "BigCodeBench/215", "solution": "import requests\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nHEADERS = {\n    'accept': 'application/json'\n}\ndef task_func(url, parameters):\n    try:\n        response = requests.get(url, headers=HEADERS, params=parameters)\n        response.raise_for_status()\n        data = response.json()\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Error fetching data from {url}: {e}\")\n\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        raise Exception(\"No data returned from the API\")\n\n    # Calculate correlation matrix\n    corr = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Heatmap')\n    plt.show()\n\n    return df, ax", "raw_solution": "import requests\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nHEADERS = {\n    'accept': 'application/json'\n}\n\ndef task_func(url, parameters):\n    try:\n        response = requests.get(url, headers=HEADERS, params=parameters)\n        response.raise_for_status()\n        data = response.json()\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Error fetching data from {url}: {e}\")\n\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        raise Exception(\"No data returned from the API\")\n\n    # Calculate correlation matrix\n    corr = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Heatmap')\n    plt.show()\n\n    return df, ax\n```\nThis script fetches data from a given URL with specified parameters, converts it into a pandas DataFrame, calculates the correlation matrix, and plots a heatmap using seaborn. If there are any issues with the request or the data, it raises an exception.\n"}
{"task_id": "BigCodeBench/216", "solution": "import pandas as pd\nimport os\nimport json\nfrom collections import Counter\ndef task_func(json_dir_path, word_count):\n    word_counter = Counter()\n    \n    for filename in os.listdir(json_dir_path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(json_dir_path, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                text = data.get('text', '')\n                words = text.split()\n                word_counter.update(words)\n    \n    most_common_words = word_counter.most_common(word_count)\n    \n    return most_common_words", "raw_solution": "import pandas as pd\nimport os\nimport json\nfrom collections import Counter\n\ndef task_func(json_dir_path, word_count):\n    word_counter = Counter()\n    \n    for filename in os.listdir(json_dir_path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(json_dir_path, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                text = data.get('text', '')\n                words = text.split()\n                word_counter.update(words)\n    \n    most_common_words = word_counter.most_common(word_count)\n    \n    return most_common_words\n\n# Example usage:\n# Assuming we have a directory 'data' with two json files 'fake_data_1.json' and 'fake_data_2.json'\n# with the content as provided in the example in the docstring.\n# print(task_func('data', 2))\n# Output: [('add', 2), ('Top', 1)]\n```\n"}
{"task_id": "BigCodeBench/217", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    \n    mean, std = np.mean(sample), np.std(sample)\n    \n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n    \n    pdf = stats.norm.pdf(bins, mu, sigma)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n    \n    ax.set_title(f'Normal Distribution with $\\mu = {mu:.2f}, \\sigma = {sigma:.2f}$')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax, mean, std", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    \n    mean, std = np.mean(sample), np.std(sample)\n    \n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n    \n    pdf = stats.norm.pdf(bins, mu, sigma)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n    \n    ax.set_title(f'Normal Distribution with $\\mu = {mu:.2f}, \\sigma = {sigma:.2f}$')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax, mean, std\n```\n"}
{"task_id": "BigCodeBench/218", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\ndef task_func(df, dict_mapping, plot_histogram=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    if not all(feature in df.columns for feature in FEATURES + [TARGET]):\n        raise ValueError(\"Input df must contain the specified features and target column.\")\n    \n    # Replace values in the DataFrame using the provided dictionary mapping\n    for column, mapping in dict_mapping.items():\n        if column in df.columns:\n            df[column] = df[column].map(mapping)\n    \n    # Standardize the features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    \n    # Plot histogram if required\n    ax = None\n    if plot_histogram:\n        ax = df[TARGET].plot(kind='hist', title='Target Distribution')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\n\ndef task_func(df, dict_mapping, plot_histogram=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    if not all(feature in df.columns for feature in FEATURES + [TARGET]):\n        raise ValueError(\"Input df must contain the specified features and target column.\")\n    \n    # Replace values in the DataFrame using the provided dictionary mapping\n    for column, mapping in dict_mapping.items():\n        if column in df.columns:\n            df[column] = df[column].map(mapping)\n    \n    # Standardize the features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    \n    # Plot histogram if required\n    ax = None\n    if plot_histogram:\n        ax = df[TARGET].plot(kind='hist', title='Target Distribution')\n    \n    return df, ax\n\n# Example usage:\n# df = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6], 'feature3': [7, 8, 9],'feature4': [10, 11, 12], 'feature5': [13, 14, 15], 'target': [0, 1, 1]})\n# dict_mapping = {1: 11, 0: 22}\n# df, ax = task_func(df, dict_mapping, plot_histogram=True)\n# print(isinstance(ax, plt.Axes))  # Should print True\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/219", "solution": "import math\nimport statistics\nimport numpy as np\ndef task_func(input_list):\n    # Sort the input list\n    sorted_list = sorted(input_list)\n    \n    # Calculate mean, median, and mode for the sorted list\n    mean = statistics.mean(sorted_list)\n    median = statistics.median(sorted_list)\n    mode = statistics.mode(sorted_list)\n    \n    # Calculate the degree values\n    degrees = np.deg2rad(sorted_list)\n    \n    # Calculate the FFT of the degree values\n    fft_values = np.fft.fft(degrees)\n    \n    # Calculate the magnitude of the FFT values\n    magnitude = np.abs(fft_values)\n    \n    # Calculate mean, median, and mode for the magnitude\n    mean_magnitude = statistics.mean(magnitude)\n    median_magnitude = statistics.median(magnitude)\n    mode_magnitude = statistics.mode(magnitude)\n    \n    # Round the mean, median, and mode to the nearest integer\n    mean = round(mean)\n    median = round(median)\n    mode = round(mode)\n    mean_magnitude = round(mean_magnitude)\n    median_magnitude = round(median_magnitude)\n    mode_magnitude = round(mode_magnitude)\n    \n    return mean, median, mode, mean_magnitude, median_magnitude, mode_magnitude\ninput_list = [30, 45, 60, 90, 180]", "raw_solution": "import math\nimport statistics\nimport numpy as np\n\ndef task_func(input_list):\n    # Sort the input list\n    sorted_list = sorted(input_list)\n    \n    # Calculate mean, median, and mode for the sorted list\n    mean = statistics.mean(sorted_list)\n    median = statistics.median(sorted_list)\n    mode = statistics.mode(sorted_list)\n    \n    # Calculate the degree values\n    degrees = np.deg2rad(sorted_list)\n    \n    # Calculate the FFT of the degree values\n    fft_values = np.fft.fft(degrees)\n    \n    # Calculate the magnitude of the FFT values\n    magnitude = np.abs(fft_values)\n    \n    # Calculate mean, median, and mode for the magnitude\n    mean_magnitude = statistics.mean(magnitude)\n    median_magnitude = statistics.median(magnitude)\n    mode_magnitude = statistics.mode(magnitude)\n    \n    # Round the mean, median, and mode to the nearest integer\n    mean = round(mean)\n    median = round(median)\n    mode = round(mode)\n    mean_magnitude = round(mean_magnitude)\n    median_magnitude = round(median_magnitude)\n    mode_magnitude = round(mode_magnitude)\n    \n    return mean, median, mode, mean_magnitude, median_magnitude, mode_magnitude\n\n# Test cases\ninput_list = [30, 45, 60, 90, 180]\nstats = task_func(input_list)"}
{"task_id": "BigCodeBench/220", "solution": "from random import choice\nimport turtle\nimport time\ndef task_func(colors):\n    \"\"\"\n    Turtle graphics to draw a sequence of squares with random colors, one after another.\n    Each square is drawn with a pause of one second between each one.\n\n    Parameters:\n    - colors (list): A list of color strings to be used for drawing squares.\n\n    Returns:\n    - None\n\n    Requirements:\n    - random.choice\n    - turtle\n    - time\n\n    Example:\n    - task_func(['red', 'blue', 'green', 'yellow', 'purple'])\n    - This will open a turtle graphics window and draw squares sequentially.\n    - After drawing, the window will remain open.\n\n    \"\"\"\n    screen = turtle.Screen()\n    screen.title(\"Colorful Squares\")\n    screen.bgcolor(\"black\")\n\n    square_turtle = turtle.Turtle()\n    square_turtle.speed(1)\n    square_turtle.penup()\n    square_turtle.goto(-200, 200)\n    square_turtle.pendown()\n\n    for color in colors:\n        square_turtle.color(color)\n        square_turtle.begin_fill()\n        for _ in range(4):\n            square_turtle.forward(200)\n            square_turtle.right(90)\n        square_turtle.end_fill()\n        time.sleep(1)\n\n    screen.mainloop()", "raw_solution": "from random import choice\nimport turtle\nimport time\n\ndef task_func(colors):\n    \"\"\"\n    Turtle graphics to draw a sequence of squares with random colors, one after another.\n    Each square is drawn with a pause of one second between each one.\n\n    Parameters:\n    - colors (list): A list of color strings to be used for drawing squares.\n\n    Returns:\n    - None\n\n    Requirements:\n    - random.choice\n    - turtle\n    - time\n\n    Example:\n    - task_func(['red', 'blue', 'green', 'yellow', 'purple'])\n    - This will open a turtle graphics window and draw squares sequentially.\n    - After drawing, the window will remain open.\n\n    \"\"\"\n    screen = turtle.Screen()\n    screen.title(\"Colorful Squares\")\n    screen.bgcolor(\"black\")\n\n    square_turtle = turtle.Turtle()\n    square_turtle.speed(1)\n    square_turtle.penup()\n    square_turtle.goto(-200, 200)\n    square_turtle.pendown()\n\n    for color in colors:\n        square_turtle.color(color)\n        square_turtle.begin_fill()\n        for _ in range(4):\n            square_turtle.forward(200)\n            square_turtle.right(90)\n        square_turtle.end_fill()\n        time.sleep(1)\n\n    screen.mainloop()\n```\n"}
{"task_id": "BigCodeBench/221", "solution": "import numpy as np\nfrom scipy import stats\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\ndef task_func(df, dct):\n    result = {}\n    for feature in FEATURES:\n        if feature not in df.columns:\n            return \"Invalid input\"\n        data = df[feature].replace(dct)\n        result[feature] = {\n            'mean': np.mean(data),\n            'median': np.median(data),\n            'mode': stats.mode(data)[0][0],\n            'variance': np.var(data)\n        }\n    return result", "raw_solution": "import numpy as np\nfrom scipy import stats\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n\ndef task_func(df, dct):\n    result = {}\n    for feature in FEATURES:\n        if feature not in df.columns:\n            return \"Invalid input\"\n        data = df[feature].replace(dct)\n        result[feature] = {\n            'mean': np.mean(data),\n            'median': np.median(data),\n            'mode': stats.mode(data)[0][0],\n            'variance': np.var(data)\n        }\n    return result\n\n# Example usage:\n# df = pd.DataFrame({'feature1': [1, 2, 3, 4, 5], 'feature2': [5, 4, 3, 2, 1], 'feature3': [2, 2, 2, 2, 2], 'feature4': [1, 1, 3, 3, 5], 'feature5': [0, 1, 1, 1, 1]})\n# dct = {}\n# print(task_func(df, dct))\n```\nThis script calculates the mean, median, mode, and variance for each feature in the DataFrame, replacing any values as specified in the dictionary. If a feature is not present in the DataFrame, it returns \"Invalid input\".\n"}
{"task_id": "BigCodeBench/222", "solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(list_input):\n    # Sort the list in degree value order\n    sorted_list = sorted(list_input, key=math.degrees)\n\n    # Calculate the cumulative sum\n    cumsum = np.cumsum(sorted_list)\n\n    # Plot the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumsum, label='Cumulative Sum')\n    ax.set_title('Cumulative Sum Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.legend()\n\n    return cumsum, ax", "raw_solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(list_input):\n    # Sort the list in degree value order\n    sorted_list = sorted(list_input, key=math.degrees)\n\n    # Calculate the cumulative sum\n    cumsum = np.cumsum(sorted_list)\n\n    # Plot the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumsum, label='Cumulative Sum')\n    ax.set_title('Cumulative Sum Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.legend()\n\n    return cumsum, ax\n```\n"}
{"task_id": "BigCodeBench/223", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df, dct, columns=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Encode categorical columns\n    if columns is None:\n        columns = df.select_dtypes(include=['object']).columns.tolist()\n\n    le = LabelEncoder()\n    for column in columns:\n        if column in df.columns:\n            df[column] = le.fit_transform(df[column].map(dct))\n\n    # Standardize numerical columns\n    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n    df[numerical_cols] = (df[numerical_cols] - df[numerical_cols].mean()) / df[numerical_cols].std()\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(df, dct, columns=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Encode categorical columns\n    if columns is None:\n        columns = df.select_dtypes(include=['object']).columns.tolist()\n\n    le = LabelEncoder()\n    for column in columns:\n        if column in df.columns:\n            df[column] = le.fit_transform(df[column].map(dct))\n\n    # Standardize numerical columns\n    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n    df[numerical_cols] = (df[numerical_cols] - df[numerical_cols].mean()) / df[numerical_cols].std()\n\n    return df\n\n# Example usage:\n# df = pd.DataFrame({'col1': ['a', 'b', 'c'], 'col2': [1, 2, 3]})\n# dct = {'a': 'x', 'b': 'y'}\n# result = task_func(df, dct)\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/224", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    x = np.arange(range_start, range_end, step)\n    sin_x = np.sin(x)\n    cos_x = np.cos(x)\n    abs_diff = np.abs(sin_x - cos_x)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, sin_x, label='sin(x)')\n    plt.plot(x, cos_x, label='cos(x)')\n    plt.fill_between(x, abs_diff, alpha=0.2, label='abs(sin(x) - cos(x))')\n    plt.legend()\n    plt.title('sin(x) vs cos(x) and their absolute difference')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    ax = plt.gca()\n\n    fft_values = fft(abs_diff)\n    fft_mean = np.mean(np.abs(fft_values))\n    fft_median = np.median(np.abs(fft_values))\n\n    data_gen = zip(x, sin_x, cos_x, abs_diff)\n\n    return data_gen, ax, fft_mean, fft_median", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    x = np.arange(range_start, range_end, step)\n    sin_x = np.sin(x)\n    cos_x = np.cos(x)\n    abs_diff = np.abs(sin_x - cos_x)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, sin_x, label='sin(x)')\n    plt.plot(x, cos_x, label='cos(x)')\n    plt.fill_between(x, abs_diff, alpha=0.2, label='abs(sin(x) - cos(x))')\n    plt.legend()\n    plt.title('sin(x) vs cos(x) and their absolute difference')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    ax = plt.gca()\n\n    fft_values = fft(abs_diff)\n    fft_mean = np.mean(np.abs(fft_values))\n    fft_median = np.median(np.abs(fft_values))\n\n    data_gen = zip(x, sin_x, cos_x, abs_diff)\n\n    return data_gen, ax, fft_mean, fft_median\n```\n"}
{"task_id": "BigCodeBench/225", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n    \n    # Replace values in the DataFrame\n    df = df.replace(dct)\n    \n    # Plot histograms if specified\n    if plot_histograms and columns:\n        for column in columns:\n            if column in df.columns:\n                plt.figure()\n                df[column].hist()\n                plt.title(f'Histogram of {column}')\n                plt.xlabel(column)\n                plt.ylabel('Frequency')\n                plt.show()\n    \n    return df", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n    \n    # Replace values in the DataFrame\n    df = df.replace(dct)\n    \n    # Plot histograms if specified\n    if plot_histograms and columns:\n        for column in columns:\n            if column in df.columns:\n                plt.figure()\n                df[column].hist()\n                plt.title(f'Histogram of {column}')\n                plt.xlabel(column)\n                plt.ylabel('Frequency')\n                plt.show()\n    \n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/226", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(range_start=0, range_end=10, step=0.1):\n    x_values = np.arange(range_start, range_end, step)\n    y_values = np.exp(x_values)\n\n    plt.figure()\n    plt.plot(x_values, y_values, label='e^x')\n    plt.title('Exponential Function Plot')\n    plt.xlabel('x')\n    plt.ylabel('e^x')\n    plt.legend()\n    ax = plt.gca()\n\n    def generator():\n        for x, y in zip(x_values, y_values):\n            yield (x, y)\n\n    return generator(), ax", "raw_solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(range_start=0, range_end=10, step=0.1):\n    x_values = np.arange(range_start, range_end, step)\n    y_values = np.exp(x_values)\n\n    plt.figure()\n    plt.plot(x_values, y_values, label='e^x')\n    plt.title('Exponential Function Plot')\n    plt.xlabel('x')\n    plt.ylabel('e^x')\n    plt.legend()\n    ax = plt.gca()\n\n    def generator():\n        for x, y in zip(x_values, y_values):\n            yield (x, y)\n\n    return generator(), ax\n```\n"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file {audio_file} does not exist.\")\n\n    # Convert list to MxN matrix\n    matrix = np.array(L).reshape(M, N)\n\n    # Load audio file\n    y, sr = librosa.load(audio_file, sr=None)\n\n    # Calculate SPL\n    spl = 20 * np.log10(np.sqrt(np.mean(y**2)))\n\n    # Normalize matrix based on SPL\n    matrix = matrix / (spl + 1e-9)\n\n    # Create spectrogram\n    fig, ax = plt.subplots()\n    img = librosa.display.specshow(librosa.amplitude_to_db(matrix, ref=np.max), sr=sr, x_axis='time', y_axis='log', ax=ax)\n    ax.set_title('Spectrogram')\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n\n    return matrix, fig", "raw_solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file {audio_file} does not exist.\")\n\n    # Convert list to MxN matrix\n    matrix = np.array(L).reshape(M, N)\n\n    # Load audio file\n    y, sr = librosa.load(audio_file, sr=None)\n\n    # Calculate SPL\n    spl = 20 * np.log10(np.sqrt(np.mean(y**2)))\n\n    # Normalize matrix based on SPL\n    matrix = matrix / (spl + 1e-9)\n\n    # Create spectrogram\n    fig, ax = plt.subplots()\n    img = librosa.display.specshow(librosa.amplitude_to_db(matrix, ref=np.max), sr=sr, x_axis='time', y_axis='log', ax=ax)\n    ax.set_title('Spectrogram')\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n\n    return matrix, fig\n```\n"}
{"task_id": "BigCodeBench/228", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(df, dct):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Replace values in DataFrame using the provided dictionary\n    df = df.replace(dct)\n\n    # Calculate Pearson correlation matrix\n    correlation_matrix = df.corr(method='pearson')\n\n    return correlation_matrix", "raw_solution": "import pandas as pd\nimport numpy as np\n\n# Constants\nCOLUMNS = ['column1', 'column2', 'column3', 'column4', 'column5']\n\ndef task_func(df, dct):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Replace values in DataFrame using the provided dictionary\n    df = df.replace(dct)\n\n    # Calculate Pearson correlation matrix\n    correlation_matrix = df.corr(method='pearson')\n\n    return correlation_matrix\n\n# Example usage:"}
{"task_id": "BigCodeBench/229", "solution": "import json\nimport random\nfrom datetime import datetime, timedelta\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\ndef task_func(file_path, num_entries, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    log_entries = []\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(['login', 'logout', 'view', 'edit'])\n        timestamp = datetime.now() - timedelta(days=random.randint(1, 30), hours=random.randint(0, 23), minutes=random.randint(0, 59))\n        log_entries.append({'user': user, 'action': action, 'timestamp': timestamp.isoformat()})\n\n    with open(file_path, 'w') as f:\n        json.dump(log_entries, f, indent=4)\n\n    return file_path", "raw_solution": "import json\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\n\ndef task_func(file_path, num_entries, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    log_entries = []\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(['login', 'logout', 'view', 'edit'])\n        timestamp = datetime.now() - timedelta(days=random.randint(1, 30), hours=random.randint(0, 23), minutes=random.randint(0, 59))\n        log_entries.append({'user': user, 'action': action, 'timestamp': timestamp.isoformat()})\n\n    with open(file_path, 'w') as f:\n        json.dump(log_entries, f, indent=4)\n\n    return file_path\n```\n"}
{"task_id": "BigCodeBench/230", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in COLUMNS):\n        return \"Invalid input\"\n\n    # Create a histogram of scores\n    fig, ax1 = plt.subplots()\n    sns.histplot(df['Score'], bins=10, ax=ax1)\n    ax1.set_title('Histogram of Scores')\n\n    # Create a boxplot of scores by country\n    fig, ax2 = plt.subplots()\n    sns.boxplot(x='Country', y='Score', data=df, ax=ax2)\n    ax2.set_title('Boxplot of Scores by Country')\n\n    return fig", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in COLUMNS):\n        return \"Invalid input\"\n\n    # Create a histogram of scores\n    fig, ax1 = plt.subplots()\n    sns.histplot(df['Score'], bins=10, ax=ax1)\n    ax1.set_title('Histogram of Scores')\n\n    # Create a boxplot of scores by country\n    fig, ax2 = plt.subplots()\n    sns.boxplot(x='Country', y='Score', data=df, ax=ax2)\n    ax2.set_title('Boxplot of Scores by Country')\n\n    return fig\n```\n"}
{"task_id": "BigCodeBench/231", "solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\ndef task_func(obj_list) -> Axes:\n    values = [obj.value for obj in obj_list]\n    if not values:\n        values = [0]\n\n    mu, std = np.mean(values), np.std(values)\n\n    x = np.linspace(mu - 4*std, mu + 4*std, 100)\n    p = stats.norm.pdf(x, mu, std)\n\n    fig, ax = plt.subplots()\n    ax.hist(values, bins=10, density=True, alpha=0.6, color='g')\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title('Histogram and Normal Distribution Curve')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\n\nclass ValueObject:\n    value = 0\n\n    def __init__(self, mu=0, std=1, seed=77):\n        random.seed(seed)\n        self.value = random.gauss(mu, std)\n\ndef task_func(obj_list) -> Axes:\n    values = [obj.value for obj in obj_list]\n    if not values:\n        values = [0]\n\n    mu, std = np.mean(values), np.std(values)\n\n    x = np.linspace(mu - 4*std, mu + 4*std, 100)\n    p = stats.norm.pdf(x, mu, std)\n\n    fig, ax = plt.subplots()\n    ax.hist(values, bins=10, density=True, alpha=0.6, color='g')\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title('Histogram and Normal Distribution Curve')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/232", "solution": "import pandas as pd\nimport collections\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    # Calculate total sales\n    total_sales = df['Sales'].sum()\n\n    # Find the most popular category\n    category_counts = collections.Counter(df['Category'])\n    most_popular_category = category_counts.most_common(1)[0][0]\n\n    return {'Total Sales': total_sales, 'Most Popular Category': most_popular_category}", "raw_solution": "import pandas as pd\nimport collections\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    # Calculate total sales\n    total_sales = df['Sales'].sum()\n\n    # Find the most popular category\n    category_counts = collections.Counter(df['Category'])\n    most_popular_category = category_counts.most_common(1)[0][0]\n\n    return {'Total Sales': total_sales, 'Most Popular Category': most_popular_category}\n\n# Example usage:\ndata = pd.DataFrame([{'Customer': 'John', 'Category': 'Electronics', 'Sales': 500}, {'Customer': 'Mary', 'Category': 'Home', 'Sales': 300}])\nreport = task_func(data)"}
{"task_id": "BigCodeBench/233", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(obj_list, attr, num_bins=30, seed=0):\n    random.seed(seed)\n    values = [getattr(obj, attr) for obj in obj_list]\n    \n    plt.hist(values, bins=num_bins)\n    plt.xlabel(attr)\n    plt.ylabel('Count')\n    plt.title(f'{attr} distribution')\n    ax = plt.gca()\n    return ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\nclass Object:\n    value = 0\n\n    def __init__(self, value=None):\n        if value is None:\n            self.value = random.gauss(0, 1)\n        else:\n            self.value = value\n\ndef task_func(obj_list, attr, num_bins=30, seed=0):\n    random.seed(seed)\n    values = [getattr(obj, attr) for obj in obj_list]\n    \n    plt.hist(values, bins=num_bins)\n    plt.xlabel(attr)\n    plt.ylabel('Count')\n    plt.title(f'{attr} distribution')\n    ax = plt.gca()\n    return ax\n```\n"}
{"task_id": "BigCodeBench/234", "solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n\n    # Remove duplicates\n    df = df.drop_duplicates()\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['age'], df['score'])\n\n    # Plotting\n    plt.scatter(df['age'], df['score'], label='Data points')\n    plt.plot(df['age'], intercept + slope * df['age'], 'r', label=f'Fitted line: y={intercept:.2f} + {slope:.2f}x')\n\n    plt.xlabel('Age')\n    plt.ylabel('Score')\n    plt.title('Linear Regression')\n    plt.legend()\n\n    return plt, plt.gca()", "raw_solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n\n    # Remove duplicates\n    df = df.drop_duplicates()\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['age'], df['score'])\n\n    # Plotting\n    plt.scatter(df['age'], df['score'], label='Data points')\n    plt.plot(df['age'], intercept + slope * df['age'], 'r', label=f'Fitted line: y={intercept:.2f} + {slope:.2f}x')\n\n    plt.xlabel('Age')\n    plt.ylabel('Score')\n    plt.title('Linear Regression')\n    plt.legend()\n\n    return plt, plt.gca()\n```\n"}
{"task_id": "BigCodeBench/235", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, num_samples)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, num_bins, density=True, alpha=0.6, color='blue', edgecolor='black')\n    \n    # Overlay PDF\n    pdf = (1/(sigma * np.sqrt(2 * np.pi)) * np.exp(- (bins - mu)**2 / (2 * sigma**2)))\n    ax.plot(bins, pdf, 'r--', linewidth=2, label='PDF')\n    \n    # Fit OLS regression\n    x = bins[:-1]\n    y = count\n    df = pd.DataFrame({'x': x, 'y': y})\n    model = ols('y ~ x + I(x**2)', data=df).fit()\n    ax.plot(bins, model.predict({'x': bins}), 'g--', linewidth=2, label='OLS Fit')\n    \n    ax.set_title('Histogram with PDF and OLS Fit')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    \n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\n\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, num_samples)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, num_bins, density=True, alpha=0.6, color='blue', edgecolor='black')\n    \n    # Overlay PDF\n    pdf = (1/(sigma * np.sqrt(2 * np.pi)) * np.exp(- (bins - mu)**2 / (2 * sigma**2)))\n    ax.plot(bins, pdf, 'r--', linewidth=2, label='PDF')\n    \n    # Fit OLS regression\n    x = bins[:-1]\n    y = count\n    df = pd.DataFrame({'x': x, 'y': y})\n    model = ols('y ~ x + I(x**2)', data=df).fit()\n    ax.plot(bins, model.predict({'x': bins}), 'g--', linewidth=2, label='OLS Fit')\n    \n    ax.set_title('Histogram with PDF and OLS Fit')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/236", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\ndef task_func(df, test_size=0.2, random_state=42):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    # Drop duplicate rows\n    df = df.drop_duplicates()\n\n    # Features and target\n    X = df[['Age']]\n    y = df['category']\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(random_state=random_state)\n    clf.fit(X_train, y_train)\n\n    # Predict and calculate accuracy\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef task_func(df, test_size=0.2, random_state=42):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    # Drop duplicate rows\n    df = df.drop_duplicates()\n\n    # Features and target\n    X = df[['Age']]\n    y = df['category']\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(random_state=random_state)\n    clf.fit(X_train, y_train)\n\n    # Predict and calculate accuracy\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy\n```\n"}
{"task_id": "BigCodeBench/237", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, save_plot=False, plot_path=None):\n    # Convert data to numpy array\n    data_array = np.array([point[1:] for point in data])\n    \n    # Apply PCA to reduce dimensionality to 2D\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(data_array)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n    ax.set_title('2D PCA of Data')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    \n    # Save plot if required\n    if save_plot:\n        if plot_path is None:\n            raise ValueError(\"plot_path must be provided if save_plot is True\")\n        plt.savefig(plot_path)\n    \n    return coordinates_2d, ax", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(data, save_plot=False, plot_path=None):\n    # Convert data to numpy array\n    data_array = np.array([point[1:] for point in data])\n    \n    # Apply PCA to reduce dimensionality to 2D\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(data_array)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n    ax.set_title('2D PCA of Data')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    \n    # Save plot if required\n    if save_plot:\n        if plot_path is None:\n            raise ValueError(\"plot_path must be provided if save_plot is True\")\n        plt.savefig(plot_path)\n    \n    return coordinates_2d, ax\n```\n"}
{"task_id": "BigCodeBench/238", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    # Remove duplicates based on 'Name' column\n    df = df.drop_duplicates(subset='Name')\n\n    # Standardize 'Age' and 'score' columns\n    scaler = StandardScaler()\n    df[['Age', 'score']] = scaler.fit_transform(df[['Age', 'score']])\n\n    # Create a scatter plot of standardized 'Age' and 'score'\n    fig, ax = plt.subplots()\n    ax.scatter(df['Age'], df['score'])\n    ax.set_title('Scatter Plot of Standardized Age and score')\n    ax.set_xlabel('Age (standardized)')\n    ax.set_ylabel('score (standardized)')\n\n    return df, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    # Remove duplicates based on 'Name' column\n    df = df.drop_duplicates(subset='Name')\n\n    # Standardize 'Age' and 'score' columns\n    scaler = StandardScaler()\n    df[['Age', 'score']] = scaler.fit_transform(df[['Age', 'score']])\n\n    # Create a scatter plot of standardized 'Age' and 'score'\n    fig, ax = plt.subplots()\n    ax.scatter(df['Age'], df['score'])\n    ax.set_title('Scatter Plot of Standardized Age and score')\n    ax.set_xlabel('Age (standardized)')\n    ax.set_ylabel('score (standardized)')\n\n    return df, ax\n\n# Example usage:\ndata = pd.DataFrame([{'Name': 'James', 'Age': 30, 'score': 85},\n                      {'Name': 'James', 'Age': 35, 'score': 90},\n                      {'Name': 'Lily', 'Age': 28, 'score': 92},\n                      {'Name': 'Sam', 'Age': 40, 'score': 88},\n                      {'Name': 'Nick', 'Age': 50, 'score': 80}])\n\nmodified_df, plot_axes = task_func(data)"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values from the tuples\n    arr = np.array([val for _, val in original])\n    \n    # Calculate basic statistics\n    mean, std = np.mean(arr), np.std(arr)\n    minimum, maximum = np.min(arr), np.max(arr)\n    \n    stats_dict = {\n        'mean': mean,\n        'std': std,\n        'min': minimum,\n        'max': maximum\n    }\n    \n    # Plot histogram with PDF\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(arr, density=True, alpha=0.6, bins='auto')\n    \n    # Overlay PDF\n    pdf = stats.norm.pdf(bins, mean, std)\n    ax.plot(bins, pdf, 'r--', linewidth=2)\n    \n    return arr, stats_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values from the tuples\n    arr = np.array([val for _, val in original])\n    \n    # Calculate basic statistics\n    mean, std = np.mean(arr), np.std(arr)\n    minimum, maximum = np.min(arr), np.max(arr)\n    \n    stats_dict = {\n        'mean': mean,\n        'std': std,\n        'min': minimum,\n        'max': maximum\n    }\n    \n    # Plot histogram with PDF\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(arr, density=True, alpha=0.6, bins='auto')\n    \n    # Overlay PDF\n    pdf = stats.norm.pdf(bins, mean, std)\n    ax.plot(bins, pdf, 'r--', linewidth=2)\n    \n    return arr, stats_dict, ax\n```\n"}
{"task_id": "BigCodeBench/240", "solution": "import pandas as pd\nfrom random import uniform\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    data = [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[column_name])\n    return df", "raw_solution": "import pandas as pd\nfrom random import uniform\n\n\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    data = [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[column_name])\n    return df\n\n\n# Example usage:\n# random.seed(0)\n# data = task_func()\n# print(data.shape[0])  # Should print 1000\n```\n"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list into a numpy array\n    arr = np.array(original, dtype=float)\n    \n    # Extract the second element of each tuple (assumed to be numeric)\n    data = arr[:, 1]\n    \n    # Normalize the data using Min-Max scaling\n    norm_arr = preprocessing.minmax_scale(data)\n    \n    # Plot the original and normalized data\n    fig, ax = plt.subplots()\n    ax.plot(original, label='Original Data')\n    ax.plot(norm_arr, label='Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Original vs Normalized Data')\n    ax.legend()\n    \n    return arr, norm_arr, ax\noriginal = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list into a numpy array\n    arr = np.array(original, dtype=float)\n    \n    # Extract the second element of each tuple (assumed to be numeric)\n    data = arr[:, 1]\n    \n    # Normalize the data using Min-Max scaling\n    norm_arr = preprocessing.minmax_scale(data)\n    \n    # Plot the original and normalized data\n    fig, ax = plt.subplots()\n    ax.plot(original, label='Original Data')\n    ax.plot(norm_arr, label='Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Original vs Normalized Data')\n    ax.legend()\n    \n    return arr, norm_arr, ax\n\n# Example usage:\noriginal = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\narr, norm_arr, ax = task_func(original)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/242", "solution": "import cv2\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(image_path, kernel_size):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"Kernel size must be a positive integer.\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be read as an image.\")\n\n    # Apply Gaussian blur to the image\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n\n    # Plot the original and blurred images\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax1.set_title('Original')\n    ax1.axis('off')\n\n    ax2.imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n    ax2.set_title('Blurred')\n    ax2.axis('off')\n\n    plt.show()\n\n    return blurred_image, ax1, ax2", "raw_solution": "import cv2\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(image_path, kernel_size):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"Kernel size must be a positive integer.\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be read as an image.\")\n\n    # Apply Gaussian blur to the image\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n\n    # Plot the original and blurred images\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax1.set_title('Original')\n    ax1.axis('off')\n\n    ax2.imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n    ax2.set_title('Blurred')\n    ax2.axis('off')\n\n    plt.show()\n\n    return blurred_image, ax1, ax2\n```\n"}
{"task_id": "BigCodeBench/243", "solution": "import pandas as pd\nimport random\nN_DATA_POINTS = 10000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\ndef task_func(n_data_points=N_DATA_POINTS):\n    if n_data_points == 0:\n        return pd.DataFrame(columns=['Value'])\n\n    data = [round(random.uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=['Value'])\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\n# Constants\nN_DATA_POINTS = 10000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef task_func(n_data_points=N_DATA_POINTS):\n    if n_data_points == 0:\n        return pd.DataFrame(columns=['Value'])\n\n    data = [round(random.uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=['Value'])\n\n    return df\n\n# Example usage:\n# random.seed(0)\n# data = task_func(20)\n# print(data.shape)  # Output: (20, 1)\n# assert MIN_VALUE <= data.iloc[0]['Value'] <= MAX_VALUE\n```\n"}
{"task_id": "BigCodeBench/244", "solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\ndef task_func(original):\n    if not original:\n        return np.array([]), np.array([]), None\n\n    # Convert the list of tuples into a numpy array\n    arr = np.array([x[1] for x in original])\n\n    # Perform Fast Fourier Transform\n    fft_data = fft(arr)\n\n    # Plot the magnitude of fft_data\n    fig, ax = plt.subplots()\n    ax.hist(np.abs(fft_data), bins=10, color='blue', alpha=0.7)\n    ax.set_title('Magnitude of FFT Data')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return arr, fft_data, ax\noriginal = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]", "raw_solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\n\ndef task_func(original):\n    if not original:\n        return np.array([]), np.array([]), None\n\n    # Convert the list of tuples into a numpy array\n    arr = np.array([x[1] for x in original])\n\n    # Perform Fast Fourier Transform\n    fft_data = fft(arr)\n\n    # Plot the magnitude of fft_data\n    fig, ax = plt.subplots()\n    ax.hist(np.abs(fft_data), bins=10, color='blue', alpha=0.7)\n    ax.set_title('Magnitude of FFT Data')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return arr, fft_data, ax\n\n# Example usage:\noriginal = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\narr, fft_data, ax = task_func(original)"}
{"task_id": "BigCodeBench/245", "solution": "import pandas as pd\nimport random\nfrom scipy import stats\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate random data points\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=['values'])\n    \n    # Calculate statistics\n    mean_value = df['values'].mean()\n    median_value = df['values'].median()\n    mode_value = stats.mode(df['values'])[0][0]\n    \n    # Return the statistics in a dictionary\n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'mode': mode_value\n    }", "raw_solution": "import pandas as pd\nimport random\nfrom scipy import stats\n\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate random data points\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=['values'])\n    \n    # Calculate statistics\n    mean_value = df['values'].mean()\n    median_value = df['values'].median()\n    mode_value = stats.mode(df['values'])[0][0]\n    \n    # Return the statistics in a dictionary\n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'mode': mode_value\n    }\n\n# Example usage\nrandom.seed(0)\nstats = task_func(1000, 5.0, 5.0)"}
{"task_id": "BigCodeBench/246", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\nANGLES = np.arange(0, 2*np.pi, 0.01)\ndef task_func(n_waves, seed=0):\n    if n_waves < 1:\n        return [], np.array([]), None\n\n    np.random.seed(seed)\n    sine_waves = [np.sin(2 * np.pi * i * ANGLES) for i in range(n_waves)]\n    mixed_signal = np.sum(sine_waves, axis=0)\n\n    fft_data = fft(mixed_signal)\n    fft_magnitude = np.abs(fft_data)\n\n    fig, ax = plt.subplots()\n    ax.hist(fft_magnitude, bins=30, color='blue', alpha=0.7)\n    ax.set_title('FFT Magnitude Histogram')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return sine_waves, fft_data, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\nANGLES = np.arange(0, 2*np.pi, 0.01)\n\ndef task_func(n_waves, seed=0):\n    if n_waves < 1:\n        return [], np.array([]), None\n\n    np.random.seed(seed)\n    sine_waves = [np.sin(2 * np.pi * i * ANGLES) for i in range(n_waves)]\n    mixed_signal = np.sum(sine_waves, axis=0)\n\n    fft_data = fft(mixed_signal)\n    fft_magnitude = np.abs(fft_data)\n\n    fig, ax = plt.subplots()\n    ax.hist(fft_magnitude, bins=30, color='blue', alpha=0.7)\n    ax.set_title('FFT Magnitude Histogram')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return sine_waves, fft_data, ax\n```\n"}
{"task_id": "BigCodeBench/247", "solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    if max_value < min_value:\n        raise ValueError(\"max_value must be greater than or equal to min_value\")\n\n    data = [random.uniform(min_value, max_value) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[\"Value\"])\n\n    scaler = StandardScaler()\n    df[\"Normalized Value\"] = scaler.fit_transform(df[[\"Value\"]])\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nN_DATA_POINTS = 5000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    if max_value < min_value:\n        raise ValueError(\"max_value must be greater than or equal to min_value\")\n\n    data = [random.uniform(min_value, max_value) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[\"Value\"])\n\n    scaler = StandardScaler()\n    df[\"Normalized Value\"] = scaler.fit_transform(df[[\"Value\"]])\n\n    return df\n\n# Example usage:\nrandom.seed(0)\nnormalized_data = task_func(5000, 5, 5)"}
{"task_id": "BigCodeBench/248", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"data_list cannot be empty\")\n\n    # Unzipping the data\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=np.nan))\n\n    # Plotting the numerical data\n    fig, ax = plt.subplots()\n    for i in range(1, len(unzipped_data)):\n        ax.plot(unzipped_data[0], unzipped_data[i], label=f'Column {i}')\n    \n    ax.set_xlabel('Character')\n    ax.set_ylabel('Numerical Value')\n    ax.set_title('Plot of Numerical Values')\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"data_list cannot be empty\")\n\n    # Unzipping the data\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=np.nan))\n\n    # Plotting the numerical data\n    fig, ax = plt.subplots()\n    for i in range(1, len(unzipped_data)):\n        ax.plot(unzipped_data[0], unzipped_data[i], label=f'Column {i}')\n    \n    ax.set_xlabel('Character')\n    ax.set_ylabel('Numerical Value')\n    ax.set_title('Plot of Numerical Values')\n    ax.legend()\n\n    return ax\n\n# Example usage:\n# plot = task_func([('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)])\n# print(type(plot))\n```\nThis script first checks if the input list is empty and raises a ValueError if it is. It then unzips the data using itertools.zip_longest to handle lists of unequal length. After unzipping, it plots the numerical data using matplotlib. The function returns the Axes object for further manipulation if needed.\n"}
{"task_id": "BigCodeBench/249", "solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate random data\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['Value'])\n    \n    # Split the data into train and test sets\n    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n    \n    return train_df, test_df", "raw_solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate random data\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['Value'])\n    \n    # Split the data into train and test sets\n    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n    \n    return train_df, test_df\n\n# Example usage:\n# random.seed(0)\n# train_data, test_data = task_func()\n# print(train_data.shape[0])\n# print(test_data.shape[0])\n# print(test_data.iloc[0]['Value'])\n```\n"}
{"task_id": "BigCodeBench/250", "solution": "import numpy as np\nimport itertools\nimport json\nimport tempfile\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    result = {}\n    for i, (letter, *values) in enumerate(data_list):\n        mean_value = np.mean(values)\n        result[f'Position {i+1}'] = mean_value\n\n    if json_file_name:\n        with open(json_file_name, 'w') as f:\n            json.dump(result, f)\n\n    return result", "raw_solution": "import numpy as np\nimport itertools\nimport json\nimport tempfile\n\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    result = {}\n    for i, (letter, *values) in enumerate(data_list):\n        mean_value = np.mean(values)\n        result[f'Position {i+1}'] = mean_value\n\n    if json_file_name:\n        with open(json_file_name, 'w') as f:\n            json.dump(result, f)\n\n    return result\n\n# Example usage:\n# json_file = tempfile.NamedTemporaryFile(delete=False)\n# print(task_func([('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)], json_file.name))\n```\nThis script calculates the mean of numeric values in each tuple of the list and stores the results in a dictionary. It also has an optional parameter to save the results to a JSON file. The example usage demonstrates how to use the function and save the results to a temporary JSON file.\n"}
{"task_id": "BigCodeBench/251", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    # Count the number of jobs\n    job_counts = data['Job'].value_counts()\n\n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(job_counts, labels=job_counts.index, autopct='%1.1f%%', startangle=90)\n    ax.set_title('Job Distribution')\n\n    return fig\ndata = pd.DataFrame({'Name': ['John', 'Jane', 'Joe'],\n                       'Date': ['01/03/2012', '02/05/2013', '03/08/2014'],\n                       'Job': ['Engineer', 'doctor', 'Lawyer']})", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    # Count the number of jobs\n    job_counts = data['Job'].value_counts()\n\n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(job_counts, labels=job_counts.index, autopct='%1.1f%%', startangle=90)\n    ax.set_title('Job Distribution')\n\n    return fig\n\n# Example usage:\ndata = pd.DataFrame({'Name': ['John', 'Jane', 'Joe'],\n                       'Date': ['01/03/2012', '02/05/2013', '03/08/2014'],\n                       'Job': ['Engineer', 'doctor', 'Lawyer']})\nfig = task_func(data)"}
{"task_id": "BigCodeBench/252", "solution": "import matplotlib.pyplot as plt\nfrom itertools import zip_longest\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\ndef task_func(data, labels):\n    # Handle cases where data and labels are not of the same length\n    colors = list(zip_longest(COLORS, data, fillvalue='black'))[0]\n    labels = list(zip_longest(labels, data, fillvalue='Unlabeled'))[0]\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    for i, (color, series) in enumerate(zip(colors, data)):\n        ax.plot(series, label=labels[i], color=color)\n\n    ax.set_title('Data Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Values')\n    ax.legend()\n\n    return ax\ndata = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]]\nlabels = ['Series 1', 'Series 2', 'Series 3']", "raw_solution": "import matplotlib.pyplot as plt\nfrom itertools import zip_longest\n\n# Constants\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\n\ndef task_func(data, labels):\n    # Handle cases where data and labels are not of the same length\n    colors = list(zip_longest(COLORS, data, fillvalue='black'))[0]\n    labels = list(zip_longest(labels, data, fillvalue='Unlabeled'))[0]\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    for i, (color, series) in enumerate(zip(colors, data)):\n        ax.plot(series, label=labels[i], color=color)\n\n    ax.set_title('Data Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Values')\n    ax.legend()\n\n    return ax\n\n# Example usage:\ndata = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]]\nlabels = ['Series 1', 'Series 2', 'Series 3']\nax = task_func(data, labels)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/253", "solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\ndef task_func(ax):\n    # Generate random data for the sign wave\n    theta = np.linspace(0, 2 * np.pi, 100)\n    r = np.sign(np.sin(theta)) + 1.5\n\n    # Choose a random color\n    color = random.choice(COLORS)\n\n    # Plot the sign wave\n    ax.plot(theta, r, color=color)\n\n    # Set random radial labels\n    ax.set_rticks([1, 2, 3])\n    ax.set_rlabel_position(random.randint(0, 360))\n\n    return color", "raw_solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef task_func(ax):\n    # Generate random data for the sign wave\n    theta = np.linspace(0, 2 * np.pi, 100)\n    r = np.sign(np.sin(theta)) + 1.5\n\n    # Choose a random color\n    color = random.choice(COLORS)\n\n    # Plot the sign wave\n    ax.plot(theta, r, color=color)\n\n    # Set random radial labels\n    ax.set_rticks([1, 2, 3])\n    ax.set_rlabel_position(random.randint(0, 360))\n\n    return color\n\n# Example usage"}
{"task_id": "BigCodeBench/254", "solution": "import json\nimport math\ndef task_func(decimal_value, precision=2):\n    \"\"\"\n    Diye gaye decimal value ka square ek certain precision tak gyat karein aur phir result ko JSON string ke roop mein encode karein.\n\n    Params:\n    - decimal_value (float): Decimal value.\n    - precision (int, Optional): square ko round karne ke liye decimal places ki number. defaults roop se 2.\n\n    Returns:\n        str: decimal maan ka square JSON string ke roop mein encode kiya gaya.\n\n    Reqs:\n    - json\n    - math\n\n    Examples:\n    - >>> decimal_value = 3.9\n    - >>> json_str = task_func(decimal_value, decimal_value)\n    - >>> print(json_str)\n    - \"1.97\"\n\n    \"\"\"\n    square_value = math.sqrt(decimal_value)\n    rounded_square_value = round(square_value, precision)\n    json_str = json.dumps(rounded_square_value)\n    return json_str", "raw_solution": "import json\nimport math\n\ndef task_func(decimal_value, precision=2):\n    \"\"\"\n    Diye gaye decimal value ka square ek certain precision tak gyat karein aur phir result ko JSON string ke roop mein encode karein.\n\n    Params:\n    - decimal_value (float): Decimal value.\n    - precision (int, Optional): square ko round karne ke liye decimal places ki number. defaults roop se 2.\n\n    Returns:\n        str: decimal maan ka square JSON string ke roop mein encode kiya gaya.\n\n    Reqs:\n    - json\n    - math\n\n    Examples:\n    - >>> decimal_value = 3.9\n    - >>> json_str = task_func(decimal_value, decimal_value)\n    - >>> print(json_str)\n    - \"1.97\"\n\n    \"\"\"\n    square_value = math.sqrt(decimal_value)\n    rounded_square_value = round(square_value, precision)\n    json_str = json.dumps(rounded_square_value)\n    return json_str\n\n# Test cases"}
{"task_id": "BigCodeBench/255", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nFUNCTIONS = [np.sin, np.cos, np.tan]\ndef task_func(ax, func_index):\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be of type matplotlib.projections.polar.pyplot.Axes\")\n\n    func = FUNCTIONS[func_index]\n    theta = np.linspace(0, 2 * np.pi, 100)\n    r = func(theta)\n    ax.plot(theta, r)\n    ax.set_rticks([1])  # Only the outermost tick line\n    ax.set_rlabel_position(45)  # Position of the radial labels\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nFUNCTIONS = [np.sin, np.cos, np.tan]\n\ndef task_func(ax, func_index):\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be of type matplotlib.projections.polar.pyplot.Axes\")\n\n    func = FUNCTIONS[func_index]\n    theta = np.linspace(0, 2 * np.pi, 100)\n    r = func(theta)\n    ax.plot(theta, r)\n    ax.set_rticks([1])  # Only the outermost tick line\n    ax.set_rlabel_position(45)  # Position of the radial labels\n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/256", "solution": "import json\nimport random\nimport hashlib\nfrom datetime import datetime\nimport pytz\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    if not isinstance(utc_datetime, datetime) or not isinstance(salt, str):\n        raise ValueError(\"utc_datetime must be a datetime object and salt must be a string.\")\n\n    random.seed(seed)\n    password = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=password_length))\n    password_hash = hashlib.sha256((password + salt).encode()).hexdigest()\n\n    utc_datetime_str = utc_datetime.astimezone(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S %Z%z')\n\n    data = {\n        'password': password_hash,\n        'datetime': utc_datetime_str\n    }\n\n    return json.dumps(data)", "raw_solution": "import json\nimport random\nimport hashlib\nfrom datetime import datetime\nimport pytz\n\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    if not isinstance(utc_datetime, datetime) or not isinstance(salt, str):\n        raise ValueError(\"utc_datetime must be a datetime object and salt must be a string.\")\n\n    random.seed(seed)\n    password = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=password_length))\n    password_hash = hashlib.sha256((password + salt).encode()).hexdigest()\n\n    utc_datetime_str = utc_datetime.astimezone(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S %Z%z')\n\n    data = {\n        'password': password_hash,\n        'datetime': utc_datetime_str\n    }\n\n    return json.dumps(data)\n\n# Example usage:\nutc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\npassword_json_str = task_func(utc_time)"}
{"task_id": "BigCodeBench/257", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(ax, num_turns):\n    \"\"\"\n    ax polar diagram par 'num_turns' turns ki specified number ke saath ek spiral banaen.\n    spiral center se shuru hota hai aur pratyek ghumaav ke saath bahar ki or failta hai.\n    plate par radial tick ko 45 degree se guna kie gae turns ki number ke anuroop intervals par rakha jaata hai.\n\n    Params:\n    - ax (matplotlib.axes._axes.Axes): spiral plot karne ke lie ax object.\n    - num_turns (int): spiral ke lie turns ki number.\n\n    Returns:\n        matplotlib.axes._axes.Axes: spiral plot ke saath sanshodhit ax object.\n\n    Reqs:\n    - numpy\n    - math\n    - matplotlib\n\n    Examples:\n    - >>> fig, ax = plt.subplots(subplot_kw={'polar': True})\n    - >>> ax = task_func(ax, 3)\n    - >>> ax.get_rlabel_position()\n    - 135.0\n\n    \"\"\"\n    theta = np.linspace(0, 2 * np.pi, 1000)\n    r = np.linspace(0, num_turns, 1000)\n    R, Theta = np.meshgrid(r, theta)\n    Z = np.exp(1j * Theta) * R\n\n    ax.plot(theta, np.abs(Z), color='b')\n    ax.set_rticks(np.arange(0, num_turns + 1, 1))\n    ax.set_rlabel_position(45)\n    ax.set_title('Spiral Plot')\n\n    return ax", "raw_solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(ax, num_turns):\n    \"\"\"\n    ax polar diagram par 'num_turns' turns ki specified number ke saath ek spiral banaen.\n    spiral center se shuru hota hai aur pratyek ghumaav ke saath bahar ki or failta hai.\n    plate par radial tick ko 45 degree se guna kie gae turns ki number ke anuroop intervals par rakha jaata hai.\n\n    Params:\n    - ax (matplotlib.axes._axes.Axes): spiral plot karne ke lie ax object.\n    - num_turns (int): spiral ke lie turns ki number.\n\n    Returns:\n        matplotlib.axes._axes.Axes: spiral plot ke saath sanshodhit ax object.\n\n    Reqs:\n    - numpy\n    - math\n    - matplotlib\n\n    Examples:\n    - >>> fig, ax = plt.subplots(subplot_kw={'polar': True})\n    - >>> ax = task_func(ax, 3)\n    - >>> ax.get_rlabel_position()\n    - 135.0\n\n    \"\"\"\n    theta = np.linspace(0, 2 * np.pi, 1000)\n    r = np.linspace(0, num_turns, 1000)\n    R, Theta = np.meshgrid(r, theta)\n    Z = np.exp(1j * Theta) * R\n\n    ax.plot(theta, np.abs(Z), color='b')\n    ax.set_rticks(np.arange(0, num_turns + 1, 1))\n    ax.set_rlabel_position(45)\n    ax.set_title('Spiral Plot')\n\n    return ax\n\n# Example usage:\n# fig, ax = plt.subplots(subplot_kw={'polar': True})\n# ax = task_func(ax, 3)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/258", "solution": "import json\nimport random\nfrom datetime import datetime, timezone\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\ndef task_func(utc_datetime, seed=0):\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = utc_datetime.replace(tzinfo=timezone.utc).isoformat()\n    person_json_str = json.dumps(person)\n    return person_json_str", "raw_solution": "import json\nimport random\nfrom datetime import datetime, timezone\n\n# Constants\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\n\ndef task_func(utc_datetime, seed=0):\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = utc_datetime.replace(tzinfo=timezone.utc).isoformat()\n    person_json_str = json.dumps(person)\n    return person_json_str\n\n# Example usage:"}
{"task_id": "BigCodeBench/259", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, num_points):\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be a matplotlib.axes._axes.Axes object.\")\n    if not isinstance(num_points, int) or num_points <= 0:\n        raise ValueError(\"num_points must be a positive integer.\")\n\n    # Generate random data\n    theta = np.linspace(0, 2 * np.pi, num_points)\n    r = np.random.rand(num_points)\n    \n    # Plot data\n    ax.plot(theta, r)\n    \n    # Set radial ticks\n    ax.set_rticks([0.2, 0.4, 0.6, 0.8, 1.0])\n    \n    # Set label position\n    ax.set_rlabel_position(10)\n    \n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(ax, num_points):\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be a matplotlib.axes._axes.Axes object.\")\n    if not isinstance(num_points, int) or num_points <= 0:\n        raise ValueError(\"num_points must be a positive integer.\")\n\n    # Generate random data\n    theta = np.linspace(0, 2 * np.pi, num_points)\n    r = np.random.rand(num_points)\n    \n    # Plot data\n    ax.plot(theta, r)\n    \n    # Set radial ticks\n    ax.set_rticks([0.2, 0.4, 0.6, 0.8, 1.0])\n    \n    # Set label position\n    ax.set_rlabel_position(10)\n    \n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/260", "solution": "import json\nimport os\nimport glob\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\ndef task_func(directory):\n    updated_files_count = 0\n    for file_path in glob.glob(os.path.join(directory, '*.json')):\n        with open(file_path, 'r+') as file:\n            data = json.load(file)\n            if KEY not in data:\n                data[KEY] = VALUE\n                file.seek(0)\n                json.dump(data, file, indent=4)\n                updated_files_count += 1\n    return updated_files_count", "raw_solution": "import json\nimport os\nimport glob\n\n# Constants\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\n\ndef task_func(directory):\n    updated_files_count = 0\n    for file_path in glob.glob(os.path.join(directory, '*.json')):\n        with open(file_path, 'r+') as file:\n            data = json.load(file)\n            if KEY not in data:\n                data[KEY] = VALUE\n                file.seek(0)\n                json.dump(data, file, indent=4)\n                updated_files_count += 1\n    return updated_files_count\n```\nThis script will update all `.json` files in the specified directory by adding the new key-value pair if it doesn't already exist. It returns the count of updated files.\n"}
{"task_id": "BigCodeBench/261", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, radius):\n    if not isinstance(ax, plt.Axes) or not ax.get_subplotspec().is_polar():\n        raise TypeError(\"The provided ax is not a polar plot.\")\n    if radius < 0:\n        raise ValueError(\"The radius must be non-negative.\")\n    \n    theta = np.linspace(0, 2 * np.pi, 100)\n    y = np.full_like(theta, radius)\n    ax.plot(theta, y, 'r')\n    ax.set_rticks([radius])  # Set radial ticks\n    ax.set_rlabel_position(0)  # Move radial labels to the center\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(ax, radius):\n    if not isinstance(ax, plt.Axes) or not ax.get_subplotspec().is_polar():\n        raise TypeError(\"The provided ax is not a polar plot.\")\n    if radius < 0:\n        raise ValueError(\"The radius must be non-negative.\")\n    \n    theta = np.linspace(0, 2 * np.pi, 100)\n    y = np.full_like(theta, radius)\n    ax.plot(theta, y, 'r')\n    ax.set_rticks([radius])  # Set radial ticks\n    ax.set_rlabel_position(0)  # Move radial labels to the center\n\n    return ax\n\n# Example usage:"}
{"task_id": "BigCodeBench/262", "solution": "import collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(dictionary, new_key, new_value):\n    # Update the dictionary with the new key-value pair\n    dictionary[new_key] = new_value\n\n    # Count the frequency of each value in the dictionary\n    value_counts = collections.Counter(dictionary.values())\n\n    # Plot the frequency distribution of the values\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=list(value_counts.keys()), y=list(value_counts.values()))\n    ax.set_title('Value Frequency Distribution')\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n\n    return dictionary, ax", "raw_solution": "import collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(dictionary, new_key, new_value):\n    # Update the dictionary with the new key-value pair\n    dictionary[new_key] = new_value\n\n    # Count the frequency of each value in the dictionary\n    value_counts = collections.Counter(dictionary.values())\n\n    # Plot the frequency distribution of the values\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=list(value_counts.keys()), y=list(value_counts.values()))\n    ax.set_title('Value Frequency Distribution')\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n\n    return dictionary, ax\n```\n"}
{"task_id": "BigCodeBench/263", "solution": "import os\nimport glob\nimport shutil\nimport time\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\ndef task_func(my_path: str, days_old: int) -> str:\n    # Calculate the threshold time\n    threshold_time = time.time() - days_old * 86400\n\n    # Create the archive directory if it doesn't exist\n    archive_dir = os.path.join(my_path, 'archive')\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Iterate over each file extension\n    for ext in FILE_EXTENSIONS:\n        # Find all files with the current extension\n        files = glob.glob(os.path.join(my_path, f'*{ext}'))\n        \n        # Move files older than 'days_old' days to the archive directory\n        for file in files:\n            if os.path.getmtime(file) < threshold_time:\n                shutil.move(file, archive_dir)\n\n    return archive_dir", "raw_solution": "import os\nimport glob\nimport shutil\nimport time\n\n# Constants\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\n\ndef task_func(my_path: str, days_old: int) -> str:\n    # Calculate the threshold time\n    threshold_time = time.time() - days_old * 86400\n\n    # Create the archive directory if it doesn't exist\n    archive_dir = os.path.join(my_path, 'archive')\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Iterate over each file extension\n    for ext in FILE_EXTENSIONS:\n        # Find all files with the current extension\n        files = glob.glob(os.path.join(my_path, f'*{ext}'))\n        \n        # Move files older than 'days_old' days to the archive directory\n        for file in files:\n            if os.path.getmtime(file) < threshold_time:\n                shutil.move(file, archive_dir)\n\n    return archive_dir\n```\n"}
{"task_id": "BigCodeBench/264", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"The provided value must be a number.\")\n\n    np.random.seed(seed)\n    mean = float(value)\n    std_dev = 1.0  # Standard deviation for the normal distribution\n\n    # Generate a random dataset\n    data = np.random.normal(loc=mean, scale=std_dev, size=n)\n    data_series = pd.Series(data)\n\n    # Plot the histogram\n    plt.hist(data, bins=bins, alpha=0.7, color='blue')\n    plt.title('Histogram of Generated Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()\n\n    # Update the dictionary\n    dictionary[key] = value\n\n    return dictionary, data_series, plt", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"The provided value must be a number.\")\n\n    np.random.seed(seed)\n    mean = float(value)\n    std_dev = 1.0  # Standard deviation for the normal distribution\n\n    # Generate a random dataset\n    data = np.random.normal(loc=mean, scale=std_dev, size=n)\n    data_series = pd.Series(data)\n\n    # Plot the histogram\n    plt.hist(data, bins=bins, alpha=0.7, color='blue')\n    plt.title('Histogram of Generated Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()\n\n    # Update the dictionary\n    dictionary[key] = value\n\n    return dictionary, data_series, plt\n```\n"}
{"task_id": "BigCodeBench/265", "solution": "import collections\nimport json\nimport os\ndef task_func(data, json_file_name='data.json'):\n    # Check if input is a dictionary\n    if not isinstance(data, dict):\n        raise ValueError(\"Input data must be a dictionary.\")\n\n    # Calculate frequency distribution\n    freq = collections.Counter(data.values())\n\n    # Add frequency distribution to the dictionary\n    data['freq'] = freq\n\n    # Save the updated dictionary to a JSON file\n    with open(json_file_name, 'w') as json_file:\n        json.dump(data, json_file)\n\n    # Return the path of the JSON file\n    return os.path.abspath(json_file_name)", "raw_solution": "import collections\nimport json\nimport os\n\ndef task_func(data, json_file_name='data.json'):\n    # Check if input is a dictionary\n    if not isinstance(data, dict):\n        raise ValueError(\"Input data must be a dictionary.\")\n\n    # Calculate frequency distribution\n    freq = collections.Counter(data.values())\n\n    # Add frequency distribution to the dictionary\n    data['freq'] = freq\n\n    # Save the updated dictionary to a JSON file\n    with open(json_file_name, 'w') as json_file:\n        json.dump(data, json_file)\n\n    # Return the path of the JSON file\n    return os.path.abspath(json_file_name)\n\n# Example usage:\n# import tempfile\n# json_file = tempfile.NamedTemporaryFile(delete=False)\n# data = {'key1': 'value1', 'key2': 'value2', 'key3': 'value1'}\n# print(task_func(data, json_file.name) is not None)  # Should return True\n```\n"}
{"task_id": "BigCodeBench/266", "solution": "import os\nimport os.path\nimport csv\nimport collections\nFILE_NAME = 'file_sizes.csv'\ndef task_func(my_path):\n    \"\"\"\n    Ek directory mein file ke aakaar par ek report banaen aur ise ek csv file mein likhen.\n\n    Params:\n    - my_path (str): The directory path.\n\n    Returns:\n        str: csv file ka path.\n\n    Reqs:\n    - os\n    - os.path\n    - csv\n    - collections\n\n    Examples:\n    - >>> task_func('/usr/my_directory')\n\n    \"\"\"\n    if not os.path.isdir(my_path):\n        raise ValueError(f\"The path {my_path} is not a valid directory.\")\n\n    file_sizes = collections.defaultdict(int)\n\n    for root, _, files in os.walk(my_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            if os.path.isfile(file_path):\n                file_sizes[file] = os.path.getsize(file_path)\n\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        fieldnames = ['File Name', 'Size (bytes)']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for file, size in file_sizes.items():\n            writer.writerow({'File Name': file, 'Size (bytes)': size})\n\n    return os.path.abspath(FILE_NAME)", "raw_solution": "import os\nimport os.path\nimport csv\nimport collections\n\n# Constants\nFILE_NAME = 'file_sizes.csv'\n\ndef task_func(my_path):\n    \"\"\"\n    Ek directory mein file ke aakaar par ek report banaen aur ise ek csv file mein likhen.\n\n    Params:\n    - my_path (str): The directory path.\n\n    Returns:\n        str: csv file ka path.\n\n    Reqs:\n    - os\n    - os.path\n    - csv\n    - collections\n\n    Examples:\n    - >>> task_func('/usr/my_directory')\n\n    \"\"\"\n    if not os.path.isdir(my_path):\n        raise ValueError(f\"The path {my_path} is not a valid directory.\")\n\n    file_sizes = collections.defaultdict(int)\n\n    for root, _, files in os.walk(my_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            if os.path.isfile(file_path):\n                file_sizes[file] = os.path.getsize(file_path)\n\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        fieldnames = ['File Name', 'Size (bytes)']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for file, size in file_sizes.items():\n            writer.writerow({'File Name': file, 'Size (bytes)': size})\n\n    return os.path.abspath(FILE_NAME)\n```\n"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # Add a new key 'a' with value 1 to the dictionary\n    data['a'] = 1\n\n    # Create a signal from the dictionary values\n    signal = np.array(list(data.values()))\n\n    # Perform Fast Fourier Transform (FFT)\n    fft_result = fftpack.fft(signal)\n\n    # Plot the FFT\n    fig, ax = plt.subplots()\n    ax.plot(fftpack.fftfreq(len(signal), d=1/sample_rate))\n    ax.plot(np.abs(fft_result))\n    ax.set_title('FFT of the Signal')\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Amplitude')\n\n    return fft_result, ax\ndata = {'key1': 1, 'key2': 2, 'key3': 3}", "raw_solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Add a new key 'a' with value 1 to the dictionary\n    data['a'] = 1\n\n    # Create a signal from the dictionary values\n    signal = np.array(list(data.values()))\n\n    # Perform Fast Fourier Transform (FFT)\n    fft_result = fftpack.fft(signal)\n\n    # Plot the FFT\n    fig, ax = plt.subplots()\n    ax.plot(fftpack.fftfreq(len(signal), d=1/sample_rate))\n    ax.plot(np.abs(fft_result))\n    ax.set_title('FFT of the Signal')\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Amplitude')\n\n    return fft_result, ax\n\n# Example usage:\ndata = {'key1': 1, 'key2': 2, 'key3': 3}\nfft, ax = task_func(data)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/268", "solution": "import collections\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\ndef task_func(n_keys, n_values):\n    \"\"\"\n    Ek Python dictionary banaen jismein ek **specified** number mein **keys** aur **values** hon.\n\n    Notes:\n        **Keys** ko pehle se paribhashit **list** se randomly roop se chuna jaata hai, **and** **values** 1 se shuru hone wale lagataar **integers** hain.\n        **Key** selection mein randomlyta ke kaaran, actual **keys** **dictionary** mein har **execution** mein true ho sakte hain.\n\n    Params:\n    - n_keys (int): utpann karne ke liye **keys** ki number.\n    - n_values (int): pratyek **key** ke liye **values** ki number (1 se shuru hone wale lagataar **integers**).\n\n    Returns:\n        dict: ek Python **dictionary** jismein **keys** **strings** ke roop mein aur **values** **lists** ke **integers** ke roop mein hon.\n\n    Reqs:\n    - collections\n    - random\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> task_func(3, 5)\n    - {'g': [1, 2, 3, 4, 5], 'a': [1, 2, 3, 4, 5]}\n    - >>> result = task_func(1, 5)\n    - >>> list(result)[0] in LETTERS\n    - True\n\n    \"\"\"\n    random.seed(0)\n    keys = random.sample(LETTERS, n_keys)\n    values = [[i for i in range(1, n_values + 1)] for _ in range(n_keys)]\n    \n    return dict(zip(keys, values))", "raw_solution": "import collections\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef task_func(n_keys, n_values):\n    \"\"\"\n    Ek Python dictionary banaen jismein ek **specified** number mein **keys** aur **values** hon.\n\n    Notes:\n        **Keys** ko pehle se paribhashit **list** se randomly roop se chuna jaata hai, **and** **values** 1 se shuru hone wale lagataar **integers** hain.\n        **Key** selection mein randomlyta ke kaaran, actual **keys** **dictionary** mein har **execution** mein true ho sakte hain.\n\n    Params:\n    - n_keys (int): utpann karne ke liye **keys** ki number.\n    - n_values (int): pratyek **key** ke liye **values** ki number (1 se shuru hone wale lagataar **integers**).\n\n    Returns:\n        dict: ek Python **dictionary** jismein **keys** **strings** ke roop mein aur **values** **lists** ke **integers** ke roop mein hon.\n\n    Reqs:\n    - collections\n    - random\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> task_func(3, 5)\n    - {'g': [1, 2, 3, 4, 5], 'a': [1, 2, 3, 4, 5]}\n    - >>> result = task_func(1, 5)\n    - >>> list(result)[0] in LETTERS\n    - True\n\n    \"\"\"\n    random.seed(0)\n    keys = random.sample(LETTERS, n_keys)\n    values = [[i for i in range(1, n_values + 1)] for _ in range(n_keys)]\n    \n    return dict(zip(keys, values))\n```\n"}
{"task_id": "BigCodeBench/269", "solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    # Ensure 'a' key exists in the dictionary\n    if 'a' not in data_dict:\n        data_dict['a'] = 1\n\n    # Extract the values from the dictionary\n    values = np.array(list(data_dict.values())).reshape(-1, 1)\n\n    # Calculate mean, median, and mode\n    mean_value = np.mean(values)\n    median_value = np.median(values)\n    mode_value = stats.mode(values)[0][0]\n\n    # Scale the values to the range [0, 1]\n    scaler = MinMaxScaler()\n    scaled_values = scaler.fit_transform(values)\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(scaled_values, bins=10, color='blue', alpha=0.7)\n    ax.set_title('Scaled Values Histogram')\n    ax.set_xlabel('Scaled Value')\n    ax.set_ylabel('Frequency')\n\n    # Return the results\n    return {\n        'data': data_dict,\n        'stats': {\n            'mean': round(mean_value, 2),\n            'median': round(median_value, 2),\n            'mode': mode_value\n        },\n        'plot': ax\n    }", "raw_solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    # Ensure 'a' key exists in the dictionary\n    if 'a' not in data_dict:\n        data_dict['a'] = 1\n\n    # Extract the values from the dictionary\n    values = np.array(list(data_dict.values())).reshape(-1, 1)\n\n    # Calculate mean, median, and mode\n    mean_value = np.mean(values)\n    median_value = np.median(values)\n    mode_value = stats.mode(values)[0][0]\n\n    # Scale the values to the range [0, 1]\n    scaler = MinMaxScaler()\n    scaled_values = scaler.fit_transform(values)\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(scaled_values, bins=10, color='blue', alpha=0.7)\n    ax.set_title('Scaled Values Histogram')\n    ax.set_xlabel('Scaled Value')\n    ax.set_ylabel('Frequency')\n\n    # Return the results\n    return {\n        'data': data_dict,\n        'stats': {\n            'mean': round(mean_value, 2),\n            'median': round(median_value, 2),\n            'mode': mode_value\n        },\n        'plot': ax\n    }\n```\n"}
{"task_id": "BigCodeBench/270", "solution": "import re\nfrom collections import Counter\ndef task_func(sentence):\n    # Use regular expression to find all words in the sentence\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    \n    # Use Counter to count occurrences of each word\n    word_counts = Counter(words)\n    \n    return dict(word_counts)", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(sentence):\n    # Use regular expression to find all words in the sentence\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    \n    # Use Counter to count occurrences of each word\n    word_counts = Counter(words)\n    \n    return dict(word_counts)\n\n# Test cases"}
{"task_id": "BigCodeBench/271", "solution": "import random\nimport string\nimport hashlib\nimport time\ndef task_func(data_dict: dict, seed=0) -> dict:\n    random.seed(seed)\n    hashed_dict = {}\n    \n    for key, value in data_dict.items():\n        # Step 1: Add a key \"a\" with value 1\n        hashed_dict[\"a\"] = 1\n        \n        # Step 2: Generate a random salt of 5 characters\n        salt = ''.join(random.choice(string.ascii_lowercase) for _ in range(5))\n        \n        # Step 3: Hash the value with the salt using SHA-256\n        hash_object = hashlib.sha256((value + salt).encode())\n        hashed_value = hash_object.hexdigest()\n        \n        # Step 4: Update the value in the dictionary with the hashed value\n        hashed_dict[key] = hashed_value\n        \n        # Step 5: Add the current UNIX timestamp to the dictionary\n        hashed_dict[\"current\"] = int(time.time())\n    \n    return hashed_dict", "raw_solution": "import random\nimport string\nimport hashlib\nimport time\n\ndef task_func(data_dict: dict, seed=0) -> dict:\n    random.seed(seed)\n    hashed_dict = {}\n    \n    for key, value in data_dict.items():\n        # Step 1: Add a key \"a\" with value 1\n        hashed_dict[\"a\"] = 1\n        \n        # Step 2: Generate a random salt of 5 characters\n        salt = ''.join(random.choice(string.ascii_lowercase) for _ in range(5))\n        \n        # Step 3: Hash the value with the salt using SHA-256\n        hash_object = hashlib.sha256((value + salt).encode())\n        hashed_value = hash_object.hexdigest()\n        \n        # Step 4: Update the value in the dictionary with the hashed value\n        hashed_dict[key] = hashed_value\n        \n        # Step 5: Add the current UNIX timestamp to the dictionary\n        hashed_dict[\"current\"] = int(time.time())\n    \n    return hashed_dict\n\n# Example usage:\n# print(task_func({'key': 'value'}))\n```\n"}
{"task_id": "BigCodeBench/272", "solution": "import cgi\nimport http.server\nimport json\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Content-Type header application/json nahi hai\"}).encode())\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Invalid JSON\"}).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Koi data prapt nahi hua\"}).encode())\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps({\"status\": \"success\", \"message\": \"Data successfully prapt hua.\"}).encode())\ndef task_func():\n    return RequestHandler", "raw_solution": "import cgi\nimport http.server\nimport json\n\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Content-Type header application/json nahi hai\"}).encode())\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Invalid JSON\"}).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Koi data prapt nahi hua\"}).encode())\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps({\"status\": \"success\", \"message\": \"Data successfully prapt hua.\"}).encode())\n\ndef task_func():\n    return RequestHandler\n```\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\ndef task_func():\n    return RequestHandler", "raw_solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE).encode('utf-8'))\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\n\ndef task_func():\n    return RequestHandler\n```\n"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Content-Type must be application/json')\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        message = self.rfile.read(length)\n        try:\n            email_data = json.loads(message)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n            return\n\n        if not all(key in email_data for key in ['subject', 'message', 'to']):\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Missing keys in JSON data')\n            return\n\n        try:\n            with smtplib.SMTP(self.server.smtp_server, self.server.smtp_port) as server:\n                server.starttls()\n                server.login(self.server.smtp_username, self.server.smtp_password)\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = self.server.smtp_username\n                msg['To'] = email_data['to']\n                server.sendmail(self.server.smtp_username, [email_data['to']], msg.as_string())\n        except smtplib.SMTPAuthenticationError:\n            self.send_response(535)\n            self.end_headers()\n            self.wfile.write(b'Authentication Failed')\n            return\n\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(b'Success')\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class SMTPRequestHandler(RequestHandler):\n        def __init__(self, *args, **kwargs):\n            self.smtp_server = smtp_server\n            self.smtp_port = smtp_port\n            self.smtp_username = smtp_username\n            self.smtp_password = smtp_password\n            super().__init__(*args, **kwargs)\n    return SMTPRequestHandler", "raw_solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        ctype, pdict = cgi.parse_header(self.headers.get('Content-Type'))\n        if ctype != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Content-Type must be application/json')\n            return\n\n        length = int(self.headers.get('Content-Length'))\n        message = self.rfile.read(length)\n        try:\n            email_data = json.loads(message)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n            return\n\n        if not all(key in email_data for key in ['subject', 'message', 'to']):\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Missing keys in JSON data')\n            return\n\n        try:\n            with smtplib.SMTP(self.server.smtp_server, self.server.smtp_port) as server:\n                server.starttls()\n                server.login(self.server.smtp_username, self.server.smtp_password)\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = self.server.smtp_username\n                msg['To'] = email_data['to']\n                server.sendmail(self.server.smtp_username, [email_data['to']], msg.as_string())\n        except smtplib.SMTPAuthenticationError:\n            self.send_response(535)\n            self.end_headers()\n            self.wfile.write(b'Authentication Failed')\n            return\n\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(b'Success')\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class SMTPRequestHandler(RequestHandler):\n        def __init__(self, *args, **kwargs):\n            self.smtp_server = smtp_server\n            self.smtp_port = smtp_port\n            self.smtp_username = smtp_username\n            self.smtp_password = smtp_password\n            super().__init__(*args, **kwargs)\n    return SMTPRequestHandler\n```\n"}
{"task_id": "BigCodeBench/275", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(n):\n    if n < 1:\n        raise ValueError(\"Input must be an integer greater than or equal to 1.\")\n    \n    # Generate all possible pairs using combinations from itertools\n    pairs = list(combinations(range(1, n+1), 2))\n    \n    return pairs", "raw_solution": "import numpy as np\nfrom itertools import combinations\n\ndef task_func(n):\n    if n < 1:\n        raise ValueError(\"Input must be an integer greater than or equal to 1.\")\n    \n    # Generate all possible pairs using combinations from itertools\n    pairs = list(combinations(range(1, n+1), 2))\n    \n    return pairs\n\n# Test cases"}
{"task_id": "BigCodeBench/276", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(matrix):\n    matrix = np.array(matrix)\n    max_values = np.max(matrix, axis=1)\n    \n    skewness = stats.skew(max_values)\n    kurt = stats.kurtosis(max_values)\n    \n    fig, ax = plt.subplots()\n    ax.hist(max_values, bins='auto', alpha=0.7, color='blue')\n    ax.set_title('Histogram of Maximum Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return skewness, kurt, ax", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    matrix = np.array(matrix)\n    max_values = np.max(matrix, axis=1)\n    \n    skewness = stats.skew(max_values)\n    kurt = stats.kurtosis(max_values)\n    \n    fig, ax = plt.subplots()\n    ax.hist(max_values, bins='auto', alpha=0.7, color='blue')\n    ax.set_title('Histogram of Maximum Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return skewness, kurt, ax\n```\n"}
{"task_id": "BigCodeBench/277", "solution": "import random\nfrom itertools import combinations\nimport math\ndef task_func(n):\n    if n < 2:\n        return None\n\n    points = [(random.random(), random.random()) for _ in range(n)]\n    all_pairs = combinations(points, 2)\n    max_distance = 0\n    closest_pair = None\n\n    for pair in all_pairs:\n        distance = math.sqrt((pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2)\n        if distance > max_distance:\n            max_distance = distance\n            closest_pair = pair\n\n    return closest_pair", "raw_solution": "import random\nfrom itertools import combinations\nimport math\n\ndef task_func(n):\n    if n < 2:\n        return None\n\n    points = [(random.random(), random.random()) for _ in range(n)]\n    all_pairs = combinations(points, 2)\n    max_distance = 0\n    closest_pair = None\n\n    for pair in all_pairs:\n        distance = math.sqrt((pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2)\n        if distance > max_distance:\n            max_distance = distance\n            closest_pair = pair\n\n    return closest_pair\n\n# Example usage:\nrandom.seed(0)"}
{"task_id": "BigCodeBench/278", "solution": "import numpy as np\nfrom sympy import symbols, solve\ndef task_func(precision=2, seed=0):\n    np.random.seed(seed)\n    a, b, c = np.random.uniform(-10, 10, 3)\n    x = symbols('x')\n    equation = a*x**2 + b*x + c\n    solutions = solve(equation, x)\n    \n    rounded_solutions = tuple(\n        complex(round(float(sol), precision)) for sol in solutions\n    )\n    \n    return rounded_solutions", "raw_solution": "import numpy as np\nfrom sympy import symbols, solve\n\ndef task_func(precision=2, seed=0):\n    np.random.seed(seed)\n    a, b, c = np.random.uniform(-10, 10, 3)\n    x = symbols('x')\n    equation = a*x**2 + b*x + c\n    solutions = solve(equation, x)\n    \n    rounded_solutions = tuple(\n        complex(round(float(sol), precision)) for sol in solutions\n    )\n    \n    return rounded_solutions\n```\n"}
{"task_id": "BigCodeBench/279", "solution": "import random\nfrom collections import Counter\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\ndef task_func(x=1):\n    hands = []\n    for _ in range(x):\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n    \n    # Flatten the list of hands and count the occurrences of each card\n    all_cards = [card for hand in hands for card in hand]\n    card_counter = Counter(all_cards)\n\n    return hands, card_counter", "raw_solution": "import random\nfrom collections import Counter\n\n# Constants\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n\ndef task_func(x=1):\n    hands = []\n    for _ in range(x):\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n    \n    # Flatten the list of hands and count the occurrences of each card\n    all_cards = [card for hand in hands for card in hand]\n    card_counter = Counter(all_cards)\n\n    return hands, card_counter\n\n# Example usage:\n# random.seed(0)  # For reproducibility\n# result = task_func(1)\n# print(result[0])  # Prints a list of 5-card hands\n# print(result[1])  # Prints a counter of the cards collections\n```\n"}
{"task_id": "BigCodeBench/280", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(signal, precision=2, seed=777):\n    np.random.seed(seed)\n    transformed_signal = fft(signal)\n    rounded_transformed_signal = np.round(transformed_signal, precision)\n    \n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n    ax1.plot(signal)\n    ax1.set_title('Mool Signal')\n    ax2.plot(rounded_transformed_signal)\n    ax2.set_title('Transform kiya gaya Signal')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return rounded_transformed_signal, (ax1, ax2)\nsignal = np.array([0., 1., 0., -1.])", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(signal, precision=2, seed=777):\n    np.random.seed(seed)\n    transformed_signal = fft(signal)\n    rounded_transformed_signal = np.round(transformed_signal, precision)\n    \n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n    ax1.plot(signal)\n    ax1.set_title('Mool Signal')\n    ax2.plot(rounded_transformed_signal)\n    ax2.set_title('Transform kiya gaya Signal')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return rounded_transformed_signal, (ax1, ax2)\n\n# Example usage:\nsignal = np.array([0., 1., 0., -1.])\ntransformed_signal, (ax1, ax2) = task_func(signal)"}
{"task_id": "BigCodeBench/281", "solution": "import re\nimport os\nfrom collections import Counter\ndef task_func(folder_path: str) -> dict:\n    ip_pattern = re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b')\n    ip_counter = Counter()\n\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            if file.endswith('.log'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    content = f.read()\n                    ip_addresses = ip_pattern.findall(content)\n                    ip_counter.update(ip_addresses)\n\n    return dict(ip_counter)", "raw_solution": "import re\nimport os\nfrom collections import Counter\n\ndef task_func(folder_path: str) -> dict:\n    ip_pattern = re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b')\n    ip_counter = Counter()\n\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            if file.endswith('.log'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    content = f.read()\n                    ip_addresses = ip_pattern.findall(content)\n                    ip_counter.update(ip_addresses)\n\n    return dict(ip_counter)\n```\n"}
{"task_id": "BigCodeBench/282", "solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\ndef task_func(file_path, onpick):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Load the image\n    img = cv2.imread(file_path)\n    if img is None:\n        raise ValueError(f\"The file {file_path} is not a valid image.\")\n\n    # Convert the image to RGB\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Calculate the color histogram\n    hist = cv2.calcHist([img_rgb], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n\n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_title('3D Color Histogram')\n\n    # Plot the histogram\n    x = np.arange(0, 8)\n    y = np.arange(0, 8)\n    x, y = np.meshgrid(x, y)\n    z = hist.flatten()\n    ax.scatter(x, y, z, picker=True)\n\n    # Set the color map\n    ax.set_zlim(0, np.max(hist))\n\n    # Define the onpick function\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\n\ndef task_func(file_path, onpick):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Load the image\n    img = cv2.imread(file_path)\n    if img is None:\n        raise ValueError(f\"The file {file_path} is not a valid image.\")\n\n    # Convert the image to RGB\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Calculate the color histogram\n    hist = cv2.calcHist([img_rgb], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n\n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_title('3D Color Histogram')\n\n    # Plot the histogram\n    x = np.arange(0, 8)\n    y = np.arange(0, 8)\n    x, y = np.meshgrid(x, y)\n    z = hist.flatten()\n    ax.scatter(x, y, z, picker=True)\n\n    # Set the color map\n    ax.set_zlim(0, np.max(hist))\n\n    # Define the onpick function\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/283", "solution": "import os\nimport json\nfrom collections import Counter\ndef task_func(json_files_path='./json_files/', key='name'):\n    counter = Counter()\n\n    for filename in os.listdir(json_files_path):\n        if filename.endswith('.json'):\n            with open(os.path.join(json_files_path, filename), 'r') as file:\n                data = json.load(file)\n                if key in data:\n                    counter[data[key]] += 1\n\n    return dict(counter)", "raw_solution": "import os\nimport json\nfrom collections import Counter\n\ndef task_func(json_files_path='./json_files/', key='name'):\n    counter = Counter()\n\n    for filename in os.listdir(json_files_path):\n        if filename.endswith('.json'):\n            with open(os.path.join(json_files_path, filename), 'r') as file:\n                data = json.load(file)\n                if key in data:\n                    counter[data[key]] += 1\n\n    return dict(counter)\n```\n"}
{"task_id": "BigCodeBench/284", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\ndef task_func(url):\n    # Create a browser object\n    br = mechanize.Browser()\n    br.set_handle_robots(False)  # Ignore robots.txt\n    br.addheaders = [('User-agent', 'Mozilla/5.0')]\n\n    # Open the URL\n    br.open(url)\n\n    # Read the page content\n    html = br.response().read()\n\n    # Parse the HTML content\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Find all hyperlinks\n    links = []\n    for link in soup.find_all('a', href=True):\n        href = link['href']\n        absolute_url = urljoin(url, href)\n        links.append(absolute_url)\n\n    return links", "raw_solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef task_func(url):\n    # Create a browser object\n    br = mechanize.Browser()\n    br.set_handle_robots(False)  # Ignore robots.txt\n    br.addheaders = [('User-agent', 'Mozilla/5.0')]\n\n    # Open the URL\n    br.open(url)\n\n    # Read the page content\n    html = br.response().read()\n\n    # Parse the HTML content\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Find all hyperlinks\n    links = []\n    for link in soup.find_all('a', href=True):\n        href = link['href']\n        absolute_url = urljoin(url, href)\n        links.append(absolute_url)\n\n    return links\n```\n"}
{"task_id": "BigCodeBench/285", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\ndef task_func(url, form_id, data):\n    \"\"\"\n    Mechanize is used to interact with the webpage and submit a form. The function returns the title of the response page.\n\n    Notes:\n        If the page has no title, it returns 'No Title'.\n\n    Params:\n    - url (str): The URL of the webpage with the form.\n    - form_id (int): The index of the form to be submitted.\n    - data (dict): A dictionary containing form data keys and values.\n\n    Returns:\n        str: The title of the form submission response page.\n\n    Reqs:\n    - mechanize\n    - bs4.BeautifulSoup\n\n    Examples:\n    - >>> data = {'username': 'admin', 'password': 'password'}\n    - >>> title = task_func('https://www.example.com/login', 0, data)\n    - >>> isinstance(title, str)\n    - True\n    \"\"\"\n    # Create a browser object\n    br = mechanize.Browser()\n    br.set_handle_robots(False)\n    br.addheaders = [('User-agent', 'Mozilla/5.0')]\n\n    # Open the URL\n    br.open(url)\n\n    # Select the form\n    br.select_form(nr=form_id)\n\n    # Fill the form\n    for key, value in data.items():\n        br.form[key] = value\n\n    # Submit the form\n    response = br.submit()\n\n    # Parse the response with BeautifulSoup\n    soup = BeautifulSoup(response.read(), 'html.parser')\n\n    # Get the title of the page\n    title = soup.title.string if soup.title else 'No Title'\n\n    return title", "raw_solution": "import mechanize\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, form_id, data):\n    \"\"\"\n    Mechanize is used to interact with the webpage and submit a form. The function returns the title of the response page.\n\n    Notes:\n        If the page has no title, it returns 'No Title'.\n\n    Params:\n    - url (str): The URL of the webpage with the form.\n    - form_id (int): The index of the form to be submitted.\n    - data (dict): A dictionary containing form data keys and values.\n\n    Returns:\n        str: The title of the form submission response page.\n\n    Reqs:\n    - mechanize\n    - bs4.BeautifulSoup\n\n    Examples:\n    - >>> data = {'username': 'admin', 'password': 'password'}\n    - >>> title = task_func('https://www.example.com/login', 0, data)\n    - >>> isinstance(title, str)\n    - True\n    \"\"\"\n    # Create a browser object\n    br = mechanize.Browser()\n    br.set_handle_robots(False)\n    br.addheaders = [('User-agent', 'Mozilla/5.0')]\n\n    # Open the URL\n    br.open(url)\n\n    # Select the form\n    br.select_form(nr=form_id)\n\n    # Fill the form\n    for key, value in data.items():\n        br.form[key] = value\n\n    # Submit the form\n    response = br.submit()\n\n    # Parse the response with BeautifulSoup\n    soup = BeautifulSoup(response.read(), 'html.parser')\n\n    # Get the title of the page\n    title = soup.title.string if soup.title else 'No Title'\n\n    return title\n```\n"}
{"task_id": "BigCodeBench/286", "solution": "from collections import Counter\nimport os\nimport csv\ndef task_func(output_file, test_directory):\n    \"\"\"\n    Ek specific directory mein kayi dictionary files (.txt) mein words ki number ginen,\n    counter ko CSV file mein niryaat karen, aur phir words ki total number lautayen.\n\n    Notes:\n        CSV output file ke liye shirshak \"Word\", \"Count\"\n        Yadi input invalid hai or error hui to 0 lautayen\n\n    Params:\n    - filename (str): Output CSV file ka naam.\n    - test_directory (str): dictionary files (.txt) wali directory.\n\n    Returns:\n        int: .txt files mein words ki total number\n\n    Reqs:\n    - collections.Counter\n    - os\n    - csv\n\n    Examples:\n    - >>> task_func('word_counts.csv')\n    - 10\n\n    \"\"\"\n    total_words = 0\n    word_counter = Counter()\n\n    try:\n        for filename in os.listdir(test_directory):\n            if filename.endswith('.txt'):\n                with open(os.path.join(test_directory, filename), 'r') as file:\n                    words = file.read().split()\n                    word_counter.update(words)\n                    total_words += len(words)\n\n        with open(output_file, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['Word', 'Count'])\n            for word, count in word_counter.items():\n                writer.writerow([word, count])\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0\n\n    return total_words", "raw_solution": "from collections import Counter\nimport os\nimport csv\n\n# Constants\nFILE_DIR = './yourdictfiles/'\n\ndef task_func(output_file, test_directory):\n    \"\"\"\n    Ek specific directory mein kayi dictionary files (.txt) mein words ki number ginen,\n    counter ko CSV file mein niryaat karen, aur phir words ki total number lautayen.\n\n    Notes:\n        CSV output file ke liye shirshak \"Word\", \"Count\"\n        Yadi input invalid hai or error hui to 0 lautayen\n\n    Params:\n    - filename (str): Output CSV file ka naam.\n    - test_directory (str): dictionary files (.txt) wali directory.\n\n    Returns:\n        int: .txt files mein words ki total number\n\n    Reqs:\n    - collections.Counter\n    - os\n    - csv\n\n    Examples:\n    - >>> task_func('word_counts.csv')\n    - 10\n\n    \"\"\"\n    total_words = 0\n    word_counter = Counter()\n\n    try:\n        for filename in os.listdir(test_directory):\n            if filename.endswith('.txt'):\n                with open(os.path.join(test_directory, filename), 'r') as file:\n                    words = file.read().split()\n                    word_counter.update(words)\n                    total_words += len(words)\n\n        with open(output_file, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['Word', 'Count'])\n            for word, count in word_counter.items():\n                writer.writerow([word, count])\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0\n\n    return total_words\n```\n"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    word_counter = Counter()\n    total_words = 0\n\n    for file in os.listdir(directory):\n        if file.endswith('.txt'):\n            with open(os.path.join(directory, file), 'r') as f:\n                words = f.read().split()\n                word_counter.update(words)\n                total_words += len(words)\n\n    with open(filename, 'w') as f:\n        json.dump(word_counter, f)\n\n    return total_words", "raw_solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    word_counter = Counter()\n    total_words = 0\n\n    for file in os.listdir(directory):\n        if file.endswith('.txt'):\n            with open(os.path.join(directory, file), 'r') as f:\n                words = f.read().split()\n                word_counter.update(words)\n                total_words += len(words)\n\n    with open(filename, 'w') as f:\n        json.dump(word_counter, f)\n\n    return total_words\n\n# Example usage:\n# Assuming the directory './testdir/' contains a file named 'single_file.txt' with the content 'hello world hello'\n# with open(\"./testdir/single_file.txt\",\"r\") as f: print(f.read())\n# Output: 'hello world hello'\n# count = task_func('single_file.txt', './testdir/')\n# print(count)\n# Output: 3\n```\n"}
{"task_id": "BigCodeBench/288", "solution": "import collections\nimport json\nimport os\ndef task_func(directory_path: str) -> dict:\n    key_counter = collections.Counter()\n\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory_path, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                key_counter.update(data.keys())\n\n    return dict(key_counter)", "raw_solution": "import collections\nimport json\nimport os\n\ndef task_func(directory_path: str) -> dict:\n    key_counter = collections.Counter()\n\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory_path, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                key_counter.update(data.keys())\n\n    return dict(key_counter)\n```\n"}
{"task_id": "BigCodeBench/289", "solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(X, y, n_splits=5, batch_size=32, epochs=1):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Define the model\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(20, activation='relu', input_shape=(X_scaled.shape[1],)),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Perform k-fold cross-validation\n    kfold = KFold(n_splits=n_splits, shuffle=True)\n    histories = []\n\n    for train_index, val_index in kfold.split(X_scaled):\n        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n\n        # Train the model\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n\n        # Store the history\n        histories.append(history)\n\n    return histories", "raw_solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(X, y, n_splits=5, batch_size=32, epochs=1):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Define the model\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(20, activation='relu', input_shape=(X_scaled.shape[1],)),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Perform k-fold cross-validation\n    kfold = KFold(n_splits=n_splits, shuffle=True)\n    histories = []\n\n    for train_index, val_index in kfold.split(X_scaled):\n        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n\n        # Train the model\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n\n        # Store the history\n        histories.append(history)\n\n    return histories\n```\n"}
{"task_id": "BigCodeBench/290", "solution": "import nltk\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(directory_path):\n    \"\"\"\n    Ek specified directory mein sabhi '.txt' files mein unique non-stop shabdon ki sankhya ginen.\n\n    Params:\n    - directory_path (str): Us directory ka path jismein '.txt' files hain.\n\n    Returns:\n        int: Sabhi files mein unique non-stop shabdon ki kul count.\n\n    Reqs:\n    - collections.Counter\n    - os\n    - nltk.corpus.stopwords\n\n    Examples:\n    - >>> task_func('./yourdictfiles/')\n    - 1500\n\n    \"\"\"\n    word_counter = Counter()\n\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                for line in file:\n                    words = line.split()\n                    for word in words:\n                        word = word.lower().strip('.,!?;:\"()')\n                        if word and word not in STOPWORDS:\n                            word_counter[word] += 1\n\n    return len(word_counter)", "raw_solution": "import nltk\nnltk.download('stopwords')\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(directory_path):\n    \"\"\"\n    Ek specified directory mein sabhi '.txt' files mein unique non-stop shabdon ki sankhya ginen.\n\n    Params:\n    - directory_path (str): Us directory ka path jismein '.txt' files hain.\n\n    Returns:\n        int: Sabhi files mein unique non-stop shabdon ki kul count.\n\n    Reqs:\n    - collections.Counter\n    - os\n    - nltk.corpus.stopwords\n\n    Examples:\n    - >>> task_func('./yourdictfiles/')\n    - 1500\n\n    \"\"\"\n    word_counter = Counter()\n\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                for line in file:\n                    words = line.split()\n                    for word in words:\n                        word = word.lower().strip('.,!?;:\"()')\n                        if word and word not in STOPWORDS:\n                            word_counter[word] += 1\n\n    return len(word_counter)\n```\nThis script counts the unique non-stop words in all '.txt' files in the specified directory. It uses the `nltk.corpus.stopwords` to filter out common stop words, and `collections.Counter` to count the occurrences of each word. The function returns the total count of unique non-stop words.\n"}
{"task_id": "BigCodeBench/291", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\ndef task_func(mu, sigma, seed=0):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, 1000)\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(data, bins=30, kde=True, color='skyblue')\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    plt.show()\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ndef task_func(mu, sigma, seed=0):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, 1000)\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(data, bins=30, kde=True, color='skyblue')\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    plt.show()\n    return ax\n```\n"}
{"task_id": "BigCodeBench/292", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(df):\n    # Check if the required columns exist in the dataframe\n    if not {'id', 'age', 'income'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n\n    # Group by 'id' and apply MinMaxScaler to 'age' and 'income'\n    scaler = MinMaxScaler()\n    df_grouped = df.groupby('id').apply(lambda x: pd.DataFrame(scaler.fit_transform(x[['age', 'income']]), columns=['age', 'income'])).reset_index(drop=True)\n\n    # Generate histogram data for 'income'\n    income_hist = df_grouped['income'].hist(bins=10)\n\n    return df_grouped, income_hist", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(df):\n    # Check if the required columns exist in the dataframe\n    if not {'id', 'age', 'income'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n\n    # Group by 'id' and apply MinMaxScaler to 'age' and 'income'\n    scaler = MinMaxScaler()\n    df_grouped = df.groupby('id').apply(lambda x: pd.DataFrame(scaler.fit_transform(x[['age', 'income']]), columns=['age', 'income'])).reset_index(drop=True)\n\n    # Generate histogram data for 'income'\n    income_hist = df_grouped['income'].hist(bins=10)\n\n    return df_grouped, income_hist\n\n# Example usage:\n# df = pd.DataFrame({'id': [1, 1, 2, 2, 3, 3], 'age': [25, 26, 35, 36, 28, 29],'income': [50000, 60000, 70000, 80000, 90000, 100000]})\n# df_scaled, income_hist = task_func(df)\n# print(df_scaled.iloc[0]['age'])\n# print(df_scaled.iloc[0]['income'])\n```\n"}
{"task_id": "BigCodeBench/293", "solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(elements, subset_size):\n    # Generate all combinations of the specified subset size\n    combinations = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sum of each combination\n    sums = [sum(comb) for comb in combinations]\n    \n    # Plot a histogram of the sums\n    fig, ax = plt.subplots()\n    ax.hist(sums, bins=np.arange(min(sums), max(sums) + 1) - 0.5, edgecolor='black')\n    ax.set_xlabel('Sum')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Subset Sums')\n    \n    return ax, combinations, sums", "raw_solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(elements, subset_size):\n    # Generate all combinations of the specified subset size\n    combinations = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sum of each combination\n    sums = [sum(comb) for comb in combinations]\n    \n    # Plot a histogram of the sums\n    fig, ax = plt.subplots()\n    ax.hist(sums, bins=np.arange(min(sums), max(sums) + 1) - 0.5, edgecolor='black')\n    ax.set_xlabel('Sum')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Subset Sums')\n    \n    return ax, combinations, sums\n```\n"}
{"task_id": "BigCodeBench/294", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    if not {'id', 'age', 'income'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n    \n    scaler = StandardScaler()\n    df['age'] = scaler.fit_transform(df[['age']])\n    df['income'] = scaler.fit_transform(df[['income']])\n    \n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    if not {'id', 'age', 'income'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n    \n    scaler = StandardScaler()\n    df['age'] = scaler.fit_transform(df[['age']])\n    df['income'] = scaler.fit_transform(df[['income']])\n    \n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/295", "solution": "import itertools\nimport statistics\ndef task_func(elements, subset_size):\n    subsets = list(itertools.combinations(elements, subset_size))\n    subset_means = [statistics.mean(subset) for subset in subsets]\n    mean = statistics.mean(subset_means)\n    median = statistics.median(subset_means)\n    mode = statistics.mode(subset_means)\n\n    return {'mean': mean, 'median': median, 'mode': mode}", "raw_solution": "import itertools\nimport statistics\n\ndef task_func(elements, subset_size):\n    subsets = list(itertools.combinations(elements, subset_size))\n    subset_means = [statistics.mean(subset) for subset in subsets]\n    mean = statistics.mean(subset_means)\n    median = statistics.median(subset_means)\n    mode = statistics.mode(subset_means)\n\n    return {'mean': mean, 'median': median, 'mode': mode}\n\n# Test cases"}
{"task_id": "BigCodeBench/296", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'value' not in df.columns:\n        raise ValueError(\"Input must be a pandas DataFrame with a 'value' column.\")\n\n    # Calculate the count of each unique value\n    value_counts = df['value'].value_counts()\n\n    # Create a bar chart\n    ax = value_counts.plot(kind='bar', color='skyblue', figsize=(10, 6))\n    ax.set_title('Value Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Count')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'value' not in df.columns:\n        raise ValueError(\"Input must be a pandas DataFrame with a 'value' column.\")\n\n    # Calculate the count of each unique value\n    value_counts = df['value'].value_counts()\n\n    # Create a bar chart\n    ax = value_counts.plot(kind='bar', color='skyblue', figsize=(10, 6))\n    ax.set_title('Value Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Count')\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'id': [1, 1, 2, 2, 3, 3],'value': ['A', 'B', 'A', 'B', 'A', 'B']})\n# ax = task_func(df)\n# print(len(ax.patches))  # Should print 2\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/297", "solution": "import itertools\nimport collections\ndef task_func(elements, subset_size):\n    \"\"\"\n    Tuple ke sabhi 2-element subsets banaen aur subsets mein pratyek sum ki occurrences ginen.\n\n    Returns:\n        dict: ek dict jismein sum aur unki counts hon.\n\n    Reqs:\n    - itertools\n    - random\n    - collections\n\n    Examples:\n    - >>> dict(task_func((1, 2, 3, 4, 5), 2))\n    - {3: 1, 4: 1, 5: 2, 6: 2, 7: 2, 8: 1, 9: 1}\n\n    \"\"\"\n    # Generate all possible subsets of the given size\n    subsets = itertools.combinations(elements, subset_size)\n    \n    # Calculate the sum of each subset\n    subset_sums = [sum(subset) for subset in subsets]\n    \n    # Count occurrences of each sum\n    sum_counts = collections.Counter(subset_sums)\n    \n    return dict(sum_counts)", "raw_solution": "import itertools\nimport collections\n\ndef task_func(elements, subset_size):\n    \"\"\"\n    Tuple ke sabhi 2-element subsets banaen aur subsets mein pratyek sum ki occurrences ginen.\n\n    Returns:\n        dict: ek dict jismein sum aur unki counts hon.\n\n    Reqs:\n    - itertools\n    - random\n    - collections\n\n    Examples:\n    - >>> dict(task_func((1, 2, 3, 4, 5), 2))\n    - {3: 1, 4: 1, 5: 2, 6: 2, 7: 2, 8: 1, 9: 1}\n\n    \"\"\"\n    # Generate all possible subsets of the given size\n    subsets = itertools.combinations(elements, subset_size)\n    \n    # Calculate the sum of each subset\n    subset_sums = [sum(subset) for subset in subsets]\n    \n    # Count occurrences of each sum\n    sum_counts = collections.Counter(subset_sums)\n    \n    return dict(sum_counts)\n```\n"}
{"task_id": "BigCodeBench/298", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(\"DataFrame must contain 'Date' and 'Value' columns\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    # Convert the 'Value' column from string to list of floats\n    df['Value'] = df['Value'].apply(eval)\n\n    # Create a DataFrame with one column for each element in the 'Value' list\n    value_df = pd.DataFrame(df['Value'].to_list(), index=df.index)\n\n    # Scale the DataFrame\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(value_df)\n    scaled_df = pd.DataFrame(scaled_values, index=df.index, columns=[f'Scaled_{i+1}' for i in range(scaled_values.shape[1])])\n\n    if plot:\n        ax = scaled_df.plot(kind='bar', figsize=(10, 6))\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Scaled Value')\n        ax.set_title('Scaled Values Over Time')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return scaled_df, ax\n\n    return scaled_df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(\"DataFrame must contain 'Date' and 'Value' columns\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    # Convert the 'Value' column from string to list of floats\n    df['Value'] = df['Value'].apply(eval)\n\n    # Create a DataFrame with one column for each element in the 'Value' list\n    value_df = pd.DataFrame(df['Value'].to_list(), index=df.index)\n\n    # Scale the DataFrame\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(value_df)\n    scaled_df = pd.DataFrame(scaled_values, index=df.index, columns=[f'Scaled_{i+1}' for i in range(scaled_values.shape[1])])\n\n    if plot:\n        ax = scaled_df.plot(kind='bar', figsize=(10, 6))\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Scaled Value')\n        ax.set_title('Scaled Values Over Time')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return scaled_df, ax\n\n    return scaled_df\n```\n"}
{"task_id": "BigCodeBench/299", "solution": "import itertools\nimport math\nfrom pandas import Series\ndef task_func(elements, subset_size, top_n=2):\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n    product = math.prod(sums)\n\n    if top_n is not None:\n        top_sums = Series(sums).nlargest(top_n)\n    else:\n        top_sums = []\n\n    return product, top_sums", "raw_solution": "import itertools\nimport math\nfrom pandas import Series\n\ndef task_func(elements, subset_size, top_n=2):\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n    product = math.prod(sums)\n\n    if top_n is not None:\n        top_sums = Series(sums).nlargest(top_n)\n    else:\n        top_sums = []\n\n    return product, top_sums\n\n# Example usage:\n# prod, sums = task_func((1, 2, 3), 2)\n# print(prod)  # Output: 60\n# print(list(sums))  # Output: [5, 4]\n```\n"}
{"task_id": "BigCodeBench/300", "solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise KeyError(\"DataFrame must contain 'Date' and 'Value' columns.\")\n\n    # Convert 'Date' to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Flatten the 'Value' list\n    df['Value'] = df['Value'].apply(pd.Series).stack().reset_index(drop=True)\n\n    # Calculate Z-scores\n    df['Z-Score'] = zscore(df['Value'])\n\n    # Create a box plot of Z-Scores over time\n    fig, ax = plt.subplots()\n    df.boxplot(column='Z-Score', by='Date', ax=ax)\n    ax.set_title('Z-Scores Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Z-Score')\n\n    return df, fig", "raw_solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise KeyError(\"DataFrame must contain 'Date' and 'Value' columns.\")\n\n    # Convert 'Date' to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Flatten the 'Value' list\n    df['Value'] = df['Value'].apply(pd.Series).stack().reset_index(drop=True)\n\n    # Calculate Z-scores\n    df['Z-Score'] = zscore(df['Value'])\n\n    # Create a box plot of Z-Scores over time\n    fig, ax = plt.subplots()\n    df.boxplot(column='Z-Score', by='Date', ax=ax)\n    ax.set_title('Z-Scores Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Z-Score')\n\n    return df, fig\n```\n"}
{"task_id": "BigCodeBench/301", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    date aur time ke aadhaar par solar activity kee ganana karen, 11 years ke solar cycle ko dhyaan mein rakhate hue.\n\n    Params:\n    - date_str (str): \"yyyy-mm-dd hh:mm:ss\" format mein date string.\n    - from_tz (str): die gae date string ka from_tz.\n    - to_tz (str): from_tz jisamen dee gaee date aur time ko parivartit kiya jaana chaahie.\n\n    Returns:\n        float: 0 aur 1 ke beech solar activity. yah maan solar activity ko darashaata hai\n        sabase kareeb solar cycle year ke baad se years ke aadhaar par cosine function ka upayog karake ganana kee gaee.\n\n    Reqs:\n    - pytz\n    - numpy\n    - dateutil.parser\n    - math\n\n    Examples:\n    - >>> task_func('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    - 0.14231483827328487\n    - >>> task_func('1990-01-01 00:00:00', 'UTC', 'America/New_York')\n    - 0.6548607339452851\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    from_zone = pytz.timezone(from_tz)\n    to_zone = pytz.timezone(to_tz)\n    naive_date = parse(date_str)\n    aware_date = from_zone.localize(naive_date)\n    converted_date = aware_date.astimezone(to_zone)\n\n    # Calculate the solar activity based on the solar cycle years\n    solar_activity = np.mean([math.cos(math.radians((converted_date.year - year) * 360 / 11)) for year in SOLAR_CYCLE_YEARS])\n\n    return solar_activity", "raw_solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    date aur time ke aadhaar par solar activity kee ganana karen, 11 years ke solar cycle ko dhyaan mein rakhate hue.\n\n    Params:\n    - date_str (str): \"yyyy-mm-dd hh:mm:ss\" format mein date string.\n    - from_tz (str): die gae date string ka from_tz.\n    - to_tz (str): from_tz jisamen dee gaee date aur time ko parivartit kiya jaana chaahie.\n\n    Returns:\n        float: 0 aur 1 ke beech solar activity. yah maan solar activity ko darashaata hai\n        sabase kareeb solar cycle year ke baad se years ke aadhaar par cosine function ka upayog karake ganana kee gaee.\n\n    Reqs:\n    - pytz\n    - numpy\n    - dateutil.parser\n    - math\n\n    Examples:\n    - >>> task_func('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    - 0.14231483827328487\n    - >>> task_func('1990-01-01 00:00:00', 'UTC', 'America/New_York')\n    - 0.6548607339452851\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    from_zone = pytz.timezone(from_tz)\n    to_zone = pytz.timezone(to_tz)\n    naive_date = parse(date_str)\n    aware_date = from_zone.localize(naive_date)\n    converted_date = aware_date.astimezone(to_zone)\n\n    # Calculate the solar activity based on the solar cycle years\n    solar_activity = np.mean([math.cos(math.radians((converted_date.year - year) * 360 / 11)) for year in SOLAR_CYCLE_YEARS])\n\n    return solar_activity\n```\n"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    if df.empty or not all(col in df.columns for col in COLUMNS):\n        raise ValueError(\"DataFrame must be non-empty and contain 'Date' and 'Value' columns.\")\n\n    # Convert 'Value' column to list of lists\n    df['Value'] = df['Value'].apply(lambda x: [float(i) for i in x])\n\n    # Calculate correlation matrix\n    corr_matrix = df['Value'].apply(pd.Series).corr()\n\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})\n        plt.title('Correlation Heatmap')\n        plt.show()\n        return corr_matrix, ax\n\n    return corr_matrix", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    if df.empty or not all(col in df.columns for col in COLUMNS):\n        raise ValueError(\"DataFrame must be non-empty and contain 'Date' and 'Value' columns.\")\n\n    # Convert 'Value' column to list of lists\n    df['Value'] = df['Value'].apply(lambda x: [float(i) for i in x])\n\n    # Calculate correlation matrix\n    corr_matrix = df['Value'].apply(pd.Series).corr()\n\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})\n        plt.title('Correlation Heatmap')\n        plt.show()\n        return corr_matrix, ax\n\n    return corr_matrix\n\n# Example usage:\n# df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]], columns=['Date', 'Value'])\n# corr_df = task_func(df)\n# print(corr_df[0][0])\n```\n"}
{"task_id": "BigCodeBench/303", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    This function calculates the moon phase based on the given date and time, considering the 7-year cycle of the moon phases.\n\n    Params:\n    - date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    - from_tz (str): The timezone of the given date string.\n    - to_tz (str): The timezone to which the given date and time should be converted.\n\n    Returns:\n        float: The moon phase between 0 and 1. A value of 0 indicates a new moon and a value of 1 indicates a full moon.\n\n    Reqs:\n    - pytz\n    - numpy\n    - dateutil.parser\n    - math\n\n    Examples:\n    - >>> task_func('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    - 0.9749279121818237\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    from_zone = pytz.timezone(from_tz)\n    to_zone = pytz.timezone(to_tz)\n    naive_date = parse(date_str)\n    aware_date = from_zone.localize(naive_date)\n    converted_date = aware_date.astimezone(to_zone)\n\n    # Calculate the moon phase based on the 7-year cycle\n    year = converted_date.year\n    cycle_index = np.searchsorted(MOON_PHASES_YEARS, year)\n    cycle_offset = (year - MOON_PHASES_YEARS[cycle_index - 1]) / 7\n    moon_phase = (1 + math.cos(math.pi * (cycle_offset - 0.5))) / 2\n\n    return moon_phase", "raw_solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    This function calculates the moon phase based on the given date and time, considering the 7-year cycle of the moon phases.\n\n    Params:\n    - date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    - from_tz (str): The timezone of the given date string.\n    - to_tz (str): The timezone to which the given date and time should be converted.\n\n    Returns:\n        float: The moon phase between 0 and 1. A value of 0 indicates a new moon and a value of 1 indicates a full moon.\n\n    Reqs:\n    - pytz\n    - numpy\n    - dateutil.parser\n    - math\n\n    Examples:\n    - >>> task_func('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    - 0.9749279121818237\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    from_zone = pytz.timezone(from_tz)\n    to_zone = pytz.timezone(to_tz)\n    naive_date = parse(date_str)\n    aware_date = from_zone.localize(naive_date)\n    converted_date = aware_date.astimezone(to_zone)\n\n    # Calculate the moon phase based on the 7-year cycle\n    year = converted_date.year\n    cycle_index = np.searchsorted(MOON_PHASES_YEARS, year)\n    cycle_offset = (year - MOON_PHASES_YEARS[cycle_index - 1]) / 7\n    moon_phase = (1 + math.cos(math.pi * (cycle_offset - 0.5))) / 2\n\n    return moon_phase\n```\n"}
{"task_id": "BigCodeBench/304", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if df.empty:\n        return (0,0)\n\n    # Convert the lists in the 'Value' column to separate columns\n    df_expanded = df.explode('Value')\n\n    # Perform PCA on the 'Value' column\n    pca = PCA(n_components=2)\n    pca.fit(df_expanded['Value'].apply(pd.Series).values)\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Plot the explained variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(2), explained_variance_ratio)\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n    ax.set_title('Explained Variance Ratio of Principal Components')\n\n    return (explained_variance_ratio, ax)", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if df.empty:\n        return (0,0)\n\n    # Convert the lists in the 'Value' column to separate columns\n    df_expanded = df.explode('Value')\n\n    # Perform PCA on the 'Value' column\n    pca = PCA(n_components=2)\n    pca.fit(df_expanded['Value'].apply(pd.Series).values)\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Plot the explained variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(2), explained_variance_ratio)\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n    ax.set_title('Explained Variance Ratio of Principal Components')\n\n    return (explained_variance_ratio, ax)\n```\n"}
{"task_id": "BigCodeBench/305", "solution": "from collections import Counter\nimport itertools\nimport random\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    all_chars = list(itertools.chain.from_iterable(list_of_lists))\n    if not all_chars:\n        return Counter()\n\n    # Randomly sample characters from the alphabet\n    sampled_chars = random.sample(ALPHABET, len(all_chars))\n\n    # Count the occurrences of each character in the sampled characters\n    char_count = Counter(sampled_chars)\n\n    # Count the occurrences of each character in the original lists\n    for lst in list_of_lists:\n        char_count.update(lst)\n\n    return char_count", "raw_solution": "from collections import Counter\nimport itertools\nimport random\n\n# Constants\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    all_chars = list(itertools.chain.from_iterable(list_of_lists))\n    if not all_chars:\n        return Counter()\n\n    # Randomly sample characters from the alphabet\n    sampled_chars = random.sample(ALPHABET, len(all_chars))\n\n    # Count the occurrences of each character in the sampled characters\n    char_count = Counter(sampled_chars)\n\n    # Count the occurrences of each character in the original lists\n    for lst in list_of_lists:\n        char_count.update(lst)\n\n    return char_count\n\n# Example usage:\n# print(task_func([['a', 'b', 'c'], [], ['d', 'e', 'f']]))\n```\n"}
{"task_id": "BigCodeBench/306", "solution": "import os\nimport logging\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    removed_files = []\n    logger = logging.getLogger('jquery_removal')\n    logger.setLevel(logging.INFO)\n    handler = logging.FileHandler('jquery_removal.log')\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    for filename in os.listdir(directory):\n        if filename.startswith('jquery') and filename.endswith('.js'):\n            file_path = os.path.join(directory, filename)\n            os.remove(file_path)\n            removed_files.append(filename)\n            logger.info(f\"Removed file: {filename}\")\n\n    return len(removed_files), removed_files", "raw_solution": "import os\nimport logging\n\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    removed_files = []\n    logger = logging.getLogger('jquery_removal')\n    logger.setLevel(logging.INFO)\n    handler = logging.FileHandler('jquery_removal.log')\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    for filename in os.listdir(directory):\n        if filename.startswith('jquery') and filename.endswith('.js'):\n            file_path = os.path.join(directory, filename)\n            os.remove(file_path)\n            removed_files.append(filename)\n            logger.info(f\"Removed file: {filename}\")\n\n    return len(removed_files), removed_files\n```\n"}
{"task_id": "BigCodeBench/307", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    combined_data = []\n\n    for sublist in list_of_lists:\n        if not sublist:\n            sublist = [random.randint(0, 100) for _ in range(5)]\n        combined_data.extend(sublist)\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(combined_data, bins=10, kde=True)\n    ax.set_title('Combined Data Histogram')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    combined_data = []\n\n    for sublist in list_of_lists:\n        if not sublist:\n            sublist = [random.randint(0, 100) for _ in range(5)]\n        combined_data.extend(sublist)\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(combined_data, bins=10, kde=True)\n    ax.set_title('Combined Data Histogram')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax\n\n# Example usage:\n# plot = task_func([[1, 2, 3], [], [4, 5, 6]])\n# print(type(plot))\n```\nThis script first checks if a sublist is empty and if so, it fills it with 5 random integers between 0 and 100. It then combines all the integers into a single list and plots a histogram using seaborn. The function returns the matplotlib axes object for further manipulation if needed.\n"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Create a list of all fields including additional ones\n    all_fields = FIELDS + additional_fields\n\n    # Generate random grades for each student and field\n    report_data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in all_fields}\n\n    # Create a DataFrame from the report data\n    report_df = pd.DataFrame(report_data, index=STUDENTS)\n\n    # Calculate the average grade for each student\n    report_df['Average Grade'] = report_df.apply(mean, axis=1)\n\n    # Calculate the average grade for each field\n    for field in all_fields:\n        report_df[field + ' Average'] = report_df[field].apply(mean)\n\n    return report_df", "raw_solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    # Create a list of all fields including additional ones\n    all_fields = FIELDS + additional_fields\n\n    # Generate random grades for each student and field\n    report_data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in all_fields}\n\n    # Create a DataFrame from the report data\n    report_df = pd.DataFrame(report_data, index=STUDENTS)\n\n    # Calculate the average grade for each student\n    report_df['Average Grade'] = report_df.apply(mean, axis=1)\n\n    # Calculate the average grade for each field\n    for field in all_fields:\n        report_df[field + ' Average'] = report_df[field].apply(mean)\n\n    return report_df\n\n# Example usage:\nrandom.seed(0)\nreport = task_func(['Computer Science', 'Geography'])"}
{"task_id": "BigCodeBench/309", "solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_lists, seed=42):\n    random.seed(seed)\n    scaler = MinMaxScaler()\n\n    scaled_lists = []\n    for inner_list in list_of_lists:\n        if not inner_list:\n            # If the inner list is empty, generate 3 random integers between 0 and 100\n            inner_list = [random.randint(0, 100) for _ in range(3)]\n        scaled_list = scaler.fit_transform([inner_list])\n        scaled_lists.append(scaled_list.tolist()[0])\n\n    return scaled_lists", "raw_solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(list_of_lists, seed=42):\n    random.seed(seed)\n    scaler = MinMaxScaler()\n\n    scaled_lists = []\n    for inner_list in list_of_lists:\n        if not inner_list:\n            # If the inner list is empty, generate 3 random integers between 0 and 100\n            inner_list = [random.randint(0, 100) for _ in range(3)]\n        scaled_list = scaler.fit_transform([inner_list])\n        scaled_lists.append(scaled_list.tolist()[0])\n\n    return scaled_lists\n\n# Test cases"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate random data\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        person = {\n            'Name': f'Person{random.randint(1, 1000)}',\n            'Age': random.randint(18, 70),\n            'Height': random.uniform(150, 200),\n            'Weight': random.uniform(50, 120)\n        }\n        data.append(person)\n\n    # Calculate averages\n    averages = {\n        'Age': mean(person['Age'] for person in data),\n        'Height': mean(person['Height'] for person in data),\n        'Weight': mean(person['Weight'] for person in data)\n    }\n\n    # Write data to CSV\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=COLUMNS)\n        writer.writeheader()\n        writer.writerows(data)\n        writer.writerow({'Name': 'Average', 'Age': averages['Age'], 'Height': averages['Height'], 'Weight': averages['Weight']})\n\n    return os.path.abspath(filename)", "raw_solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate random data\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        person = {\n            'Name': f'Person{random.randint(1, 1000)}',\n            'Age': random.randint(18, 70),\n            'Height': random.uniform(150, 200),\n            'Weight': random.uniform(50, 120)\n        }\n        data.append(person)\n\n    # Calculate averages\n    averages = {\n        'Age': mean(person['Age'] for person in data),\n        'Height': mean(person['Height'] for person in data),\n        'Weight': mean(person['Weight'] for person in data)\n    }\n\n    # Write data to CSV\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=COLUMNS)\n        writer.writeheader()\n        writer.writerows(data)\n        writer.writerow({'Name': 'Average', 'Age': averages['Age'], 'Height': averages['Height'], 'Weight': averages['Weight']})\n\n    return os.path.abspath(filename)\n\n# Example usage"}
{"task_id": "BigCodeBench/311", "solution": "import numpy as np\nimport random\nfrom scipy import stats\ndef task_func(list_of_lists, size=5, seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    all_values = []\n    for lst in list_of_lists:\n        if lst:\n            all_values.extend(lst)\n        else:\n            all_values.extend(np.random.randint(0, 101, size))\n\n    mean = np.mean(all_values)\n    median = np.median(all_values)\n    mode = stats.mode(all_values)[0][0]\n\n    return {'mean': mean, 'median': median, 'mode': mode}", "raw_solution": "import numpy as np\nimport random\nfrom scipy import stats\n\ndef task_func(list_of_lists, size=5, seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    all_values = []\n    for lst in list_of_lists:\n        if lst:\n            all_values.extend(lst)\n        else:\n            all_values.extend(np.random.randint(0, 101, size))\n\n    mean = np.mean(all_values)\n    median = np.median(all_values)\n    mode = stats.mode(all_values)[0][0]\n\n    return {'mean': mean, 'median': median, 'mode': mode}\n\n# Example usage:\n# print(task_func([[1, 2, 3], [], [4, 5, 6]]))\n```\n"}
{"task_id": "BigCodeBench/312", "solution": "import random\nimport matplotlib.pyplot as plt\nDISTRIBUTION_SIZE = 1000\ndef task_func(bins=30):\n    \"\"\"\n    Ek Gaussian distribution banaen aur iska histogram plot Karen.\n\n    Params:\n    - bins (int, optional): histogram ke liye dibbe ki sankhya. Default 30 hai.\n\n    Returns:\n        tuple: distribution list aur histogram plot ka Axes patch object yukt ek tuple.\n\n    Reqs:\n    - random\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> distribution, ax = task_func()\n    - >>> len(ax.patches) == 30\n    - True\n    - >>> len(distribution)\n    - 1000\n    - >>> plt.close()\n\n    \"\"\"\n    # Generate a Gaussian distribution\n    distribution = [random.gauss(0, 1) for _ in range(DISTRIBUTION_SIZE)]\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(distribution, bins=bins, color='blue', alpha=0.7)\n    ax.set_title('Histogram of Gaussian Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return distribution, ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\n# Constants\nDISTRIBUTION_SIZE = 1000\n\ndef task_func(bins=30):\n    \"\"\"\n    Ek Gaussian distribution banaen aur iska histogram plot Karen.\n\n    Params:\n    - bins (int, optional): histogram ke liye dibbe ki sankhya. Default 30 hai.\n\n    Returns:\n        tuple: distribution list aur histogram plot ka Axes patch object yukt ek tuple.\n\n    Reqs:\n    - random\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> distribution, ax = task_func()\n    - >>> len(ax.patches) == 30\n    - True\n    - >>> len(distribution)\n    - 1000\n    - >>> plt.close()\n\n    \"\"\"\n    # Generate a Gaussian distribution\n    distribution = [random.gauss(0, 1) for _ in range(DISTRIBUTION_SIZE)]\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(distribution, bins=bins, color='blue', alpha=0.7)\n    ax.set_title('Histogram of Gaussian Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return distribution, ax\n```\n"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    files = {}\n    for root, dirs, files_in_dir in os.walk(directory):\n        for file in files_in_dir:\n            if not re.search(r'\\[\\w+\\]', file):\n                new_dir = os.path.join(root, file)\n                new_file_name = f\"{file}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n                new_file_path = os.path.join(new_dir, new_file_name)\n                shutil.move(os.path.join(root, file), new_file_path)\n                if file not in files:\n                    files[file] = []\n                files[file].append(new_file_name)\n    \n    return directory, files", "raw_solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    files = {}\n    for root, dirs, files_in_dir in os.walk(directory):\n        for file in files_in_dir:\n            if not re.search(r'\\[\\w+\\]', file):\n                new_dir = os.path.join(root, file)\n                new_file_name = f\"{file}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n                new_file_path = os.path.join(new_dir, new_file_name)\n                shutil.move(os.path.join(root, file), new_file_path)\n                if file not in files:\n                    files[file] = []\n                files[file].append(new_file_name)\n    \n    return directory, files\n```\nThis script walks through the given directory, checks for files without square brackets in their names, and moves them into a new directory with a timestamp appended to the file name. It also keeps track of the new file names in a dictionary.\n"}
{"task_id": "BigCodeBench/314", "solution": "import socket\nimport ssl\nimport http.client\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    context = ssl.create_default_context()\n\n    with socket.create_connection((SERVER_NAME, SERVER_PORT)) as sock:\n        with context.wrap_socket(sock, server_hostname=SERVER_NAME) as ssock:\n            conn = http.client.HTTPSConnection(SERVER_NAME, context=context)\n            conn.request(\"GET\", path)\n            response = conn.getresponse()\n            response_body = response.read().decode('utf-8')\n            conn.close()\n            return response_body", "raw_solution": "import socket\nimport ssl\nimport http.client\n\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    context = ssl.create_default_context()\n\n    with socket.create_connection((SERVER_NAME, SERVER_PORT)) as sock:\n        with context.wrap_socket(sock, server_hostname=SERVER_NAME) as ssock:\n            conn = http.client.HTTPSConnection(SERVER_NAME, context=context)\n            conn.request(\"GET\", path)\n            response = conn.getresponse()\n            response_body = response.read().decode('utf-8')\n            conn.close()\n            return response_body\n\n# Example usage:\n# response = task_func('www.example.com', 443, '/path/to/request')\n# print(isinstance(response, str))  # Should print: True\n```\n"}
{"task_id": "BigCodeBench/315", "solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\ndef task_func(dir, api_key, recipient_email):\n    if not os.path.exists(dir):\n        raise FileNotFoundError(f\"The directory {dir} does not exist.\")\n    \n    try:\n        files = os.listdir(dir)\n        message = Mail(\n            from_email='from@example.com',\n            to_emails=recipient_email,\n            subject='Directory Listing',\n            html_content=f'<strong>Files in {dir}:</strong><br/>{\"<br/>\".join(files)}')\n        \n        sg = SendGridAPIClient(api_key)\n        response = sg.send(message)\n        \n        return response.status_code // 100 == 2  # Check if status code is in the 2xx range\n\n    except HTTPError as e:\n        print(e)\n        return False\n    except Exception as e:\n        print(e)\n        return False", "raw_solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\n\ndef task_func(dir, api_key, recipient_email):\n    if not os.path.exists(dir):\n        raise FileNotFoundError(f\"The directory {dir} does not exist.\")\n    \n    try:\n        files = os.listdir(dir)\n        message = Mail(\n            from_email='from@example.com',\n            to_emails=recipient_email,\n            subject='Directory Listing',\n            html_content=f'<strong>Files in {dir}:</strong><br/>{\"<br/>\".join(files)}')\n        \n        sg = SendGridAPIClient(api_key)\n        response = sg.send(message)\n        \n        return response.status_code // 100 == 2  # Check if status code is in the 2xx range\n\n    except HTTPError as e:\n        print(e)\n        return False\n    except Exception as e:\n        print(e)\n        return False\n```\n"}
{"task_id": "BigCodeBench/316", "solution": "import pandas as pd\nimport random\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\ndef task_func(value_range=(0, 100)):\n    \"\"\"\n    Ek DataFrame ke roop mein specified range ke bheetar ek category distribution utpann karein aur lautayein.\n\n    Params:\n    - value_range (tuple): categories ke liye random maan utpann karne ke liye range (min, max) specified karne wala ek tuple.\n\n    Returns:\n        DataFrame: Ek pandas DataFrame jismein do columns hain: 'Category' (category ke naam) aur 'Count' (pratyek category ki count).\n\n    Reqs:\n    - pandas\n    - random\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> df = task_func()\n    - >>> df['Count'][0] >= 0\n    - true\n\n    \"\"\"\n    category_counts = {category: random.randint(*value_range) for category in CATEGORIES}\n    df = pd.DataFrame(list(category_counts.items()), columns=['Category', 'Count'])\n    return df", "raw_solution": "import pandas as pd\nimport random\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(value_range=(0, 100)):\n    \"\"\"\n    Ek DataFrame ke roop mein specified range ke bheetar ek category distribution utpann karein aur lautayein.\n\n    Params:\n    - value_range (tuple): categories ke liye random maan utpann karne ke liye range (min, max) specified karne wala ek tuple.\n\n    Returns:\n        DataFrame: Ek pandas DataFrame jismein do columns hain: 'Category' (category ke naam) aur 'Count' (pratyek category ki count).\n\n    Reqs:\n    - pandas\n    - random\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> df = task_func()\n    - >>> df['Count'][0] >= 0\n    - true\n\n    \"\"\"\n    category_counts = {category: random.randint(*value_range) for category in CATEGORIES}\n    df = pd.DataFrame(list(category_counts.items()), columns=['Category', 'Count'])\n    return df\n\n# Example usage:\nrandom.seed(0)\ndf = task_func()"}
{"task_id": "BigCodeBench/317", "solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\ndef task_func(example_str):\n    # Remove special characters and numbers\n    cleaned_str = re.sub(r'[^a-zA-Z\\s]', '', example_str)\n\n    # Convert to lowercase\n    cleaned_str = cleaned_str.lower()\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the cleaned string\n    tfidf_matrix = vectorizer.fit_transform([cleaned_str])\n\n    # Get feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Create a dictionary to hold the TF-IDF scores\n    tfidf_scores = {}\n\n    # Populate the dictionary with TF-IDF scores\n    for i, score in enumerate(tfidf_matrix.toarray()[0]):\n        tfidf_scores[feature_names[i]] = score\n\n    return tfidf_scores\nexample_str = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003] Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"", "raw_solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\ndef task_func(example_str):\n    # Remove special characters and numbers\n    cleaned_str = re.sub(r'[^a-zA-Z\\s]', '', example_str)\n\n    # Convert to lowercase\n    cleaned_str = cleaned_str.lower()\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the cleaned string\n    tfidf_matrix = vectorizer.fit_transform([cleaned_str])\n\n    # Get feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Create a dictionary to hold the TF-IDF scores\n    tfidf_scores = {}\n\n    # Populate the dictionary with TF-IDF scores\n    for i, score in enumerate(tfidf_matrix.toarray()[0]):\n        tfidf_scores[feature_names[i]] = score\n\n    return tfidf_scores\n\n# Example usage\nexample_str = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003] Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\ntfidf_scores = task_func(example_str)"}
{"task_id": "BigCodeBench/318", "solution": "import random\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(points_count=1000, radius=1):\n    # Generate random points within the circle\n    points = []\n    for _ in range(points_count):\n        angle = random.uniform(0, 2 * math.pi)\n        r = random.uniform(0, radius)\n        x = r * math.cos(angle)\n        y = r * math.sin(angle)\n        points.append((x, y))\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(*zip(*points))\n    ax.set_aspect('equal', 'box')  # Ensure the aspect ratio is equal\n    ax.set_title('Random Points within a Circle')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n\n    return ax", "raw_solution": "import random\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(points_count=1000, radius=1):\n    # Generate random points within the circle\n    points = []\n    for _ in range(points_count):\n        angle = random.uniform(0, 2 * math.pi)\n        r = random.uniform(0, radius)\n        x = r * math.cos(angle)\n        y = r * math.sin(angle)\n        points.append((x, y))\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(*zip(*points))\n    ax.set_aspect('equal', 'box')  # Ensure the aspect ratio is equal\n    ax.set_title('Random Points within a Circle')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n\n    return ax\n\n# Example usage:\n# random.seed(0)\n# ax = task_func(500, 0.5)\n# print(len(ax.collections[0].get_offsets()))  # Should print 500\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/319", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\ndef task_func(example_str, top_n=30):\n    # Extract words from the string\n    words = re.findall(r'\\b\\w+\\b', example_str)\n\n    # Create a frequency distribution\n    freq_dist = FreqDist(words)\n\n    # Get the top n words\n    top_n_words = freq_dist.most_common(top_n)\n\n    # Create a bar plot of the frequency distribution\n    fig, ax = plt.subplots()\n    words, frequencies = zip(*top_n_words)\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Top {} Words Frequency Distribution'.format(top_n))\n\n    return ax, dict(top_n_words)", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\n\ndef task_func(example_str, top_n=30):\n    # Extract words from the string\n    words = re.findall(r'\\b\\w+\\b', example_str)\n\n    # Create a frequency distribution\n    freq_dist = FreqDist(words)\n\n    # Get the top n words\n    top_n_words = freq_dist.most_common(top_n)\n\n    # Create a bar plot of the frequency distribution\n    fig, ax = plt.subplots()\n    words, frequencies = zip(*top_n_words)\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Top {} Words Frequency Distribution'.format(top_n))\n\n    return ax, dict(top_n_words)\n```\n"}
{"task_id": "BigCodeBench/320", "solution": "import subprocess\nimport os\nimport random\ndef task_func(directory, file_list):\n    if not file_list:\n        return None\n\n    random_file = random.choice(file_list)\n    file_path = os.path.join(directory, random_file)\n\n    if not os.path.isfile(file_path):\n        return None\n\n    try:\n        process = subprocess.run(['cmd', '/c', file_path], check=True, text=True, capture_output=True)\n        return process.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode\n    except FileNotFoundError:\n        return None", "raw_solution": "import subprocess\nimport os\nimport random\n\ndef task_func(directory, file_list):\n    if not file_list:\n        return None\n\n    random_file = random.choice(file_list)\n    file_path = os.path.join(directory, random_file)\n\n    if not os.path.isfile(file_path):\n        return None\n\n    try:\n        process = subprocess.run(['cmd', '/c', file_path], check=True, text=True, capture_output=True)\n        return process.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode\n    except FileNotFoundError:\n        return None\n```\n"}
{"task_id": "BigCodeBench/321", "solution": "import pandas as pd\nimport re\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Extract names from the text\n    names = re.findall(r'\\b\\w+\\s\\w+\\b', text)\n    \n    # Calculate frequency of each name\n    name_freqs = pd.Series(names).value_counts().rename(None)\n    \n    # Plot the frequency chart\n    ax = name_freqs.plot(kind='bar', title='Name Frequencies')\n    plt.xlabel('Name')\n    plt.ylabel('Frequency')\n    \n    # Calculate skew and kurtosis\n    skewness = name_freqs.skew()\n    kurt = name_freqs.kurt()\n    \n    return name_freqs, ax, skewness, kurt", "raw_solution": "import pandas as pd\nimport re\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Extract names from the text\n    names = re.findall(r'\\b\\w+\\s\\w+\\b', text)\n    \n    # Calculate frequency of each name\n    name_freqs = pd.Series(names).value_counts().rename(None)\n    \n    # Plot the frequency chart\n    ax = name_freqs.plot(kind='bar', title='Name Frequencies')\n    plt.xlabel('Name')\n    plt.ylabel('Frequency')\n    \n    # Calculate skew and kurtosis\n    skewness = name_freqs.skew()\n    kurt = name_freqs.kurt()\n    \n    return name_freqs, ax, skewness, kurt\n\n# Example usage\ntext_input = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nname_freqs, plot, skew, kurtosis = task_func(text_input)"}
{"task_id": "BigCodeBench/322", "solution": "import subprocess\nimport os\nimport shutil\nimport sys\nDIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server\\\\Backup'\ndef task_func(filename):\n    \"\"\"\n    This function copies a specified file from the original directory to the backup directory and then executes it using a subprocess.\n\n    Params:\n    - filename (str): The name of the file to be backed up and executed.\n\n    Returns:\n    - int: The exit code of the subprocess, or -1 if the backup fails.\n\n    Requirements:\n    - subprocess\n    - shutil\n\n    Examples:\n    - >>> task_func('vmware-cmd.bat') # Assuming successful execution\n    - 0\n    - >>> task_func('nonexistent.bat') # If backup fails or file doesn't exist\n    - -1\n    \"\"\"\n    # Check if the file exists in the original directory\n    original_file_path = os.path.join(DIRECTORY, filename)\n    if not os.path.isfile(original_file_path):\n        return -1\n\n    # Check if the backup directory exists, if not, create it\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n\n    # Copy the file to the backup directory\n    backup_file_path = os.path.join(BACKUP_DIRECTORY, filename)\n    try:\n        shutil.copy(original_file_path, backup_file_path)\n    except Exception as e:\n        print(f\"Error during backup: {e}\")\n        return -1\n\n    # Execute the file using a subprocess\n    try:\n        result = subprocess.run([backup_file_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during execution: {e}\")\n        return -1", "raw_solution": "import subprocess\nimport os\nimport shutil\nimport sys\n\n# Constants\nDIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\\\Program Files\\\\VMware\\\\VMware Server\\\\Backup'\n\ndef task_func(filename):\n    \"\"\"\n    This function copies a specified file from the original directory to the backup directory and then executes it using a subprocess.\n\n    Params:\n    - filename (str): The name of the file to be backed up and executed.\n\n    Returns:\n    - int: The exit code of the subprocess, or -1 if the backup fails.\n\n    Requirements:\n    - subprocess\n    - shutil\n\n    Examples:\n    - >>> task_func('vmware-cmd.bat') # Assuming successful execution\n    - 0\n    - >>> task_func('nonexistent.bat') # If backup fails or file doesn't exist\n    - -1\n    \"\"\"\n    # Check if the file exists in the original directory\n    original_file_path = os.path.join(DIRECTORY, filename)\n    if not os.path.isfile(original_file_path):\n        return -1\n\n    # Check if the backup directory exists, if not, create it\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n\n    # Copy the file to the backup directory\n    backup_file_path = os.path.join(BACKUP_DIRECTORY, filename)\n    try:\n        shutil.copy(original_file_path, backup_file_path)\n    except Exception as e:\n        print(f\"Error during backup: {e}\")\n        return -1\n\n    # Execute the file using a subprocess\n    try:\n        result = subprocess.run([backup_file_path], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during execution: {e}\")\n        return -1\n```\n"}
{"task_id": "BigCodeBench/323", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\ndef task_func(text, num_gaussians=1, seed=42):\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians must be greater than 0.\")\n    \n    # Extract names from text\n    names = re.findall(r'\\b\\w+\\b(?: \\[.*?\\])+', text)\n    \n    # Tokenize names into words\n    words = []\n    for name in names:\n        words.extend(re.findall(r'\\b\\w+\\b', name))\n    \n    # Count the frequency of each word\n    word_freqs = Counter(words)\n    \n    # Check if num_gaussians is greater than the number of unique words\n    if num_gaussians > len(word_freqs):\n        raise Exception(\"num_gaussians cannot be greater than the number of unique words.\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Fit a Gaussian Mixture Model\n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    word_frequencies = np.array(list(word_freqs.values())).reshape(-1, 1)\n    gmm.fit(word_frequencies)\n    \n    # Get the means and variances of the fitted Gaussians\n    means = gmm.means_.flatten()\n    variances = gmm.covariances_.flatten()\n    \n    return word_freqs, means, variances", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\n\ndef task_func(text, num_gaussians=1, seed=42):\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians must be greater than 0.\")\n    \n    # Extract names from text\n    names = re.findall(r'\\b\\w+\\b(?: \\[.*?\\])+', text)\n    \n    # Tokenize names into words\n    words = []\n    for name in names:\n        words.extend(re.findall(r'\\b\\w+\\b', name))\n    \n    # Count the frequency of each word\n    word_freqs = Counter(words)\n    \n    # Check if num_gaussians is greater than the number of unique words\n    if num_gaussians > len(word_freqs):\n        raise Exception(\"num_gaussians cannot be greater than the number of unique words.\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Fit a Gaussian Mixture Model\n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    word_frequencies = np.array(list(word_freqs.values())).reshape(-1, 1)\n    gmm.fit(word_frequencies)\n    \n    # Get the means and variances of the fitted Gaussians\n    means = gmm.means_.flatten()\n    variances = gmm.covariances_.flatten()\n    \n    return word_freqs, means, variances\n\n# Example usage:\n# freqs, means, variances = task_func(\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\")\n# print(freqs)\n# print(means)\n# print(variances)\n```\n"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    def run_file(file):\n        try:\n            # subprocess call to run the file\n            result = subprocess.run([\"cmd\", \"/c\", file], check=True)\n            return result.returncode\n        except subprocess.CalledProcessError as e:\n            return e.returncode\n\n    # List to store the exit codes\n    exit_codes = []\n\n    # Create and start threads for each file\n    threads = []\n    for file in file_list:\n        thread = threading.Thread(target=lambda: exit_codes.append(run_file(file)))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes", "raw_solution": "import subprocess\nimport time\nimport threading\n\ndef task_func(file_list):\n    def run_file(file):\n        try:\n            # subprocess call to run the file\n            result = subprocess.run([\"cmd\", \"/c\", file], check=True)\n            return result.returncode\n        except subprocess.CalledProcessError as e:\n            return e.returncode\n\n    # List to store the exit codes\n    exit_codes = []\n\n    # Create and start threads for each file\n    threads = []\n    for file in file_list:\n        thread = threading.Thread(target=lambda: exit_codes.append(run_file(file)))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes\n\n# Example usage:\n# Assuming the files \"task_func_data/file1.bat\" and \"task_func_data/file2.bat\" exist and are executable.\n# print(task_func([\"task_func_data/file1.bat\", \"task_func_data/file2.bat\"]))\n```\nThis script uses the `subprocess` module to run each file in a separate thread. It captures the exit code of each subprocess and stores it in a list. The script waits for all threads to finish before returning the list of exit codes.\n"}
{"task_id": "BigCodeBench/325", "solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\nREGEX_PATTERN = r'\\\\(.+?\\\\)|\\\\w'\ndef task_func(directory_path: str, regex_pattern: str = REGEX_PATTERN) -> dict:\n    matches = {}\n    path = Path(directory_path)\n    \n    if not path.exists() or not path.is_dir():\n        raise ValueError(\"Provided path is not a valid directory.\")\n    \n    for file_path in glob.glob(str(path / '*.txt')):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            file_matches = re.findall(regex_pattern, content)\n            matches[os.path.basename(file_path)] = file_matches\n    \n    return matches", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\n\nREGEX_PATTERN = r'\\\\(.+?\\\\)|\\\\w'\n\ndef task_func(directory_path: str, regex_pattern: str = REGEX_PATTERN) -> dict:\n    matches = {}\n    path = Path(directory_path)\n    \n    if not path.exists() or not path.is_dir():\n        raise ValueError(\"Provided path is not a valid directory.\")\n    \n    for file_path in glob.glob(str(path / '*.txt')):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            file_matches = re.findall(regex_pattern, content)\n            matches[os.path.basename(file_path)] = file_matches\n    \n    return matches\n\n# Example usage:\n# matches = task_func('/path/to/directory')\n# print(matches)\n```\nThis script defines a function `task_func` that takes a directory path and a regular expression pattern as input. It then searches for all text files in the specified directory, reads their content, and finds all matches of the provided regular expression pattern. The function returns a dictionary where the keys are the names of the text files and the values are lists of matches found in each file.\n\nPlease replace `'/path/to/directory'` with the actual path to the directory containing the text files you want to search.\n"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n    results = []\n\n    for bat_file in bat_files:\n        try:\n            result = subprocess.run([sys.executable, bat_file], check=True, capture_output=True)\n            results.append((os.path.basename(bat_file), result.returncode))\n        except subprocess.CalledProcessError as e:\n            results.append((os.path.basename(bat_file), None))\n\n    return results", "raw_solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n    results = []\n\n    for bat_file in bat_files:\n        try:\n            result = subprocess.run([sys.executable, bat_file], check=True, capture_output=True)\n            results.append((os.path.basename(bat_file), result.returncode))\n        except subprocess.CalledProcessError as e:\n            results.append((os.path.basename(bat_file), None))\n\n    return results\n```\nThis script uses the `glob` module to find all `.bat` files in the specified directory. It then runs each file using `subprocess.run`, capturing the return code. If the file cannot be executed, it appends a tuple with the file name and `None` for the exit code. The function returns a list of tuples, each containing the file name and its exit code.\n"}
{"task_id": "BigCodeBench/327", "solution": "import csv\nimport re\nfrom collections import Counter\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    matches = []\n    with open(file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            for cell in row:\n                matches.extend(re.findall(regex_pattern, cell))\n    \n    counts = Counter(matches)\n    return dict(counts)", "raw_solution": "import csv\nimport re\nfrom collections import Counter\n\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    matches = []\n    with open(file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            for cell in row:\n                matches.extend(re.findall(regex_pattern, cell))\n    \n    counts = Counter(matches)\n    return dict(counts)\n\n# Example usage:\n# Assuming the CSV file has the following content:\n# a,b,(abc)\n# This will output: {'a': 1, ' ': 1, 'b': 1, ' (': 1, 'abc': 1, ')': 1}\n```\n"}
{"task_id": "BigCodeBench/328", "solution": "import collections\nimport random\nfrom queue import PriorityQueue\ndef task_func(number_teams=5):\n    # Initialize a priority queue\n    pq = PriorityQueue()\n\n    # Generate random team names and points\n    for i in range(1, number_teams + 1):\n        team_name = f'Team {i}'\n        points = random.randint(1, 100)\n        # Use negative points to sort in descending order\n        pq.put((-points, team_name))\n\n    # Initialize an ordered dictionary\n    ranking = collections.OrderedDict()\n\n    # Pop items from the priority queue and add to the ordered dictionary\n    while not pq.empty():\n        points, team_name = pq.get()\n        ranking[team_name] = -points\n\n    return ranking", "raw_solution": "import collections\nimport random\nfrom queue import PriorityQueue\n\ndef task_func(number_teams=5):\n    # Initialize a priority queue\n    pq = PriorityQueue()\n\n    # Generate random team names and points\n    for i in range(1, number_teams + 1):\n        team_name = f'Team {i}'\n        points = random.randint(1, 100)\n        # Use negative points to sort in descending order\n        pq.put((-points, team_name))\n\n    # Initialize an ordered dictionary\n    ranking = collections.OrderedDict()\n\n    # Pop items from the priority queue and add to the ordered dictionary\n    while not pq.empty():\n        points, team_name = pq.get()\n        ranking[team_name] = -points\n\n    return ranking\n\n# Example usage"}
{"task_id": "BigCodeBench/329", "solution": "import re\nimport json\nimport os\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    matches = []\n    for key, value in data.items():\n        matches.extend(re.findall(regex_pattern, value))\n    \n    return {os.path.basename(file_path): matches}", "raw_solution": "import re\nimport json\nimport os\n\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    matches = []\n    for key, value in data.items():\n        matches.extend(re.findall(regex_pattern, value))\n    \n    return {os.path.basename(file_path): matches}\n```\n"}
{"task_id": "BigCodeBench/330", "solution": "import heapq\nimport random\ndef task_func(list_length: int, k: int):\n    \"\"\"\n    Find k largest numbers from a randomly generated list using heapq.\n\n    Params:\n    - list_length (int): The length of the randomly generated list of ints.\n    - k (int): The number of largest elements to find.\n\n    Returns:\n        Tuple: A tuple containing two lists:\n        list[int]: A randomly generated list of ints with the specified length.\n        list[int]: The k largest numbers found using heapq.\n\n    Reqs:\n    - heapq\n    - random\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> rand_list, top_k = task_func(5, 3)\n    - >>> top_k[0] in rand_list\n    - True\n\n    \"\"\"\n    # Generate a random list of integers\n    rand_list = [random.randint(0, 100) for _ in range(list_length)]\n    \n    # Find the k largest numbers using heapq\n    top_k = heapq.nlargest(k, rand_list)\n    \n    return rand_list, top_k", "raw_solution": "import heapq\nimport random\n\n\ndef task_func(list_length: int, k: int):\n    \"\"\"\n    Find k largest numbers from a randomly generated list using heapq.\n\n    Params:\n    - list_length (int): The length of the randomly generated list of ints.\n    - k (int): The number of largest elements to find.\n\n    Returns:\n        Tuple: A tuple containing two lists:\n        list[int]: A randomly generated list of ints with the specified length.\n        list[int]: The k largest numbers found using heapq.\n\n    Reqs:\n    - heapq\n    - random\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> rand_list, top_k = task_func(5, 3)\n    - >>> top_k[0] in rand_list\n    - True\n\n    \"\"\"\n    # Generate a random list of integers\n    rand_list = [random.randint(0, 100) for _ in range(list_length)]\n    \n    # Find the k largest numbers using heapq\n    top_k = heapq.nlargest(k, rand_list)\n    \n    return rand_list, top_k\n```\n"}
{"task_id": "BigCodeBench/331", "solution": "import bisect\nimport random\ndef task_func(num, list_length = 5, min_value = 0, max_value = 0):\n    # Generate a random list of integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    random_list.sort()  # Sort the list\n\n    # Insert the number into the sorted list at the correct position\n    bisect.insort(random_list, num)\n\n    # Return the original and modified lists\n    return random_list[:-1], random_list", "raw_solution": "import bisect\nimport random\n\ndef task_func(num, list_length = 5, min_value = 0, max_value = 0):\n    # Generate a random list of integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    random_list.sort()  # Sort the list\n\n    # Insert the number into the sorted list at the correct position\n    bisect.insort(random_list, num)\n\n    # Return the original and modified lists\n    return random_list[:-1], random_list\n\n# Test cases"}
{"task_id": "BigCodeBench/332", "solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\ndef task_func(text: str) -> dict:\n    # Ensure stopwords are downloaded\n    try:\n        stop_words = set(stopwords.words('english'))\n    except LookupError:\n        import nltk\n        nltk.download('stopwords')\n        stop_words = set(stopwords.words('english'))\n\n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Filter out stop words\n    filtered_words = [word for word in words if word not in stop_words]\n\n    # Count occurrences of each word\n    word_count = Counter(filtered_words)\n\n    return dict(word_count)", "raw_solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\ndef task_func(text: str) -> dict:\n    # Ensure stopwords are downloaded\n    try:\n        stop_words = set(stopwords.words('english'))\n    except LookupError:\n        import nltk\n        nltk.download('stopwords')\n        stop_words = set(stopwords.words('english'))\n\n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Filter out stop words\n    filtered_words = [word for word in words if word not in stop_words]\n\n    # Count occurrences of each word\n    word_count = Counter(filtered_words)\n\n    return dict(word_count)\n```\n"}
{"task_id": "BigCodeBench/333", "solution": "import heapq\nimport random\ndef task_func(k, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a list of random integers\n    rand_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    \n    # Find the k smallest numbers using heapq\n    least_k = heapq.nsmallest(k, rand_list)\n    \n    return rand_list, least_k", "raw_solution": "import heapq\nimport random\n\ndef task_func(k, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a list of random integers\n    rand_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    \n    # Find the k smallest numbers using heapq\n    least_k = heapq.nsmallest(k, rand_list)\n    \n    return rand_list, least_k\n\n# Test cases\nrandom.seed(0)\nrand_list, least_k = task_func(3)\nassert least_k[0] in rand_list\n\nrand_list, least_k = task_func(3, 5, 100, 100)\nassert least_k == [100, 100, 100]\n"}
{"task_id": "BigCodeBench/334", "solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\ndef task_func(documents):\n    # Tokenize the documents\n    tokenized_docs = [' '.join(word_tokenize(doc)) for doc in documents]\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the documents\n    tfidf_matrix = vectorizer.fit_transform(tokenized_docs)\n\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df", "raw_solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\ndef task_func(documents):\n    # Tokenize the documents\n    tokenized_docs = [' '.join(word_tokenize(doc)) for doc in documents]\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the documents\n    tfidf_matrix = vectorizer.fit_transform(tokenized_docs)\n\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\n\n# Example usage:\ndocs = ['yah first document hai.', 'yah document second document hai.', 'aur yah third hai.', 'kya yah first document hai?']\ntfidf = task_func(docs)"}
{"task_id": "BigCodeBench/335", "solution": "import collections\nfrom queue import PriorityQueue\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(string_length=100):\n    # Generate a random string of the specified length\n    random_string = ''.join(random.choices(LETTERS, k=string_length))\n\n    # Calculate the frequency of each letter in the string\n    frequency = collections.Counter(random_string)\n\n    # Create a priority queue to sort the letters by frequency\n    pq = PriorityQueue()\n    for letter, count in frequency.items():\n        pq.put((-count, letter))  # Use negative count for max-heap behavior\n\n    # Create an ordered dictionary to store the sorted frequencies\n    sorted_frequency = collections.OrderedDict()\n    while not pq.empty():\n        count, letter = pq.get()\n        sorted_frequency[letter] = -count\n\n    return sorted_frequency", "raw_solution": "import collections\nfrom queue import PriorityQueue\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef task_func(string_length=100):\n    # Generate a random string of the specified length\n    random_string = ''.join(random.choices(LETTERS, k=string_length))\n\n    # Calculate the frequency of each letter in the string\n    frequency = collections.Counter(random_string)\n\n    # Create a priority queue to sort the letters by frequency\n    pq = PriorityQueue()\n    for letter, count in frequency.items():\n        pq.put((-count, letter))  # Use negative count for max-heap behavior\n\n    # Create an ordered dictionary to store the sorted frequencies\n    sorted_frequency = collections.OrderedDict()\n    while not pq.empty():\n        count, letter = pq.get()\n        sorted_frequency[letter] = -count\n\n    return sorted_frequency\n\n# Example usage"}
{"task_id": "BigCodeBench/336", "solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\ndef task_func(pattern, directory, extensions):\n    matches = []\n    for ext in extensions:\n        for file_path in glob.glob(os.path.join(directory, ext)):\n            with open(file_path, 'r') as file:\n                content = file.read()\n                if re.search(pattern, content, re.IGNORECASE):\n                    matches.append(Path(file_path).resolve())\n    return matches", "raw_solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\n\ndef task_func(pattern, directory, extensions):\n    matches = []\n    for ext in extensions:\n        for file_path in glob.glob(os.path.join(directory, ext)):\n            with open(file_path, 'r') as file:\n                content = file.read()\n                if re.search(pattern, content, re.IGNORECASE):\n                    matches.append(Path(file_path).resolve())\n    return matches\n\n# Example usage:\n# temp_dir = 'path/to/temp/directory'\n# matches = task_func('Hello', temp_dir, ['*.txt', '*.md'])\n# print(matches)\n```\nThis script defines a function `task_func` that takes a regex pattern, a directory path, and a list of file extensions as input. It then searches for files matching the provided extensions in the specified directory, reads their contents, and checks if the pattern is present (case insensitive). If the pattern is found, the absolute path of the file is added to the list of matches. The function returns a list of absolute paths to the files that match the pattern.\n"}
{"task_id": "BigCodeBench/337", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col):\n    if not np.issubdtype(df[value_col].dtype, np.number):\n        raise TypeError(\"Values in the specified column must be numeric.\")\n\n    groups = df[group_col].unique()\n    values = df.groupby(group_col)[value_col].mean()\n    errors = df.groupby(group_col)[value_col].std()\n\n    fig, ax = plt.subplots()\n    for i, group in enumerate(groups):\n        ax.errorbar(i, values[group], yerr=errors[group], fmt='o', color=COLORS[i % len(COLORS)], label=group)\n\n    ax.set_xticks(np.arange(len(groups)))\n    ax.set_xticklabels(groups)\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    ax.set_title(f\"Bar chart of {value_col} by {group_col}\")\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col):\n    if not np.issubdtype(df[value_col].dtype, np.number):\n        raise TypeError(\"Values in the specified column must be numeric.\")\n\n    groups = df[group_col].unique()\n    values = df.groupby(group_col)[value_col].mean()\n    errors = df.groupby(group_col)[value_col].std()\n\n    fig, ax = plt.subplots()\n    for i, group in enumerate(groups):\n        ax.errorbar(i, values[group], yerr=errors[group], fmt='o', color=COLORS[i % len(COLORS)], label=group)\n\n    ax.set_xticks(np.arange(len(groups)))\n    ax.set_xticklabels(groups)\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    ax.set_title(f\"Bar chart of {value_col} by {group_col}\")\n    ax.legend()\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'Group': ['A', 'B', 'A', 'B', 'A', 'B'], 'Value': [1, 2, 3, 4, 5, 6]})\n# ax = task_func(df, 'Group', 'Value')\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/338", "solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\ndef task_func(elements, seed=100):\n    random.seed(seed)\n    patterns = []\n    counts = {}\n\n    for element in elements:\n        random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n        formatted_string = f'% {random_string}%'\n        patterns.append(formatted_string)\n\n        for char in formatted_string:\n            if char in counts:\n                counts[char] += 1\n            else:\n                counts[char] = 1\n\n    # Plotting the histogram\n    fig, ax = plt.subplots()\n    ax.bar(counts.keys(), counts.values())\n    ax.set_xlabel('Characters')\n    ax.set_ylabel('Occurrences')\n    ax.set_title('Character Occurrences in Formatted Strings')\n\n    return patterns, ax, counts", "raw_solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\n\ndef task_func(elements, seed=100):\n    random.seed(seed)\n    patterns = []\n    counts = {}\n\n    for element in elements:\n        random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n        formatted_string = f'% {random_string}%'\n        patterns.append(formatted_string)\n\n        for char in formatted_string:\n            if char in counts:\n                counts[char] += 1\n            else:\n                counts[char] = 1\n\n    # Plotting the histogram\n    fig, ax = plt.subplots()\n    ax.bar(counts.keys(), counts.values())\n    ax.set_xlabel('Characters')\n    ax.set_ylabel('Occurrences')\n    ax.set_title('Character Occurrences in Formatted Strings')\n\n    return patterns, ax, counts\n\n# Example usage:\n# patterns, ax, counts = task_func(['abc', 'def'])\n# print(patterns)\n# print(counts)\n```\n"}
{"task_id": "BigCodeBench/339", "solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\ndef task_func(req_data, secret_key):\n    if not isinstance(req_data, dict):\n        raise TypeError(\"req_data should be a dictionary.\")\n\n    # Convert dictionary to JSON string\n    json_data = json.dumps(req_data)\n\n    # Create HMAC signature\n    signature = hmac.new(secret_key.encode(), json_data.encode(), hashlib.sha256).hexdigest()\n\n    # Encode signature with URL encoding\n    encoded_signature = urllib.parse.quote(signature)\n\n    # Replace spaces with '+'\n    encoded_signature = encoded_signature.replace(' ', '+')\n\n    return encoded_signature", "raw_solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\n\ndef task_func(req_data, secret_key):\n    if not isinstance(req_data, dict):\n        raise TypeError(\"req_data should be a dictionary.\")\n\n    # Convert dictionary to JSON string\n    json_data = json.dumps(req_data)\n\n    # Create HMAC signature\n    signature = hmac.new(secret_key.encode(), json_data.encode(), hashlib.sha256).hexdigest()\n\n    # Encode signature with URL encoding\n    encoded_signature = urllib.parse.quote(signature)\n\n    # Replace spaces with '+'\n    encoded_signature = encoded_signature.replace(' ', '+')\n\n    return encoded_signature\n\n# Test cases"}
{"task_id": "BigCodeBench/340", "solution": "import json\nimport hashlib\nimport blake3\ndef task_func(req_data):\n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(req_data, sort_keys=True)\n    \n    # Generate BLAKE3 hash\n    blake3_hash = blake3.blake3(json_str.encode('utf-8')).hexdigest()\n    \n    # Generate MD5 hash of the BLAKE3 hash\n    md5_hash = hashlib.md5(blake3_hash.encode('utf-8')).hexdigest()\n    \n    return blake3_hash, md5_hash", "raw_solution": "import json\nimport hashlib\nimport blake3\n\ndef task_func(req_data):\n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(req_data, sort_keys=True)\n    \n    # Generate BLAKE3 hash\n    blake3_hash = blake3.blake3(json_str.encode('utf-8')).hexdigest()\n    \n    # Generate MD5 hash of the BLAKE3 hash\n    md5_hash = hashlib.md5(blake3_hash.encode('utf-8')).hexdigest()\n    \n    return blake3_hash, md5_hash\n\n# Test cases"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame) or df.empty or col not in df.columns:\n        raise ValueError(\"Input df must be a non-empty pandas DataFrame and the specified column must exist.\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(df[col], kde=True, ax=axes[0])\n        sns.boxplot(x=df[col], ax=axes[1])\n    elif pd.api.types.is_categorical_dtype(df[col]):\n        sns.countplot(x=df[col], ax=axes[0])\n        sns.boxplot(x=df[col], ax=axes[1])\n    else:\n        raise ValueError(\"Specified column must be numeric or categorical.\")\n\n    plt.tight_layout()\n    return fig", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame) or df.empty or col not in df.columns:\n        raise ValueError(\"Input df must be a non-empty pandas DataFrame and the specified column must exist.\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(df[col], kde=True, ax=axes[0])\n        sns.boxplot(x=df[col], ax=axes[1])\n    elif pd.api.types.is_categorical_dtype(df[col]):\n        sns.countplot(x=df[col], ax=axes[0])\n        sns.boxplot(x=df[col], ax=axes[1])\n    else:\n        raise ValueError(\"Specified column must be numeric or categorical.\")\n\n    plt.tight_layout()\n    return fig\n\n# Example usage:\n# df = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n# fig = task_func(df, 'value')\n# print(type(fig))\n# plt.close()\n\n# df = pd.DataFrame({'category': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B']})\n# fig = task_func(df, 'category')\n# print(type(fig))\n# print(len(fig.axes))\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/342", "solution": "import string\nimport random\nimport re\ndef task_func(elements, pattern, seed=100):\n    random.seed(seed)\n    replaced_elements = []\n\n    for element in elements:\n        # Replace each character in the element with a random character\n        new_element = ''.join(random.choice(string.ascii_letters) for char in element)\n        # Format the new element with the given pattern\n        formatted_element = pattern % new_element\n        replaced_elements.append(formatted_element)\n\n    # Join all formatted elements into a single string\n    all_formatted_elements = ' '.join(replaced_elements)\n\n    # Search for the pattern in the formatted string\n    result = bool(re.search(pattern, all_formatted_elements))\n\n    return replaced_elements, result\npattern = \"%s\"", "raw_solution": "import string\nimport random\nimport re\n\ndef task_func(elements, pattern, seed=100):\n    random.seed(seed)\n    replaced_elements = []\n\n    for element in elements:\n        # Replace each character in the element with a random character\n        new_element = ''.join(random.choice(string.ascii_letters) for char in element)\n        # Format the new element with the given pattern\n        formatted_element = pattern % new_element\n        replaced_elements.append(formatted_element)\n\n    # Join all formatted elements into a single string\n    all_formatted_elements = ' '.join(replaced_elements)\n\n    # Search for the pattern in the formatted string\n    result = bool(re.search(pattern, all_formatted_elements))\n\n    return replaced_elements, result\n\n# Example usage:\nELEMENTS = [\"abc\", \"def\"]\npattern = \"%s\"\nreplaced_elements, result = task_func(ELEMENTS, pattern, 234)"}
{"task_id": "BigCodeBench/343", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b', 'y', 'm']\ndef task_func(df, col, title=None):\n    if not isinstance(df, pd.DataFrame) or df.empty or col not in df.columns:\n        raise ValueError(\"Input df must be a non-empty pandas DataFrame with the specified column.\")\n\n    value_counts = df[col].value_counts()\n    labels = value_counts.index\n    sizes = value_counts.values\n\n    fig, ax = plt.subplots()\n    ax.pie(sizes, labels=labels, colors=COLORS, autopct='%1.1f%%', startangle=140)\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    if title:\n        ax.set_title(title)\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants for pie chart colors\nCOLORS = ['r', 'g', 'b', 'y', 'm']\n\ndef task_func(df, col, title=None):\n    if not isinstance(df, pd.DataFrame) or df.empty or col not in df.columns:\n        raise ValueError(\"Input df must be a non-empty pandas DataFrame with the specified column.\")\n\n    value_counts = df[col].value_counts()\n    labels = value_counts.index\n    sizes = value_counts.values\n\n    fig, ax = plt.subplots()\n    ax.pie(sizes, labels=labels, colors=COLORS, autopct='%1.1f%%', startangle=140)\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    if title:\n        ax.set_title(title)\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'fruit': ['apple', 'banana', 'orange', 'apple', 'banana', 'banana']})\n# ax = task_func(df, 'fruit', title='Fruit Distribution')\n# print(ax.get_title())\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/344", "solution": "import os\nimport shutil\ndef task_func(src_folder, backup_dir):\n    if not os.path.exists(src_folder):\n        raise ValueError(f\"Source folder '{src_folder}' does not exist.\")\n    \n    try:\n        # Create a backup of the source folder\n        shutil.copytree(src_folder, os.path.join(backup_dir, os.path.basename(src_folder)))\n        \n        # Remove the source folder\n        shutil.rmtree(src_folder)\n        \n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False", "raw_solution": "import os\nimport shutil\n\ndef task_func(src_folder, backup_dir):\n    if not os.path.exists(src_folder):\n        raise ValueError(f\"Source folder '{src_folder}' does not exist.\")\n    \n    try:\n        # Create a backup of the source folder\n        shutil.copytree(src_folder, os.path.join(backup_dir, os.path.basename(src_folder)))\n        \n        # Remove the source folder\n        shutil.rmtree(src_folder)\n        \n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```\n"}
{"task_id": "BigCodeBench/345", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col1, col2):\n    if not isinstance(df, pd.DataFrame) or df.empty or col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a non-empty DataFrame containing the specified columns.\")\n    \n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise TypeError(\"Columns must be numeric.\")\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.regplot(x=col1, y=col2, data=df)\n    plt.title(f'Regression Line and Scatter Plot of {col1} vs {col2}')\n    plt.xlabel(col1)\n    plt.ylabel(col2)\n    plt.show()\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col1, col2):\n    if not isinstance(df, pd.DataFrame) or df.empty or col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Invalid 'df': must be a non-empty DataFrame containing the specified columns.\")\n    \n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise TypeError(\"Columns must be numeric.\")\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.regplot(x=col1, y=col2, data=df)\n    plt.title(f'Regression Line and Scatter Plot of {col1} vs {col2}')\n    plt.xlabel(col1)\n    plt.ylabel(col2)\n    plt.show()\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [2, 4, 6, 8, 10]})\n# plot = task_func(df, 'X', 'Y')\n# print(len(plot.collections[0].get_offsets().data))  # Should print 5\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script not found: {script_path}\")\n\n    try:\n        result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        if wait:\n            return result.returncode\n        else:\n            return None\n    except subprocess.CalledProcessError as e:\n        print(f\"An error occurred while running the script: {e}\", file=sys.stderr)\n        return e.returncode if wait else None", "raw_solution": "import subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script not found: {script_path}\")\n\n    try:\n        result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        if wait:\n            return result.returncode\n        else:\n            return None\n    except subprocess.CalledProcessError as e:\n        print(f\"An error occurred while running the script: {e}\", file=sys.stderr)\n        return e.returncode if wait else None\n```\n"}
{"task_id": "BigCodeBench/347", "solution": "import pandas as pd\nimport re\nimport numpy as np\nPATTERN = r\"([a-fA-F\\d]{32})\"\ndef task_func(df, column):\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in DataFrame\")\n\n    matches = df[column].apply(lambda x: re.findall(PATTERN, x))\n    flat_matches = [item for sublist in matches for item in sublist]\n    unique_matches, counts = np.unique(flat_matches, return_counts=True)\n    result = pd.Series(counts, index=unique_matches)\n\n    return result\ndata = pd.DataFrame({\"text\": [\"6f96cfdfe5ccc627cadf24b41725caa4 gorilla\", \"6f96cfdfe5ccc627cadf24b41725caa4 banana\", \"1234567890abcdef1234567890abcdef apple\"]})", "raw_solution": "import pandas as pd\nimport re\nimport numpy as np\n\n# Constants\nPATTERN = r\"([a-fA-F\\d]{32})\"\n\ndef task_func(df, column):\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in DataFrame\")\n\n    matches = df[column].apply(lambda x: re.findall(PATTERN, x))\n    flat_matches = [item for sublist in matches for item in sublist]\n    unique_matches, counts = np.unique(flat_matches, return_counts=True)\n    result = pd.Series(counts, index=unique_matches)\n\n    return result\n\n# Example usage:\ndata = pd.DataFrame({\"text\": [\"6f96cfdfe5ccc627cadf24b41725caa4 gorilla\", \"6f96cfdfe5ccc627cadf24b41725caa4 banana\", \"1234567890abcdef1234567890abcdef apple\"]})\ncounts = task_func(data, \"text\")"}
{"task_id": "BigCodeBench/348", "solution": "import subprocess\nimport os\nimport signal\nimport time\ndef task_func(process_name: str) -> int:\n    \"\"\"\n    This function kills all running processes with a specific name.\n\n    Notes:\n        It sends a termination signal to all processes with the given name and waits for 1 second.\n        There is no guarantee that all processes will be terminated within this time.\n\n    Params:\n    - process_name (str): The name of the processes to be killed.\n\n    Returns:\n        int: The number of processes killed. If no processes were found, it returns 0.\n\n    Reqs:\n    - subprocess\n    - os\n    - signal\n    - time\n\n    Examples:\n    - >>> pids = task_func('test_name') # Dummy example, should return 0\n    - >>> pids\n    - 0\n    \"\"\"\n    # Get all running processes\n    processes = subprocess.check_output(['pgrep', process_name]).decode('utf-8').strip().split('\\n')\n\n    # If no processes are found, return 0\n    if not processes:\n        return 0\n\n    # Send termination signal to each process\n    for pid in processes:\n        os.kill(int(pid), signal.SIGTERM)\n        time.sleep(1)  # Wait for 1 second\n\n    # Return the number of processes killed\n    return len(processes)", "raw_solution": "import subprocess\nimport os\nimport signal\nimport time\n\ndef task_func(process_name: str) -> int:\n    \"\"\"\n    This function kills all running processes with a specific name.\n\n    Notes:\n        It sends a termination signal to all processes with the given name and waits for 1 second.\n        There is no guarantee that all processes will be terminated within this time.\n\n    Params:\n    - process_name (str): The name of the processes to be killed.\n\n    Returns:\n        int: The number of processes killed. If no processes were found, it returns 0.\n\n    Reqs:\n    - subprocess\n    - os\n    - signal\n    - time\n\n    Examples:\n    - >>> pids = task_func('test_name') # Dummy example, should return 0\n    - >>> pids\n    - 0\n    \"\"\"\n    # Get all running processes\n    processes = subprocess.check_output(['pgrep', process_name]).decode('utf-8').strip().split('\\n')\n\n    # If no processes are found, return 0\n    if not processes:\n        return 0\n\n    # Send termination signal to each process\n    for pid in processes:\n        os.kill(int(pid), signal.SIGTERM)\n        time.sleep(1)  # Wait for 1 second\n\n    # Return the number of processes killed\n    return len(processes)\n```\n"}
{"task_id": "BigCodeBench/349", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories):\n    report_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        bikri_matra = random.randint(1, 100)\n        revenue = random.uniform(10, 100)\n        report_data.append([product, category, bikri_matra, revenue])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Category', 'Bikri ki matra', 'Revenue'])\n\n    return report_df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories):\n    report_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        bikri_matra = random.randint(1, 100)\n        revenue = random.uniform(10, 100)\n        report_data.append([product, category, bikri_matra, revenue])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Category', 'Bikri ki matra', 'Revenue'])\n\n    return report_df\n\n# Example usage:\nrandom.seed(0)\nreport = task_func(['Product 1', 'Product 2', 'Product 3'], ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports'])"}
{"task_id": "BigCodeBench/350", "solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\nimport gzip\ndef task_func(src_folder, dst_folder):\n    if not os.path.exists(dst_folder):\n        os.makedirs(dst_folder)\n\n    files = glob(os.path.join(src_folder, '*'))\n    failed_files = []\n\n    for file in files:\n        try:\n            # Compress the file using gzip\n            with open(file, 'rb') as f_in, gzip.open(file + '.gz', 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n\n            # Move the compressed file to the destination folder\n            shutil.move(file + '.gz', dst_folder)\n\n        except Exception as e:\n            failed_files.append(file)\n\n    if failed_files:\n        return {\n            'success': False,\n            'message': 'Some files failed to compress or move.',\n            'failed_files': failed_files\n        }\n\n    return {\n        'success': True,\n        'message': 'All files compressed and moved successfully.',\n        'failed_files': []\n    }", "raw_solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\nimport gzip\n\ndef task_func(src_folder, dst_folder):\n    if not os.path.exists(dst_folder):\n        os.makedirs(dst_folder)\n\n    files = glob(os.path.join(src_folder, '*'))\n    failed_files = []\n\n    for file in files:\n        try:\n            # Compress the file using gzip\n            with open(file, 'rb') as f_in, gzip.open(file + '.gz', 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n\n            # Move the compressed file to the destination folder\n            shutil.move(file + '.gz', dst_folder)\n\n        except Exception as e:\n            failed_files.append(file)\n\n    if failed_files:\n        return {\n            'success': False,\n            'message': 'Some files failed to compress or move.',\n            'failed_files': failed_files\n        }\n\n    return {\n        'success': True,\n        'message': 'All files compressed and moved successfully.',\n        'failed_files': []\n    }\n```\n"}
{"task_id": "BigCodeBench/351", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    report_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = quantity_sold * random.randint(1, 100)\n\n        report_data.append({\n            'Product': product,\n            'Category': category,\n            'Quantity Sold': quantity_sold,\n            'Revenue': revenue\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    report_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = quantity_sold * random.randint(1, 100)\n\n        report_data.append({\n            'Product': product,\n            'Category': category,\n            'Quantity Sold': quantity_sold,\n            'Revenue': revenue\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df\n\n# Example usage:\nrandom.seed(0)\nreport = task_func(['Product 1'], ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports'], 100, 100)"}
{"task_id": "BigCodeBench/352", "solution": "import pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text_dict, word_keys, top_k=2):\n    if top_k < 0:\n        raise ValueError(\"top_k must be a non-negative integer\")\n\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(list(text_dict.items()), columns=['Word', 'Frequency'])\n\n    # Create a DataFrame for word_keys\n    word_keys_df = pd.DataFrame(word_keys, columns=['Word'])\n    word_keys_df['Frequency'] = 0\n\n    # Merge the two DataFrames\n    merged_df = pd.concat([df, word_keys_df]).reset_index(drop=True)\n    merged_df = merged_df.groupby('Word').sum().reset_index()\n\n    # Sort the DataFrame by Frequency\n    merged_df = merged_df.sort_values(by='Frequency', ascending=False)\n\n    # Get the top_k words\n    top_k_words = merged_df.head(top_k).to_dict('records')\n    top_k_dict = {word['Word']: word['Frequency'] for word in top_k_words}\n\n    # Plot the frequencies\n    ax = merged_df.plot(kind='bar', x='Word', y='Frequency', legend=False)\n    ax.set_title('Word Frequencies')\n    ax.set_ylabel('Frequency')\n    ax.set_xlabel('Word')\n\n    return ax, top_k_dict", "raw_solution": "import pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text_dict, word_keys, top_k=2):\n    if top_k < 0:\n        raise ValueError(\"top_k must be a non-negative integer\")\n\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(list(text_dict.items()), columns=['Word', 'Frequency'])\n\n    # Create a DataFrame for word_keys\n    word_keys_df = pd.DataFrame(word_keys, columns=['Word'])\n    word_keys_df['Frequency'] = 0\n\n    # Merge the two DataFrames\n    merged_df = pd.concat([df, word_keys_df]).reset_index(drop=True)\n    merged_df = merged_df.groupby('Word').sum().reset_index()\n\n    # Sort the DataFrame by Frequency\n    merged_df = merged_df.sort_values(by='Frequency', ascending=False)\n\n    # Get the top_k words\n    top_k_words = merged_df.head(top_k).to_dict('records')\n    top_k_dict = {word['Word']: word['Frequency'] for word in top_k_words}\n\n    # Plot the frequencies\n    ax = merged_df.plot(kind='bar', x='Word', y='Frequency', legend=False)\n    ax.set_title('Word Frequencies')\n    ax.set_ylabel('Frequency')\n    ax.set_xlabel('Word')\n\n    return ax, top_k_dict\n\n# Example usage:\n# text_dict = Counter({'the': 3, 'be': 1, 'to': 1, 'that': 2, 'and': 1, 'a': 1, 'in': 1, 'have': 1, 'I': 1})\n# word_keys = ['the', 'and', 'I']\n# ax, frequencies = task_func(text_dict, word_keys, 3)\n# print(type(ax))\n# print(frequencies)\n```\n"}
{"task_id": "BigCodeBench/353", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    report_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = random.uniform(10.0, 100.0)\n        total_revenue = quantity_sold * revenue\n\n        report_data.append({\n            'Product': product,\n            'Category': category,\n            'Quantity Sold': quantity_sold,\n            'Revenue': revenue,\n            'Total Revenue': total_revenue\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    report_data = []\n\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = random.uniform(10.0, 100.0)\n        total_revenue = quantity_sold * revenue\n\n        report_data.append({\n            'Product': product,\n            'Category': category,\n            'Quantity Sold': quantity_sold,\n            'Revenue': revenue,\n            'Total Revenue': total_revenue\n        })\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df\n\n# Example usage:\nrandom.seed(0)\nreport = task_func(['Product 1'], ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports'], 100, 100)"}
{"task_id": "BigCodeBench/354", "solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(sentences_dict, word_keys):\n    # Initialize a counter to count occurrences of each word\n    word_counter = collections.Counter()\n\n    # Iterate over each sentence in the dictionary\n    for sentence in sentences_dict.values():\n        # Split the sentence into words and update the counter\n        words = sentence.split()\n        word_counter.update(words)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(word_counter.items(), columns=['Word', 'Frequency'])\n\n    # Filter the DataFrame to only include the words in word_keys\n    df = df[df['Word'].isin(word_keys)]\n\n    # Plot the frequencies as a bar chart\n    ax = df.plot(kind='bar', x='Word', y='Frequency', legend=False)\n    ax.set_title('Word Frequencies')\n    ax.set_ylabel('Frequency')\n\n    return ax\nsentences_dict = {'Sentence1': 'the quick brown fox', 'Sentence2': 'jumps over the lazy dog', 'Sentence3': 'the dog is brown'}\nword_keys = ['the', 'dog']", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nWORDS = ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I']\n\ndef task_func(sentences_dict, word_keys):\n    # Initialize a counter to count occurrences of each word\n    word_counter = collections.Counter()\n\n    # Iterate over each sentence in the dictionary\n    for sentence in sentences_dict.values():\n        # Split the sentence into words and update the counter\n        words = sentence.split()\n        word_counter.update(words)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(word_counter.items(), columns=['Word', 'Frequency'])\n\n    # Filter the DataFrame to only include the words in word_keys\n    df = df[df['Word'].isin(word_keys)]\n\n    # Plot the frequencies as a bar chart\n    ax = df.plot(kind='bar', x='Word', y='Frequency', legend=False)\n    ax.set_title('Word Frequencies')\n    ax.set_ylabel('Frequency')\n\n    return ax\n\n# Example usage\nsentences_dict = {'Sentence1': 'the quick brown fox', 'Sentence2': 'jumps over the lazy dog', 'Sentence3': 'the dog is brown'}\nword_keys = ['the', 'dog']\nax = task_func(sentences_dict, word_keys)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/355", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\ndef task_func(amplitude, frequency, time):\n    # Generate Hann window\n    window = get_window('hann', len(time))\n\n    # Generate complex wave\n    wave = amplitude * np.exp(1j * 2 * np.pi * frequency * time) * window\n\n    # Plot the complex wave\n    fig, ax = plt.subplots()\n    ax.plot(time, np.real(wave), label='Real part')\n    ax.plot(time, np.imag(wave), label='Imaginary part')\n    ax.set_title('Hann window ke saath complex wave')\n    ax.set_xlabel('time')\n    ax.set_ylabel('float')\n    ax.legend()\n\n    return wave, fig, ax", "raw_solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\n\ndef task_func(amplitude, frequency, time):\n    # Generate Hann window\n    window = get_window('hann', len(time))\n\n    # Generate complex wave\n    wave = amplitude * np.exp(1j * 2 * np.pi * frequency * time) * window\n\n    # Plot the complex wave\n    fig, ax = plt.subplots()\n    ax.plot(time, np.real(wave), label='Real part')\n    ax.plot(time, np.imag(wave), label='Imaginary part')\n    ax.set_title('Hann window ke saath complex wave')\n    ax.set_xlabel('time')\n    ax.set_ylabel('float')\n    ax.legend()\n\n    return wave, fig, ax\n```\n"}
{"task_id": "BigCodeBench/356", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\ndef task_func(x, y):\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"Both x and y must be numpy.ndarray objects.\")\n    if x.size != y.size:\n        raise ValueError(\"x and y must have the same length.\")\n\n    # Calculate the phase of a complex function\n    Z = np.array([[cmath.exp(complex(i, j)) for j in y] for i in x])\n\n    # Plot the phase\n    fig, ax = plt.subplots()\n    cax = ax.contourf(x, y, np.angle(Z), cmap='hsv')\n    fig.colorbar(cax)\n    ax.set_title('Phase Plot')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    return ax, Z", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\n\ndef task_func(x, y):\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"Both x and y must be numpy.ndarray objects.\")\n    if x.size != y.size:\n        raise ValueError(\"x and y must have the same length.\")\n\n    # Calculate the phase of a complex function\n    Z = np.array([[cmath.exp(complex(i, j)) for j in y] for i in x])\n\n    # Plot the phase\n    fig, ax = plt.subplots()\n    cax = ax.contourf(x, y, np.angle(Z), cmap='hsv')\n    fig.colorbar(cax)\n    ax.set_title('Phase Plot')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    return ax, Z\n\n# Example usage:\n# ax, Z = task_func(np.array([1, 2, 3]), np.array([1, 2, 3]))\n# print(isinstance(ax, plt.Axes), isinstance(Z, np.ndarray))  # Should print (True, True)\n```\n"}
{"task_id": "BigCodeBench/357", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy.ndarray\")\n\n    real_part = norm.pdf(x, loc=0, scale=1)\n    imaginary_part = norm.pdf(x, loc=2, scale=2)\n\n    result = real_part + imaginary_part * 1j\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, np.real(result), label='Real Part')\n    plt.plot(x, np.imag(result), label='Imaginary Part')\n    plt.title('Complex Distribution')\n    plt.xlabel('x')\n    plt.ylabel('Probability Density')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return result\nX = np.linspace(-10, 10, 1000)", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy.ndarray\")\n\n    real_part = norm.pdf(x, loc=0, scale=1)\n    imaginary_part = norm.pdf(x, loc=2, scale=2)\n\n    result = real_part + imaginary_part * 1j\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, np.real(result), label='Real Part')\n    plt.plot(x, np.imag(result), label='Imaginary Part')\n    plt.title('Complex Distribution')\n    plt.xlabel('x')\n    plt.ylabel('Probability Density')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return result\n\n# Example usage:\nX = np.linspace(-10, 10, 1000)\nresult = task_func(X)"}
{"task_id": "BigCodeBench/358", "solution": "import itertools\nimport json\ndef task_func(json_list, r):\n    try:\n        data = json.loads(json_list)\n        number_list = data['number_list']\n    except (json.JSONDecodeError, KeyError):\n        raise ValueError(\"Invalid JSON or missing 'number_list' key\")\n\n    if not number_list:\n        raise ValueError(\"The number list is empty\")\n\n    combinations = list(itertools.combinations(number_list, r))\n    return combinations", "raw_solution": "import itertools\nimport json\n\ndef task_func(json_list, r):\n    try:\n        data = json.loads(json_list)\n        number_list = data['number_list']\n    except (json.JSONDecodeError, KeyError):\n        raise ValueError(\"Invalid JSON or missing 'number_list' key\")\n\n    if not number_list:\n        raise ValueError(\"The number list is empty\")\n\n    combinations = list(itertools.combinations(number_list, r))\n    return combinations\n\n# Example usage:\n# combinations = task_func('{\"number_list\": [1, 2, 3, 4, 5]}', 3)\n# print(combinations)\n```\n"}
{"task_id": "BigCodeBench/359", "solution": "from scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(data_dict, data_keys):\n    # Extract data from the dictionary\n    x = data_dict[data_keys[0]]\n    y = data_dict[data_keys[1]]\n\n    # Calculate the correlation coefficient\n    correlation, _ = stats.pearsonr(x, y)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y, label=f'Correlation: {correlation:.4f}')\n    ax.set_xlabel(data_keys[0])\n    ax.set_ylabel(data_keys[1])\n    ax.set_title('Scatter Plot with Correlation')\n    ax.legend()\n\n    return correlation, ax", "raw_solution": "from scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict, data_keys):\n    # Extract data from the dictionary\n    x = data_dict[data_keys[0]]\n    y = data_dict[data_keys[1]]\n\n    # Calculate the correlation coefficient\n    correlation, _ = stats.pearsonr(x, y)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y, label=f'Correlation: {correlation:.4f}')\n    ax.set_xlabel(data_keys[0])\n    ax.set_ylabel(data_keys[1])\n    ax.set_title('Scatter Plot with Correlation')\n    ax.legend()\n\n    return correlation, ax\n```\n"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n    \n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file {file_location}.\")\n\n    stats = {}\n    for column in df.columns:\n        mean = np.mean(df[column])\n        std = np.std(df[column])\n        stats[column] = {'mean': mean, 'std': std}\n\n    fig, ax = plt.subplots()\n    for column, values in stats.items():\n        ax.bar(column, values['mean'], yerr=values['std'], capsize=5)\n\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Mean')\n    ax.set_title('Mean and Standard Deviation')\n\n    return stats, fig", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n    \n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file {file_location}.\")\n\n    stats = {}\n    for column in df.columns:\n        mean = np.mean(df[column])\n        std = np.std(df[column])\n        stats[column] = {'mean': mean, 'std': std}\n\n    fig, ax = plt.subplots()\n    for column, values in stats.items():\n        ax.bar(column, values['mean'], yerr=values['std'], capsize=5)\n\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Mean')\n    ax.set_title('Mean and Standard Deviation')\n\n    return stats, fig\n```\n"}
{"task_id": "BigCodeBench/361", "solution": "import pandas as pd\nimport logging\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    \"\"\"\n    This function reads data from an excel spreadsheet, converts it to a csv file, and calculates the sum of each column.\n\n    Notes:\n        Make sure the excel file contains only numerical data for accurate sum calculations.\n\n    Params:\n    - sheet_name (str): The name of the sheet to load data from.\n    - excel_file_location (str): The path to the excel file. Default is 'test.xlsx'.\n    - csv_file_location (str): The path where the csv file will be saved. Default is 'test.csv'.\n\n    Returns:\n        dict: A dictionary containing the sum of each column.\n\n    Reqs:\n    - pandas\n    - logging\n\n    Raises:\n    - FileNotFoundError: If the specified excel file does not exist.\n    - ValueError: If the specified sheet name does not exist in the excel file.\n\n    Examples:\n    - >>> test_excel_file = 'dummy_test.xlsx'\n    - >>> test_csv_file = 'dummy_test.csv'\n    - >>> test_sheet_name = 'TestSheet'\n    - >>> data = {'A': [10, 20, 30], 'B': [40, 50, 60]}\n    - >>> df = pd.DataFrame(data)\n    - >>> df.to_excel(test_excel_file, sheet_name=test_sheet_name, index=False)\n    - >>> task_func(sheet_name='TestSheet', excel_file_location=test_excel_file, csv_file_location=test_csv_file) # {'Column1': sum_value1, 'Column2': sum_value2, ...}\n    - {'A': 60, 'B': 150}\n    - >>> os.remove(test_excel_file)\n    - >>> os.remove(test_csv_file)\n    \"\"\"\n    try:\n        # Read the excel file\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n        logging.info(f\"Successfully read the excel file: {excel_file_location}\")\n\n        # Convert the dataframe to a csv file\n        df.to_csv(csv_file_location, index=False)\n        logging.info(f\"Successfully converted the excel file to csv: {csv_file_location}\")\n\n        # Calculate the sum of each column\n        column_sums = df.sum()\n        logging.info(f\"Successfully calculated the sum of each column\")\n\n        return column_sums.to_dict()\n    except FileNotFoundError:\n        logging.error(f\"The file {excel_file_location} does not exist.\")\n        raise FileNotFoundError(f\"The file {excel_file_location} does not exist.\")\n    except ValueError:\n        logging.error(f\"The sheet {sheet_name} does not exist in the excel file.\")\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the excel file.\")", "raw_solution": "import pandas as pd\nimport logging\n\n# Set up basic configuration for logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    \"\"\"\n    This function reads data from an excel spreadsheet, converts it to a csv file, and calculates the sum of each column.\n\n    Notes:\n        Make sure the excel file contains only numerical data for accurate sum calculations.\n\n    Params:\n    - sheet_name (str): The name of the sheet to load data from.\n    - excel_file_location (str): The path to the excel file. Default is 'test.xlsx'.\n    - csv_file_location (str): The path where the csv file will be saved. Default is 'test.csv'.\n\n    Returns:\n        dict: A dictionary containing the sum of each column.\n\n    Reqs:\n    - pandas\n    - logging\n\n    Raises:\n    - FileNotFoundError: If the specified excel file does not exist.\n    - ValueError: If the specified sheet name does not exist in the excel file.\n\n    Examples:\n    - >>> test_excel_file = 'dummy_test.xlsx'\n    - >>> test_csv_file = 'dummy_test.csv'\n    - >>> test_sheet_name = 'TestSheet'\n    - >>> data = {'A': [10, 20, 30], 'B': [40, 50, 60]}\n    - >>> df = pd.DataFrame(data)\n    - >>> df.to_excel(test_excel_file, sheet_name=test_sheet_name, index=False)\n    - >>> task_func(sheet_name='TestSheet', excel_file_location=test_excel_file, csv_file_location=test_csv_file) # {'Column1': sum_value1, 'Column2': sum_value2, ...}\n    - {'A': 60, 'B': 150}\n    - >>> os.remove(test_excel_file)\n    - >>> os.remove(test_csv_file)\n    \"\"\"\n    try:\n        # Read the excel file\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n        logging.info(f\"Successfully read the excel file: {excel_file_location}\")\n\n        # Convert the dataframe to a csv file\n        df.to_csv(csv_file_location, index=False)\n        logging.info(f\"Successfully converted the excel file to csv: {csv_file_location}\")\n\n        # Calculate the sum of each column\n        column_sums = df.sum()\n        logging.info(f\"Successfully calculated the sum of each column\")\n\n        return column_sums.to_dict()\n    except FileNotFoundError:\n        logging.error(f\"The file {excel_file_location} does not exist.\")\n        raise FileNotFoundError(f\"The file {excel_file_location} does not exist.\")\n    except ValueError:\n        logging.error(f\"The sheet {sheet_name} does not exist in the excel file.\")\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the excel file.\")\n```\n"}
{"task_id": "BigCodeBench/362", "solution": "import pandas as pd\nimport os\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    if not os.path.exists(original_file_location):\n        raise FileNotFoundError(f\"The file {original_file_location} does not exist.\")\n    \n    try:\n        df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n    except Exception as e:\n        raise ValueError(f\"Error reading the excel file: {e}\")\n\n    df.to_excel(new_file_location, sheet_name=sheet_name, index=False)\n\n    return df", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    if not os.path.exists(original_file_location):\n        raise FileNotFoundError(f\"The file {original_file_location} does not exist.\")\n    \n    try:\n        df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n    except Exception as e:\n        raise ValueError(f\"Error reading the excel file: {e}\")\n\n    df.to_excel(new_file_location, sheet_name=sheet_name, index=False)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/363", "solution": "from multiprocessing import Pool\nimport math\ndef calculate_factorial(number: int) -> tuple:\n    return number, math.factorial(number)\ndef task_func(numbers: list) -> dict:\n    if not all(isinstance(n, int) and n >= 0 for n in numbers):\n        raise ValueError(\"All elements in the input list must be non-negative integers.\")\n    \n    with Pool() as pool:\n        results = pool.map(calculate_factorial, numbers)\n    \n    return dict(results)", "raw_solution": "from multiprocessing import Pool\nimport math\n\ndef calculate_factorial(number: int) -> tuple:\n    return number, math.factorial(number)\n    \ndef task_func(numbers: list) -> dict:\n    if not all(isinstance(n, int) and n >= 0 for n in numbers):\n        raise ValueError(\"All elements in the input list must be non-negative integers.\")\n    \n    with Pool() as pool:\n        results = pool.map(calculate_factorial, numbers)\n    \n    return dict(results)\n\n# Example usage:\n# factorials = task_func([5, 6, 7, 8, 9])\n# print(factorials)  # Output: {5: 120, 6: 720, 7: 5040, 8: 40320, 9: 362880}\n```\nThis script uses the `multiprocessing` module to parallelize the calculation of factorials for a list of numbers. The `task_func` function takes a list of numbers, checks that all elements are non-negative integers, and then calculates the factorial for each number using a pool of processes. The results are returned as a dictionary where the keys are the input numbers and the values are their factorials.\n"}
{"task_id": "BigCodeBench/364", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n\n    # Splitting the data into training and testing sets\n    X = df[FEATURES]\n    y = df[TARGET]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Creating and training the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n\n    # Splitting the data into training and testing sets\n    X = df[FEATURES]\n    y = df[TARGET]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Creating and training the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model\n\n# Example usage:"}
{"task_id": "BigCodeBench/365", "solution": "from collections import Counter\nimport json\nimport random\nimport tempfile\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\ndef task_func(n, file_name, seed=77):\n    \"\"\"\n    Ek constant list se n randomly ruup se chune gaye words ki ek json file banaen.\n\n    Params:\n    - n (int): List se chune jaane wale words ki number.\n    - file_name (str): Utpann ki jaane wali json file ka naam.\n    - seed (int, Optional): Random number generator ke liye seed. defaults 77 hai.\n\n    Returns:\n        str: Utpann json file ka naam.\n\n    Reqs:\n    - collections\n    - json\n    - random\n\n    Examples:\n    - >>> import tempfile\n    - >>> temp_dir = tempfile.mkdtemp()\n    - >>> file_name = temp_dir + \"/word_counts.json\"\n    - >>> task_func(5, file_name, 29).endswith('word_counts.json')\n    - True\n\n    \"\"\"\n    random.seed(seed)\n    chosen_words = random.sample(WORDS, n)\n    word_counts = Counter(chosen_words)\n    \n    with open(file_name, 'w') as f:\n        json.dump(word_counts, f)\n    \n    return file_name", "raw_solution": "from collections import Counter\nimport json\nimport random\nimport tempfile\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\n\ndef task_func(n, file_name, seed=77):\n    \"\"\"\n    Ek constant list se n randomly ruup se chune gaye words ki ek json file banaen.\n\n    Params:\n    - n (int): List se chune jaane wale words ki number.\n    - file_name (str): Utpann ki jaane wali json file ka naam.\n    - seed (int, Optional): Random number generator ke liye seed. defaults 77 hai.\n\n    Returns:\n        str: Utpann json file ka naam.\n\n    Reqs:\n    - collections\n    - json\n    - random\n\n    Examples:\n    - >>> import tempfile\n    - >>> temp_dir = tempfile.mkdtemp()\n    - >>> file_name = temp_dir + \"/word_counts.json\"\n    - >>> task_func(5, file_name, 29).endswith('word_counts.json')\n    - True\n\n    \"\"\"\n    random.seed(seed)\n    chosen_words = random.sample(WORDS, n)\n    word_counts = Counter(chosen_words)\n    \n    with open(file_name, 'w') as f:\n        json.dump(word_counts, f)\n    \n    return file_name\n```\n"}
{"task_id": "BigCodeBench/366", "solution": "import matplotlib.pyplot as plt\nimport random\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\ndef task_func(number_list, bins):\n    \"\"\"\n    A function to create a histogram plot from a list of numbers.\n\n    Notes:\n        This function uses matplotlib.pyplot to create a histogram plot.\n        The plot has a title 'histogram'.\n        The x-axis label is 'number'.\n        The y-axis label is 'frequency'.\n        The histogram bars are colored from a set of predefined colors, chosen randomly.\n\n    Params:\n    - number_list (list): A list of numbers to plot.\n    - bins (int or sequence): If an int, it defines the number of bins for the histogram.\n      If a sequence, it defines the bin edges.\n\n    Returns:\n        matplotlib.axes._axes.Axes: The axis object of the histogram plot.\n\n    Reqs:\n    - matplotlib.pyplot\n    - random\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> number_list = [1, 2, 2, 3, 3, 3, 4, 4, 5]\n    - >>> bins = 5\n    - >>> ax = task_func(number_list, bins)\n    - >>> ax.patches[0].get_height()\n    - 1.0\n    - >>> ax.patches[2].get_height() > ax.patches[0].get_height()\n    - True\n    - >>> plt.close()\n    \"\"\"\n    # Randomly select a color from the COLORS list\n    random_color = random.choice(COLORS)\n    \n    # Create the histogram plot\n    fig, ax = plt.subplots()\n    ax.hist(number_list, bins=bins, color=random_color)\n    ax.set_title('histogram')\n    ax.set_xlabel('number')\n    ax.set_ylabel('frequency')\n    \n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\ndef task_func(number_list, bins):\n    \"\"\"\n    A function to create a histogram plot from a list of numbers.\n\n    Notes:\n        This function uses matplotlib.pyplot to create a histogram plot.\n        The plot has a title 'histogram'.\n        The x-axis label is 'number'.\n        The y-axis label is 'frequency'.\n        The histogram bars are colored from a set of predefined colors, chosen randomly.\n\n    Params:\n    - number_list (list): A list of numbers to plot.\n    - bins (int or sequence): If an int, it defines the number of bins for the histogram.\n      If a sequence, it defines the bin edges.\n\n    Returns:\n        matplotlib.axes._axes.Axes: The axis object of the histogram plot.\n\n    Reqs:\n    - matplotlib.pyplot\n    - random\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> number_list = [1, 2, 2, 3, 3, 3, 4, 4, 5]\n    - >>> bins = 5\n    - >>> ax = task_func(number_list, bins)\n    - >>> ax.patches[0].get_height()\n    - 1.0\n    - >>> ax.patches[2].get_height() > ax.patches[0].get_height()\n    - True\n    - >>> plt.close()\n    \"\"\"\n    # Randomly select a color from the COLORS list\n    random_color = random.choice(COLORS)\n    \n    # Create the histogram plot\n    fig, ax = plt.subplots()\n    ax.hist(number_list, bins=bins, color=random_color)\n    ax.set_title('histogram')\n    ax.set_xlabel('number')\n    ax.set_ylabel('frequency')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n\n    # Count the number of activities per day of the week\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime('%A')\n        activity_counts[day_of_week] += 1\n\n    # Prepare data for plotting\n    days_of_week = list(activity_counts.keys())\n    counts = list(activity_counts.values())\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.bar(days_of_week, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax", "raw_solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n\n    # Count the number of activities per day of the week\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime('%A')\n        activity_counts[day_of_week] += 1\n\n    # Prepare data for plotting\n    days_of_week = list(activity_counts.keys())\n    counts = list(activity_counts.values())\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.bar(days_of_week, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax\n\n# Example usage:\n# activities = [datetime(2023, 10, 25), datetime(2023, 10, 26)]\n# ax = task_func(activities)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    random.seed(seed)\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    if not files:\n        raise FileNotFoundError(\"No files found in the source directory.\")\n    selected_file = random.choice(files)\n    shutil.move(os.path.join(src_dir, selected_file), dest_dir)\n    return selected_file", "raw_solution": "import os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    random.seed(seed)\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    if not files:\n        raise FileNotFoundError(\"No files found in the source directory.\")\n    selected_file = random.choice(files)\n    shutil.move(os.path.join(src_dir, selected_file), dest_dir)\n    return selected_file\n```\n"}
{"task_id": "BigCodeBench/369", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    '''\n    Draw a histogram of the given array with a Gaussian fit.\n\n    Parameters:\n    l (numpy array): The input array.\n\n    Returns:\n    ax (matplotlib.axes._axes.Axes): Axes object with the plot.\n\n    Note:\n    - This function use \"Fit results: mu = {mean},  std = {standard deviation}\" as the title of the plot, \n    where the values are rounded to two decimal points.\n\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> l = np.array([5, 5, 5, 5, 5])\n    >>> ax = task_func(l)\n    >>> print(ax.get_title())\n    Fit results: mu = 5.00,  std = 0.00\n    >>> plt.close()\n    '''\n    mu, std = stats.norm.fit(l)\n    fig, ax = plt.subplots()\n    ax.hist(l, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n\n    return ax", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(l):\n    '''\n    Draw a histogram of the given array with a Gaussian fit.\n\n    Parameters:\n    l (numpy array): The input array.\n\n    Returns:\n    ax (matplotlib.axes._axes.Axes): Axes object with the plot.\n\n    Note:\n    - This function use \"Fit results: mu = {mean},  std = {standard deviation}\" as the title of the plot, \n    where the values are rounded to two decimal points.\n\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> l = np.array([5, 5, 5, 5, 5])\n    >>> ax = task_func(l)\n    >>> print(ax.get_title())\n    Fit results: mu = 5.00,  std = 0.00\n    >>> plt.close()\n    '''\n    mu, std = stats.norm.fit(l)\n    fig, ax = plt.subplots()\n    ax.hist(l, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/370", "solution": "import os\nimport re\nimport json\nimport glob\ndef task_func(directory_path: str) -> list:\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n\n    processed_files = []\n\n    for file_path in glob.glob(os.path.join(directory_path, '*.json')):\n        with open(file_path, 'r') as file:\n            content = file.read()\n\n        # Escape double quotes\n        escaped_content = re.sub(r'(?<!\\\\)\"', r'\\\\\\\\\"', content)\n\n        with open(file_path, 'w') as file:\n            file.write(escaped_content)\n\n        processed_files.append(file_path)\n\n    return processed_files", "raw_solution": "import os\nimport re\nimport json\nimport glob\n\ndef task_func(directory_path: str) -> list:\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n\n    processed_files = []\n\n    for file_path in glob.glob(os.path.join(directory_path, '*.json')):\n        with open(file_path, 'r') as file:\n            content = file.read()\n\n        # Escape double quotes\n        escaped_content = re.sub(r'(?<!\\\\)\"', r'\\\\\\\\\"', content)\n\n        with open(file_path, 'w') as file:\n            file.write(escaped_content)\n\n        processed_files.append(file_path)\n\n    return processed_files\n```\n"}
{"task_id": "BigCodeBench/371", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(l):\n    \"\"\"\n    Input field ko [0, 1] ki range mein scale karein aur ise ek DataFrame ke roop mein dikhayein.\n\n    Notes:\n        Return DataFrame 'Scaled Values' ka use column naam ke roop mein karta hai.\n\n    Params:\n    - l (numpy array): Input array.\n\n    Returns:\n        DataFrame: Scaled array ka ek pandas DataFrame.\n\n    Reqs:\n    - numpy\n    - sklearn.preprocessing\n    - pandas\n\n    Examples:\n    - >>> import numpy as np\n    - >>> l = np.array([10, 20, 30, 40, 50])\n    - >>> df = task_func(l)\n    - >>> print(int(df.iloc[0]['Scaled Values']))\n    - 0\n\n    \"\"\"\n    scaler = MinMaxScaler()\n    scaled_values = scaler.fit_transform(l.reshape(-1, 1))\n    df = pd.DataFrame(scaled_values, columns=['Scaled Values'])\n    return df", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(l):\n    \"\"\"\n    Input field ko [0, 1] ki range mein scale karein aur ise ek DataFrame ke roop mein dikhayein.\n\n    Notes:\n        Return DataFrame 'Scaled Values' ka use column naam ke roop mein karta hai.\n\n    Params:\n    - l (numpy array): Input array.\n\n    Returns:\n        DataFrame: Scaled array ka ek pandas DataFrame.\n\n    Reqs:\n    - numpy\n    - sklearn.preprocessing\n    - pandas\n\n    Examples:\n    - >>> import numpy as np\n    - >>> l = np.array([10, 20, 30, 40, 50])\n    - >>> df = task_func(l)\n    - >>> print(int(df.iloc[0]['Scaled Values']))\n    - 0\n\n    \"\"\"\n    scaler = MinMaxScaler()\n    scaled_values = scaler.fit_transform(l.reshape(-1, 1))\n    df = pd.DataFrame(scaled_values, columns=['Scaled Values'])\n    return df\n```\n"}
{"task_id": "BigCodeBench/372", "solution": "import re\nimport glob\nfrom docx import Document\ndef task_func(directory_path: str) -> int:\n    # Find all .docx files in the given directory\n    docx_files = glob.glob(f\"{directory_path}/*.docx\")\n    processed_files = 0\n\n    for file in docx_files:\n        # Load the document\n        doc = Document(file)\n\n        # Iterate through each paragraph in the document\n        for para in doc.paragraphs:\n            # Replace double quotes with double quotes preceded by a backslash\n            new_text = re.sub(r'\"', r'\\\"', para.text)\n            para.text = new_text\n\n        # Save the modified document\n        doc.save(file)\n        processed_files += 1\n\n    return processed_files", "raw_solution": "import re\nimport glob\nfrom docx import Document\n\ndef task_func(directory_path: str) -> int:\n    # Find all .docx files in the given directory\n    docx_files = glob.glob(f\"{directory_path}/*.docx\")\n    processed_files = 0\n\n    for file in docx_files:\n        # Load the document\n        doc = Document(file)\n\n        # Iterate through each paragraph in the document\n        for para in doc.paragraphs:\n            # Replace double quotes with double quotes preceded by a backslash\n            new_text = re.sub(r'\"', r'\\\"', para.text)\n            para.text = new_text\n\n        # Save the modified document\n        doc.save(file)\n        processed_files += 1\n\n    return processed_files\n```\n"}
{"task_id": "BigCodeBench/373", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(l, x_data, plot=False):\n    def quadratic(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    params, _ = curve_fit(quadratic, x_data, l)\n    fitted_values = quadratic(x_data, *params)\n\n    if plot:\n        fig, ax = plt.subplots()\n        ax.scatter(x_data, l, label='Data')\n        ax.plot(x_data, fitted_values, 'r-', label='Fitted curve')\n        ax.set_xlabel('x')\n        ax.set_ylabel('y')\n        ax.set_title('Quadratic Fit')\n        ax.legend()\n        plt.show()\n        return params, fitted_values, ax\n\n    return params, fitted_values\nl = np.array([1, 4, 9, 16, 25])\nx_data = np.array([1, 2, 3, 4, 5])", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(l, x_data, plot=False):\n    def quadratic(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    params, _ = curve_fit(quadratic, x_data, l)\n    fitted_values = quadratic(x_data, *params)\n\n    if plot:\n        fig, ax = plt.subplots()\n        ax.scatter(x_data, l, label='Data')\n        ax.plot(x_data, fitted_values, 'r-', label='Fitted curve')\n        ax.set_xlabel('x')\n        ax.set_ylabel('y')\n        ax.set_title('Quadratic Fit')\n        ax.legend()\n        plt.show()\n        return params, fitted_values, ax\n\n    return params, fitted_values\n\n# Example usage:\nl = np.array([1, 4, 9, 16, 25])\nx_data = np.array([1, 2, 3, 4, 5])\nparams, fitted_values = task_func(l, x_data, plot=True)"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Check if the directory exists\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n    \n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Initialize a counter for modified files\n    modified_count = 0\n    \n    for file in xlsx_files:\n        # Load the workbook\n        workbook = load_workbook(file)\n        \n        # Iterate through all sheets\n        for sheet in workbook.worksheets:\n            # Iterate through all cells\n            for row in sheet.iter_rows():\n                for cell in row:\n                    # Check if the cell value is a string\n                    if isinstance(cell.value, str):\n                        # Replace double quotes with double backslashes\n                        cell.value = re.sub(r'\"', r'\\\\\\\\\"', cell.value)\n                        modified_count += 1\n        \n        # Save the workbook\n        workbook.save(file)\n    \n    return modified_count", "raw_solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Check if the directory exists\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n    \n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Initialize a counter for modified files\n    modified_count = 0\n    \n    for file in xlsx_files:\n        # Load the workbook\n        workbook = load_workbook(file)\n        \n        # Iterate through all sheets\n        for sheet in workbook.worksheets:\n            # Iterate through all cells\n            for row in sheet.iter_rows():\n                for cell in row:\n                    # Check if the cell value is a string\n                    if isinstance(cell.value, str):\n                        # Replace double quotes with double backslashes\n                        cell.value = re.sub(r'\"', r'\\\\\\\\\"', cell.value)\n                        modified_count += 1\n        \n        # Save the workbook\n        workbook.save(file)\n    \n    return modified_count\n```\n"}
{"task_id": "BigCodeBench/375", "solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(l)\n    \n    fig, ax = plt.subplots()\n    ax.scatter(principalComponents[:, 0], principalComponents[:, 1])\n    ax.set_title('PCA Result')\n    ax.set_xlabel('First Principal Component')\n    ax.set_ylabel('Second Principal Component')\n    \n    return ax", "raw_solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(l):\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(l)\n    \n    fig, ax = plt.subplots()\n    ax.scatter(principalComponents[:, 0], principalComponents[:, 1])\n    ax.set_title('PCA Result')\n    ax.set_xlabel('First Principal Component')\n    ax.set_ylabel('Second Principal Component')\n    \n    return ax\n\n# Example usage:\n# import numpy as np\n# l = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# ax = task_func(l)\n# print(len(ax.collections[0].get_offsets()))\n# print(ax.get_title())\n# plt.close()\n```\n"}
{"task_id": "BigCodeBench/376", "solution": "import nltk\nimport re\nfrom collections import Counter\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(text):\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove punctuation using regular expressions\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Tokenize the text into words\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Count word frequencies\n    word_counts = Counter(words)\n\n    return dict(word_counts)", "raw_solution": "import nltk\nimport re\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef task_func(text):\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove punctuation using regular expressions\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Tokenize the text into words\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Count word frequencies\n    word_counts = Counter(words)\n\n    return dict(word_counts)\n\n# Example usage:\n# print(task_func('This is a sample text. This text is for testing.'))\n# Output: {'sample': 1, 'text': 2, 'testing': 1}\n```\n"}
{"task_id": "BigCodeBench/377", "solution": "from texttable import Texttable\nimport os\nimport psutil\ndef task_func():\n    # Create a table\n    table = Texttable()\n    table.add_row([\"Ek table banaen jo system ke cpu upayog, memory upayog, aur disk upayog ko pradarshit karti hai.\"])\n    table.add_row([\"Aitem\", \"Maan\"])\n    \n    # Get CPU usage\n    cpu_usage = psutil.cpu_percent(interval=1)\n    table.add_row([\"CPU Usage (%)\", f\"{cpu_usage}%\"])\n    \n    # Get Memory usage\n    memory_info = psutil.virtual_memory()\n    table.add_row([\"Memory Usage (%)\", f\"{memory_info.percent}%\"])\n    \n    # Get Disk usage\n    disk_info = psutil.disk_usage(os.sep)\n    table.add_row([\"Disk Usage (%)\", f\"{disk_info.percent}%\"])\n    \n    # Return the table as a string\n    return table.draw()", "raw_solution": "from texttable import Texttable\nimport os\nimport psutil\n\ndef task_func():\n    # Create a table\n    table = Texttable()\n    table.add_row([\"Ek table banaen jo system ke cpu upayog, memory upayog, aur disk upayog ko pradarshit karti hai.\"])\n    table.add_row([\"Aitem\", \"Maan\"])\n    \n    # Get CPU usage\n    cpu_usage = psutil.cpu_percent(interval=1)\n    table.add_row([\"CPU Usage (%)\", f\"{cpu_usage}%\"])\n    \n    # Get Memory usage\n    memory_info = psutil.virtual_memory()\n    table.add_row([\"Memory Usage (%)\", f\"{memory_info.percent}%\"])\n    \n    # Get Disk usage\n    disk_info = psutil.disk_usage(os.sep)\n    table.add_row([\"Disk Usage (%)\", f\"{disk_info.percent}%\"])\n    \n    # Return the table as a string\n    return table.draw()\n\n# Example usage\ntable_str = task_func()"}
{"task_id": "BigCodeBench/378", "solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\ndef task_func(data_dir='./data/'):\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"The directory {data_dir} does not exist.\")\n    \n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n    if not csv_files:\n        raise ValueError(f\"No CSV files found in the directory {data_dir}.\")\n\n    table = Texttable()\n    table.add_row([\"File\", \"Rows\", \"Columns\"])\n\n    for csv_file in csv_files:\n        try:\n            df = pd.read_csv(csv_file)\n            table.add_row([os.path.basename(csv_file), df.shape[0], df.shape[1]])\n        except pd.errors.EmptyDataError:\n            raise pd.errors.EmptyDataError(f\"The file {csv_file} is empty.\")\n    \n    return table.draw()", "raw_solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\n\ndef task_func(data_dir='./data/'):\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"The directory {data_dir} does not exist.\")\n    \n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n    if not csv_files:\n        raise ValueError(f\"No CSV files found in the directory {data_dir}.\")\n\n    table = Texttable()\n    table.add_row([\"File\", \"Rows\", \"Columns\"])\n\n    for csv_file in csv_files:\n        try:\n            df = pd.read_csv(csv_file)\n            table.add_row([os.path.basename(csv_file), df.shape[0], df.shape[1]])\n        except pd.errors.EmptyDataError:\n            raise pd.errors.EmptyDataError(f\"The file {csv_file} is empty.\")\n    \n    return table.draw()\n```\n"}
{"task_id": "BigCodeBench/379", "solution": "import pandas as pd\nimport numpy as np\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length):\n    \"\"\"\n    Ek nirdisht length and random data ke saath ek Pandas DataFrame banaen and phir data record karen.\n\n    Params:\n    - length (int): DataFrame banane int length.\n\n    Returns:\n        DataFrame: random data ke saath ek Pandas DataFrame.\n\n    Reqs:\n    - pandas\n    - numpy\n\n    Examples:\n    - >>> np.random.seed(0)\n    - >>> df = task_func(5)\n    - >>> df.shape\n    - (5, 5)\n\n    \"\"\"\n    # Generating random data\n    data = np.random.rand(length, len(COLUMNS))\n    \n    # Creating DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(length):\n    \"\"\"\n    Ek nirdisht length and random data ke saath ek Pandas DataFrame banaen and phir data record karen.\n\n    Params:\n    - length (int): DataFrame banane int length.\n\n    Returns:\n        DataFrame: random data ke saath ek Pandas DataFrame.\n\n    Reqs:\n    - pandas\n    - numpy\n\n    Examples:\n    - >>> np.random.seed(0)\n    - >>> df = task_func(5)\n    - >>> df.shape\n    - (5, 5)\n\n    \"\"\"\n    # Generating random data\n    data = np.random.rand(length, len(COLUMNS))\n    \n    # Creating DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    return df\n\n# Test cases"}
{"task_id": "BigCodeBench/380", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    # Ensure the directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Get a list of all files in the directory\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\n    # Create a dictionary to hold files grouped by their extensions\n    files_by_extension = {}\n\n    for file in files:\n        # Extract the file extension\n        extension = re.search(r'\\.(\\w+)$', file)\n        if extension:\n            extension = extension.group(1)\n            if extension not in files_by_extension:\n                # Create a new directory for this extension\n                new_dir = os.path.join(directory, extension)\n                os.makedirs(new_dir, exist_ok=True)\n                files_by_extension[extension] = new_dir\n            # Move the file to the corresponding directory\n            shutil.move(os.path.join(directory, file), files_by_extension[extension])\n\n    return None", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(directory):\n    # Ensure the directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Get a list of all files in the directory\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\n    # Create a dictionary to hold files grouped by their extensions\n    files_by_extension = {}\n\n    for file in files:\n        # Extract the file extension\n        extension = re.search(r'\\.(\\w+)$', file)\n        if extension:\n            extension = extension.group(1)\n            if extension not in files_by_extension:\n                # Create a new directory for this extension\n                new_dir = os.path.join(directory, extension)\n                os.makedirs(new_dir, exist_ok=True)\n                files_by_extension[extension] = new_dir\n            # Move the file to the corresponding directory\n            shutil.move(os.path.join(directory, file), files_by_extension[extension])\n\n    return None\n```\n"}
{"task_id": "BigCodeBench/381", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    df = pd.read_csv(file_path)\n    \n    if target_column not in df.columns:\n        raise ValueError(f\"The target column {target_column} is not in the DataFrame.\")\n    \n    df.dropna(inplace=True)\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.dropna(inplace=True)\n    \n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    if not all(df.dtypes == 'float32'):\n        raise ValueError(\"All columns except the target column must be of type float32.\")\n    \n    model = RandomForestClassifier(random_state=seed)\n    model.fit(X, y)\n    \n    importances = model.feature_importances_\n    \n    fig, ax = plt.subplots()\n    sns.barplot(x=importances, y=X.columns, ax=ax)\n    ax.set_title('Feature Importances')\n    ax.set_xlabel('Importance')\n    ax.set_ylabel('Features')\n    \n    return ax, importances", "raw_solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    df = pd.read_csv(file_path)\n    \n    if target_column not in df.columns:\n        raise ValueError(f\"The target column {target_column} is not in the DataFrame.\")\n    \n    df.dropna(inplace=True)\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.dropna(inplace=True)\n    \n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    if not all(df.dtypes == 'float32'):\n        raise ValueError(\"All columns except the target column must be of type float32.\")\n    \n    model = RandomForestClassifier(random_state=seed)\n    model.fit(X, y)\n    \n    importances = model.feature_importances_\n    \n    fig, ax = plt.subplots()\n    sns.barplot(x=importances, y=X.columns, ax=ax)\n    ax.set_title('Feature Importances')\n    ax.set_xlabel('Importance')\n    ax.set_ylabel('Features')\n    \n    return ax, importances\n\n# Example usage:\n# ax, importances = task_func('arena.csv', 'Index')\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/382", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(length):\n    # Generate a normal distribution\n    distribution = np.random.normal(loc=0, scale=1, size=length)\n\n    # Create a histogram of the distribution\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(distribution, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the probability density function\n    pdf = norm.pdf(bins, loc=0, scale=1)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n\n    return distribution, ax", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n    # Generate a normal distribution\n    distribution = np.random.normal(loc=0, scale=1, size=length)\n\n    # Create a histogram of the distribution\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(distribution, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the probability density function\n    pdf = norm.pdf(bins, loc=0, scale=1)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n\n    return distribution, ax\n```\n"}
{"task_id": "BigCodeBench/383", "solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\nimport re\ndef task_func(text, n, top_k):\n    # Preprocess the text\n    text = re.sub(r'\\W+', ' ', text.lower())\n\n    # Generate n-grams\n    ngrams = zip(*[text[i:] for i in range(n)])\n    ngram_counts = Counter(ngrams)\n\n    # Get the top k n-grams\n    top_ngrams = ngram_counts.most_common(top_k)\n\n    # Create a DataFrame\n    df = pd.DataFrame(top_ngrams, columns=['ngram', 'count'])\n\n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='count', y='ngram', data=df)\n    ax.set_title(f'Top {top_k} {n}-grams')\n    ax.set_xlabel('Count')\n    ax.set_ylabel('N-grams')\n\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\nimport re\n\ndef task_func(text, n, top_k):\n    # Preprocess the text\n    text = re.sub(r'\\W+', ' ', text.lower())\n\n    # Generate n-grams\n    ngrams = zip(*[text[i:] for i in range(n)])\n    ngram_counts = Counter(ngrams)\n\n    # Get the top k n-grams\n    top_ngrams = ngram_counts.most_common(top_k)\n\n    # Create a DataFrame\n    df = pd.DataFrame(top_ngrams, columns=['ngram', 'count'])\n\n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='count', y='ngram', data=df)\n    ax.set_title(f'Top {top_k} {n}-grams')\n    ax.set_xlabel('Count')\n    ax.set_ylabel('N-grams')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/384", "solution": "import collections\nimport random\nimport itertools\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Bear', 'Giraffe', 'Horse', 'Rabbit', 'Snake', 'Zebra']\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n    \n    # Reverse the dictionary to get animals as keys and names as values\n    reversed_dict = {animal: [] for animal in ANIMALS}\n    for name, animal in animal_dict.items():\n        if animal in reversed_dict:\n            reversed_dict[animal].append(name)\n    \n    # Count occurrences of each animal in the ANIMALS list\n    animal_counter = collections.Counter(random.choices(ANIMALS, k=len(ANIMALS)))\n    for animal, count in animal_counter.items():\n        animal_counter[animal] = min(count, max_count)\n    \n    return reversed_dict, animal_counter\nanimal_dict = {'John': 'Cat', 'Alice': 'Dog', 'Bob': 'Elephant', 'Charlie': 'Lion', 'David': 'Tiger', 'Sue': 'Pangolin'}", "raw_solution": "import collections\nimport random\nimport itertools\n\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Bear', 'Giraffe', 'Horse', 'Rabbit', 'Snake', 'Zebra']\n\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n    \n    # Reverse the dictionary to get animals as keys and names as values\n    reversed_dict = {animal: [] for animal in ANIMALS}\n    for name, animal in animal_dict.items():\n        if animal in reversed_dict:\n            reversed_dict[animal].append(name)\n    \n    # Count occurrences of each animal in the ANIMALS list\n    animal_counter = collections.Counter(random.choices(ANIMALS, k=len(ANIMALS)))\n    for animal, count in animal_counter.items():\n        animal_counter[animal] = min(count, max_count)\n    \n    return reversed_dict, animal_counter\n\n# Example usage:\nanimal_dict = {'John': 'Cat', 'Alice': 'Dog', 'Bob': 'Elephant', 'Charlie': 'Lion', 'David': 'Tiger', 'Sue': 'Pangolin'}\nreversed_dict, animal_counter = task_func(animal_dict, 15, 77)"}
{"task_id": "BigCodeBench/385", "solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit in the dictionary\n    fruit_counter = Counter(fruit_dict.values())\n\n    # Create a bar chart of the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(fruit_counter.keys(), fruit_counter.values())\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Fruit Frequency in Dictionary')\n\n    return dict(fruit_counter), ax\nfruit_dict = {'John': 'Apple', 'Alice': 'Banana', 'Bob': 'Cherry', 'Charlie': 'Date', 'David': 'Apple'}", "raw_solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\n\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry', 'Fig', 'Grape', 'Honeydew', 'Indian Prune', 'Jackfruit']\n\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit in the dictionary\n    fruit_counter = Counter(fruit_dict.values())\n\n    # Create a bar chart of the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(fruit_counter.keys(), fruit_counter.values())\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Fruit Frequency in Dictionary')\n\n    return dict(fruit_counter), ax\n\n# Example usage:\nfruit_dict = {'John': 'Apple', 'Alice': 'Banana', 'Bob': 'Cherry', 'Charlie': 'Date', 'David': 'Apple'}\nfreq, ax = task_func(fruit_dict)"}
{"task_id": "BigCodeBench/386", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length, min_value = 0, max_value = 100):\n    \"\"\"\n    This function generates a pandas DataFrame with specified length and columns, and calculates the cumulative distribution function (CDF).\n\n    Notes:\n        The columns are named according to the COLUMNS constant.\n\n    Params:\n    - length (int): The length of the columns to be generated.\n    - min_value (int, optional): The minimum value for data generation. Default is 0.\n    - max_value (int, optional): The maximum value for data generation. Default is 100.\n\n    Returns:\n        DataFrame: The DataFrame with CDF values.\n\n    Reqs:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> np.random.seed(0)\n    - >>> cdf = task_func(100, 0, 1)\n    - >>> print(len(cdf))\n    - 1\n    \"\"\"\n\n    # Generate random data\n    data = np.random.randint(min_value, max_value, size=(length, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Calculate CDF\n    cdf = df.cumsum()\n\n    # Plot CDF\n    plt.figure(figsize=(10, 6))\n    for column in COLUMNS:\n        plt.plot(cdf[column], label=column)\n    plt.title('Cumulative Distribution Function (CDF)')\n    plt.xlabel('Index')\n    plt.ylabel('Cumulative Sum')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return cdf", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(length, min_value = 0, max_value = 100):\n    \"\"\"\n    This function generates a pandas DataFrame with specified length and columns, and calculates the cumulative distribution function (CDF).\n\n    Notes:\n        The columns are named according to the COLUMNS constant.\n\n    Params:\n    - length (int): The length of the columns to be generated.\n    - min_value (int, optional): The minimum value for data generation. Default is 0.\n    - max_value (int, optional): The maximum value for data generation. Default is 100.\n\n    Returns:\n        DataFrame: The DataFrame with CDF values.\n\n    Reqs:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> np.random.seed(0)\n    - >>> cdf = task_func(100, 0, 1)\n    - >>> print(len(cdf))\n    - 1\n    \"\"\"\n\n    # Generate random data\n    data = np.random.randint(min_value, max_value, size=(length, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Calculate CDF\n    cdf = df.cumsum()\n\n    # Plot CDF\n    plt.figure(figsize=(10, 6))\n    for column in COLUMNS:\n        plt.plot(cdf[column], label=column)\n    plt.title('Cumulative Distribution Function (CDF)')\n    plt.xlabel('Index')\n    plt.ylabel('Cumulative Sum')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return cdf\n```\n"}
{"task_id": "BigCodeBench/387", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\ndef task_func(city_dict, max_range=1000000, seed=0):\n    np.random.seed(seed)\n    population_dict = {}\n    \n    for person, city in city_dict.items():\n        if city in CITIES:\n            population_dict[person] = np.random.randint(1, max_range)\n        else:\n            population_dict[person] = -1\n    \n    city_populations = {city: 0 for city in CITIES}\n    for person, city in city_dict.items():\n        if city in CITIES:\n            city_populations[city] += population_dict[person]\n    \n    fig, ax = plt.subplots()\n    ax.bar(city_populations.keys(), city_populations.values())\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('Population by City')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return population_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\n\ndef task_func(city_dict, max_range=1000000, seed=0):\n    np.random.seed(seed)\n    population_dict = {}\n    \n    for person, city in city_dict.items():\n        if city in CITIES:\n            population_dict[person] = np.random.randint(1, max_range)\n        else:\n            population_dict[person] = -1\n    \n    city_populations = {city: 0 for city in CITIES}\n    for person, city in city_dict.items():\n        if city in CITIES:\n            city_populations[city] += population_dict[person]\n    \n    fig, ax = plt.subplots()\n    ax.bar(city_populations.keys(), city_populations.values())\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('Population by City')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return population_dict, ax\n```\n"}
{"task_id": "BigCodeBench/388", "solution": "import collections\nimport pandas as pd\ndef task_func(my_tuple, path_csv_files):\n    result = {}\n    for column in my_tuple:\n        counter = collections.Counter()\n        for file in path_csv_files:\n            df = pd.read_csv(file)\n            counter.update(df[column])\n        result[column] = counter\n    return result", "raw_solution": "import collections\nimport pandas as pd\n\ndef task_func(my_tuple, path_csv_files):\n    result = {}\n    for column in my_tuple:\n        counter = collections.Counter()\n        for file in path_csv_files:\n            df = pd.read_csv(file)\n            counter.update(df[column])\n        result[column] = counter\n    return result\n\n# Example usage:\n# Assuming we have two CSV files: 'file1.csv' and 'file2.csv'\n# with the following content:\n# file1.csv:\n# Country,Gender\n# USA,Male\n# Canada,Female\n# USA,Male\n# file2.csv:\n# Country,Gender\n# UK,Male\n# USA,Male\n# Germany,Female\n\n# from unittest.mock import MagicMock\n# import pandas as pd\n# df1 = pd.DataFrame({'Country': ['USA', 'Canada', 'USA'], 'Gender': ['Male', 'Female', 'Male']})\n# df2 = pd.DataFrame({'Country': ['UK', 'USA', 'Germany'], 'Gender': ['Male', 'Male', 'Female']})\n# pd.read_csv = MagicMock(side_effect=[df1, df2])\n# result = task_func(('Country', 'Gender'), ['file1.csv', 'file2.csv'])\n# print(result['Country'])\n# print(result['Gender'])\n```\n"}
{"task_id": "BigCodeBench/389", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    # Create a new subdirectory named \"Interesting Files\"\n    new_dir = os.path.join(directory, \"Interesting Files\")\n    os.makedirs(new_dir, exist_ok=True)\n\n    # List to store moved files\n    moved_files = []\n\n    # Regex pattern to match filenames containing \"like\" or \"what\"\n    pattern = re.compile(r'.*(like|what).*', re.IGNORECASE)\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the filename matches the pattern\n        if pattern.match(filename):\n            # Move the file to the new subdirectory\n            shutil.move(os.path.join(directory, filename), new_dir)\n            moved_files.append(filename)\n\n    return moved_files", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(directory):\n    # Create a new subdirectory named \"Interesting Files\"\n    new_dir = os.path.join(directory, \"Interesting Files\")\n    os.makedirs(new_dir, exist_ok=True)\n\n    # List to store moved files\n    moved_files = []\n\n    # Regex pattern to match filenames containing \"like\" or \"what\"\n    pattern = re.compile(r'.*(like|what).*', re.IGNORECASE)\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the filename matches the pattern\n        if pattern.match(filename):\n            # Move the file to the new subdirectory\n            shutil.move(os.path.join(directory, filename), new_dir)\n            moved_files.append(filename)\n\n    return moved_files\n```\n"}
{"task_id": "BigCodeBench/390", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    if not csv_url_dict or \"URL\" not in csv_url_dict:\n        raise ValueError(\"The dictionary is empty or 'URL' key is missing.\")\n    \n    url = csv_url_dict[\"URL\"]\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to fetch data from {url}\")\n\n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    df = df.sort_values(by=sort_by_column)\n    \n    return df", "raw_solution": "import pandas as pd\nimport requests\nfrom io import StringIO\n\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    if not csv_url_dict or \"URL\" not in csv_url_dict:\n        raise ValueError(\"The dictionary is empty or 'URL' key is missing.\")\n    \n    url = csv_url_dict[\"URL\"]\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to fetch data from {url}\")\n\n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    df = df.sort_values(by=sort_by_column)\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/391", "solution": "import os\nimport glob\nimport shutil\ndef task_func(directory, archive_dir='archive'):\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n    errors = []\n\n    for file in json_files:\n        try:\n            shutil.move(file, archive_dir)\n        except Exception as e:\n            errors.append(f\"Error moving {file}: {str(e)}\")\n\n    return len(errors) == 0, errors", "raw_solution": "import os\nimport glob\nimport shutil\n\ndef task_func(directory, archive_dir='archive'):\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n    errors = []\n\n    for file in json_files:\n        try:\n            shutil.move(file, archive_dir)\n        except Exception as e:\n            errors.append(f\"Error moving {file}: {str(e)}\")\n\n    return len(errors) == 0, errors\n```\n"}
{"task_id": "BigCodeBench/392", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col, group_name):\n    if group_name not in df[group_col].values:\n        raise ValueError(f\"Group name '{group_name}' not found in DataFrame.\")\n\n    # Filter the DataFrame for the specific group\n    group_df = df[df[group_col] == group_name]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(group_df[group_col], group_df[value_col], color=COLORS[0])\n\n    # Set labels and title\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    ax.set_title(f'{value_col} ke lie {group_name} ka bar chart')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col, group_name):\n    if group_name not in df[group_col].values:\n        raise ValueError(f\"Group name '{group_name}' not found in DataFrame.\")\n\n    # Filter the DataFrame for the specific group\n    group_df = df[df[group_col] == group_name]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(group_df[group_col], group_df[value_col], color=COLORS[0])\n\n    # Set labels and title\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    ax.set_title(f'{value_col} ke lie {group_name} ka bar chart')\n\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'Group': ['A', 'B', 'C'], 'Value': [10, 20, 30]})\n# ax = task_func(df, 'Group', 'Value', 'B')\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/393", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Histogram\n    ax[0].hist(samples, bins=30, alpha=0.7, color='blue')\n    ax[0].set_title('Histogram of Samples')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Q-Q plot\n    stats.probplot(samples, dist=\"norm\", plot=ax[1])\n    ax[1].set_title('Q-Q Plot of Samples')\n    ax[1].get_lines()[1].set_color('red')\n\n    plt.tight_layout()\n    return fig", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Histogram\n    ax[0].hist(samples, bins=30, alpha=0.7, color='blue')\n    ax[0].set_title('Histogram of Samples')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Q-Q plot\n    stats.probplot(samples, dist=\"norm\", plot=ax[1])\n    ax[1].set_title('Q-Q Plot of Samples')\n    ax[1].get_lines()[1].set_color('red')\n\n    plt.tight_layout()\n    return fig\n```\n"}
{"task_id": "BigCodeBench/394", "solution": "import collections\nimport string\nimport random\ndef task_func(length, seed=0):\n    random.seed(seed)\n    ascii_letters = string.ascii_letters\n    random_string = ''.join(random.choice(ascii_letters) for _ in range(length))\n    frequency = collections.Counter(random_string)\n    return dict(frequency)", "raw_solution": "import collections\nimport string\nimport random\n\n\ndef task_func(length, seed=0):\n    random.seed(seed)\n    ascii_letters = string.ascii_letters\n    random_string = ''.join(random.choice(ascii_letters) for _ in range(length))\n    frequency = collections.Counter(random_string)\n    return dict(frequency)\n```\n"}
{"task_id": "BigCodeBench/395", "solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    file_paths = glob.glob(os.path.join(directory, file_pattern))\n    if not file_paths:\n        raise ValueError(f\"No files matching pattern {file_pattern} found in directory {directory}.\")\n    \n    file_paths = natsort.natsorted(file_paths)\n    data = []\n\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n            numbers = re.findall(regex, content)\n            data.append({'Filename': os.path.basename(file_path), 'Numeric Data': list(map(int, numbers))})\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\n\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    file_paths = glob.glob(os.path.join(directory, file_pattern))\n    if not file_paths:\n        raise ValueError(f\"No files matching pattern {file_pattern} found in directory {directory}.\")\n    \n    file_paths = natsort.natsorted(file_paths)\n    data = []\n\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n            numbers = re.findall(regex, content)\n            data.append({'Filename': os.path.basename(file_path), 'Numeric Data': list(map(int, numbers))})\n\n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\n# data_dir = './data/'\n# create_dummy_files(data_dir)\n# df = task_func('./data/', '*.txt', r'([0-9]+)')\n# tear_down_files(data_dir)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/396", "solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, sample_size, seed=0):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, sample_size)\n    density = stats.gaussian_kde(data)\n\n    x = np.linspace(min(data), max(data), 1000)\n    y = density(x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'N({mu}, {sigma}^2)')\n    ax.set_title('Gaussian Kernel Density Estimate')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n\n    return ax", "raw_solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mu, sigma, sample_size, seed=0):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, sample_size)\n    density = stats.gaussian_kde(data)\n\n    x = np.linspace(min(data), max(data), 1000)\n    y = density(x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'N({mu}, {sigma}^2)')\n    ax.set_title('Gaussian Kernel Density Estimate')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/397", "solution": "import re\nimport urllib.request\nimport json\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(API_URL):\n    \"\"\"\n    Fetches the public IP address of the current host using an API.\n\n    Params:\n    - API_URL (str): The URL of the API that returns the public IP in JSON format.\n\n    Returns:\n    - str: The public IP address.\n\n    Reqs:\n    - re\n    - urllib.request\n    - json\n\n    Raises:\n    - If the API request fails, an error message is returned.\n\n    Examples:\n    - >>> import json\n    - >>> from unittest.mock import MagicMock\n    - >>> mock_response = MagicMock()\n    - >>> mock_response.read.return_value = json.dumps({'ip': '192.168.1.1'}).encode('utf-8')\n    - >>> mock_urlopen = MagicMock(return_value=mock_response)\n    - >>> with unittest.mock.patch('urllib.request.urlopen', mock_urlopen):\n    - ...     task_func('https://api.ipify.org?format=json')\n    - '192.168.1.1'\n    \"\"\"\n    try:\n        with urllib.request.urlopen(API_URL) as response:\n            data = json.loads(response.read().decode())\n            ip = data.get('ip', '')\n            if re.match(IP_REGEX, ip):\n                return ip\n            else:\n                raise ValueError(\"Invalid IP address format.\")\n    except Exception as e:\n        return f\"An error occurred: {e}\"", "raw_solution": "import re\nimport urllib.request\nimport json\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef task_func(API_URL):\n    \"\"\"\n    Fetches the public IP address of the current host using an API.\n\n    Params:\n    - API_URL (str): The URL of the API that returns the public IP in JSON format.\n\n    Returns:\n    - str: The public IP address.\n\n    Reqs:\n    - re\n    - urllib.request\n    - json\n\n    Raises:\n    - If the API request fails, an error message is returned.\n\n    Examples:\n    - >>> import json\n    - >>> from unittest.mock import MagicMock\n    - >>> mock_response = MagicMock()\n    - >>> mock_response.read.return_value = json.dumps({'ip': '192.168.1.1'}).encode('utf-8')\n    - >>> mock_urlopen = MagicMock(return_value=mock_response)\n    - >>> with unittest.mock.patch('urllib.request.urlopen', mock_urlopen):\n    - ...     task_func('https://api.ipify.org?format=json')\n    - '192.168.1.1'\n    \"\"\"\n    try:\n        with urllib.request.urlopen(API_URL) as response:\n            data = json.loads(response.read().decode())\n            ip = data.get('ip', '')\n            if re.match(IP_REGEX, ip):\n                return ip\n            else:\n                raise ValueError(\"Invalid IP address format.\")\n    except Exception as e:\n        return f\"An error occurred: {e}\"\n```\n"}
{"task_id": "BigCodeBench/398", "solution": "import json\nimport os\ndef task_func(file_path):\n    \"\"\"\n    Jaanche ki ek JSON file mein data, dictionaries (JavaScript mein objects) ki ek list hai ya nahi.\n\n    Params:\n    - file_path (str): JSON file ka path.\n\n    Returns:\n        bool: true yadi data dictionaries ki ek list hai, anyatha False.\n\n    Reqs:\n    - json\n    - os\n\n    Examples:\n    - >>> import tempfile\n    - >>> import json\n    - >>> temp_dir = tempfile.mkdtemp()\n    - >>> file_path = os.path.join(temp_dir, 'data.json')\n    - >>> with open(file_path, 'w') as f:\n    - ...     json.dump([{'name': 'Alice', 'age': 30}, {'name': 'Bob', 'age': 25}], f)\n    - >>> task_func(file_path)\n    - true\n    - >>> task_func('./invalid_data.json') # File does not exist\n    - False\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        return False\n\n    with open(file_path, 'r') as f:\n        try:\n            data = json.load(f)\n            return isinstance(data, list)\n        except json.JSONDecodeError:\n            return False", "raw_solution": "import json\nimport os\n\ndef task_func(file_path):\n    \"\"\"\n    Jaanche ki ek JSON file mein data, dictionaries (JavaScript mein objects) ki ek list hai ya nahi.\n\n    Params:\n    - file_path (str): JSON file ka path.\n\n    Returns:\n        bool: true yadi data dictionaries ki ek list hai, anyatha False.\n\n    Reqs:\n    - json\n    - os\n\n    Examples:\n    - >>> import tempfile\n    - >>> import json\n    - >>> temp_dir = tempfile.mkdtemp()\n    - >>> file_path = os.path.join(temp_dir, 'data.json')\n    - >>> with open(file_path, 'w') as f:\n    - ...     json.dump([{'name': 'Alice', 'age': 30}, {'name': 'Bob', 'age': 25}], f)\n    - >>> task_func(file_path)\n    - true\n    - >>> task_func('./invalid_data.json') # File does not exist\n    - False\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        return False\n\n    with open(file_path, 'r') as f:\n        try:\n            data = json.load(f)\n            return isinstance(data, list)\n        except json.JSONDecodeError:\n            return False\n```\n"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be a positive integer.\")\n\n    # Generate time values\n    t = np.linspace(0, 1, sample_size)\n\n    # Generate sine and cosine waves\n    sine_wave = np.sin(2 * math.pi * frequency * t)\n    cosine_wave = np.cos(2 * math.pi * frequency * t)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n    ax.legend()\n\n    return fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be a positive integer.\")\n\n    # Generate time values\n    t = np.linspace(0, 1, sample_size)\n\n    # Generate sine and cosine waves\n    sine_wave = np.sin(2 * math.pi * frequency * t)\n    cosine_wave = np.cos(2 * math.pi * frequency * t)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n    ax.legend()\n\n    return fig, ax\n```\n"}
{"task_id": "BigCodeBench/400", "solution": "import json\nfrom pathlib import Path\nfrom glob import glob\ndef task_func(directory, string):\n    # Initialize an empty list to store file paths\n    matching_files = []\n\n    # Use glob to find all JSON files in the directory and subdirectories\n    for file_path in glob(f\"{directory}/**/*.json\", recursive=True):\n        # Open and read the JSON file\n        with open(file_path, \"r\") as file:\n            try:\n                data = json.load(file)\n                # Convert the JSON data to a string and check if the string is present\n                data_str = json.dumps(data)\n                if string in data_str:\n                    matching_files.append(file_path)\n            except json.JSONDecodeError:\n                # Skip files that are not valid JSON\n                continue\n\n    return matching_files", "raw_solution": "import json\nfrom pathlib import Path\nfrom glob import glob\n\ndef task_func(directory, string):\n    # Initialize an empty list to store file paths\n    matching_files = []\n\n    # Use glob to find all JSON files in the directory and subdirectories\n    for file_path in glob(f\"{directory}/**/*.json\", recursive=True):\n        # Open and read the JSON file\n        with open(file_path, \"r\") as file:\n            try:\n                data = json.load(file)\n                # Convert the JSON data to a string and check if the string is present\n                data_str = json.dumps(data)\n                if string in data_str:\n                    matching_files.append(file_path)\n            except json.JSONDecodeError:\n                # Skip files that are not valid JSON\n                continue\n\n    return matching_files\n```\n"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    app = Flask(app_name)\n    mail = Mail(app)\n\n    # Fetching environment variables or using defaults\n    mail_server = os.getenv('MAIL_SERVER', 'localhost')\n    mail_port = int(os.getenv('MAIL_PORT', 25))\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'false').lower() in ['true', '1', 't']\n    mail_username = os.getenv('MAIL_USERNAME', None)\n    mail_password = os.getenv('MAIL_PASSWORD', None)\n\n    # Configuring mail\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password\n    )\n\n    return mail, app.config", "raw_solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    app = Flask(app_name)\n    mail = Mail(app)\n\n    # Fetching environment variables or using defaults\n    mail_server = os.getenv('MAIL_SERVER', 'localhost')\n    mail_port = int(os.getenv('MAIL_PORT', 25))\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'false').lower() in ['true', '1', 't']\n    mail_username = os.getenv('MAIL_USERNAME', None)\n    mail_password = os.getenv('MAIL_PASSWORD', None)\n\n    # Configuring mail\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password\n    )\n\n    return mail, app.config\n\n# Example usage:\n# mail, configs = task_func(\"test\")\n# print(mail.__getattribute__(\"app\").name)  # Output: test\n```\n"}
{"task_id": "BigCodeBench/402", "solution": "import re\nimport requests\nimport json\nimport csv\nimport os\nAPI_URL = 'https://api.example.com/data'\ndef task_func(pattern):\n    response = requests.get(API_URL)\n    data = response.json()\n\n    matches = []\n    for item in data['data']:\n        if re.search(pattern, item):\n            matches.append(item)\n\n    csv_file_path = 'matched_data.csv'\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for match in matches:\n            writer.writerow([match])\n\n    return os.path.abspath(csv_file_path)", "raw_solution": "import re\nimport requests\nimport json\nimport csv\nimport os  \n\n# Constants\nAPI_URL = 'https://api.example.com/data'\n\ndef task_func(pattern):\n    response = requests.get(API_URL)\n    data = response.json()\n\n    matches = []\n    for item in data['data']:\n        if re.search(pattern, item):\n            matches.append(item)\n\n    csv_file_path = 'matched_data.csv'\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for match in matches:\n            writer.writerow([match])\n\n    return os.path.abspath(csv_file_path)\n```\nThis script sends a GET request to the API, extracts the data, and writes the matching items to a CSV file. The function returns the absolute path of the CSV file. If no matches are found, the CSV file will be empty.\n"}
{"task_id": "BigCodeBench/403", "solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\ndef task_func(img_path, blur_radius=5):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image using PIL\n    original_img = Image.open(img_path)\n    original_array = np.array(original_img)\n\n    # Convert the image to grayscale\n    grayscale_img = original_img.convert('L')\n    grayscale_array = np.array(grayscale_img)\n\n    # Apply the blur filter\n    blurred_img = grayscale_img.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n    blurred_array = np.array(blurred_img)\n\n    # Display the original and processed images using OpenCV\n    cv2.imshow('Original Image', original_array)\n    cv2.imshow('Blurred and Grayscale Image', blurred_array)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    return original_array, blurred_array", "raw_solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path, blur_radius=5):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image using PIL\n    original_img = Image.open(img_path)\n    original_array = np.array(original_img)\n\n    # Convert the image to grayscale\n    grayscale_img = original_img.convert('L')\n    grayscale_array = np.array(grayscale_img)\n\n    # Apply the blur filter\n    blurred_img = grayscale_img.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n    blurred_array = np.array(blurred_img)\n\n    # Display the original and processed images using OpenCV\n    cv2.imshow('Original Image', original_array)\n    cv2.imshow('Blurred and Grayscale Image', blurred_array)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    return original_array, blurred_array\n\n# Example usage:\n# image_path = 'sample.png'\n# create_dummy_image(image_path=image_path)\n# original, processed = task_func(image_path)\n# os.remove(image_path)\n```\nThis script loads an image using PIL, converts it to grayscale, applies a blur filter, and then displays both the original and processed images using OpenCV. The function returns the numpy arrays of the original and processed images.\n"}
{"task_id": "BigCodeBench/404", "solution": "import cv2\nimport numpy as np\nimport os\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image\n    img = cv2.imread(img_path)\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Find contours in the image\n    contours, hierarchy = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Draw contours on the original image\n    result_img = cv2.drawContours(img, contours, -1, (0, 255, 0), 2)\n\n    return result_img, contours", "raw_solution": "import cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image\n    img = cv2.imread(img_path)\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Find contours in the image\n    contours, hierarchy = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Draw contours on the original image\n    result_img = cv2.drawContours(img, contours, -1, (0, 255, 0), 2)\n\n    return result_img, contours\n```\n"}
{"task_id": "BigCodeBench/405", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(points: int):\n    random_numbers = [random.random() for _ in range(points)]\n    fig, ax = plt.subplots()\n    ax.plot(random_numbers)\n    ax.set_title('Random Numbers Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Random Value')\n    plt.show()\n    return random_numbers, ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\ndef task_func(points: int):\n    random_numbers = [random.random() for _ in range(points)]\n    fig, ax = plt.subplots()\n    ax.plot(random_numbers)\n    ax.set_title('Random Numbers Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Random Value')\n    plt.show()\n    return random_numbers, ax\n```\n"}
{"task_id": "BigCodeBench/406", "solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\ndef task_func(img_path, angle):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image\n    original_img = Image.open(img_path)\n\n    # Rotate the image\n    rotated_img = original_img.rotate(angle)\n\n    # Convert images to numpy arrays\n    original_img_array = np.array(original_img)\n    rotated_img_array = np.array(rotated_img)\n\n    # Plot the original and rotated images\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(original_img_array)\n    axs[0].set_title('Original Image')\n    axs[0].axis('off')\n    axs[1].imshow(rotated_img_array)\n    axs[1].set_title('Rotated Image')\n    axs[1].axis('off')\n    plt.show()\n\n    return original_img_array, rotated_img_array", "raw_solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\ndef task_func(img_path, angle):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"No file found at {img_path}\")\n\n    # Load the image\n    original_img = Image.open(img_path)\n\n    # Rotate the image\n    rotated_img = original_img.rotate(angle)\n\n    # Convert images to numpy arrays\n    original_img_array = np.array(original_img)\n    rotated_img_array = np.array(rotated_img)\n\n    # Plot the original and rotated images\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(original_img_array)\n    axs[0].set_title('Original Image')\n    axs[0].axis('off')\n    axs[1].imshow(rotated_img_array)\n    axs[1].set_title('Rotated Image')\n    axs[1].axis('off')\n    plt.show()\n\n    return original_img_array, rotated_img_array\n\n# Example usage:\n# img_path = 'sample.png'\n# task_func(img_path, 45)\n# os.remove(img_path)\n```\nThis script loads an image from the given path, rotates it by the specified angle, converts the images to numpy arrays, and displays them using matplotlib. It also removes the image file after processing.\n"}
{"task_id": "BigCodeBench/407", "solution": "import os\nimport csv\nfrom openpyxl import load_workbook\ndef task_func(file_name, excel_file_path, csv_file_path) -> str:\n    excel_file = os.path.join(excel_file_path, file_name)\n    csv_file = os.path.join(csv_file_path, file_name.replace('.xlsx', '.csv'))\n\n    if not os.path.exists(excel_file):\n        raise FileNotFoundError(f\"[Errno 2] No such file or directory: '{excel_file}'\")\n\n    workbook = load_workbook(filename=excel_file)\n    sheet = workbook.active\n\n    with open(csv_file, 'w', newline='') as csvfile:\n        csvwriter = csv.writer(csvfile)\n        for row in sheet.iter_rows(values_only=True):\n            csvwriter.writerow(row)\n\n    return os.path.basename(csv_file)", "raw_solution": "import os\nimport csv\nfrom openpyxl import load_workbook\n\ndef task_func(file_name, excel_file_path, csv_file_path) -> str:\n    excel_file = os.path.join(excel_file_path, file_name)\n    csv_file = os.path.join(csv_file_path, file_name.replace('.xlsx', '.csv'))\n\n    if not os.path.exists(excel_file):\n        raise FileNotFoundError(f\"[Errno 2] No such file or directory: '{excel_file}'\")\n\n    workbook = load_workbook(filename=excel_file)\n    sheet = workbook.active\n\n    with open(csv_file, 'w', newline='') as csvfile:\n        csvwriter = csv.writer(csvfile)\n        for row in sheet.iter_rows(values_only=True):\n            csvwriter.writerow(row)\n\n    return os.path.basename(csv_file)\n```\n"}
{"task_id": "BigCodeBench/408", "solution": "import sqlite3\nimport pandas as pd\ndef task_func(db_file: str, query: str) -> pd.DataFrame:\n    \"\"\"\n    Fetch data from a SQLite database and return it as a pandas DataFrame.\n    \n    This function connects to a SQLite database specified by db_file, executes the SQL query,\n    and returns the results as a pandas DataFrame.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - query (str): SQL query to execute.\n\n    Returns:\n    - pd.DataFrame: DataFrame containing the results of the query.\n\n    Requirements:\n    - sqlite3\n    - pandas\n\n    Examples:\n    - >>> db_file = 'sample_database.db'\n    - >>> df = task_func(db_file, \"SELECT * FROM users WHERE name = 'John Doe'\")\n    - >>> df\n    - id        name  age\n    - --  ----------  ---\n    - ..  John Doe   ..\n    - >>> df = task_func(db_file, \"SELECT age, COUNT(*) AS count FROM users GROUP BY age\")\n    - >>> df\n    - age  count\n    - ---  -----\n    - 25   3\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    # Execute the query and load the results into a pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # Close the database connection\n    conn.close()\n    \n    return df", "raw_solution": "import sqlite3\nimport pandas as pd\n\ndef task_func(db_file: str, query: str) -> pd.DataFrame:\n    \"\"\"\n    Fetch data from a SQLite database and return it as a pandas DataFrame.\n    \n    This function connects to a SQLite database specified by db_file, executes the SQL query,\n    and returns the results as a pandas DataFrame.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - query (str): SQL query to execute.\n\n    Returns:\n    - pd.DataFrame: DataFrame containing the results of the query.\n\n    Requirements:\n    - sqlite3\n    - pandas\n\n    Examples:\n    - >>> db_file = 'sample_database.db'\n    - >>> df = task_func(db_file, \"SELECT * FROM users WHERE name = 'John Doe'\")\n    - >>> df\n    - id        name  age\n    - --  ----------  ---\n    - ..  John Doe   ..\n    - >>> df = task_func(db_file, \"SELECT age, COUNT(*) AS count FROM users GROUP BY age\")\n    - >>> df\n    - age  count\n    - ---  -----\n    - 25   3\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    # Execute the query and load the results into a pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # Close the database connection\n    conn.close()\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    file_path = os.path.join(excel_file_path, file_name)\n    \n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist in the directory {excel_file_path}.\")\n\n    df = pd.read_excel(file_path)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the file {file_name}.\")\n\n    data = df[column_name].dropna()\n\n    mean_value = np.mean(data)\n    median_value = np.median(data)\n    std_dev_value = np.std(data)\n\n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'std_dev': std_dev_value\n    }", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    file_path = os.path.join(excel_file_path, file_name)\n    \n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist in the directory {excel_file_path}.\")\n\n    df = pd.read_excel(file_path)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the file {file_name}.\")\n\n    data = df[column_name].dropna()\n\n    mean_value = np.mean(data)\n    median_value = np.median(data)\n    std_dev_value = np.std(data)\n\n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'std_dev': std_dev_value\n    }\n\n# Example usage:\n# Assuming the directory './data/' and file 'excel_file1.xls' exist and have a column named 'Sales'\n# stats = task_func('./data/', 'excel_file1.xls', 'Sales')\n# print(stats)\n```\n"}
{"task_id": "BigCodeBench/410", "solution": "import os\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> pd.DataFrame:\n    if not os.path.exists(excel_directory):\n        raise FileNotFoundError(f\"The directory {excel_directory} does not exist.\")\n    \n    file_path = os.path.join(excel_directory, file_name)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist in the directory {excel_directory}.\")\n\n    df = pd.read_excel(file_path)\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n    \n    try:\n        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    except ValueError:\n        raise ValueError(\"The start_date and end_date must be in 'yyyy-mm-dd' format.\")\n    \n    df[column_name] = pd.to_datetime(df[column_name])\n    filtered_df = df[(df[column_name] >= start_date) & (df[column_name] <= end_date)]\n    \n    return filtered_df", "raw_solution": "import os\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> pd.DataFrame:\n    if not os.path.exists(excel_directory):\n        raise FileNotFoundError(f\"The directory {excel_directory} does not exist.\")\n    \n    file_path = os.path.join(excel_directory, file_name)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist in the directory {excel_directory}.\")\n\n    df = pd.read_excel(file_path)\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n    \n    try:\n        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    except ValueError:\n        raise ValueError(\"The start_date and end_date must be in 'yyyy-mm-dd' format.\")\n    \n    df[column_name] = pd.to_datetime(df[column_name])\n    filtered_df = df[(df[column_name] >= start_date) & (df[column_name] <= end_date)]\n    \n    return filtered_df\n\n# Example usage:\n# data_dir, file_name = './excel_files/', 'excel_file1.xls'\n# test_file = create_dummy_file(data_dir, file_name)\n# filtered_df = task_func(data_dir, file_name, 'Date', '2020-01-01', '2020-12-31')\n# os.remove(test_file)\n# os.rmdir(data_dir)\n# print(filtered_df.head())\n```\n"}
{"task_id": "BigCodeBench/411", "solution": "import pandas as pd\nimport json\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    df = pd.DataFrame(data)\n    if 'c' in df.columns:\n        df = df.drop(columns=['c'])\n    df_dict = df.to_dict(orient='index')\n    with open(output_path, 'w') as f:\n        json.dump(df_dict, f)\n    return output_path", "raw_solution": "import pandas as pd\nimport json\n\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    df = pd.DataFrame(data)\n    if 'c' in df.columns:\n        df = df.drop(columns=['c'])\n    df_dict = df.to_dict(orient='index')\n    with open(output_path, 'w') as f:\n        json.dump(df_dict, f)\n    return output_path\n\n# Test cases"}
{"task_id": "BigCodeBench/412", "solution": "import json\nimport base64\nimport unicodedata\ndef task_func(json_file: str) -> dict:\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    decoded_data = {}\n    for key, encoded_value in data.items():\n        decoded_value = base64.b64decode(encoded_value).decode('utf-8')\n        normalized_value = unicodedata.normalize('NFC', decoded_value)\n        decoded_data[key] = normalized_value\n\n    return decoded_data", "raw_solution": "import json\nimport base64\nimport unicodedata\n\ndef task_func(json_file: str) -> dict:\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    decoded_data = {}\n    for key, encoded_value in data.items():\n        decoded_value = base64.b64decode(encoded_value).decode('utf-8')\n        normalized_value = unicodedata.normalize('NFC', decoded_value)\n        decoded_data[key] = normalized_value\n\n    return decoded_data\n\n# Example usage:\n# Assuming 'example.json' contains {\"key1\": \"SGVsbG8gV29ybGQ=\", \"key2\": \"UHl0aG9uIENvZGUgUmVmaW5lcg==\"}\n# print(task_func('example.json'))\n# Output: {'key1': 'Hello World', 'key2': 'Python Code Refiner'}\n```\n"}
{"task_id": "BigCodeBench/413", "solution": "import os\nfrom flask import Flask\nfrom flask_mail import Mail, Message\ndef task_func(app):\n    mail = Mail(app)\n\n    # Fetching environment variables or using defaults\n    mail_server = os.getenv('MAIL_SERVER', 'smtp.example.com')\n    mail_port = int(os.getenv('MAIL_PORT', 587))\n    mail_username = os.getenv('MAIL_USERNAME', 'user@example.com')\n    mail_password = os.getenv('MAIL_PASSWORD', 'password')\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'true').lower() in ['true', '1', 't']\n    mail_use_ssl = os.getenv('MAIL_USE_SSL', 'false').lower() in ['true', '1', 't']\n\n    # Configuring the mail settings\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USE_SSL=mail_use_ssl\n    )\n\n    return mail, app.config", "raw_solution": "import os\nfrom flask import Flask\nfrom flask_mail import Mail, Message\n\ndef task_func(app):\n    mail = Mail(app)\n\n    # Fetching environment variables or using defaults\n    mail_server = os.getenv('MAIL_SERVER', 'smtp.example.com')\n    mail_port = int(os.getenv('MAIL_PORT', 587))\n    mail_username = os.getenv('MAIL_USERNAME', 'user@example.com')\n    mail_password = os.getenv('MAIL_PASSWORD', 'password')\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'true').lower() in ['true', '1', 't']\n    mail_use_ssl = os.getenv('MAIL_USE_SSL', 'false').lower() in ['true', '1', 't']\n\n    # Configuring the mail settings\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USE_SSL=mail_use_ssl\n    )\n\n    return mail, app.config\n\n# Example usage"}
{"task_id": "BigCodeBench/414", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    df = pd.DataFrame(data)\n    \n    if column in df.columns:\n        df = df.drop(columns=[column])\n    \n    if df.select_dtypes(include=[np.number]).shape[1] > 0:\n        ax = df.plot(kind='line')\n        plt.show()\n    else:\n        ax = None\n\n    return df, ax\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column=\"c\"):\n    df = pd.DataFrame(data)\n    \n    if column in df.columns:\n        df = df.drop(columns=[column])\n    \n    if df.select_dtypes(include=[np.number]).shape[1] > 0:\n        ax = df.plot(kind='line')\n        plt.show()\n    else:\n        ax = None\n\n    return df, ax\n\n# Example usage:\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\nmodified_df, ax = task_func(data)"}
{"task_id": "BigCodeBench/415", "solution": "import pandas as pd\nimport codecs\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame.\")\n    \n    if 'unicodeString' not in dataframe.columns:\n        raise KeyError(\"DataFrame must contain 'unicodeString' column.\")\n    \n    dataframe['unicodeString'] = dataframe['unicodeString'].apply(lambda x: codecs.decode(x, 'unicode_escape'))\n    \n    return dataframe", "raw_solution": "import pandas as pd\nimport codecs\n\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame.\")\n    \n    if 'unicodeString' not in dataframe.columns:\n        raise KeyError(\"DataFrame must contain 'unicodeString' column.\")\n    \n    dataframe['unicodeString'] = dataframe['unicodeString'].apply(lambda x: codecs.decode(x, 'unicode_escape'))\n    \n    return dataframe\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Name': ['John', 'Anna', 'Peter'],\n#     'Age': [27, 23, 29],\n#     'Salary': [50000, 60000, 70000],\n#     'unicodeString': ['\\\\u004A\\\\u006F\\\\u0068\\\\u006E', '\\\\u0041\\\\u006E\\\\u006E\\\\u0061', '\\\\u0050\\\\u0065\\\\u0074\\\\u0065\\\\u0072']\n# })\n# print(task_func(df))\n```\n"}
{"task_id": "BigCodeBench/416", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n    \n    if not pd.api.types.is_numeric_dtype(data[column]):\n        raise ValueError(f\"Column '{column}' must be numeric.\")\n    \n    corr_matrix = data.corr()\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column=\"c\"):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n    \n    if not pd.api.types.is_numeric_dtype(data[column]):\n        raise ValueError(f\"Column '{column}' must be numeric.\")\n    \n    corr_matrix = data.corr()\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n    return ax\n\n# Example usage:\n# df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]})\n# task_func(df)\n```\n"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Create a sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.1), metrics=['accuracy'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='train loss')\n    ax.plot(history.history['val_loss'], label='test loss')\n    ax.set_title('model loss')\n    ax.set_xlabel('epoch')\n    ax.set_ylabel('loss')\n    ax.legend()\n\n    return model, ax", "raw_solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Create a sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.1), metrics=['accuracy'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='train loss')\n    ax.plot(history.history['val_loss'], label='test loss')\n    ax.set_title('model loss')\n    ax.set_xlabel('epoch')\n    ax.set_ylabel('loss')\n    ax.legend()\n\n    return model, ax\n```\n"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=2, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.1))\n\n    # Fit the model\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n    ax.set_title('ROC curve')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend()\n\n    return model, ax", "raw_solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=2, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.1))\n\n    # Fit the model\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n    ax.set_title('ROC curve')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend()\n\n    return model, ax\n```\n"}
{"task_id": "BigCodeBench/419", "solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Splitting the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Creating a Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=X.shape[1], activation='sigmoid')\n    ])\n\n    # Compiling the model\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n    # Training the model\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predicting probabilities for the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Calculating precision and recall\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred_prob)\n\n    # Plotting the precision-recall curve\n    fig, ax = plt.subplots()\n    ax.plot(recall, precision, marker='.')\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    ax.set_title('Precision-Recall Curve')\n    plt.show()\n\n    return model, ax", "raw_solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Splitting the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Creating a Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=X.shape[1], activation='sigmoid')\n    ])\n\n    # Compiling the model\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n    # Training the model\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predicting probabilities for the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Calculating precision and recall\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred_prob)\n\n    # Plotting the precision-recall curve\n    fig, ax = plt.subplots()\n    ax.plot(recall, precision, marker='.')\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    ax.set_title('Precision-Recall Curve')\n    plt.show()\n\n    return model, ax\n```\n"}
{"task_id": "BigCodeBench/420", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data):\n    df = pd.DataFrame(data)\n    scaler = StandardScaler()\n\n    # Check if all columns are numeric\n    numeric_cols = df.select_dtypes(include=['number']).columns\n    if len(numeric_cols) == len(df.columns):\n        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    else:\n        df = df.apply(pd.to_numeric, errors='coerce').fillna(df)\n        df = scaler.fit_transform(df)\n\n    return pd.DataFrame(df, columns=df.columns)", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data):\n    df = pd.DataFrame(data)\n    scaler = StandardScaler()\n\n    # Check if all columns are numeric\n    numeric_cols = df.select_dtypes(include=['number']).columns\n    if len(numeric_cols) == len(df.columns):\n        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    else:\n        df = df.apply(pd.to_numeric, errors='coerce').fillna(df)\n        df = scaler.fit_transform(df)\n\n    return pd.DataFrame(df, columns=df.columns)\n```\n"}
{"task_id": "BigCodeBench/421", "solution": "import requests\nimport os\nimport json\nimport time\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\ndef task_func(url, directory, metadata):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    if not isinstance(url, str):\n        raise TypeError(\"The URL must be a string.\")\n    \n    responses = []\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as file:\n                file_data = {\n                    'file': (filename, file),\n                    'metadata': json.dumps(metadata)\n                }\n                response = requests.post(url, headers=HEADERS, files=file_data)\n                responses.append(response.status_code)\n                time.sleep(1)  # Wait for 1 second before uploading the next file\n\n    return responses", "raw_solution": "import requests\nimport os\nimport json\nimport time\n\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\n\ndef task_func(url, directory, metadata):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    if not isinstance(url, str):\n        raise TypeError(\"The URL must be a string.\")\n    \n    responses = []\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as file:\n                file_data = {\n                    'file': (filename, file),\n                    'metadata': json.dumps(metadata)\n                }\n                response = requests.post(url, headers=HEADERS, files=file_data)\n                responses.append(response.status_code)\n                time.sleep(1)  # Wait for 1 second before uploading the next file\n\n    return responses\n```\n"}
{"task_id": "BigCodeBench/422", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    df = pd.DataFrame(df)\n    if column_to_remove in df.columns:\n        df = df.drop(columns=[column_to_remove])\n\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    return X_train, X_test, y_train, y_test", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    df = pd.DataFrame(df)\n    if column_to_remove in df.columns:\n        df = df.drop(columns=[column_to_remove])\n\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    return X_train, X_test, y_train, y_test\n\n# Example usage:\ndata = {\n    'a': [1, 2, 3, 4],\n    'b': [5, 6, 7, 8],\n    'c': [9, 10, 11, 12],\n    'target': [0, 1, 0, 1]\n}\nX_train, X_test, y_train, y_test = task_func(data, 'target')"}
{"task_id": "BigCodeBench/423", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n\n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"No image found at {image_path}\")\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Apply threshold to get binary image\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    return gray_image, binary_image", "raw_solution": "import numpy as np\nimport cv2\nimport os\n\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n\n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"No image found at {image_path}\")\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Apply threshold to get binary image\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    return gray_image, binary_image\n```\n"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise ValueError(f\"Unable to load image at {image_path}\")\n\n    # Convert the image from BGR to RGB\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to a 2D array of pixels\n    pixel_values = image.reshape((-1, 3))\n    pixel_values = np.float32(pixel_values)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n\n    # Replace pixels with their nearest centroid color\n    segmented_image = np.uint8(kmeans.cluster_centers_[kmeans.labels_])\n    segmented_image = segmented_image.reshape(image.shape)\n\n    # Convert the segmented image back to RGB\n    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n\n    return image, segmented_image", "raw_solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise ValueError(f\"Unable to load image at {image_path}\")\n\n    # Convert the image from BGR to RGB\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to a 2D array of pixels\n    pixel_values = image.reshape((-1, 3))\n    pixel_values = np.float32(pixel_values)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n\n    # Replace pixels with their nearest centroid color\n    segmented_image = np.uint8(kmeans.cluster_centers_[kmeans.labels_])\n    segmented_image = segmented_image.reshape(image.shape)\n\n    # Convert the segmented image back to RGB\n    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n\n    return image, segmented_image\n```\n"}
{"task_id": "BigCodeBench/425", "solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n\n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Calculate the histogram\n    histogram = cv2.calcHist([image], [0], None, [256], [0, 256])\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.plot(histogram)\n    ax.set_title('Grayscale Histogram')\n    ax.set_xlabel('Pixel Value')\n    ax.set_ylabel('Frequency')\n\n    # Save the histogram plot\n    plt.savefig(histogram_path)\n\n    return ax", "raw_solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\n\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n\n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Calculate the histogram\n    histogram = cv2.calcHist([image], [0], None, [256], [0, 256])\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.plot(histogram)\n    ax.set_title('Grayscale Histogram')\n    ax.set_xlabel('Pixel Value')\n    ax.set_ylabel('Frequency')\n\n    # Save the histogram plot\n    plt.savefig(histogram_path)\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/426", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be read.\")\n    \n    # Convert to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Apply threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    # Save the binary image\n    cv2.imwrite('binary_image.jpg', binary_image)\n\n    return gray_image, binary_image", "raw_solution": "import numpy as np\nimport cv2\nimport os\n\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    if not isinstance(threshold, int) or not (0 <= threshold <= 255):\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file {image_path} could not be read.\")\n    \n    # Convert to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Apply threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    # Save the binary image\n    cv2.imwrite('binary_image.jpg', binary_image)\n\n    return gray_image, binary_image\n```\n"}
{"task_id": "BigCodeBench/427", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on=\"id\")\n\n    # Extract features and target from the merged dataframe\n    X = merged_df[features]\n    y = merged_df[target]\n\n    # Create and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Calculate residuals\n    y_pred = model.predict(X)\n    residuals = y - y_pred\n\n    # Plot residuals\n    fig, ax = plt.subplots()\n    ax.scatter(y_pred, residuals)\n    ax.set_title(\"Residuals Plot\")\n    ax.set_xlabel(\"Predicted Values\")\n    ax.set_ylabel(\"Residuals\")\n\n    # Return the results\n    return {\n        \"coefficients\": model.coef_.tolist(),\n        \"intercept\": model.intercept_,\n        \"residuals_plot\": ax\n    }", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on=\"id\")\n\n    # Extract features and target from the merged dataframe\n    X = merged_df[features]\n    y = merged_df[target]\n\n    # Create and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Calculate residuals\n    y_pred = model.predict(X)\n    residuals = y - y_pred\n\n    # Plot residuals\n    fig, ax = plt.subplots()\n    ax.scatter(y_pred, residuals)\n    ax.set_title(\"Residuals Plot\")\n    ax.set_xlabel(\"Predicted Values\")\n    ax.set_ylabel(\"Residuals\")\n\n    # Return the results\n    return {\n        \"coefficients\": model.coef_.tolist(),\n        \"intercept\": model.intercept_,\n        \"residuals_plot\": ax\n    }\n```\n"}
{"task_id": "BigCodeBench/428", "solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df1, df2):\n    # Merge the dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n\n    # Select only numeric columns for scaling\n    numeric_cols = merged_df.select_dtypes(include=['float64', 'int64']).columns\n\n    # Scale numeric columns\n    scaler = StandardScaler()\n    merged_df[numeric_cols] = scaler.fit_transform(merged_df[numeric_cols])\n\n    # Create a pair plot of the scaled dataframe\n    pair_plot = sns.pairplot(merged_df[numeric_cols])\n\n    return merged_df, pair_plot", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df1, df2):\n    # Merge the dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n\n    # Select only numeric columns for scaling\n    numeric_cols = merged_df.select_dtypes(include=['float64', 'int64']).columns\n\n    # Scale numeric columns\n    scaler = StandardScaler()\n    merged_df[numeric_cols] = scaler.fit_transform(merged_df[numeric_cols])\n\n    # Create a pair plot of the scaled dataframe\n    pair_plot = sns.pairplot(merged_df[numeric_cols])\n\n    return merged_df, pair_plot\n```\n"}
{"task_id": "BigCodeBench/429", "solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2):\n    # Ensure df2 has a 'target' column\n    if 'target' not in df2.columns:\n        raise ValueError(\"DataFrame df2 must contain a 'target' column.\")\n\n    # Ensure df1 and df2 have a common 'i' column\n    if 'i' not in df1.columns or 'i' not in df2.columns:\n        raise ValueError(\"DataFrames must have a common 'i' column.\")\n\n    # Merge the dataframes on 'i' column\n    merged_df = pd.merge(df1, df2, on='i')\n\n    # Select the features and target\n    X = merged_df.drop(columns=['target', 'i'])\n    y = merged_df['target']\n\n    # Apply SelectKBest\n    selector = SelectKBest(score_func=f_classif, k=2)\n    selected_features = selector.fit_transform(X, y)\n\n    # Get the names of the selected features\n    feature_names = X.columns[selector.get_support(indices=True)]\n\n    # Create a correlation matrix\n    corr_matrix = X[feature_names].corr()\n\n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Feature Correlation Heatmap')\n    plt.show()\n\n    return list(feature_names), heatmap", "raw_solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2):\n    # Ensure df2 has a 'target' column\n    if 'target' not in df2.columns:\n        raise ValueError(\"DataFrame df2 must contain a 'target' column.\")\n\n    # Ensure df1 and df2 have a common 'i' column\n    if 'i' not in df1.columns or 'i' not in df2.columns:\n        raise ValueError(\"DataFrames must have a common 'i' column.\")\n\n    # Merge the dataframes on 'i' column\n    merged_df = pd.merge(df1, df2, on='i')\n\n    # Select the features and target\n    X = merged_df.drop(columns=['target', 'i'])\n    y = merged_df['target']\n\n    # Apply SelectKBest\n    selector = SelectKBest(score_func=f_classif, k=2)\n    selected_features = selector.fit_transform(X, y)\n\n    # Get the names of the selected features\n    feature_names = X.columns[selector.get_support(indices=True)]\n\n    # Create a correlation matrix\n    corr_matrix = X[feature_names].corr()\n\n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Feature Correlation Heatmap')\n    plt.show()\n\n    return list(feature_names), heatmap\n```\n"}
{"task_id": "BigCodeBench/430", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the dataframes\n    df = pd.merge(df1, df2, on='id')\n\n    # Extract the relevant columns\n    data = df[[column1, column2]]\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    labels = kmeans.fit_predict(data)\n\n    # Plot the clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data[column1], data[column2], c=labels, cmap='viridis')\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.set_title('KMeans Clustering')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\n    return labels, ax", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the dataframes\n    df = pd.merge(df1, df2, on='id')\n\n    # Extract the relevant columns\n    data = df[[column1, column2]]\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    labels = kmeans.fit_predict(data)\n\n    # Plot the clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data[column1], data[column2], c=labels, cmap='viridis')\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.set_title('KMeans Clustering')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\n    return labels, ax\n```\n"}
{"task_id": "BigCodeBench/431", "solution": "import cv2\nimport os\nimport numpy as np\ndef task_func(image_file: str) -> np.ndarray:\n    if not os.path.exists(image_file):\n        raise FileNotFoundError(f\"The file {image_file} does not exist.\")\n    \n    image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        raise ValueError(f\"The file {image_file} is not a valid image.\")\n    \n    histogram = np.zeros(256, dtype=np.uint32)\n    for pixel_value in image.flatten():\n        histogram[pixel_value] += 1\n\n    return histogram", "raw_solution": "import cv2\nimport os\nimport numpy as np\n\ndef task_func(image_file: str) -> np.ndarray:\n    if not os.path.exists(image_file):\n        raise FileNotFoundError(f\"The file {image_file} does not exist.\")\n    \n    image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        raise ValueError(f\"The file {image_file} is not a valid image.\")\n    \n    histogram = np.zeros(256, dtype=np.uint32)\n    for pixel_value in image.flatten():\n        histogram[pixel_value] += 1\n\n    return histogram\n\n# Example usage:\n# dummy_image_path = 'dummy_image.png'\n# np.random.seed(48)\n# dummy_image = np.random.randint(0, 256, (10, 10), dtype=np.uint8)\n# cv2.imwrite(dummy_image_path, dummy_image)\n# histogram = task_func(dummy_image_path)\n# os.remove(dummy_image_path)\n# print(histogram.shape)  # Output: (256,)\n```\n"}
{"task_id": "BigCodeBench/432", "solution": "import seaborn as sns\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id')\n\n    # Create contingency table\n    contingency_table = pd.crosstab(merged_df[column1], merged_df[column2])\n\n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    # Create heatmap\n    plt.figure(figsize=(10, 6))\n    heatmap = sns.heatmap(contingency_table, annot=True, fmt='d', cmap='Blues', cbar=True)\n    plt.xlabel(column2)\n    plt.ylabel(column1)\n    plt.title('Contingency Table Heatmap')\n\n    return p, heatmap", "raw_solution": "import seaborn as sns\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id')\n\n    # Create contingency table\n    contingency_table = pd.crosstab(merged_df[column1], merged_df[column2])\n\n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    # Create heatmap\n    plt.figure(figsize=(10, 6))\n    heatmap = sns.heatmap(contingency_table, annot=True, fmt='d', cmap='Blues', cbar=True)\n    plt.xlabel(column2)\n    plt.ylabel(column1)\n    plt.title('Contingency Table Heatmap')\n\n    return p, heatmap\n```\n"}
{"task_id": "BigCodeBench/433", "solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\ndef task_func(s, signature, secret_key):\n    # Decode the base64 encoded message\n    decoded_message = base64.b64decode(s)\n\n    # Create a new HMAC object using the secret key and SHA-1 hash function\n    hmac_obj = hmac.new(binascii.a2b_utf8(secret_key), decoded_message, hashlib.sha1)\n\n    # Compute the HMAC signature\n    computed_signature = hmac_obj.hexdigest()\n\n    # Compare the computed signature with the provided signature\n    return computed_signature == signature", "raw_solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\n\ndef task_func(s, signature, secret_key):\n    # Decode the base64 encoded message\n    decoded_message = base64.b64decode(s)\n\n    # Create a new HMAC object using the secret key and SHA-1 hash function\n    hmac_obj = hmac.new(binascii.a2b_utf8(secret_key), decoded_message, hashlib.sha1)\n\n    # Compute the HMAC signature\n    computed_signature = hmac_obj.hexdigest()\n\n    # Compare the computed signature with the provided signature\n    return computed_signature == signature\n\n# Test cases"}
{"task_id": "BigCodeBench/434", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    random.seed(seed)\n    products = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\n    data = []\n    \n    for line in s.split('\\n'):\n        parts = line.split()\n        if len(parts) != 5:\n            raise ValueError(\"Each line must contain exactly 5 fields: ID, Quantity, Code, Price, and Description.\")\n        \n        id, quantity, code, price, description = parts\n        quantity = int(quantity)\n        price = int(price)\n        \n        # Remove trailing whitespace from description\n        description = description.strip()\n        \n        # Assign a random product name\n        product = random.choice(products)\n        \n        data.append([id, quantity, code, price, product, description])\n    \n    df = pd.DataFrame(data, columns=['ID', 'Quantity', 'Code', 'Price', 'Product', 'Description'])\n    return df\ns = '1 10 A10B 100 This is a description with spaces\\n2 20 B20C 200 Another description example'", "raw_solution": "import pandas as pd\nimport re\nimport random\n\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    random.seed(seed)\n    products = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\n    data = []\n    \n    for line in s.split('\\n'):\n        parts = line.split()\n        if len(parts) != 5:\n            raise ValueError(\"Each line must contain exactly 5 fields: ID, Quantity, Code, Price, and Description.\")\n        \n        id, quantity, code, price, description = parts\n        quantity = int(quantity)\n        price = int(price)\n        \n        # Remove trailing whitespace from description\n        description = description.strip()\n        \n        # Assign a random product name\n        product = random.choice(products)\n        \n        data.append([id, quantity, code, price, product, description])\n    \n    df = pd.DataFrame(data, columns=['ID', 'Quantity', 'Code', 'Price', 'Product', 'Description'])\n    return df\n\n# Example usage:\ns = '1 10 A10B 100 This is a description with spaces\\n2 20 B20C 200 Another description example'\ndf = task_func(s)"}
{"task_id": "BigCodeBench/435", "solution": "import pandas as pd\nfrom random import randint\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    names = ['John', 'Alice', 'Bob', 'Charlie', 'David']\n    if name not in names:\n        raise ValueError(\"Name must be one of 'John', 'Alice', 'Bob', 'Charlie', 'David'.\")\n\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'developer', 'Tester']\n    job_title = job_titles[randint(0, len(job_titles) - 1)]\n\n    data = {\n        'Name': [name],\n        'Age': [age],\n        'Code': [code],\n        'Salary': [salary],\n        'Bio': [bio],\n        'Job Title': [job_title]\n    }\n\n    data_df = pd.DataFrame(data)\n\n    return data_df", "raw_solution": "import pandas as pd\nfrom random import randint\n\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    names = ['John', 'Alice', 'Bob', 'Charlie', 'David']\n    if name not in names:\n        raise ValueError(\"Name must be one of 'John', 'Alice', 'Bob', 'Charlie', 'David'.\")\n\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'developer', 'Tester']\n    job_title = job_titles[randint(0, len(job_titles) - 1)]\n\n    data = {\n        'Name': [name],\n        'Age': [age],\n        'Code': [code],\n        'Salary': [salary],\n        'Bio': [bio],\n        'Job Title': [job_title]\n    }\n\n    data_df = pd.DataFrame(data)\n\n    return data_df\n\n# Example usage:\n# df = task_func(\"John\", 30, \"A10B\", 5000.0, \"This is a bio with spaces\")\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/436", "solution": "import string\nimport matplotlib.pyplot as plt\ndef task_func(s):\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string.\")\n    \n    # Convert string to lowercase\n    s = s.lower()\n    \n    # Initialize frequency dictionary\n    freqs = {letter: 0 for letter in string.ascii_lowercase}\n    \n    # Calculate frequency of each letter\n    for char in s:\n        if char in freqs:\n            freqs[char] += 1\n    \n    # Plot frequency bar chart\n    letters = list(freqs.keys())\n    frequencies = list(freqs.values())\n    \n    fig, ax = plt.subplots()\n    ax.bar(letters, frequencies)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n    \n    return freqs, ax", "raw_solution": "import string\nimport matplotlib.pyplot as plt\n\ndef task_func(s):\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string.\")\n    \n    # Convert string to lowercase\n    s = s.lower()\n    \n    # Initialize frequency dictionary\n    freqs = {letter: 0 for letter in string.ascii_lowercase}\n    \n    # Calculate frequency of each letter\n    for char in s:\n        if char in freqs:\n            freqs[char] += 1\n    \n    # Plot frequency bar chart\n    letters = list(freqs.keys())\n    frequencies = list(freqs.values())\n    \n    fig, ax = plt.subplots()\n    ax.bar(letters, frequencies)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n    \n    return freqs, ax\n```\n"}
{"task_id": "BigCodeBench/437", "solution": "import pickle\nimport os\nimport pandas as pd\nimport numpy as np\ndef task_func(df, file_name=\"save.pkl\"):\n    # Save the DataFrame to a pickle file\n    with open(file_name, 'wb') as file:\n        pickle.dump(df, file)\n\n    # Load the DataFrame from the pickle file\n    with open(file_name, 'rb') as file:\n        loaded_df = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(file_name)\n\n    return loaded_df\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))", "raw_solution": "import pickle\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df, file_name=\"save.pkl\"):\n    # Save the DataFrame to a pickle file\n    with open(file_name, 'wb') as file:\n        pickle.dump(df, file)\n\n    # Load the DataFrame from the pickle file\n    with open(file_name, 'rb') as file:\n        loaded_df = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(file_name)\n\n    return loaded_df\n\n# Example usage:\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\nloaded_df = task_func(df, 'test_file.pkl')\n\n# Verify that the loaded DataFrame is the same as the original\nassert df.equals(loaded_df), \"The loaded DataFrame does not match the original.\"\n\n# Verify the types of the original and loaded DataFrames\nassert type(df) == type(loaded_df), \"The types of the original and loaded DataFrames do not match.\"\n\n# Verify the head of the original DataFrame\nassert df.head(2).equals(loaded_df.head(2)), \"The head of the original and loaded DataFrames do not match.\"\n"}
{"task_id": "BigCodeBench/438", "solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list.\")\n\n    # Generate a plot\n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n    ax.set_title('Random Numbers Plot')\n\n    # Save the figure to a pickle file\n    with open(file_path, 'wb') as file:\n        pickle.dump(fig, file)\n\n    # Load the figure from the pickle file\n    with open(file_path, 'rb') as file:\n        loaded_fig = pickle.load(file)\n\n    # Clean up by removing the pickle file\n    os.remove(file_path)\n\n    return loaded_fig\nnumbers = [random.random() for _ in range(100)]", "raw_solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\nimport random\n\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list.\")\n\n    # Generate a plot\n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n    ax.set_title('Random Numbers Plot')\n\n    # Save the figure to a pickle file\n    with open(file_path, 'wb') as file:\n        pickle.dump(fig, file)\n\n    # Load the figure from the pickle file\n    with open(file_path, 'rb') as file:\n        loaded_fig = pickle.load(file)\n\n    # Clean up by removing the pickle file\n    os.remove(file_path)\n\n    return loaded_fig\n\n# Example usage:\nnumbers = [random.random() for _ in range(100)]\nloaded_fig = task_func(numbers)"}
{"task_id": "BigCodeBench/439", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(P, T):\n    # Perform matrix-tensor multiplication\n    product = np.einsum('ij,ijk->ijk', P, T)\n\n    # Create a heatmap of the result\n    fig, ax = plt.subplots()\n    sns.heatmap(product[0], ax=ax, annot=True, fmt=\".2f\", cmap=\"viridis\")\n    ax.set_title('Heatmap of the Result')\n\n    return product, ax", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(P, T):\n    # Perform matrix-tensor multiplication\n    product = np.einsum('ij,ijk->ijk', P, T)\n\n    # Create a heatmap of the result\n    fig, ax = plt.subplots()\n    sns.heatmap(product[0], ax=ax, annot=True, fmt=\".2f\", cmap=\"viridis\")\n    ax.set_title('Heatmap of the Result')\n\n    return product, ax\n```\n"}
{"task_id": "BigCodeBench/440", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(P, T):\n    if P.size == 0 or T.size == 0:\n        raise ValueError(\"Input matrices cannot be empty.\")\n\n    # Perform matrix-tensor multiplication\n    result = np.tensordot(P, T, axes=1)\n\n    # Standardize the result\n    scaler = StandardScaler()\n    result_scaled = scaler.fit_transform(result)\n\n    # Create a DataFrame\n    df = pd.DataFrame(result_scaled, columns=[f'feature_{i}' for i in range(result_scaled.shape[1])])\n\n    return df\nP = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\nT = np.random.rand(3, 5, 5)", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(P, T):\n    if P.size == 0 or T.size == 0:\n        raise ValueError(\"Input matrices cannot be empty.\")\n\n    # Perform matrix-tensor multiplication\n    result = np.tensordot(P, T, axes=1)\n\n    # Standardize the result\n    scaler = StandardScaler()\n    result_scaled = scaler.fit_transform(result)\n\n    # Create a DataFrame\n    df = pd.DataFrame(result_scaled, columns=[f'feature_{i}' for i in range(result_scaled.shape[1])])\n\n    return df\n\n# Example usage:\nnp.random.seed(0)\nP = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\nT = np.random.rand(3, 5, 5)\nresult = task_func(P, T)"}
{"task_id": "BigCodeBench/441", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(P, T):\n    \"\"\"\n    numpy ka upayog karake ek matrix \"P\" aur ek 3d tensor \"T\" ka product calculate karein aur phir matplotlib ke saath 3d mein result ko visualize karein. Matrix aur tensor ka product Einstein summation par aadhaarit hai.\n\n    Notes:\n        Yeh function sirf numpy matrices/arrays ko sweekar karta hai.\n\n    Params:\n    - P (numpy.ndarray): Input matrix jiska shape (N, 3) hai, jahan N rows ki sankhya hai.\n    - T (numpy.ndarray): Input tensor jiska shape (3, 3, 3) hai.\n\n    Returns:\n        tuple:\n        result (numpy.ndarray): Matrix P aur tensor T ka product jiska shape (N, 3) hai.\n        ax (mpl_toolkits.mplot3d.axes3d.Axes3d): Result ka 3d visualization.\n\n    Reqs:\n    - numpy\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1]])\n    - >>> T = np.random.rand(3, 3, 3)\n    - >>> result, ax = task_func(P, T)\n    - >>> type(result)\n    - <class 'numpy.ndarray'>\n    - >>> type(ax)\n    - <class 'mpl_toolkits.mplot3d.axes3d.Axes3d'>\n    \"\"\"\n    # Check if the shapes of P and T are as expected\n    if P.shape[1] != 3 or T.shape != (3, 3, 3):\n        raise ValueError(\"Invalid shapes for P and T\")\n\n    # Calculate the product using numpy's einsum\n    result = np.einsum('ij,ijk->ik', P, T)\n\n    # Plotting the result\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n\n    return result, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(P, T):\n    \"\"\"\n    numpy ka upayog karake ek matrix \"P\" aur ek 3d tensor \"T\" ka product calculate karein aur phir matplotlib ke saath 3d mein result ko visualize karein. Matrix aur tensor ka product Einstein summation par aadhaarit hai.\n\n    Notes:\n        Yeh function sirf numpy matrices/arrays ko sweekar karta hai.\n\n    Params:\n    - P (numpy.ndarray): Input matrix jiska shape (N, 3) hai, jahan N rows ki sankhya hai.\n    - T (numpy.ndarray): Input tensor jiska shape (3, 3, 3) hai.\n\n    Returns:\n        tuple:\n        result (numpy.ndarray): Matrix P aur tensor T ka product jiska shape (N, 3) hai.\n        ax (mpl_toolkits.mplot3d.axes3d.Axes3d): Result ka 3d visualization.\n\n    Reqs:\n    - numpy\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1]])\n    - >>> T = np.random.rand(3, 3, 3)\n    - >>> result, ax = task_func(P, T)\n    - >>> type(result)\n    - <class 'numpy.ndarray'>\n    - >>> type(ax)\n    - <class 'mpl_toolkits.mplot3d.axes3d.Axes3d'>\n    \"\"\"\n    # Check if the shapes of P and T are as expected\n    if P.shape[1] != 3 or T.shape != (3, 3, 3):\n        raise ValueError(\"Invalid shapes for P and T\")\n\n    # Calculate the product using numpy's einsum\n    result = np.einsum('ij,ijk->ik', P, T)\n\n    # Plotting the result\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n\n    return result, ax\n```\n"}
{"task_id": "BigCodeBench/442", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    # Flatten the tensor T\n    T_flatten = T.reshape(-1, np.prod(tensor_shape))\n\n    # Perform PCA on the flattened tensor\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(T_flatten)\n\n    # Plot the PCA result\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n\n    return pca_result, ax\nP = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\nT = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    # Flatten the tensor T\n    T_flatten = T.reshape(-1, np.prod(tensor_shape))\n\n    # Perform PCA on the flattened tensor\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(T_flatten)\n\n    # Plot the PCA result\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n\n    return pca_result, ax\n\n# Example usage:\nP = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\nT = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\npca_result, ax = task_func(P, T)"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Flatten the matrix P\n    P_flat = P.reshape(-1, 1)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(P_flat)\n\n    # Plot the clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(P_flat, cluster_result, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans clustering visualization')\n    ax.set_xlabel('Data points')\n    ax.set_ylabel('Cluster labels')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\n    return cluster_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Flatten the matrix P\n    P_flat = P.reshape(-1, 1)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(P_flat)\n\n    # Plot the clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(P_flat, cluster_result, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans clustering visualization')\n    ax.set_xlabel('Data points')\n    ax.set_ylabel('Cluster labels')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\n    return cluster_result, ax\n```\n"}
{"task_id": "BigCodeBench/444", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(n_points=100, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random points in 3D space\n    points = np.random.rand(n_points, 3)\n\n    # Create a 3D scatter plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n\n    # Set plot labels\n    ax.set_xlabel('X axis')\n    ax.set_ylabel('Y axis')\n    ax.set_zlabel('Z axis')\n\n    return points, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(n_points=100, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random points in 3D space\n    points = np.random.rand(n_points, 3)\n\n    # Create a 3D scatter plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n\n    # Set plot labels\n    ax.set_xlabel('X axis')\n    ax.set_ylabel('Y axis')\n    ax.set_zlabel('Z axis')\n\n    return points, ax\n```\n"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    if not isinstance(points, np.ndarray) or points.shape[1] != 2:\n        raise ValueError(\"Input must be a numpy ndarray with shape (n_points, 2)\")\n\n    vor = Voronoi(points)\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    plt.show()\n\n    return vor, ax", "raw_solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    if not isinstance(points, np.ndarray) or points.shape[1] != 2:\n        raise ValueError(\"Input must be a numpy ndarray with shape (n_points, 2)\")\n\n    vor = Voronoi(points)\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    plt.show()\n\n    return vor, ax\n\n# Example usage:\n# points = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# vor, ax = task_func(points)\n```\n"}
{"task_id": "BigCodeBench/446", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n    \n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    ax.set_title('Blobs')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax, label='Cluster Label')\n    \n    return X, y, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n    \n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    ax.set_title('Blobs')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax, label='Cluster Label')\n    \n    return X, y, ax\n```\n"}
{"task_id": "BigCodeBench/447", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, n_components=2, random_state=None):\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    ax.set_title('PCA Result')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return {\n        \"transformed_data\": transformed_data,\n        \"ax\": ax\n    }", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(data, n_components=2, random_state=None):\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    ax.set_title('PCA Result')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return {\n        \"transformed_data\": transformed_data,\n        \"ax\": ax\n    }\n```\n"}
{"task_id": "BigCodeBench/448", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'N({mu}, {sigma}^2)')\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('x')\n    ax.set_ylabel('Probability Density')\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'N({mu}, {sigma}^2)')\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('x')\n    ax.set_ylabel('Probability Density')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/449", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Plotting histograms for each feature\n    axes_list = []\n    for column in standardized_data.columns:\n        ax = standardized_data[column].plot(kind='hist', bins=20, title=f'Histogram of {column}')\n        axes_list.append(ax)\n        plt.show()\n\n    return standardized_data, axes_list", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    # Standardizing the data\n    scaler = StandardScaler()\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Plotting histograms for each feature\n    axes_list = []\n    for column in standardized_data.columns:\n        ax = standardized_data[column].plot(kind='hist', bins=20, title=f'Histogram of {column}')\n        axes_list.append(ax)\n        plt.show()\n\n    return standardized_data, axes_list\n```\nThis script standardizes the input data and then plots histograms for each feature in the standardized data. The function returns the standardized data and a list of matplotlib Axes objects for each histogram.\n"}
{"task_id": "BigCodeBench/450", "solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate synthetic dataset\n    X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=random_seed)\n\n    # Calculate distances between samples\n    distances = cdist(X, X, metric='euclidean')\n\n    # Plot the dataset\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    ax.set_title('Synthetic Dataset')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax, label='Cluster Label')\n\n    # Save or show the plot\n    if plot_path:\n        plt.savefig(plot_path)\n        plt.close(fig)\n        return distances, None, plot_path\n    else:\n        return distances, ax, None", "raw_solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate synthetic dataset\n    X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=random_seed)\n\n    # Calculate distances between samples\n    distances = cdist(X, X, metric='euclidean')\n\n    # Plot the dataset\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    ax.set_title('Synthetic Dataset')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax, label='Cluster Label')\n\n    # Save or show the plot\n    if plot_path:\n        plt.savefig(plot_path)\n        plt.close(fig)\n        return distances, None, plot_path\n    else:\n        return distances, ax, None\n```\n"}
{"task_id": "BigCodeBench/451", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a random high-dimensional dataset\n    data = np.random.rand(N_SAMPLES, N_FEATURES)\n\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Plot the covariance matrix heatmap if n_components > 1\n    if n_components > 1:\n        covariance_matrix = np.cov(transformed_data, rowvar=False)\n        fig, ax = plt.subplots()\n        sns.heatmap(covariance_matrix, ax=ax, cmap='viridis')\n        ax.set_title('Covariance Matrix Heatmap')\n        plt.show()\n        return transformed_data, ax\n    else:\n        return transformed_data, None", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a random high-dimensional dataset\n    data = np.random.rand(N_SAMPLES, N_FEATURES)\n\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Plot the covariance matrix heatmap if n_components > 1\n    if n_components > 1:\n        covariance_matrix = np.cov(transformed_data, rowvar=False)\n        fig, ax = plt.subplots()\n        sns.heatmap(covariance_matrix, ax=ax, cmap='viridis')\n        ax.set_title('Covariance Matrix Heatmap')\n        plt.show()\n        return transformed_data, ax\n    else:\n        return transformed_data, None\n```\n"}
{"task_id": "BigCodeBench/452", "solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=random_seed)\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n    \n    # Create and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n    \n    # Get the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n    \n    # Calculate Mean Squared Error (MSE)\n    mse = np.mean((predictions - y_test) ** 2)\n    \n    return predictions, coefficients, intercept, mse", "raw_solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=random_seed)\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n    \n    # Create and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n    \n    # Get the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n    \n    # Calculate Mean Squared Error (MSE)\n    mse = np.mean((predictions - y_test) ** 2)\n    \n    return predictions, coefficients, intercept, mse\n```\n"}
{"task_id": "BigCodeBench/453", "solution": "import re\nimport string\nfrom random import choice\ndef task_func(n, pattern):\n    while True:\n        # Generate a random string of length n\n        random_string = ''.join(choice(string.ascii_letters) for _ in range(n))\n        \n        # Check if the generated string matches the pattern\n        if re.match(pattern, random_string):\n            return random_string", "raw_solution": "import re\nimport string\nfrom random import choice\n\ndef task_func(n, pattern):\n    while True:\n        # Generate a random string of length n\n        random_string = ''.join(choice(string.ascii_letters) for _ in range(n))\n        \n        # Check if the generated string matches the pattern\n        if re.match(pattern, random_string):\n            return random_string\n\n# Test cases"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    if not os.path.exists(src_dir) or not os.path.exists(dest_dir):\n        raise FileNotFoundError(\"Source or destination directory does not exist.\")\n    \n    files = glob.glob(os.path.join(src_dir, f'*.{ext}'))\n    moved_files = []\n\n    for file in files:\n        file_name = os.path.basename(file)\n        dest_file = os.path.join(dest_dir, file_name)\n        \n        if not os.path.exists(dest_file):\n            shutil.move(file, dest_file)\n            moved_files.append(dest_file)\n    \n    return moved_files", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    if not os.path.exists(src_dir) or not os.path.exists(dest_dir):\n        raise FileNotFoundError(\"Source or destination directory does not exist.\")\n    \n    files = glob.glob(os.path.join(src_dir, f'*.{ext}'))\n    moved_files = []\n\n    for file in files:\n        file_name = os.path.basename(file)\n        dest_file = os.path.join(dest_dir, file_name)\n        \n        if not os.path.exists(dest_file):\n            shutil.move(file, dest_file)\n            moved_files.append(dest_file)\n    \n    return moved_files\n```\n"}
{"task_id": "BigCodeBench/455", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, n):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, n)\n\n    # Plot histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot probability density function\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title(\"Histogram and PDF of Normal Distribution\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    return samples", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, n):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, n)\n\n    # Plot histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot probability density function\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title(\"Histogram and PDF of Normal Distribution\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    return samples\n\n# Example usage:\n# task_func(5, 2, 500)\n```\nThis script generates a set of samples from a normal distribution with a given mean and standard deviation, then plots both a histogram of the samples and the probability density function (PDF) of the normal distribution. The function returns the array of samples.\n"}
{"task_id": "BigCodeBench/456", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(normalized_data, cmap=\"YlGnBu\", cbar_kws={'label': 'Normalized Value'})\n    plt.title('Heatmap of Normalized Data')\n    plt.xlabel('Features')\n    plt.ylabel('Samples')\n\n    return normalized_data, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(normalized_data, cmap=\"YlGnBu\", cbar_kws={'label': 'Normalized Value'})\n    plt.title('Heatmap of Normalized Data')\n    plt.xlabel('Features')\n    plt.ylabel('Samples')\n\n    return normalized_data, ax\n```\n"}
{"task_id": "BigCodeBench/457", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in L):\n        raise TypeError(\"Input must be a list of lists containing integers.\")\n\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Create a DataFrame\n    df = pd.DataFrame(flat_list, columns=['Values'])\n\n    # Plot the histogram\n    ax = df['Values'].plot(kind='hist', bins=len(df['Values'].unique()), rwidth=0.8)\n\n    # Set xticks to the unique values\n    ax.set_xticks(np.arange(len(df['Values'].unique())))\n\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in L):\n        raise TypeError(\"Input must be a list of lists containing integers.\")\n\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Create a DataFrame\n    df = pd.DataFrame(flat_list, columns=['Values'])\n\n    # Plot the histogram\n    ax = df['Values'].plot(kind='hist', bins=len(df['Values'].unique()), rwidth=0.8)\n\n    # Set xticks to the unique values\n    ax.set_xticks(np.arange(len(df['Values'].unique())))\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    # Load the JSON string into a dictionary\n    data = json.loads(json_str)\n\n    # Initialize an empty list to store processed data\n    processed_data = []\n\n    # Iterate over the dictionary items\n    for key, value in data.items():\n        if isinstance(value, list):\n            # Process numerical values in the list\n            for item in value:\n                if isinstance(item, (int, float)):\n                    processed_data.append([key, item])\n        elif isinstance(value, (int, float)):\n            # Process single numerical values\n            processed_data.append([key, value])\n\n    # Create a DataFrame from the processed data\n    df = pd.DataFrame(processed_data, columns=['Key', 'Value'])\n\n    return df\njson_str = '{\"a\": [1, 2, 3], \"b\": 4.9, \"c\": \"5\"}'", "raw_solution": "import json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    # Load the JSON string into a dictionary\n    data = json.loads(json_str)\n\n    # Initialize an empty list to store processed data\n    processed_data = []\n\n    # Iterate over the dictionary items\n    for key, value in data.items():\n        if isinstance(value, list):\n            # Process numerical values in the list\n            for item in value:\n                if isinstance(item, (int, float)):\n                    processed_data.append([key, item])\n        elif isinstance(value, (int, float)):\n            # Process single numerical values\n            processed_data.append([key, value])\n\n    # Create a DataFrame from the processed data\n    df = pd.DataFrame(processed_data, columns=['Key', 'Value'])\n\n    return df\n\n# Example usage\njson_str = '{\"a\": [1, 2, 3], \"b\": 4.9, \"c\": \"5\"}'\ndf = task_func(json_str)"}
{"task_id": "BigCodeBench/459", "solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\ndef task_func(script_dir, scripts, delay):\n    if delay < 0:\n        raise ValueError(\"Delay must be a non-negative integer.\")\n    if not scripts:\n        raise ValueError(\"At least one script must be provided.\")\n\n    timestamps = []\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.isfile(script_path):\n            raise FileNotFoundError(f\"The script {script} was not found.\")\n\n        start_time = datetime.now()\n        timestamps.append(start_time.strftime('%Y-%m-%d %H:%M:%S'))\n\n        subprocess.run(['bash', script_path], check=True)\n\n        time.sleep(delay)\n\n    return timestamps", "raw_solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\n\ndef task_func(script_dir, scripts, delay):\n    if delay < 0:\n        raise ValueError(\"Delay must be a non-negative integer.\")\n    if not scripts:\n        raise ValueError(\"At least one script must be provided.\")\n\n    timestamps = []\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.isfile(script_path):\n            raise FileNotFoundError(f\"The script {script} was not found.\")\n\n        start_time = datetime.now()\n        timestamps.append(start_time.strftime('%Y-%m-%d %H:%M:%S'))\n\n        subprocess.run(['bash', script_path], check=True)\n\n        time.sleep(delay)\n\n    return timestamps\n```\n"}
{"task_id": "BigCodeBench/460", "solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(script_path, output_file_path):\n    try:\n        # Run the script using subprocess\n        subprocess.run([\"bash\", script_path, output_file_path], check=True)\n    except subprocess.CalledProcessError:\n        raise ValueError(\"Script execution failed or output file is invalid.\")\n\n    # Load the CSV file into a DataFrame\n    try:\n        df = pd.read_csv(output_file_path)\n    except FileNotFoundError:\n        raise ValueError(\"Output file not found.\")\n    except pd.errors.EmptyDataError:\n        raise ValueError(\"Output file is empty.\")\n    except pd.errors.ParserError:\n        raise ValueError(\"Output file is not a valid CSV.\")\n\n    if df.shape[1] != 2:\n        raise ValueError(\"CSV file must contain exactly two columns.\")\n\n    # Plot the data\n    ax = df.plot(kind='bar', x=df.columns[0], y=df.columns[1], legend=False)\n    plt.xlabel(df.columns[0])\n    plt.ylabel(df.columns[1])\n    plt.title('Bar Graph')\n    plt.show()\n\n    return df, ax", "raw_solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(script_path, output_file_path):\n    try:\n        # Run the script using subprocess\n        subprocess.run([\"bash\", script_path, output_file_path], check=True)\n    except subprocess.CalledProcessError:\n        raise ValueError(\"Script execution failed or output file is invalid.\")\n\n    # Load the CSV file into a DataFrame\n    try:\n        df = pd.read_csv(output_file_path)\n    except FileNotFoundError:\n        raise ValueError(\"Output file not found.\")\n    except pd.errors.EmptyDataError:\n        raise ValueError(\"Output file is empty.\")\n    except pd.errors.ParserError:\n        raise ValueError(\"Output file is not a valid CSV.\")\n\n    if df.shape[1] != 2:\n        raise ValueError(\"CSV file must contain exactly two columns.\")\n\n    # Plot the data\n    ax = df.plot(kind='bar', x=df.columns[0], y=df.columns[1], legend=False)\n    plt.xlabel(df.columns[0])\n    plt.ylabel(df.columns[1])\n    plt.title('Bar Graph')\n    plt.show()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.isfile(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} was not found.\")\n    \n    # Start the script as a subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # Initialize CPU and memory usage\n    cpu_usage = 0.0\n    memory_usage = 0.0\n    \n    try:\n        # Wait for the script to complete or timeout\n        process.wait(timeout=timeout)\n        \n        # Get the process object\n        proc = psutil.Process(process.pid)\n        \n        # Get CPU and memory usage\n        cpu_usage = proc.cpu_percent(interval=1)\n        memory_usage = proc.memory_info().rss\n        \n    except subprocess.TimeoutExpired:\n        # If the script exceeds the timeout, kill the process\n        process.kill()\n        process.wait()\n        \n        # Get the process object\n        proc = psutil.Process(process.pid)\n        \n        # Get CPU and memory usage\n        cpu_usage = proc.cpu_percent(interval=1)\n        memory_usage = proc.memory_info().rss\n        \n    finally:\n        # Ensure the process is cleaned up\n        if process.poll() is None:\n            process.kill()\n            process.wait()\n    \n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}", "raw_solution": "import subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.isfile(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} was not found.\")\n    \n    # Start the script as a subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # Initialize CPU and memory usage\n    cpu_usage = 0.0\n    memory_usage = 0.0\n    \n    try:\n        # Wait for the script to complete or timeout\n        process.wait(timeout=timeout)\n        \n        # Get the process object\n        proc = psutil.Process(process.pid)\n        \n        # Get CPU and memory usage\n        cpu_usage = proc.cpu_percent(interval=1)\n        memory_usage = proc.memory_info().rss\n        \n    except subprocess.TimeoutExpired:\n        # If the script exceeds the timeout, kill the process\n        process.kill()\n        process.wait()\n        \n        # Get the process object\n        proc = psutil.Process(process.pid)\n        \n        # Get CPU and memory usage\n        cpu_usage = proc.cpu_percent(interval=1)\n        memory_usage = proc.memory_info().rss\n        \n    finally:\n        # Ensure the process is cleaned up\n        if process.poll() is None:\n            process.kill()\n            process.wait()\n    \n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}\n```\n"}
{"task_id": "BigCodeBench/462", "solution": "import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be at least 1\")\n\n    random.seed(random_seed)\n\n    data = {\n        \"Category\": [random.choice(categories) for _ in range(num_rows)],\n        \"value\": [random.randint(1, 100) for _ in range(num_rows)]\n    }\n\n    df = pd.DataFrame(data)\n\n    ax = df['Category'].value_counts().plot(kind='bar', title='Category Counts')\n    plt.xlabel('Category')\n    plt.ylabel('Counts')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be at least 1\")\n\n    random.seed(random_seed)\n\n    data = {\n        \"Category\": [random.choice(categories) for _ in range(num_rows)],\n        \"value\": [random.randint(1, 100) for _ in range(num_rows)]\n    }\n\n    df = pd.DataFrame(data)\n\n    ax = df['Category'].value_counts().plot(kind='bar', title='Category Counts')\n    plt.xlabel('Category')\n    plt.ylabel('Counts')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/463", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_str, separator=\",\", bins=20):\n    try:\n        # Convert the string to a list of integers\n        data = list(map(int, data_str.split(separator)))\n        \n        # Create a pandas Series from the data\n        series = pd.Series(data)\n        \n        # Plot the histogram\n        ax = series.plot.hist(bins=bins, grid=True, rwidth=0.9, color='#607c8e')\n        \n        return series, ax\n    except ValueError as e:\n        raise ValueError(\"Invalid data format\") from e\n    except Exception as e:\n        raise Exception(\"An error occurred\") from e", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_str, separator=\",\", bins=20):\n    try:\n        # Convert the string to a list of integers\n        data = list(map(int, data_str.split(separator)))\n        \n        # Create a pandas Series from the data\n        series = pd.Series(data)\n        \n        # Plot the histogram\n        ax = series.plot.hist(bins=bins, grid=True, rwidth=0.9, color='#607c8e')\n        \n        return series, ax\n    except ValueError as e:\n        raise ValueError(\"Invalid data format\") from e\n    except Exception as e:\n        raise Exception(\"An error occurred\") from e\n\n# Example usage:\n# series, ax = task_func('1,2,3,4,5,5,5,4,3,2,1')\n# print(type(series), series.tolist())\n# print(type(ax))\n```\n"}
{"task_id": "BigCodeBench/464", "solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\ndef task_func(my_obj):\n    class CustomEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, datetime):\n                return obj.isoformat()\n            elif isinstance(obj, Decimal):\n                return str(obj)\n            return super().default(obj)\n\n    return json.dumps(my_obj, cls=CustomEncoder)", "raw_solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\n\ndef task_func(my_obj):\n    class CustomEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, datetime):\n                return obj.isoformat()\n            elif isinstance(obj, Decimal):\n                return str(obj)\n            return super().default(obj)\n\n    return json.dumps(my_obj, cls=CustomEncoder)\n\n# Test cases"}
{"task_id": "BigCodeBench/465", "solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        else:\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)", "raw_solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\n\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        else:\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)\n\n# Test cases"}
{"task_id": "BigCodeBench/466", "solution": "import json\nfrom enum import Enum\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n    BLUE = 3\nclass ColorEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Color):\n            return obj.name\n        return super().default(obj)\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=ColorEncoder)", "raw_solution": "import json\nfrom enum import Enum\n\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n    BLUE = 3\n\nclass ColorEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Color):\n            return obj.name\n        return super().default(obj)\n\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=ColorEncoder)\n\n# Test cases"}
{"task_id": "BigCodeBench/467", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    x = np.random.rand(n)\n    y = np.random.rand(n)\n    \n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Random Points Scatter Plot')\n    \n    points = list(zip(x, y))\n    \n    return fig, points", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    x = np.random.rand(n)\n    y = np.random.rand(n)\n    \n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Random Points Scatter Plot')\n    \n    points = list(zip(x, y))\n    \n    return fig, points\n\n# Example usage:\n# fig, points = task_func(5)\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/468", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Read the csv into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n\n    # Change the numeric values to floats\n    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n\n    # Create a line chart of the data in the specified column\n    ax = df[columns].plot(kind='line')\n\n    # Calculate the croot of the data\n    croot_series = df[columns].applymap(lambda x: np.cbrt(x) if pd.notnull(x) else np.nan)\n\n    return df, ax, croot_series", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Read the csv into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n\n    # Change the numeric values to floats\n    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n\n    # Create a line chart of the data in the specified column\n    ax = df[columns].plot(kind='line')\n\n    # Calculate the croot of the data\n    croot_series = df[columns].applymap(lambda x: np.cbrt(x) if pd.notnull(x) else np.nan)\n\n    return df, ax, croot_series\n\n# Example usage:\n# df, ax, croot = task_func('path_to_csv.csv', ['Column1', 'Column2', 'Column3'])\n# print(df)\n# print(ax)\n# print(croot)\n```\n"}
{"task_id": "BigCodeBench/469", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Convert all grades to uppercase and remove leading/trailing whitespaces\n    student_grades = [grade.strip().upper() for grade in student_grades]\n\n    # Count the occurrences of each grade\n    grade_counts = Counter(student_grades)\n\n    # Create a DataFrame from the counts\n    report_df = pd.DataFrame.from_dict(grade_counts, orient='index', columns=['Count'])\n    report_df.index.name = 'Grade'\n\n    # Add missing grades with count 0\n    for grade in possible_grades:\n        if grade not in report_df.index:\n            report_df.loc[grade] = 0\n\n    # Sort the DataFrame by grade\n    report_df = report_df.reindex(possible_grades)\n\n    # Plot the grade distribution\n    ax = report_df.plot(kind='bar', legend=False)\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n    ax.set_title('Grade Distribution')\n\n    return report_df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Convert all grades to uppercase and remove leading/trailing whitespaces\n    student_grades = [grade.strip().upper() for grade in student_grades]\n\n    # Count the occurrences of each grade\n    grade_counts = Counter(student_grades)\n\n    # Create a DataFrame from the counts\n    report_df = pd.DataFrame.from_dict(grade_counts, orient='index', columns=['Count'])\n    report_df.index.name = 'Grade'\n\n    # Add missing grades with count 0\n    for grade in possible_grades:\n        if grade not in report_df.index:\n            report_df.loc[grade] = 0\n\n    # Sort the DataFrame by grade\n    report_df = report_df.reindex(possible_grades)\n\n    # Plot the grade distribution\n    ax = report_df.plot(kind='bar', legend=False)\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n    ax.set_title('Grade Distribution')\n\n    return report_df, ax\n```\n"}
{"task_id": "BigCodeBench/470", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(myList):\n    # Create a histogram of the list\n    fig, ax = plt.subplots()\n    ax.hist(myList, bins=np.arange(min(myList), max(myList) + 1) - 0.5, color='black', edgecolor='black')\n    \n    # Set labels and title\n    ax.set_xlabel('maan')\n    ax.set_ylabel('frequency')\n    ax.set_title('maano ka histogram')\n    \n    return ax\nmyList = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(myList):\n    # Create a histogram of the list\n    fig, ax = plt.subplots()\n    ax.hist(myList, bins=np.arange(min(myList), max(myList) + 1) - 0.5, color='black', edgecolor='black')\n    \n    # Set labels and title\n    ax.set_xlabel('maan')\n    ax.set_ylabel('frequency')\n    ax.set_title('maano ka histogram')\n    \n    return ax\n\n# Example usage:\nmyList = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\nax = task_func(myList)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/471", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(myList):\n    # Convert all strings to lowercase and remove leading/trailing whitespaces\n    cleanedList = [word.strip().lower() for word in myList if word.strip()]\n\n    # Count the frequency of each word\n    wordCounts = Counter(cleanedList)\n\n    # Create a DataFrame from the word counts\n    df = pd.DataFrame(wordCounts.items(), columns=['Word', 'Count'])\n\n    return df\nmyList = ['apple', 'banana', 'apple', 'cherry', 'banana', 'banana']", "raw_solution": "from collections import Counter\nimport pandas as pd\n\ndef task_func(myList):\n    # Convert all strings to lowercase and remove leading/trailing whitespaces\n    cleanedList = [word.strip().lower() for word in myList if word.strip()]\n\n    # Count the frequency of each word\n    wordCounts = Counter(cleanedList)\n\n    # Create a DataFrame from the word counts\n    df = pd.DataFrame(wordCounts.items(), columns=['Word', 'Count'])\n\n    return df\n\n# Example usage:\nmyList = ['apple', 'banana', 'apple', 'cherry', 'banana', 'banana']"}
{"task_id": "BigCodeBench/472", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\ndef task_func(myList, n_clusters):\n    if not isinstance(myList, list) or not all(isinstance(sublist, list) and len(sublist) == 2 for sublist in myList):\n        raise ValueError(\"Invalid input. 'myList' must be a list of 2D points.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"Invalid input. 'n_clusters' must be a positive integer.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=1)\n    kmeans.fit(myList)\n    labels = kmeans.labels_\n    centers = kmeans.cluster_centers_\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter([point[0] for point in myList], [point[1] for point in myList], c=labels, cmap='viridis')\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', marker='x')\n    ax.set_title('KMeans Clustering')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\ndef task_func(myList, n_clusters):\n    if not isinstance(myList, list) or not all(isinstance(sublist, list) and len(sublist) == 2 for sublist in myList):\n        raise ValueError(\"Invalid input. 'myList' must be a list of 2D points.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"Invalid input. 'n_clusters' must be a positive integer.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=1)\n    kmeans.fit(myList)\n    labels = kmeans.labels_\n    centers = kmeans.cluster_centers_\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter([point[0] for point in myList], [point[1] for point in myList], c=labels, cmap='viridis')\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', marker='x')\n    ax.set_title('KMeans Clustering')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/473", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(n_walks, n_steps, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Define the possible colors\n    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n    color_cycle = itertools.cycle(colors)\n\n    # Initialize the plot\n    fig, ax = plt.subplots()\n\n    # Generate random walks\n    for _ in range(n_walks):\n        # Generate random steps\n        steps = np.random.choice([-1, 1], size=n_steps)\n        walk = np.cumsum(steps)\n\n        # Plot each walk with a different color\n        ax.plot(walk, color=next(color_cycle))\n\n    # Set the x-axis limits\n    ax.set_xlim(0, n_steps)\n\n    # Set the x-axis ticks\n    ax.set_xticks(np.arange(0, n_steps + 1, n_steps // 8))\n    ax.set_xticklabels([str(i * n_steps // 8) for i in range(9)])\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(n_walks, n_steps, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Define the possible colors\n    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n    color_cycle = itertools.cycle(colors)\n\n    # Initialize the plot\n    fig, ax = plt.subplots()\n\n    # Generate random walks\n    for _ in range(n_walks):\n        # Generate random steps\n        steps = np.random.choice([-1, 1], size=n_steps)\n        walk = np.cumsum(steps)\n\n        # Plot each walk with a different color\n        ax.plot(walk, color=next(color_cycle))\n\n    # Set the x-axis limits\n    ax.set_xlim(0, n_steps)\n\n    # Set the x-axis ticks\n    ax.set_xticks(np.arange(0, n_steps + 1, n_steps // 8))\n    ax.set_xticklabels([str(i * n_steps // 8) for i in range(9)])\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/474", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    np.random.seed(random_seed)\n    samples = np.random.normal(mu, sigma, n_samples)\n    \n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    \n    pdf = norm.pdf(bins, mu, sigma)\n    ax.plot(bins, pdf, linewidth=2, color='r')\n    \n    ax.set_title('Histogram and PDF of Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    return ax, samples", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    np.random.seed(random_seed)\n    samples = np.random.normal(mu, sigma, n_samples)\n    \n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    \n    pdf = norm.pdf(bins, mu, sigma)\n    ax.plot(bins, pdf, linewidth=2, color='r')\n    \n    ax.set_title('Histogram and PDF of Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    return ax, samples\n```\n"}
{"task_id": "BigCodeBench/475", "solution": "import pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\ndef task_func(data, date_format, country, country_codes=None):\n    if not isinstance(data, pd.DataFrame) or not isinstance(date_format, str) or not isinstance(country, str) or (country_codes is not None and not isinstance(country_codes, dict)):\n        raise ValueError(\"Invalid input types\")\n\n    if country_codes is None:\n        default_country_codes = {\n            'Russia': 'ru_RU',\n            'Germany': 'de_DE',\n            'France': 'fr_FR',\n            'Spain': 'es_ES',\n            'Italy': 'it_IT'\n        }\n        country_codes = default_country_codes\n\n    if country not in country_codes:\n        raise ValueError(f\"Country '{country}' not found in country codes\")\n\n    try:\n        data['dates'] = pd.to_datetime(data['dates'], format=date_format)\n    except Exception as e:\n        raise ValueError(f\"Error converting dates: {e}\")\n\n    ax = data['dates'].dt.to_period('M').value_counts().sort_index().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Date Distribution')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Frequency')\n\n    return ax\ndata = pd.DataFrame({'dates': ['01/01/2000', '01/02/2000', '02/03/2000', '04/05/2000', '06/07/2000']})", "raw_solution": "import pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndef task_func(data, date_format, country, country_codes=None):\n    if not isinstance(data, pd.DataFrame) or not isinstance(date_format, str) or not isinstance(country, str) or (country_codes is not None and not isinstance(country_codes, dict)):\n        raise ValueError(\"Invalid input types\")\n\n    if country_codes is None:\n        default_country_codes = {\n            'Russia': 'ru_RU',\n            'Germany': 'de_DE',\n            'France': 'fr_FR',\n            'Spain': 'es_ES',\n            'Italy': 'it_IT'\n        }\n        country_codes = default_country_codes\n\n    if country not in country_codes:\n        raise ValueError(f\"Country '{country}' not found in country codes\")\n\n    try:\n        data['dates'] = pd.to_datetime(data['dates'], format=date_format)\n    except Exception as e:\n        raise ValueError(f\"Error converting dates: {e}\")\n\n    ax = data['dates'].dt.to_period('M').value_counts().sort_index().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Date Distribution')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Frequency')\n\n    return ax\n\n# Example usage:\ndata = pd.DataFrame({'dates': ['01/01/2000', '01/02/2000', '02/03/2000', '04/05/2000', '06/07/2000']})\nax = task_func(data, '%d/%m/%Y', 'Russia')"}
{"task_id": "BigCodeBench/476", "solution": "import matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nimport numpy as np\ndef task_func(X, Y):\n    # Define a quadratic function\n    def quadratic(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    # Fit the quadratic function to the data\n    params, _ = curve_fit(quadratic, X, Y)\n\n    # Generate data points for the fitted curve\n    X_fit = np.linspace(min(X), max(X), 100)\n    Y_fit = quadratic(X_fit, *params)\n\n    # Plot the data points and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y, label='Data Points')\n    ax.plot(X_fit, Y_fit, color='red', label='Fitted Curve')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Quadratic Fit')\n    ax.legend()\n\n    return params, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nimport numpy as np\n\ndef task_func(X, Y):\n    # Define a quadratic function\n    def quadratic(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    # Fit the quadratic function to the data\n    params, _ = curve_fit(quadratic, X, Y)\n\n    # Generate data points for the fitted curve\n    X_fit = np.linspace(min(X), max(X), 100)\n    Y_fit = quadratic(X_fit, *params)\n\n    # Plot the data points and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y, label='Data Points')\n    ax.plot(X_fit, Y_fit, color='red', label='Fitted Curve')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Quadratic Fit')\n    ax.legend()\n\n    return params, ax\n```\n"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random data\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Generate random categories\n    if N > len(CATEGORIES):\n        categories = np.random.choice(CATEGORIES, N, replace=True)\n    else:\n        categories = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['x'], df['y'], c=df['category'].cat.codes, cmap='viridis')\n    ax.set_title('Scatter Plot of x vs y')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    \n    # Add legend\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Categories\")\n    ax.add_artist(legend1)\n    \n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random data\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Generate random categories\n    if N > len(CATEGORIES):\n        categories = np.random.choice(CATEGORIES, N, replace=True)\n    else:\n        categories = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['x'], df['y'], c=df['category'].cat.codes, cmap='viridis')\n    ax.set_title('Scatter Plot of x vs y')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    \n    # Add legend\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Categories\")\n    ax.add_artist(legend1)\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/478", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    modified_strings = []\n    for string in data_list:\n        # Remove leading and trailing whitespaces\n        string = string.strip()\n        # Split the string into a list of words\n        words = string.split(', ')\n        # Randomly select a subset of words\n        selected_words = random.sample(words, len(words) - 1)\n        # Join the selected words back into a string\n        modified_string = ', '.join(selected_words)\n        modified_strings.append(modified_string)\n\n    # Create a DataFrame with the original and modified strings\n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport random\n\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    modified_strings = []\n    for string in data_list:\n        # Remove leading and trailing whitespaces\n        string = string.strip()\n        # Split the string into a list of words\n        words = string.split(', ')\n        # Randomly select a subset of words\n        selected_words = random.sample(words, len(words) - 1)\n        # Join the selected words back into a string\n        modified_string = ', '.join(selected_words)\n        modified_strings.append(modified_string)\n\n    # Create a DataFrame with the original and modified strings\n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n\n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair, bag, lamp'], seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/479", "solution": "import random\nimport string\nimport pandas as pd\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    modified_strings = []\n    \n    for s in data_list:\n        words = s.split(', ')\n        modified_words = []\n        \n        for word in words:\n            if word:\n                random_substring = random.choice(word)\n                random_string = ''.join(random.choices(string.ascii_lowercase, k=len(random_substring)))\n                modified_word = word.replace(random_substring, random_string)\n                modified_words.append(modified_word)\n            else:\n                modified_words.append(word)\n        \n        modified_strings.append(', '.join(modified_words))\n    \n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n    \n    return df", "raw_solution": "import random\nimport string\nimport pandas as pd\n\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    modified_strings = []\n    \n    for s in data_list:\n        words = s.split(', ')\n        modified_words = []\n        \n        for word in words:\n            if word:\n                random_substring = random.choice(word)\n                random_string = ''.join(random.choices(string.ascii_lowercase, k=len(random_substring)))\n                modified_word = word.replace(random_substring, random_string)\n                modified_words.append(modified_word)\n            else:\n                modified_words.append(word)\n        \n        modified_strings.append(', '.join(modified_words))\n    \n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n    \n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair, bag, lamp'])\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/480", "solution": "import re\nimport random\nimport pandas as pd\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    results = []\n    for original in data_list:\n        # Remove leading and trailing whitespaces\n        cleaned = original.strip()\n        # Split the string by commas\n        substrings = cleaned.split(',')\n        # Shuffle the substrings\n        random.shuffle(substrings)\n        # Join the substrings back into a string\n        shuffled = ', '.join(substrings)\n        results.append({'Original String': original, 'Shuffled String': shuffled})\n\n    # Create a DataFrame from the results\n    df = pd.DataFrame(results)\n    return df", "raw_solution": "import re\nimport random\nimport pandas as pd\n\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    results = []\n    for original in data_list:\n        # Remove leading and trailing whitespaces\n        cleaned = original.strip()\n        # Split the string by commas\n        substrings = cleaned.split(',')\n        # Shuffle the substrings\n        random.shuffle(substrings)\n        # Join the substrings back into a string\n        shuffled = ', '.join(substrings)\n        results.append({'Original String': original, 'Shuffled String': shuffled})\n\n    # Create a DataFrame from the results\n    df = pd.DataFrame(results)\n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair'], seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/481", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=42):\n    random.seed(seed)\n    original_strings = []\n    randomized_strings = []\n\n    for string in data_list:\n        # Split the string into substrings\n        substrings = string.split(', ')\n        # Shuffle the substrings\n        random.shuffle(substrings)\n        # Join the substrings back into a string\n        randomized_string = ', '.join(substrings)\n        # Append the original and randomized strings to their respective lists\n        original_strings.append(string)\n        randomized_strings.append(randomized_string)\n\n    # Create a DataFrame from the original and randomized strings\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Randomized String': randomized_strings\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(data_list, seed=42):\n    random.seed(seed)\n    original_strings = []\n    randomized_strings = []\n\n    for string in data_list:\n        # Split the string into substrings\n        substrings = string.split(', ')\n        # Shuffle the substrings\n        random.shuffle(substrings)\n        # Join the substrings back into a string\n        randomized_string = ', '.join(substrings)\n        # Append the original and randomized strings to their respective lists\n        original_strings.append(string)\n        randomized_strings.append(randomized_string)\n\n    # Create a DataFrame from the original and randomized strings\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Randomized String': randomized_strings\n    })\n\n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair, bag'], seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/482", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def random_operation(substring):\n        operation = random.choice(['remove', 'replace', 'shuffle', 'random'])\n        if operation == 'remove':\n            return ''.join(random.sample(substring, len(substring) - 1))\n        elif operation == 'replace':\n            return ''.join(random.sample(substring, len(substring)))\n        elif operation == 'shuffle':\n            return ''.join(random.sample(substring, len(substring)))\n        else:\n            return ''.join(random.sample(substring, len(substring)))\n\n    original_strings = []\n    modified_strings = []\n\n    for data in data_list:\n        substrings = re.split(r'\\s*,\\s*', data)\n        if len(substrings) > 1:\n            modified_substrings = [random_operation(sub) for sub in substrings]\n            modified_string = ', '.join(modified_substrings)\n        else:\n            modified_string = data\n        original_strings.append(data)\n        modified_strings.append(modified_string)\n\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Modified String': modified_strings\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def random_operation(substring):\n        operation = random.choice(['remove', 'replace', 'shuffle', 'random'])\n        if operation == 'remove':\n            return ''.join(random.sample(substring, len(substring) - 1))\n        elif operation == 'replace':\n            return ''.join(random.sample(substring, len(substring)))\n        elif operation == 'shuffle':\n            return ''.join(random.sample(substring, len(substring)))\n        else:\n            return ''.join(random.sample(substring, len(substring)))\n\n    original_strings = []\n    modified_strings = []\n\n    for data in data_list:\n        substrings = re.split(r'\\s*,\\s*', data)\n        if len(substrings) > 1:\n            modified_substrings = [random_operation(sub) for sub in substrings]\n            modified_string = ', '.join(modified_substrings)\n        else:\n            modified_string = data\n        original_strings.append(data)\n        modified_strings.append(modified_string)\n\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Modified String': modified_strings\n    })\n\n    return df\n\n# Example usage:\n# df = task_func(['lamp, bag, mirror', 'table, chair, bag, lamp'], seed=0)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/483", "solution": "import re\nimport pandas as pd\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    def reverse_words_in_match(match):\n        return ' '.join(match.group().split()[::-1])\n\n    def process_column(col):\n        return [re.sub(pattern, reverse_words_in_match, text) for text in col]\n\n    if column_name in df.columns:\n        df[column_name] = process_column(df[column_name])\n\n    return df", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    def reverse_words_in_match(match):\n        return ' '.join(match.group().split()[::-1])\n\n    def process_column(col):\n        return [re.sub(pattern, reverse_words_in_match, text) for text in col]\n\n    if column_name in df.columns:\n        df[column_name] = process_column(df[column_name])\n\n    return df\n\n# Example usage:\n# df = pd.DataFrame({'A': ['apple orange', 'red yellow green'], 'B': [1, 2]})\n# pattern = r'\b(?:apple|yellow)\b'\n# reversed_df = task_func(df, 'A', pattern)\n# print(reversed_df)\n```\n"}
{"task_id": "BigCodeBench/484", "solution": "import math\nimport numpy as np\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    np.random.seed(random_seed)\n    timestamps = np.arange(start_time, end_time, step)\n    timestamps = timestamps[timestamps <= end_time]\n    num_samples = len(timestamps)\n\n    sensor1 = np.random.normal(0, 0.1, num_samples)\n    sensor2 = np.random.cosine(0, 0.1, num_samples)\n    sensor3 = np.random.uniform(0, 1, num_samples)\n\n    sensor_status = np.random.choice(sensor_statuses, num_samples)\n\n    data = {\n        \"Timestamp\": pd.to_datetime(timestamps, unit='s'),\n        \"Sensor1\": sensor1,\n        \"Sensor2\": sensor2,\n        \"Sensor3\": sensor3,\n        \"SensorStatus\": sensor_status\n    }\n\n    df = pd.DataFrame(data, columns=columns)\n\n    return df", "raw_solution": "import math\nimport numpy as np\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    np.random.seed(random_seed)\n    timestamps = np.arange(start_time, end_time, step)\n    timestamps = timestamps[timestamps <= end_time]\n    num_samples = len(timestamps)\n\n    sensor1 = np.random.normal(0, 0.1, num_samples)\n    sensor2 = np.random.cosine(0, 0.1, num_samples)\n    sensor3 = np.random.uniform(0, 1, num_samples)\n\n    sensor_status = np.random.choice(sensor_statuses, num_samples)\n\n    data = {\n        \"Timestamp\": pd.to_datetime(timestamps, unit='s'),\n        \"Sensor1\": sensor1,\n        \"Sensor2\": sensor2,\n        \"Sensor3\": sensor3,\n        \"SensorStatus\": sensor_status\n    }\n\n    df = pd.DataFrame(data, columns=columns)\n\n    return df\n\n# Example usage:\n# df = task_func(0, 5000, 1000)\n# print(df.head(1))\n```\n"}
{"task_id": "BigCodeBench/485", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time):\n    # Convert string dates to datetime objects\n    start = datetime.strptime(start_time, \"%Y-%m-%d\")\n    end = datetime.strptime(end_time, \"%Y-%m-%d\")\n    \n    # Define the time zones\n    time_zones = ['UTC', 'America/Los_Angeles', 'Europe/Paris', 'Asia/Kolkata', 'Australia/Sydney']\n    \n    # Create a list to store the differences\n    differences = []\n    \n    # Iterate over the date range\n    current_date = start\n    while current_date <= end:\n        # Get the current date in UTC\n        utc_now = pytz.utc.localize(current_date)\n        \n        # Calculate the difference for each time zone\n        for zone in time_zones:\n            tz = pytz.timezone(zone)\n            local_now = tz.localize(current_date)\n            difference = (local_now - utc_now).total_seconds() / 3600\n            differences.append(difference)\n        \n        # Move to the next day\n        current_date += timedelta(days=1)\n    \n    # Create a numpy array for plotting\n    differences = np.array(differences)\n    \n    # Plot the differences\n    fig, ax = plt.subplots()\n    ax.plot(differences)\n    ax.set_title('Time Differences Between UTC and Time Zones')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Hours')\n    ax.set_xticks(np.arange(len(differences)))\n    ax.set_xticklabels([start + timedelta(days=i) for i in range(len(differences))], rotation=45)\n    \n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time):\n    # Convert string dates to datetime objects\n    start = datetime.strptime(start_time, \"%Y-%m-%d\")\n    end = datetime.strptime(end_time, \"%Y-%m-%d\")\n    \n    # Define the time zones\n    time_zones = ['UTC', 'America/Los_Angeles', 'Europe/Paris', 'Asia/Kolkata', 'Australia/Sydney']\n    \n    # Create a list to store the differences\n    differences = []\n    \n    # Iterate over the date range\n    current_date = start\n    while current_date <= end:\n        # Get the current date in UTC\n        utc_now = pytz.utc.localize(current_date)\n        \n        # Calculate the difference for each time zone\n        for zone in time_zones:\n            tz = pytz.timezone(zone)\n            local_now = tz.localize(current_date)\n            difference = (local_now - utc_now).total_seconds() / 3600\n            differences.append(difference)\n        \n        # Move to the next day\n        current_date += timedelta(days=1)\n    \n    # Create a numpy array for plotting\n    differences = np.array(differences)\n    \n    # Plot the differences\n    fig, ax = plt.subplots()\n    ax.plot(differences)\n    ax.set_title('Time Differences Between UTC and Time Zones')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Hours')\n    ax.set_xticks(np.arange(len(differences)))\n    ax.set_xticklabels([start + timedelta(days=i) for i in range(len(differences))], rotation=45)\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n    \n    # Convert start_time and end_time from milliseconds to datetime\n    start_time = datetime.fromtimestamp(start_time / 1000.0)\n    end_time = datetime.fromtimestamp(end_time / 1000.0)\n    \n    # Generate a range of timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=f\"{step}ms\")\n    \n    # Generate random values and apply a linear trend\n    values = np.random.normal(loc=0, scale=1, size=len(timestamps)) + trend * np.arange(len(timestamps))\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Time': timestamps, 'Value': values})\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Trend')\n    \n    return ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n    \n    # Convert start_time and end_time from milliseconds to datetime\n    start_time = datetime.fromtimestamp(start_time / 1000.0)\n    end_time = datetime.fromtimestamp(end_time / 1000.0)\n    \n    # Generate a range of timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=f\"{step}ms\")\n    \n    # Generate random values and apply a linear trend\n    values = np.random.normal(loc=0, scale=1, size=len(timestamps)) + trend * np.arange(len(timestamps))\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Time': timestamps, 'Value': values})\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Trend')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/487", "solution": "import os\nimport pandas as pd\nimport re\ndef task_func(file_path: str) -> pd.DataFrame:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    log_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{6}) - ([A-Z]+) - (.+)')\n    data = []\n\n    with open(file_path, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line)\n            if match:\n                timestamp, level, message = match.groups()\n                data.append({'Timestamp': timestamp, 'Level': level, 'Message': message})\n\n    df = pd.DataFrame(data, columns=['Timestamp', 'Level', 'Message'])\n\n    return df", "raw_solution": "import os\nimport pandas as pd\nimport re\n\ndef task_func(file_path: str) -> pd.DataFrame:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    log_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{6}) - ([A-Z]+) - (.+)')\n    data = []\n\n    with open(file_path, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line)\n            if match:\n                timestamp, level, message = match.groups()\n                data.append({'Timestamp': timestamp, 'Level': level, 'Message': message})\n\n    df = pd.DataFrame(data, columns=['Timestamp', 'Level', 'Message'])\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/488", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, amplitude, period, seed=0):\n    np.random.seed(seed)\n    \n    # Convert start_time and end_time from milliseconds to datetime\n    start_time = datetime.utcfromtimestamp(start_time / 1000.0)\n    end_time = datetime.utcfromtimestamp(end_time / 1000.0)\n    \n    # Generate a range of timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=f\"{step}ms\")\n    \n    # Generate a time series with seasonality\n    values = amplitude * np.sin(2 * np.pi * timestamps / pd.Timedelta(period, unit='ms'))\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Timestamp': timestamps, 'Value': values})\n    \n    # Plot the time series\n    fig, ax = plt.subplots()\n    ax.plot(df['Timestamp'], df['Value'])\n    ax.set_xlabel('Timestamp')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Seasonality')\n    \n    # Format x-axis to show readable timestamps\n    fig.autofmt_xdate()\n    \n    return ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, amplitude, period, seed=0):\n    np.random.seed(seed)\n    \n    # Convert start_time and end_time from milliseconds to datetime\n    start_time = datetime.utcfromtimestamp(start_time / 1000.0)\n    end_time = datetime.utcfromtimestamp(end_time / 1000.0)\n    \n    # Generate a range of timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=f\"{step}ms\")\n    \n    # Generate a time series with seasonality\n    values = amplitude * np.sin(2 * np.pi * timestamps / pd.Timedelta(period, unit='ms'))\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Timestamp': timestamps, 'Value': values})\n    \n    # Plot the time series\n    fig, ax = plt.subplots()\n    ax.plot(df['Timestamp'], df['Value'])\n    ax.set_xlabel('Timestamp')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Seasonality')\n    \n    # Format x-axis to show readable timestamps\n    fig.autofmt_xdate()\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/489", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(epoch_milliseconds, seed=0):\n    random.seed(seed)\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    if start_time > current_time:\n        raise ValueError(\"Start time cannot be later than the current time.\")\n    \n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n    \n    data = []\n    while start_time <= current_time:\n        user = random.choice(users)\n        activity = random.choice(activities)\n        data.append({'user': user, 'activity': activity, 'time': start_time})\n        start_time += timedelta(seconds=random.randint(1, 10))\n    \n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(epoch_milliseconds, seed=0):\n    random.seed(seed)\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    if start_time > current_time:\n        raise ValueError(\"Start time cannot be later than the current time.\")\n    \n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n    \n    data = []\n    while start_time <= current_time:\n        user = random.choice(users)\n        activity = random.choice(activities)\n        data.append({'user': user, 'activity': activity, 'time': start_time})\n        start_time += timedelta(seconds=random.randint(1, 10))\n    \n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\n# log = task_func(1615168051807)\n# print(log.head())\n```\n"}
{"task_id": "BigCodeBench/490", "solution": "import xmltodict\nimport json\ndef task_func(s, file_path):\n    \"\"\"\n    XML string ko ek dict representation mein badalta hai aur ise ek JSON file ke roop mein save karta hai.\n    Yeh XML format mein store kiye gaye data ko easily access aur persist karne ke liye useful hai.\n\n    Params:\n    - s (str): XML string jise convert karna hai.\n    - file_path (str): Vah path jahaan JSON file ko save kiya jaayega.\n\n    Returns:\n        dict: XML string ka ek dict representation.\n\n    Reqs:\n    - xmltodict\n    - json\n\n    Examples:\n    - Examples:\n    - >>> result = task_func('<person><name>John</name><age>30</age></person>', \"temp.json\")\n    - >>> result['person']['name'] + ', ' + result['person']['age']\n    - 'John, 30'\n    - >>> result = task_func('<school><class><student>Emma</student></class></school>', \"temp.json\")\n    - >>> result['school']['class']['student']\n    - 'Emma'\n\n    \"\"\"\n    # Convert XML string to a dictionary\n    data_dict = xmltodict.parse(s)\n\n    # Convert the dictionary to a JSON string and save it to a file\n    with open(file_path, 'w') as json_file:\n        json.dump(data_dict, json_file, indent=4)\n\n    return data_dict", "raw_solution": "import xmltodict\nimport json\n\ndef task_func(s, file_path):\n    \"\"\"\n    XML string ko ek dict representation mein badalta hai aur ise ek JSON file ke roop mein save karta hai.\n    Yeh XML format mein store kiye gaye data ko easily access aur persist karne ke liye useful hai.\n\n    Params:\n    - s (str): XML string jise convert karna hai.\n    - file_path (str): Vah path jahaan JSON file ko save kiya jaayega.\n\n    Returns:\n        dict: XML string ka ek dict representation.\n\n    Reqs:\n    - xmltodict\n    - json\n\n    Examples:\n    - Examples:\n    - >>> result = task_func('<person><name>John</name><age>30</age></person>', \"temp.json\")\n    - >>> result['person']['name'] + ', ' + result['person']['age']\n    - 'John, 30'\n    - >>> result = task_func('<school><class><student>Emma</student></class></school>', \"temp.json\")\n    - >>> result['school']['class']['student']\n    - 'Emma'\n\n    \"\"\"\n    # Convert XML string to a dictionary\n    data_dict = xmltodict.parse(s)\n\n    # Convert the dictionary to a JSON string and save it to a file\n    with open(file_path, 'w') as json_file:\n        json.dump(data_dict, json_file, indent=4)\n\n    return data_dict\n```\n"}
{"task_id": "BigCodeBench/491", "solution": "import random\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\ndef task_func(epoch_milliseconds, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a positive integer.\")\n\n    start_date = datetime.utcfromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.utcnow()\n    days = (end_date - start_date).days\n\n    categories = ['electronics', 'books', 'ghar', 'khel', 'kapde']\n    sales_data = {category: [] for category in categories}\n\n    for day in range(days):\n        for category in categories:\n            sales = random.randint(10, 50)\n            sales_data[category].append(sales)\n\n    bikri_data = {category: sum(sales) for category, sales in sales_data.items()}\n\n    fig, ax = plt.subplots()\n    ax.plot(list(bikri_data.keys()), list(bikri_data.values()), marker='o')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Bikri Units')\n    ax.set_title('Bikri Trend')\n\n    return bikri_data, ax", "raw_solution": "import random\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndef task_func(epoch_milliseconds, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a positive integer.\")\n\n    start_date = datetime.utcfromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.utcnow()\n    days = (end_date - start_date).days\n\n    categories = ['electronics', 'books', 'ghar', 'khel', 'kapde']\n    sales_data = {category: [] for category in categories}\n\n    for day in range(days):\n        for category in categories:\n            sales = random.randint(10, 50)\n            sales_data[category].append(sales)\n\n    bikri_data = {category: sum(sales) for category, sales in sales_data.items()}\n\n    fig, ax = plt.subplots()\n    ax.plot(list(bikri_data.keys()), list(bikri_data.values()), marker='o')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Bikri Units')\n    ax.set_title('Bikri Trend')\n\n    return bikri_data, ax\n```\n"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n\n    data = []\n    for date in date_range:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append([product, date, sales])\n\n    df = pd.DataFrame(data, columns=[\"Product\", \"Date\", \"Sales\"])\n    return df", "raw_solution": "import pandas as pd\nfrom datetime import datetime\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n\n    data = []\n    for date in date_range:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append([product, date, sales])\n\n    df = pd.DataFrame(data, columns=[\"Product\", \"Date\", \"Sales\"])\n    return df\n\n# Example usage:\n# sales_data = task_func(1236472051807, random_seed=42)\n# print(sales_data.head())\n```\n"}
{"task_id": "BigCodeBench/493", "solution": "from datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_date = datetime.now()\n    dates = []\n    performance_data = {team: [] for team in teams}\n\n    while start_date <= current_date:\n        dates.append(start_date)\n        for team in teams:\n            performance = sum(random.random() * 0.1 for _ in range(10))\n            performance_data[team].append(performance)\n        start_date += timedelta(days=1)\n\n    fig, ax = plt.subplots()\n    for team in teams:\n        ax.plot(dates, performance_data[team], label=team)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Performance')\n    ax.set_title('Performance Trend Over Time')\n    ax.legend()\n\n    return performance_data, fig", "raw_solution": "from datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_date = datetime.now()\n    dates = []\n    performance_data = {team: [] for team in teams}\n\n    while start_date <= current_date:\n        dates.append(start_date)\n        for team in teams:\n            performance = sum(random.random() * 0.1 for _ in range(10))\n            performance_data[team].append(performance)\n        start_date += timedelta(days=1)\n\n    fig, ax = plt.subplots()\n    for team in teams:\n        ax.plot(dates, performance_data[team], label=team)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Performance')\n    ax.set_title('Performance Trend Over Time')\n    ax.legend()\n\n    return performance_data, fig\n```\n"}
{"task_id": "BigCodeBench/494", "solution": "from datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    fake = Faker()\n    if seed is not None:\n        fake.seed_instance(seed)\n\n    if epoch_milliseconds < 0:\n        epoch_milliseconds = 0\n\n    dt = datetime.fromtimestamp(epoch_milliseconds / 1000.0, pytz.utc)\n    tz_pattern = re.compile(r\"^UTC[+-]\\d{2}:\\d{2}$\")\n\n    result = {}\n    for tz in timezones:\n        if tz not in pytz.all_timezones and not tz_pattern.match(tz):\n            print(f\"Invalid timezone: {tz}. Using UTC.\")\n            tz = \"UTC\"\n        try:\n            local_tz = pytz.timezone(tz)\n            local_dt = dt.astimezone(local_tz)\n            event_name = fake.name()\n            result[event_name] = [\n                {\n                    \"date\": local_dt.date(),\n                    \"time\": local_dt.time(),\n                    \"timezone\": tz\n                }\n            ]\n        except Exception as e:\n            print(f\"Error processing timezone {tz}: {e}\")\n\n    return result", "raw_solution": "from datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\n\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    fake = Faker()\n    if seed is not None:\n        fake.seed_instance(seed)\n\n    if epoch_milliseconds < 0:\n        epoch_milliseconds = 0\n\n    dt = datetime.fromtimestamp(epoch_milliseconds / 1000.0, pytz.utc)\n    tz_pattern = re.compile(r\"^UTC[+-]\\d{2}:\\d{2}$\")\n\n    result = {}\n    for tz in timezones:\n        if tz not in pytz.all_timezones and not tz_pattern.match(tz):\n            print(f\"Invalid timezone: {tz}. Using UTC.\")\n            tz = \"UTC\"\n        try:\n            local_tz = pytz.timezone(tz)\n            local_dt = dt.astimezone(local_tz)\n            event_name = fake.name()\n            result[event_name] = [\n                {\n                    \"date\": local_dt.date(),\n                    \"time\": local_dt.time(),\n                    \"timezone\": tz\n                }\n            ]\n        except Exception as e:\n            print(f\"Error processing timezone {tz}: {e}\")\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/495", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(days, random_seed=0):\n    np.random.seed(random_seed)\n    date_range = pd.date_range(start='2023-01-01', periods=days, freq='D')\n    data = np.random.randint(0, 101, size=(days, 5))\n    df = pd.DataFrame(data, columns=['Groceries', 'Entertainment', 'Rent', 'Utilities', 'Miscellaneous'], index=date_range)\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(days, random_seed=0):\n    np.random.seed(random_seed)\n    date_range = pd.date_range(start='2023-01-01', periods=days, freq='D')\n    data = np.random.randint(0, 101, size=(days, 5))\n    df = pd.DataFrame(data, columns=['Groceries', 'Entertainment', 'Rent', 'Utilities', 'Miscellaneous'], index=date_range)\n    return df\n\n# Example usage:\n# df = task_func(5, random_seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/496", "solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be at least 1\")\n\n    np.random.seed(random_seed)\n    today = datetime.now()\n    dates = [today - timedelta(days=i) for i in range(days_in_past)]\n    temperatures = np.random.randint(15, 36, size=days_in_past)\n\n    fig, ax = plt.subplots()\n    ax.plot(dates, temperatures, marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Temperature (degC)')\n    ax.set_title('Temperature Trend')\n    ax.set_xticks(dates)\n    ax.set_xticklabels([date.strftime('%Y-%m-%d') for date in dates], rotation=45)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be at least 1\")\n\n    np.random.seed(random_seed)\n    today = datetime.now()\n    dates = [today - timedelta(days=i) for i in range(days_in_past)]\n    temperatures = np.random.randint(15, 36, size=days_in_past)\n\n    fig, ax = plt.subplots()\n    ax.plot(dates, temperatures, marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Temperature (degC)')\n    ax.set_title('Temperature Trend')\n    ax.set_xticks(dates)\n    ax.set_xticklabels([date.strftime('%Y-%m-%d') for date in dates], rotation=45)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/497", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\ndef task_func(days_in_past=7):\n    if days_in_past < 0:\n        raise ValueError(\"days_in_past must be a non-negative integer\")\n\n    # Get the current time in UTC\n    utc_now = datetime.now(pytz.utc)\n\n    # Calculate the date 'days_in_past' days ago\n    past_date = utc_now - timedelta(days=days_in_past)\n\n    # Get the weekday of the past date\n    weekday = past_date.weekday()\n\n    # Map the weekday to the corresponding day name\n    day_name = calendar.day_name[weekday]\n\n    return day_name", "raw_solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\n\ndef task_func(days_in_past=7):\n    if days_in_past < 0:\n        raise ValueError(\"days_in_past must be a non-negative integer\")\n\n    # Get the current time in UTC\n    utc_now = datetime.now(pytz.utc)\n\n    # Calculate the date 'days_in_past' days ago\n    past_date = utc_now - timedelta(days=days_in_past)\n\n    # Get the weekday of the past date\n    weekday = past_date.weekday()\n\n    # Map the weekday to the corresponding day name\n    day_name = calendar.day_name[weekday]\n\n    return day_name\n\n# Test cases"}
{"task_id": "BigCodeBench/498", "solution": "import xmltodict\nimport json\ndef task_func(s, save_json=False, json_file_path=None):\n    if not s.strip():\n        raise ValueError(\"Input XML string is empty or only contains whitespace.\")\n    \n    # Convert XML to dictionary\n    xml_dict = xmltodict.parse(s)\n    \n    # Optionally save the dictionary to a JSON file\n    if save_json and json_file_path:\n        with open(json_file_path, 'w') as json_file:\n            json.dump(xml_dict, json_file, indent=4)\n    \n    return xml_dict", "raw_solution": "import xmltodict\nimport json\n\ndef task_func(s, save_json=False, json_file_path=None):\n    if not s.strip():\n        raise ValueError(\"Input XML string is empty or only contains whitespace.\")\n    \n    # Convert XML to dictionary\n    xml_dict = xmltodict.parse(s)\n    \n    # Optionally save the dictionary to a JSON file\n    if save_json and json_file_path:\n        with open(json_file_path, 'w') as json_file:\n            json.dump(xml_dict, json_file, indent=4)\n    \n    return xml_dict\n\n# Example usage:\n# Convert XML to dictionary\nxml_string = '<school><class><student>Emma</student></class></school>'\nresult = task_func(xml_string)"}
{"task_id": "BigCodeBench/499", "solution": "import xlwt\nimport os\nimport io\nimport csv\ndef task_func(csv_content, filename):\n    # Create a workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Read CSV content\n    csv_file = io.StringIO(csv_content)\n    reader = csv.reader(csv_file)\n\n    # Write data to sheet\n    for row_idx, row in enumerate(reader):\n        for col_idx, value in enumerate(row):\n            sheet.write(row_idx, col_idx, value)\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the created file\n    return os.path.abspath(filename)\ncsv_content = 'ID,Name,Age\\n1,John Doe,30\\n2,Jane Doe,28'", "raw_solution": "import xlwt\nimport os\nimport io\nimport csv\n\ndef task_func(csv_content, filename):\n    # Create a workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Read CSV content\n    csv_file = io.StringIO(csv_content)\n    reader = csv.reader(csv_file)\n\n    # Write data to sheet\n    for row_idx, row in enumerate(reader):\n        for col_idx, value in enumerate(row):\n            sheet.write(row_idx, col_idx, value)\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the created file\n    return os.path.abspath(filename)\n\n# Test cases\ncsv_content = 'ID,Name,Age\\n1,John Doe,30\\n2,Jane Doe,28'\nfile_path = task_func(csv_content, 'test_data.xls')"}
{"task_id": "BigCodeBench/500", "solution": "import xlwt\nimport os\nfrom collections import OrderedDict\nFIELDS = ['ID', 'Name', 'Age']\ndef task_func(values, filename):\n    # Create a new workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Write the header\n    for col, field in enumerate(FIELDS):\n        sheet.write(0, col, field)\n\n    # Write the data\n    for row, value in enumerate(values, start=1):\n        for col, field in enumerate(FIELDS):\n            sheet.write(row, col, value[field])\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the file\n    return os.path.abspath(filename)\nempty_data = []", "raw_solution": "import xlwt\nimport os\nfrom collections import OrderedDict\n\n# Constants\nFIELDS = ['ID', 'Name', 'Age']\n\ndef task_func(values, filename):\n    # Create a new workbook and sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Write the header\n    for col, field in enumerate(FIELDS):\n        sheet.write(0, col, field)\n\n    # Write the data\n    for row, value in enumerate(values, start=1):\n        for col, field in enumerate(FIELDS):\n            sheet.write(row, col, value[field])\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the file\n    return os.path.abspath(filename)\n\n# Test cases\nempty_data = []\npath = task_func(empty_data, 'empty_data.xls')\nassert os.path.exists(path) and 'empty_data.xls' in path\n\ndata = [OrderedDict([('ID', 1), ('Name', 'John Doe'), ('Age', 30)]),\n        OrderedDict([('ID', 2), ('Name', 'Jane Doe'), ('Age', 28)])]\npath = task_func(data, 'test_data.xls')\nassert os.path.exists(path) and 'test_data.xls' in path\n```\nThis script creates an Excel file with the given data and filename. It uses the `xlwt` library to create the Excel file and the `os` library to get the absolute path of the file. The function also includes test cases to verify its correctness.\n"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = pd.read_json(json_str)\n    except ValueError as e:\n        raise ValueError(\"Invalid JSON\") from e\n\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet(sheet_name)\n\n    for col, col_name in enumerate(data.columns):\n        sheet.write(0, col, col_name)\n\n    for row, row_data in data.iterrows():\n        for col, col_name in enumerate(data.columns):\n            sheet.write(row + 1, col, row_data[col_name])\n\n    workbook.save(filename)\n\n    return os.path.abspath(filename)", "raw_solution": "import xlwt\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = pd.read_json(json_str)\n    except ValueError as e:\n        raise ValueError(\"Invalid JSON\") from e\n\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet(sheet_name)\n\n    for col, col_name in enumerate(data.columns):\n        sheet.write(0, col, col_name)\n\n    for row, row_data in data.iterrows():\n        for col, col_name in enumerate(data.columns):\n            sheet.write(row + 1, col, row_data[col_name])\n\n    workbook.save(filename)\n\n    return os.path.abspath(filename)\n```\n"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n    activities = [\"Dancing\", \"Running\", \"Swimming\", \"Yoga\", \"Meditation\"]\n    dates = [datetime.now() - timedelta(days=i) for i in range(days_in_past)]\n    \n    data = []\n    for date in dates:\n        activity = random.choice(activities)\n        duration = random.randint(0, 120)\n        data.append([date.strftime(\"%Y-%m-%d\"), activity, duration])\n    \n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.lineplot(data=df, x=\"Date\", y=\"Duration\", hue=\"Activity\")\n    ax.set_title(\"Activity Duration Over Time\")\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"Duration\")\n    \n    return ax, df", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n    activities = [\"Dancing\", \"Running\", \"Swimming\", \"Yoga\", \"Meditation\"]\n    dates = [datetime.now() - timedelta(days=i) for i in range(days_in_past)]\n    \n    data = []\n    for date in dates:\n        activity = random.choice(activities)\n        duration = random.randint(0, 120)\n        data.append([date.strftime(\"%Y-%m-%d\"), activity, duration])\n    \n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.lineplot(data=df, x=\"Date\", y=\"Duration\", hue=\"Activity\")\n    ax.set_title(\"Activity Duration Over Time\")\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"Duration\")\n    \n    return ax, df\n```\n"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    np.random.seed(random_seed)\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    \n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    data = {stock: np.random.uniform(0.0, 1.0, size=days_in_past) for stock in stock_names}\n    \n    df = pd.DataFrame(data, index=date_range)\n    df = df.round(6)  # Round the prices to 6 decimal places\n    \n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    np.random.seed(random_seed)\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    \n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    data = {stock: np.random.uniform(0.0, 1.0, size=days_in_past) for stock in stock_names}\n    \n    df = pd.DataFrame(data, index=date_range)\n    df = df.round(6)  # Round the prices to 6 decimal places\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/504", "solution": "import hashlib\nimport rsa\nimport base64\ndef task_func(file_path):\n    # Load the private key\n    with open('private.pem', 'rb') as f:\n        private_key = rsa.PrivateKey.load_pkcs1(f.read())\n\n    # Read the file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # Generate SHA-256 hash of the file content\n    sha256_hash = hashlib.sha256(file_content).digest()\n\n    # Sign the hash with the private key\n    signed_hash = rsa.sign(sha256_hash, private_key, 'SHA-256')\n\n    # Encode the signed hash with base64\n    encoded_signed_hash = base64.b64encode(signed_hash).decode('utf-8')\n\n    return encoded_signed_hash", "raw_solution": "import hashlib\nimport rsa\nimport base64\n\ndef task_func(file_path):\n    # Load the private key\n    with open('private.pem', 'rb') as f:\n        private_key = rsa.PrivateKey.load_pkcs1(f.read())\n\n    # Read the file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # Generate SHA-256 hash of the file content\n    sha256_hash = hashlib.sha256(file_content).digest()\n\n    # Sign the hash with the private key\n    signed_hash = rsa.sign(sha256_hash, private_key, 'SHA-256')\n\n    # Encode the signed hash with base64\n    encoded_signed_hash = base64.b64encode(signed_hash).decode('utf-8')\n\n    return encoded_signed_hash\n```\nThis script reads a file, generates a SHA-256 hash of its contents, signs the hash with a private RSA key, and then encodes the signed hash in base64. The function returns the base64 encoded signed hash. The script assumes that 'private.pem' is a valid RSA private key file.\n"}
{"task_id": "BigCodeBench/505", "solution": "import hashlib\nimport hmac\ndef task_func(secret, message):\n    \"\"\"\n    This function generates an HMAC signature for a given message using the SHA-256 hash function.\n    The secret key is used for both the hashing and the authentication process.\n\n    Parameters:\n    - secret (str): The secret key used for HMAC generation.\n    - message (str): The message to be authenticated.\n\n    Returns:\n    - str: The HMAC signature in hexadecimal format.\n\n    Examples:\n    >>> len(task_func('mysecretkey', 'Hello, world!')) == 64\n    True\n    \"\"\"\n    # Convert the secret key to bytes\n    secret_bytes = secret.encode('utf-8')\n    \n    # Convert the message to bytes\n    message_bytes = message.encode('utf-8')\n    \n    # Create a new HMAC object using the secret key and SHA-256 hash function\n    hmac_obj = hmac.new(secret_bytes, message_bytes, hashlib.sha256)\n    \n    # Return the HMAC signature in hexadecimal format\n    return hmac_obj.hexdigest()", "raw_solution": "import hashlib\nimport hmac\n\ndef task_func(secret, message):\n    \"\"\"\n    This function generates an HMAC signature for a given message using the SHA-256 hash function.\n    The secret key is used for both the hashing and the authentication process.\n\n    Parameters:\n    - secret (str): The secret key used for HMAC generation.\n    - message (str): The message to be authenticated.\n\n    Returns:\n    - str: The HMAC signature in hexadecimal format.\n\n    Examples:\n    >>> len(task_func('mysecretkey', 'Hello, world!')) == 64\n    True\n    \"\"\"\n    # Convert the secret key to bytes\n    secret_bytes = secret.encode('utf-8')\n    \n    # Convert the message to bytes\n    message_bytes = message.encode('utf-8')\n    \n    # Create a new HMAC object using the secret key and SHA-256 hash function\n    hmac_obj = hmac.new(secret_bytes, message_bytes, hashlib.sha256)\n    \n    # Return the HMAC signature in hexadecimal format\n    return hmac_obj.hexdigest()\n\n# Test cases to verify the solution correctness\nassert len(task_func('mysecretkey', 'Hello, world!')) == 64\nassert task_func('mysecretkey', 'Hello, world!') != task_func('mysecretkey', 'Goodbye, world!')\nassert task_func('mysecretkey', 'Different message') != task_func('mysecretkey', 'Hello, world!')\n```\n"}
{"task_id": "BigCodeBench/506", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=['Date', 'Temperature', 'Humidity', 'Wind Speed', 'Precipitation'])\n\n    # Check if the specified column exists in the DataFrame\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame\")\n\n    # Calculate the sum, mean, min, and max of the specified column\n    column_data = df[column]\n    result = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max(),\n    }\n\n    # Set mean, min, and max to np.nan, np.inf, and -np.inf respectively if the column is empty\n    if column_data.empty:\n        result['mean'] = np.nan\n        result['min'] = np.inf\n        result['max'] = -np.inf\n\n    # Plot a histogram of the specified column\n    plt.hist(column_data, bins=10, alpha=0.7, color='blue')\n    plt.title(f'Histogram of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    result['plot'] = plt.gca()\n\n    return result\ndata = [\n    ['2022-01-01', -5, 80, 10, 0],\n    ['2022-01-02', -2, 83, 15, 0],\n    ['2022-01-03', -3, 85, 12, 0]\n]", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=['Date', 'Temperature', 'Humidity', 'Wind Speed', 'Precipitation'])\n\n    # Check if the specified column exists in the DataFrame\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame\")\n\n    # Calculate the sum, mean, min, and max of the specified column\n    column_data = df[column]\n    result = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max(),\n    }\n\n    # Set mean, min, and max to np.nan, np.inf, and -np.inf respectively if the column is empty\n    if column_data.empty:\n        result['mean'] = np.nan\n        result['min'] = np.inf\n        result['max'] = -np.inf\n\n    # Plot a histogram of the specified column\n    plt.hist(column_data, bins=10, alpha=0.7, color='blue')\n    plt.title(f'Histogram of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    result['plot'] = plt.gca()\n\n    return result\n\n# Example usage:\ndata = [\n    ['2022-01-01', -5, 80, 10, 0],\n    ['2022-01-02', -2, 83, 15, 0],\n    ['2022-01-03', -3, 85, 12, 0]\n]\nresult = task_func('Temperature', data)"}
{"task_id": "BigCodeBench/507", "solution": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime\ndef task_func(column, data):\n    valid_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n    if column not in valid_columns:\n        raise ValueError(f\"Invalid column name. Valid options are: {', '.join(valid_columns)}\")\n\n    df = pd.DataFrame(data, columns=valid_columns)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the data\")\n\n    if df.empty:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n\n    column_data = df[column]\n    result = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    return result\ndata = [\n    [datetime(2022, 1, 1), 95, 95, 95, 95, 10000],\n    [datetime(2022, 5, 1), 100, 100, 100, 100, 20000],\n    [datetime(2022, 2, 1), 90, 90, 90, 90, 5000]\n]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef task_func(column, data):\n    valid_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n    if column not in valid_columns:\n        raise ValueError(f\"Invalid column name. Valid options are: {', '.join(valid_columns)}\")\n\n    df = pd.DataFrame(data, columns=valid_columns)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the data\")\n\n    if df.empty:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n\n    column_data = df[column]\n    result = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    return result\n\n# Example usage:\ndata = [\n    [datetime(2022, 1, 1), 95, 95, 95, 95, 10000],\n    [datetime(2022, 5, 1), 100, 100, 100, 100, 20000],\n    [datetime(2022, 2, 1), 90, 90, 90, 90, 5000]\n]\nresults = task_func('Open', data)"}
{"task_id": "BigCodeBench/508", "solution": "import hashlib\nimport io\nimport os\ndef task_func(file_path1, file_path2):\n    if not os.path.exists(file_path1):\n        raise FileNotFoundError(f\"The file {file_path1} does not exist.\")\n    if not os.path.exists(file_path2):\n        raise FileNotFoundError(f\"The file {file_path2} does not exist.\")\n\n    def calculate_md5(file_path):\n        hash_md5 = hashlib.md5()\n        with io.open(file_path, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n\n    return calculate_md5(file_path1) == calculate_md5(file_path2)", "raw_solution": "import hashlib\nimport io\nimport os\n\ndef task_func(file_path1, file_path2):\n    if not os.path.exists(file_path1):\n        raise FileNotFoundError(f\"The file {file_path1} does not exist.\")\n    if not os.path.exists(file_path2):\n        raise FileNotFoundError(f\"The file {file_path2} does not exist.\")\n\n    def calculate_md5(file_path):\n        hash_md5 = hashlib.md5()\n        with io.open(file_path, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n\n    return calculate_md5(file_path1) == calculate_md5(file_path2)\n```\n"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\nimport os\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    if not os.path.exists(file_path1) or not os.path.exists(file_path2):\n        raise FileNotFoundError(\"One or both files do not exist.\")\n    \n    try:\n        with open(file_path1, newline='', encoding='utf-8') as f1, open(file_path2, newline='', encoding='utf-8') as f2:\n            lines1 = f1.readlines()\n            lines2 = f2.readlines()\n    except Exception as e:\n        raise Exception(f\"Error reading files: {e}\")\n\n    if not lines1 or not lines2:\n        raise ValueError(\"One or both files are empty.\")\n\n    diff = list(ndiff(lines1, lines2))\n    diff_data = []\n\n    for line in diff:\n        if line.startswith('- ') or line.startswith('+ ') or line.startswith('  '):\n            line_number = int(line.split(',')[0].strip('-+ '))\n            status = line[0]\n            content = line.split(',')[1].strip()\n            diff_data.append({'Line Number': line_number, 'Sthiti': status, 'Content': content})\n\n    df = pd.DataFrame(diff_data)\n    return df", "raw_solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\nimport os\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    if not os.path.exists(file_path1) or not os.path.exists(file_path2):\n        raise FileNotFoundError(\"One or both files do not exist.\")\n    \n    try:\n        with open(file_path1, newline='', encoding='utf-8') as f1, open(file_path2, newline='', encoding='utf-8') as f2:\n            lines1 = f1.readlines()\n            lines2 = f2.readlines()\n    except Exception as e:\n        raise Exception(f\"Error reading files: {e}\")\n\n    if not lines1 or not lines2:\n        raise ValueError(\"One or both files are empty.\")\n\n    diff = list(ndiff(lines1, lines2))\n    diff_data = []\n\n    for line in diff:\n        if line.startswith('- ') or line.startswith('+ ') or line.startswith('  '):\n            line_number = int(line.split(',')[0].strip('-+ '))\n            status = line[0]\n            content = line.split(',')[1].strip()\n            diff_data.append({'Line Number': line_number, 'Sthiti': status, 'Content': content})\n\n    df = pd.DataFrame(diff_data)\n    return df\n\n# Example usage:\n# df = task_func('file1.csv', 'file2.csv')\n# print(df.head())\n```\n"}
{"task_id": "BigCodeBench/510", "solution": "import difflib\nimport gzip\ndef task_func(file_path1, file_path2):\n    def read_gzip_file(file_path):\n        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n            return f.readlines()\n\n    file1_lines = read_gzip_file(file_path1)\n    file2_lines = read_gzip_file(file_path2)\n\n    diff = difflib.unified_diff(file1_lines, file2_lines, lineterm='', fromfile=file_path1, tofile=file_path2)\n    diff_result = '\\n'.join(diff)\n\n    return diff_result", "raw_solution": "import difflib\nimport gzip\n\ndef task_func(file_path1, file_path2):\n    def read_gzip_file(file_path):\n        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n            return f.readlines()\n\n    file1_lines = read_gzip_file(file_path1)\n    file2_lines = read_gzip_file(file_path2)\n\n    diff = difflib.unified_diff(file1_lines, file2_lines, lineterm='', fromfile=file_path1, tofile=file_path2)\n    diff_result = '\\n'.join(diff)\n\n    return diff_result\n\n# Test cases"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None\n\n    df = pd.DataFrame(data, columns=['Age', 'Salary', 'Experience'])\n\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' not found in the data.\")\n\n    column_data = df[column]\n\n    stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    ax = column_data.plot.pie(autopct='%1.1f%%')\n    plt.title(f\"Pie chart of {column}\")\n\n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not data:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None\n\n    df = pd.DataFrame(data, columns=['Age', 'Salary', 'Experience'])\n\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' not found in the data.\")\n\n    column_data = df[column]\n\n    stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    ax = column_data.plot.pie(autopct='%1.1f%%')\n    plt.title(f\"Pie chart of {column}\")\n\n    return stats, ax\n```\n"}
{"task_id": "BigCodeBench/512", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not all(len(row) == 3 for row in data):\n        raise ValueError(\"Each row in data must have exactly 3 elements.\")\n    \n    df = pd.DataFrame(data, columns=['Product', 'Quantity', 'Total Sales'])\n    \n    if (df['Quantity'] < 0).any() or (df['Total Sales'] < 0).any():\n        raise ValueError(\"Quantity and Total Sales must be non-negative.\")\n    \n    stats = {\n        'sum': df['Total Sales'].sum(),\n        'mean': df['Total Sales'].mean(),\n        'min': df['Total Sales'].min(),\n        'max': df['Total Sales'].max()\n    }\n    \n    ax = df.plot(kind='bar', x='Product', y='Total Sales', title=f\"{column} ka bar chart\")\n    ax.set_xlabel('Product')\n    \n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not all(len(row) == 3 for row in data):\n        raise ValueError(\"Each row in data must have exactly 3 elements.\")\n    \n    df = pd.DataFrame(data, columns=['Product', 'Quantity', 'Total Sales'])\n    \n    if (df['Quantity'] < 0).any() or (df['Total Sales'] < 0).any():\n        raise ValueError(\"Quantity and Total Sales must be non-negative.\")\n    \n    stats = {\n        'sum': df['Total Sales'].sum(),\n        'mean': df['Total Sales'].mean(),\n        'min': df['Total Sales'].min(),\n        'max': df['Total Sales'].max()\n    }\n    \n    ax = df.plot(kind='bar', x='Product', y='Total Sales', title=f\"{column} ka bar chart\")\n    ax.set_xlabel('Product')\n    \n    return stats, ax\n```\n"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\ndef task_func(column, data):\n    if column not in ['date', 'kadam', 'jali hui calories', 'chali gayi doori']:\n        raise KeyError(f\"Invalid column: {column}. Valid columns are 'date', 'kadam', 'jali hui calories', 'chali gayi doori'.\")\n\n    if not data:\n        raise ValueError(\"Data list is empty.\")\n\n    df = pd.DataFrame(data, columns=['date', 'kadam', 'jali hui calories', 'chali gayi doori'])\n\n    if not all(df['kadam'].apply(lambda x: isinstance(x, (int, float))) and x >= 0 for x in df['kadam']):\n        raise ValueError(\"Kadam must be a non-negative number.\")\n    if not all(df['jali hui calories'].apply(lambda x: isinstance(x, (int, float))) and x >= 0 for x in df['jali hui calories']):\n        raise ValueError(\"Burned calories must be a non-negative number.\")\n    if not all(df['chali gayi doori'].apply(lambda x: isinstance(x, (int, float))) and x >= 0 for x in df['chali gayi doori']):\n        raise ValueError(\"Chali gayi doori must be a non-negative number.\")\n\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date')\n\n    if column not in df.columns:\n        raise KeyError(f\"Invalid column: {column}. Valid columns are 'date', 'kadam', 'jali hui calories', 'chali gayi doori'.\")\n\n    stats = {\n        'sum': np.sum(df[column]),\n        'mean': np.mean(df[column]),\n        'min': np.min(df[column]),\n        'max': np.max(df[column])\n    }\n\n    ax = df.plot(x='date', y=column, title=column)\n    plt.xlabel('Date')\n    plt.ylabel(column)\n\n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\ndef task_func(column, data):\n    if column not in ['date', 'kadam', 'jali hui calories', 'chali gayi doori']:\n        raise KeyError(f\"Invalid column: {column}. Valid columns are 'date', 'kadam', 'jali hui calories', 'chali gayi doori'.\")\n\n    if not data:\n        raise ValueError(\"Data list is empty.\")\n\n    df = pd.DataFrame(data, columns=['date', 'kadam', 'jali hui calories', 'chali gayi doori'])\n\n    if not all(df['kadam'].apply(lambda x: isinstance(x, (int, float))) and x >= 0 for x in df['kadam']):\n        raise ValueError(\"Kadam must be a non-negative number.\")\n    if not all(df['jali hui calories'].apply(lambda x: isinstance(x, (int, float))) and x >= 0 for x in df['jali hui calories']):\n        raise ValueError(\"Burned calories must be a non-negative number.\")\n    if not all(df['chali gayi doori'].apply(lambda x: isinstance(x, (int, float))) and x >= 0 for x in df['chali gayi doori']):\n        raise ValueError(\"Chali gayi doori must be a non-negative number.\")\n\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date')\n\n    if column not in df.columns:\n        raise KeyError(f\"Invalid column: {column}. Valid columns are 'date', 'kadam', 'jali hui calories', 'chali gayi doori'.\")\n\n    stats = {\n        'sum': np.sum(df[column]),\n        'mean': np.mean(df[column]),\n        'min': np.min(df[column]),\n        'max': np.max(df[column])\n    }\n\n    ax = df.plot(x='date', y=column, title=column)\n    plt.xlabel('Date')\n    plt.ylabel(column)\n\n    return stats, ax\n```\n"}
{"task_id": "BigCodeBench/514", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    columns = ['A', 'B', 'C', 'D', 'E']\n    df = pd.DataFrame(array, columns=columns)\n    \n    sums = df.sum()\n    ax = sums.plot(kind='bar', color='skyblue')\n    ax.set_title('Sum of Columns')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Sum')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(array):\n    columns = ['A', 'B', 'C', 'D', 'E']\n    df = pd.DataFrame(array, columns=columns)\n    \n    sums = df.sum()\n    ax = sums.plot(kind='bar', color='skyblue')\n    ax.set_title('Sum of Columns')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Sum')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/515", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    if not array or any(len(sublist) != 5 for sublist in array):\n        raise ValueError(\"Input array must be non-empty and all sublists must be of length 5.\")\n\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n    corr_matrix = df.corr()\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(array):\n    if not array or any(len(sublist) != 5 for sublist in array):\n        raise ValueError(\"Input array must be non-empty and all sublists must be of length 5.\")\n\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n    corr_matrix = df.corr()\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/516", "solution": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\ndef task_func(\n    array: list, random_seed: int = 0\n) -> (pd.DataFrame, sm.regression.linear_model.RegressionResultsWrapper):\n    np.random.seed(random_seed)\n    data = np.array(array)\n    if data.shape[1] != 5:\n        raise ValueError(\"Each sub-list must contain exactly 5 elements.\")\n\n    df = pd.DataFrame(data, columns=['A', 'b', 'C', 'd', 'Response'])\n    X = df[['A', 'b', 'C', 'd']]\n    y = df['Response']\n    X = sm.add_constant(X)  # Adds a constant term to the predictor variables\n\n    model = sm.OLS(y, X).fit()\n    return df, model", "raw_solution": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef task_func(\n    array: list, random_seed: int = 0\n) -> (pd.DataFrame, sm.regression.linear_model.RegressionResultsWrapper):\n    np.random.seed(random_seed)\n    data = np.array(array)\n    if data.shape[1] != 5:\n        raise ValueError(\"Each sub-list must contain exactly 5 elements.\")\n\n    df = pd.DataFrame(data, columns=['A', 'b', 'C', 'd', 'Response'])\n    X = df[['A', 'b', 'C', 'd']]\n    y = df['Response']\n    X = sm.add_constant(X)  # Adds a constant term to the predictor variables\n\n    model = sm.OLS(y, X).fit()\n    return df, model\n```\n"}
{"task_id": "BigCodeBench/517", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(array: list, random_seed: int = 42) -> (pd.DataFrame, np.ndarray):\n    np.random.seed(random_seed)\n    df = pd.DataFrame(array)\n    pca = PCA(n_components=2)\n    transformed = pca.fit_transform(df)\n    return df, transformed", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\ndef task_func(array: list, random_seed: int = 42) -> (pd.DataFrame, np.ndarray):\n    np.random.seed(random_seed)\n    df = pd.DataFrame(array)\n    pca = PCA(n_components=2)\n    transformed = pca.fit_transform(df)\n    return df, transformed\n\n# Example usage:\ndata = [[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]]\ndf, transformed = task_func(data)"}
{"task_id": "BigCodeBench/518", "solution": "import pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\ndef task_func(array):\n    if not all(isinstance(sublist, list) and all(isinstance(item, (int, float)) for item in sublist) for sublist in array):\n        raise TypeError(\"All elements in the array should be integers or floats.\")\n\n    # Convert the 2d list to a DataFrame\n    df = pd.DataFrame(array, columns=[chr(65 + i) for i in range(len(array[0]))])\n\n    # Calculate the distance matrix\n    distance_matrix = pdist(df.values, metric='euclidean')\n    distance_matrix = squareform(distance_matrix)\n\n    # Convert the distance matrix to a DataFrame\n    distance_df = pd.DataFrame(distance_matrix, index=df.index, columns=df.index)\n\n    return df, distance_df", "raw_solution": "import pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\n\ndef task_func(array):\n    if not all(isinstance(sublist, list) and all(isinstance(item, (int, float)) for item in sublist) for sublist in array):\n        raise TypeError(\"All elements in the array should be integers or floats.\")\n\n    # Convert the 2d list to a DataFrame\n    df = pd.DataFrame(array, columns=[chr(65 + i) for i in range(len(array[0]))])\n\n    # Calculate the distance matrix\n    distance_matrix = pdist(df.values, metric='euclidean')\n    distance_matrix = squareform(distance_matrix)\n\n    # Convert the distance matrix to a DataFrame\n    distance_df = pd.DataFrame(distance_matrix, index=df.index, columns=df.index)\n\n    return df, distance_df\n\n# Example usage:\n# df, distance_matrix = task_func([[1,2,3,4,5], [6,7,8,9,10]])\n# print(df)\n# print(distance_matrix)\n```\n"}
{"task_id": "BigCodeBench/519", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not all(isinstance(item, dict) for item in data):\n        raise TypeError(\"Input data must be a list of dictionaries.\")\n    \n    # Convert list of dictionaries to pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Replace NaN values with 0\n    df.fillna(0, inplace=True)\n    \n    # Plotting the sales data\n    ax = df.plot(kind='line', figsize=(10, 6))\n    ax.set_title('Fruit Sales over Time')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Sales Quantity')\n    \n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not all(isinstance(item, dict) for item in data):\n        raise TypeError(\"Input data must be a list of dictionaries.\")\n    \n    # Convert list of dictionaries to pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Replace NaN values with 0\n    df.fillna(0, inplace=True)\n    \n    # Plotting the sales data\n    ax = df.plot(kind='line', figsize=(10, 6))\n    ax.set_title('Fruit Sales over Time')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Sales Quantity')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/520", "solution": "import collections\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not all(isinstance(d, dict) for d in data):\n        raise ValueError(\"All elements in the data list must be dictionaries.\")\n    \n    total_sales = collections.Counter()\n    for sales in data:\n        for fruit, quantity in sales.items():\n            if quantity < 0:\n                raise ValueError(\"Sales quantity cannot be negative.\")\n            total_sales[fruit] += quantity\n\n    if not total_sales:\n        return {}, None\n\n    fruits = list(total_sales.keys())\n    quantities = list(total_sales.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(fruits, quantities, color=['laal', 'peela', 'hara', 'neela', 'purple'])\n    ax.set_xlabel('Fruits')\n    ax.set_ylabel('Total Sales')\n    ax.set_title('Total Sales by Fruit')\n\n    return dict(total_sales), ax", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not all(isinstance(d, dict) for d in data):\n        raise ValueError(\"All elements in the data list must be dictionaries.\")\n    \n    total_sales = collections.Counter()\n    for sales in data:\n        for fruit, quantity in sales.items():\n            if quantity < 0:\n                raise ValueError(\"Sales quantity cannot be negative.\")\n            total_sales[fruit] += quantity\n\n    if not total_sales:\n        return {}, None\n\n    fruits = list(total_sales.keys())\n    quantities = list(total_sales.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(fruits, quantities, color=['laal', 'peela', 'hara', 'neela', 'purple'])\n    ax.set_xlabel('Fruits')\n    ax.set_ylabel('Total Sales')\n    ax.set_title('Total Sales by Fruit')\n\n    return dict(total_sales), ax\n```\n"}
{"task_id": "BigCodeBench/521", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_list):\n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data_list)\n\n    # Plotting the data\n    ax = df.plot(kind='line', marker='o', figsize=(10, 6))\n    ax.set_title('Student Scores Over Tests')\n    ax.set_xlabel('Test Number')\n    ax.set_ylabel('Score')\n    ax.set_xticks(range(len(df.columns)))\n    ax.set_xticklabels(df.columns, rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_list):\n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data_list)\n\n    # Plotting the data\n    ax = df.plot(kind='line', marker='o', figsize=(10, 6))\n    ax.set_title('Student Scores Over Tests')\n    ax.set_xlabel('Test Number')\n    ax.set_ylabel('Score')\n    ax.set_xticks(range(len(df.columns)))\n    ax.set_xticklabels(df.columns, rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/522", "solution": "import collections\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return None\n\n    # Initialize a defaultdict to store scores\n    scores = collections.defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        for name, score in d.items():\n            if score < 0:\n                raise ValueError(\"Score cannot be negative.\")\n            scores[name].append(score)\n\n    # Calculate average scores\n    avg_scores = {name: sum(scores[name]) / len(scores[name]) for name in scores}\n\n    # Create a bar chart\n    names = list(avg_scores.keys())\n    values = list(avg_scores.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(range(len(avg_scores)), values, tick_label=names, color=['laal', 'peela', 'hara', 'blue', 'purple'])\n\n    ax.set_xlabel('Names')\n    ax.set_ylabel('Average Score')\n    ax.set_title('Average Character Score')\n\n    return ax\ndata = [{'John': 5, 'Jane': 10, 'Joe': 7},\n        {'John': 6, 'Jane': 8, 'Joe': 10},\n        {'John': 5, 'Jane': 9, 'Joe': 8},\n        {'John': 7, 'Jane': 10, 'Joe': 9}]", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        return None\n\n    # Initialize a defaultdict to store scores\n    scores = collections.defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        for name, score in d.items():\n            if score < 0:\n                raise ValueError(\"Score cannot be negative.\")\n            scores[name].append(score)\n\n    # Calculate average scores\n    avg_scores = {name: sum(scores[name]) / len(scores[name]) for name in scores}\n\n    # Create a bar chart\n    names = list(avg_scores.keys())\n    values = list(avg_scores.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(range(len(avg_scores)), values, tick_label=names, color=['laal', 'peela', 'hara', 'blue', 'purple'])\n\n    ax.set_xlabel('Names')\n    ax.set_ylabel('Average Score')\n    ax.set_title('Average Character Score')\n\n    return ax\n\n# Example usage:\ndata = [{'John': 5, 'Jane': 10, 'Joe': 7},\n        {'John': 6, 'Jane': 8, 'Joe': 10},\n        {'John': 5, 'Jane': 9, 'Joe': 8},\n        {'John': 7, 'Jane': 10, 'Joe': 9}]\nax = task_func(data)"}
{"task_id": "BigCodeBench/523", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return None\n\n    # Convert list of dictionaries to a Pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Create a line plot\n    ax = df.plot(kind='line')\n\n    # Set the title and labels\n    ax.set_title('Data over Time')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Data Points')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        return None\n\n    # Convert list of dictionaries to a Pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Create a line plot\n    ax = df.plot(kind='line')\n\n    # Set the title and labels\n    ax.set_title('Data over Time')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Data Points')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/524", "solution": "from collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data cannot be empty.\")\n    \n    if not all(isinstance(d, dict) for d in data):\n        raise TypeError(\"Input must be a list of dictionaries.\")\n    \n    if not all(all(isinstance(v, (int, float)) for v in d.values()) for d in data):\n        raise TypeError(\"All dictionary values must be numeric.\")\n    \n    stats = defaultdict(lambda: {'mean': None, 'std': None})\n    \n    for d in data:\n        for key, value in d.items():\n            if stats[key]['mean'] is None:\n                stats[key]['mean'] = value\n                stats[key]['std'] = 0\n            else:\n                stats[key]['mean'] = (stats[key]['mean'] + value) / 2\n                stats[key]['std'] = np.std([stats[key]['mean'], value])\n    \n    axes = []\n    for key in stats:\n        values = [d[key] for d in data if key in d]\n        if values:\n            mean_value = np.mean(values)\n            std_value = np.std(values)\n            fig, ax = plt.subplots()\n            ax.hist(values, bins=10, alpha=0.7, color='blue')\n            ax.axvline(mean_value, color='red', linestyle='dashed', linewidth=1)\n            ax.set_title(f'Statistics of {key}')\n            ax.set_ylabel('Value')\n            axes.append(ax)\n    \n    return stats, axes", "raw_solution": "from collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data cannot be empty.\")\n    \n    if not all(isinstance(d, dict) for d in data):\n        raise TypeError(\"Input must be a list of dictionaries.\")\n    \n    if not all(all(isinstance(v, (int, float)) for v in d.values()) for d in data):\n        raise TypeError(\"All dictionary values must be numeric.\")\n    \n    stats = defaultdict(lambda: {'mean': None, 'std': None})\n    \n    for d in data:\n        for key, value in d.items():\n            if stats[key]['mean'] is None:\n                stats[key]['mean'] = value\n                stats[key]['std'] = 0\n            else:\n                stats[key]['mean'] = (stats[key]['mean'] + value) / 2\n                stats[key]['std'] = np.std([stats[key]['mean'], value])\n    \n    axes = []\n    for key in stats:\n        values = [d[key] for d in data if key in d]\n        if values:\n            mean_value = np.mean(values)\n            std_value = np.std(values)\n            fig, ax = plt.subplots()\n            ax.hist(values, bins=10, alpha=0.7, color='blue')\n            ax.axvline(mean_value, color='red', linestyle='dashed', linewidth=1)\n            ax.set_title(f'Statistics of {key}')\n            ax.set_ylabel('Value')\n            axes.append(ax)\n    \n    return stats, axes\n```\n"}
{"task_id": "BigCodeBench/525", "solution": "import json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(input_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    result = defaultdict(lambda: {'mean': None, 'median': None})\n    plots = []\n\n    for item in data:\n        for key, values in item.items():\n            if key not in result:\n                result[key] = {'mean': np.mean(values), 'median': np.median(values)}\n\n            fig, ax = plt.subplots()\n            ax.bar(['Mean', 'Median'], [result[key]['mean'], result[key]['median']])\n            ax.set_title(f'{key} - Mean vs Median')\n            plots.append(ax)\n\n    return result, plots", "raw_solution": "import json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(input_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    result = defaultdict(lambda: {'mean': None, 'median': None})\n    plots = []\n\n    for item in data:\n        for key, values in item.items():\n            if key not in result:\n                result[key] = {'mean': np.mean(values), 'median': np.median(values)}\n\n            fig, ax = plt.subplots()\n            ax.bar(['Mean', 'Median'], [result[key]['mean'], result[key]['median']])\n            ax.set_title(f'{key} - Mean vs Median')\n            plots.append(ax)\n\n    return result, plots\n```\nThis script reads a JSON file, calculates the mean and median for each key in the dictionaries, and visualizes the results using bar charts. The function returns a dictionary with the mean and median for each key and a list of matplotlib axes objects for the plots.\n"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    stats = defaultdict(lambda: {'values': []})\n\n    for record in data:\n        for key, value in record.items():\n            if isinstance(value, (int, float)):\n                stats[key]['values'].append(value)\n\n    stats_df = pd.DataFrame(stats).transpose()\n    stats_df['mean'] = stats_df['values'].apply(lambda x: np.mean(x) if x else np.nan)\n    stats_df['median'] = stats_df['values'].apply(lambda x: np.median(x) if x else np.nan)\n\n    stats_df = stats_df[['mean', 'median']]\n\n    return stats_df", "raw_solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    stats = defaultdict(lambda: {'values': []})\n\n    for record in data:\n        for key, value in record.items():\n            if isinstance(value, (int, float)):\n                stats[key]['values'].append(value)\n\n    stats_df = pd.DataFrame(stats).transpose()\n    stats_df['mean'] = stats_df['values'].apply(lambda x: np.mean(x) if x else np.nan)\n    stats_df['median'] = stats_df['values'].apply(lambda x: np.median(x) if x else np.nan)\n\n    stats_df = stats_df[['mean', 'median']]\n\n    return stats_df\n```\nThis script reads a JSON file, processes the data to calculate mean and median for each key, and returns a Pandas DataFrame with the results. The function handles missing values and non-numeric data appropriately.\n"}
{"task_id": "BigCodeBench/527", "solution": "import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file: str) -> plt.Axes:\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Convert data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate mean and median for each key\n    results = defaultdict(lambda: {'mean': None, 'median': None})\n    for key in df.keys():\n        results[key]['mean'] = np.mean(df[key])\n        results[key]['median'] = np.median(df[key])\n\n    # Create a box plot\n    ax = sns.boxplot(data=df)\n    ax.set_title('Values for Each Key')\n\n    return results, ax", "raw_solution": "import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file: str) -> plt.Axes:\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Convert data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate mean and median for each key\n    results = defaultdict(lambda: {'mean': None, 'median': None})\n    for key in df.keys():\n        results[key]['mean'] = np.mean(df[key])\n        results[key]['median'] = np.median(df[key])\n\n    # Create a box plot\n    ax = sns.boxplot(data=df)\n    ax.set_title('Values for Each Key')\n\n    return results, ax\n```\n"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    try:\n        # Read the CSV file\n        with open(file_path, mode='r') as file:\n            reader = csv.reader(file)\n            data = list(reader)\n        \n        # Create a DataFrame\n        df = pd.DataFrame(data[1:], columns=data[0])\n        \n        # Find duplicates\n        duplicates = Counter(tuple(row) for row in df.values)\n        \n        # Plot duplicates\n        fig, ax = plt.subplots()\n        ax.bar(duplicates.keys(), duplicates.values())\n        ax.set_xlabel('Rows')\n        ax.set_ylabel('Count')\n        ax.set_title('Duplicate Rows Count')\n        plt.xticks(rotation=90)\n        \n        return dict(duplicates), ax\n\n    except Exception as e:\n        raise ValueError(f\"Error processing the file: {e}\")", "raw_solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    try:\n        # Read the CSV file\n        with open(file_path, mode='r') as file:\n            reader = csv.reader(file)\n            data = list(reader)\n        \n        # Create a DataFrame\n        df = pd.DataFrame(data[1:], columns=data[0])\n        \n        # Find duplicates\n        duplicates = Counter(tuple(row) for row in df.values)\n        \n        # Plot duplicates\n        fig, ax = plt.subplots()\n        ax.bar(duplicates.keys(), duplicates.values())\n        ax.set_xlabel('Rows')\n        ax.set_ylabel('Count')\n        ax.set_title('Duplicate Rows Count')\n        plt.xticks(rotation=90)\n        \n        return dict(duplicates), ax\n\n    except Exception as e:\n        raise ValueError(f\"Error processing the file: {e}\")\n```\n"}
{"task_id": "BigCodeBench/529", "solution": "from collections import Counter\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rolls, num_dice, plot_path=None, random_seed=0):\n    random.seed(random_seed)\n    outcomes = [random.randint(1, 6) for _ in range(num_rolls * num_dice)]\n    counter = Counter(outcomes)\n    \n    # Plotting the distribution\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values(), width=0.8, color='blue')\n    ax.set_xlabel('Dice Outcome')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Dice Rolls')\n    \n    if plot_path:\n        plt.savefig(plot_path)\n    \n    return counter, ax", "raw_solution": "from collections import Counter\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(num_rolls, num_dice, plot_path=None, random_seed=0):\n    random.seed(random_seed)\n    outcomes = [random.randint(1, 6) for _ in range(num_rolls * num_dice)]\n    counter = Counter(outcomes)\n    \n    # Plotting the distribution\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values(), width=0.8, color='blue')\n    ax.set_xlabel('Dice Outcome')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Dice Rolls')\n    \n    if plot_path:\n        plt.savefig(plot_path)\n    \n    return counter, ax\n```\n"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty or 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame should not be empty and should contain 'name' and 'age' columns.\")\n    \n    if (df['age'] < 0).any():\n        raise ValueError(\"Age values should not be negative.\")\n    \n    df['age'] = df['age'].apply(lambda x: round(x))\n    \n    duplicates = df.duplicated(subset='name', keep=False)\n    duplicates_df = df[duplicates]\n    \n    if duplicates_df.empty:\n        return Counter(), None\n    \n    duplicates_counter = Counter(duplicates_df['age'])\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(duplicates_df['age'], bins=np.arange(duplicates_df['age'].min(), duplicates_df['age'].max() + 1) - 0.5, discrete=True)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution of Duplicate Names')\n    \n    return duplicates_counter, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty or 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame should not be empty and should contain 'name' and 'age' columns.\")\n    \n    if (df['age'] < 0).any():\n        raise ValueError(\"Age values should not be negative.\")\n    \n    df['age'] = df['age'].apply(lambda x: round(x))\n    \n    duplicates = df.duplicated(subset='name', keep=False)\n    duplicates_df = df[duplicates]\n    \n    if duplicates_df.empty:\n        return Counter(), None\n    \n    duplicates_counter = Counter(duplicates_df['age'])\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(duplicates_df['age'], bins=np.arange(duplicates_df['age'].min(), duplicates_df['age'].max() + 1) - 0.5, discrete=True)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution of Duplicate Names')\n    \n    return duplicates_counter, ax\n```\n"}
{"task_id": "BigCodeBench/531", "solution": "from collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    # Count duplicates\n    duplicates = Counter(tuple(row) for row in df.itertuples(index=False))\n\n    # Remove duplicates\n    df_unique = df.drop_duplicates()\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    df_unique['cluster'] = kmeans.fit_predict(df_unique[['x', 'y']])\n\n    # Plot clusters\n    ax = df_unique.plot.scatter(x='x', y='y', c='cluster', colormap='viridis')\n\n    return duplicates, df_unique, ax", "raw_solution": "from collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    # Count duplicates\n    duplicates = Counter(tuple(row) for row in df.itertuples(index=False))\n\n    # Remove duplicates\n    df_unique = df.drop_duplicates()\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    df_unique['cluster'] = kmeans.fit_predict(df_unique[['x', 'y']])\n\n    # Plot clusters\n    ax = df_unique.plot.scatter(x='x', y='y', c='cluster', colormap='viridis')\n\n    return duplicates, df_unique, ax\n\n# Example usage:\n# df = pd.DataFrame({'x': [1, 2, 2, 2, 3, 4], 'y': [1, 1, 1, 1, 3, 3]})\n# duplicates, df_clustered, ax = task_func(df, random_state=42)\n# print(df_clustered)\n# print(duplicates)\n```\n"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    # Count occurrences of each value\n    counter = Counter(df['value'])\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, density=True, alpha=0.6, color='g')\n    \n    # Fit a normal distribution and plot it\n    mu, std = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return counter, ax", "raw_solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    # Count occurrences of each value\n    counter = Counter(df['value'])\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, density=True, alpha=0.6, color='g')\n    \n    # Fit a normal distribution and plot it\n    mu, std = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return counter, ax\n```\n"}
{"task_id": "BigCodeBench/533", "solution": "import numpy as np\nimport secrets\nimport hashlib\nimport base64\ndef task_func(num, from_base, to_base, alphabet):\n    if from_base < 2 or to_base < 2:\n        raise ValueError(\"Both 'from_base' and 'to_base' must be greater than or equal to 2.\")\n    \n    try:\n        # Convert the number from the from_base to the base 10\n        num_decimal = int(num, from_base)\n    except ValueError:\n        raise ValueError(f\"The number '{num}' is not a valid number in base {from_base}.\")\n\n    # Generate a random salt\n    salt = secrets.token_bytes(16)\n    \n    # Hash the number using SHA-256\n    hash_object = hashlib.sha256(salt + num_decimal.to_bytes((num_decimal.bit_length() + 7) // 8, byteorder='big'))\n    hash_digest = hash_object.digest()\n    \n    # Encode the hash using base64\n    encoded_hash = base64.b64encode(hash_digest, alphabet=alphabet).decode('utf-8')\n    \n    return encoded_hash, base64.b64encode(salt).decode('utf-8')\nalphabet = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz+/\"", "raw_solution": "import numpy as np\nimport secrets\nimport hashlib\nimport base64\n\ndef task_func(num, from_base, to_base, alphabet):\n    if from_base < 2 or to_base < 2:\n        raise ValueError(\"Both 'from_base' and 'to_base' must be greater than or equal to 2.\")\n    \n    try:\n        # Convert the number from the from_base to the base 10\n        num_decimal = int(num, from_base)\n    except ValueError:\n        raise ValueError(f\"The number '{num}' is not a valid number in base {from_base}.\")\n\n    # Generate a random salt\n    salt = secrets.token_bytes(16)\n    \n    # Hash the number using SHA-256\n    hash_object = hashlib.sha256(salt + num_decimal.to_bytes((num_decimal.bit_length() + 7) // 8, byteorder='big'))\n    hash_digest = hash_object.digest()\n    \n    # Encode the hash using base64\n    encoded_hash = base64.b64encode(hash_digest, alphabet=alphabet).decode('utf-8')\n    \n    return encoded_hash, base64.b64encode(salt).decode('utf-8')\n\n# Example usage:\nalphabet = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz+/\"\nencoded, salt = task_func('A1', 16, 8, alphabet)"}
{"task_id": "BigCodeBench/534", "solution": "import numpy as np\nimport base64\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.asymmetric import rsa\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Convert the number from the source base to base 10\n    num_decimal = int(num, from_base)\n    \n    # Convert the decimal number to a byte array\n    num_bytes = (num_decimal).to_bytes((num_decimal.bit_length() + 7) // 8, byteorder='big')\n    \n    # Sign the byte array using the RSA private key\n    signature = private_key.sign(\n        num_bytes,\n        padding.PSS(\n            mgf=padding.MGF1(hashes.SHA256()),\n            salt_length=padding.PSS.MAX_LENGTH\n        ),\n        hashes.SHA256()\n    )\n    \n    # Encode the signature using the custom alphabet\n    encoded_signature = base64.b64encode(signature, alphabet).decode('utf-8')\n    \n    return encoded_signature", "raw_solution": "import numpy as np\nimport base64\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.asymmetric import rsa\n\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Convert the number from the source base to base 10\n    num_decimal = int(num, from_base)\n    \n    # Convert the decimal number to a byte array\n    num_bytes = (num_decimal).to_bytes((num_decimal.bit_length() + 7) // 8, byteorder='big')\n    \n    # Sign the byte array using the RSA private key\n    signature = private_key.sign(\n        num_bytes,\n        padding.PSS(\n            mgf=padding.MGF1(hashes.SHA256()),\n            salt_length=padding.PSS.MAX_LENGTH\n        ),\n        hashes.SHA256()\n    )\n    \n    # Encode the signature using the custom alphabet\n    encoded_signature = base64.b64encode(signature, alphabet).decode('utf-8')\n    \n    return encoded_signature\n\n# Example usage:"}
{"task_id": "BigCodeBench/535", "solution": "import sqlite3\nimport numpy as np\nfrom random import choice, seed\ndef task_func(db_path, table_name, num_entries, random_seed=None):\n    NAMES = ['John', 'jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 65)\n    HEIGHTS = range(150, 200)\n\n    if num_entries < 0:\n        raise ValueError(\"num_entries must be a non-negative integer\")\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    cursor.execute(f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        name TEXT,\n        age INTEGER,\n        height INTEGER\n    )\n    \"\"\")\n\n    # Insert random data into the table\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        cursor.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n\n    conn.commit()\n    inserted_rows = cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n    conn.close()\n\n    return inserted_rows", "raw_solution": "import sqlite3\nimport numpy as np\nfrom random import choice, seed\n\ndef task_func(db_path, table_name, num_entries, random_seed=None):\n    NAMES = ['John', 'jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 65)\n    HEIGHTS = range(150, 200)\n\n    if num_entries < 0:\n        raise ValueError(\"num_entries must be a non-negative integer\")\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    cursor.execute(f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        name TEXT,\n        age INTEGER,\n        height INTEGER\n    )\n    \"\"\")\n\n    # Insert random data into the table\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        cursor.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n\n    conn.commit()\n    inserted_rows = cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n    conn.close()\n\n    return inserted_rows\n\n# Example usage:\n# print(task_func('path_to_test.db', 'People', 100, random_seed=42))\n```\n"}
{"task_id": "BigCodeBench/536", "solution": "import sqlite3\nimport pandas as pd\nimport os\ndef task_func(db_name, table_name, csv_path=\"data.csv\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load the table into a pandas DataFrame\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n    \n    # Export the DataFrame to a CSV file\n    df.to_csv(csv_path, index=False)\n    \n    # Close the database connection\n    conn.close()\n    \n    # Return the absolute path of the CSV file\n    return os.path.abspath(csv_path)", "raw_solution": "import sqlite3\nimport pandas as pd\nimport os\n\ndef task_func(db_name, table_name, csv_path=\"data.csv\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load the table into a pandas DataFrame\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n    \n    # Export the DataFrame to a CSV file\n    df.to_csv(csv_path, index=False)\n    \n    # Close the database connection\n    conn.close()\n    \n    # Return the absolute path of the CSV file\n    return os.path.abspath(csv_path)\n```\n"}
{"task_id": "BigCodeBench/537", "solution": "import sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load data from the specified table into a pandas DataFrame\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n    \n    # Check if there are any negative ages in the data\n    if (df['age'] < 0).any():\n        raise ValueError(\"Data contains negative ages which is not allowed.\")\n    \n    # Plot the age distribution using seaborn\n    ax = sns.histplot(df['age'], bins=30, kde=True)\n    ax.set_title('Age Distribution')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Frequency')\n    \n    # Close the database connection\n    conn.close()\n    \n    return ax", "raw_solution": "import sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load data from the specified table into a pandas DataFrame\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n    \n    # Check if there are any negative ages in the data\n    if (df['age'] < 0).any():\n        raise ValueError(\"Data contains negative ages which is not allowed.\")\n    \n    # Plot the age distribution using seaborn\n    ax = sns.histplot(df['age'], bins=30, kde=True)\n    ax.set_title('Age Distribution')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Frequency')\n    \n    # Close the database connection\n    conn.close()\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/538", "solution": "import sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(db_name, table_name):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Close the connection\n    conn.close()\n    \n    # Check if there are at least two numerical columns\n    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n    if len(numerical_cols) < 2:\n        raise ValueError(\"Table must have at least two numerical columns.\")\n    \n    # Plot the relationship between the first and second numerical column\n    fig, ax = plt.subplots()\n    ax.scatter(df[numerical_cols[0]], df[numerical_cols[1]])\n    ax.set_xlabel(numerical_cols[0])\n    ax.set_ylabel(numerical_cols[1])\n    ax.set_title(f'Relationship between {numerical_cols[0]} and {numerical_cols[1]}')\n    \n    return ax", "raw_solution": "import sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(db_name, table_name):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Close the connection\n    conn.close()\n    \n    # Check if there are at least two numerical columns\n    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n    if len(numerical_cols) < 2:\n        raise ValueError(\"Table must have at least two numerical columns.\")\n    \n    # Plot the relationship between the first and second numerical column\n    fig, ax = plt.subplots()\n    ax.scatter(df[numerical_cols[0]], df[numerical_cols[1]])\n    ax.set_xlabel(numerical_cols[0])\n    ax.set_ylabel(numerical_cols[1])\n    ax.set_title(f'Relationship between {numerical_cols[0]} and {numerical_cols[1]}')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/539", "solution": "import sqlite3\nfrom random import choice, seed\nimport os\ndef task_func(db_name, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries must be a non-negative integer\")\n\n    NAMES = ['John', 'jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 66)\n    HEIGHTS = range(150, 201)\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n\n    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (name TEXT, age INTEGER, height INTEGER)\")\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        cursor.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n\n    conn.commit()\n    conn.close()\n\n    return os.path.abspath(db_name)", "raw_solution": "import sqlite3\nfrom random import choice, seed\nimport os\n\ndef task_func(db_name, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries must be a non-negative integer\")\n\n    NAMES = ['John', 'jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 66)\n    HEIGHTS = range(150, 201)\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n\n    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (name TEXT, age INTEGER, height INTEGER)\")\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        cursor.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n\n    conn.commit()\n    conn.close()\n\n    return os.path.abspath(db_name)\n```\n"}
{"task_id": "BigCodeBench/540", "solution": "from collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(list_of_menuitems, title=\"Menu Distribution\", color=\"blue\", width=1.0):\n    # Flatten the list of menu items\n    flat_menuitems = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count occurrences of each menu item\n    counter = Counter(flat_menuitems)\n    \n    # Prepare data for plotting\n    labels, values = zip(*counter.items())\n    indices = np.arange(len(labels))\n    \n    # Plotting the histogram\n    fig, ax = plt.subplots()\n    ax.bar(indices, values, width=width, color=color)\n    ax.set_xticks(indices)\n    ax.set_xticklabels(labels, rotation=45)\n    ax.set_title(title)\n    ax.set_xlabel('Menu Items')\n    ax.set_ylabel('Frequency')\n    \n    return ax", "raw_solution": "from collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(list_of_menuitems, title=\"Menu Distribution\", color=\"blue\", width=1.0):\n    # Flatten the list of menu items\n    flat_menuitems = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count occurrences of each menu item\n    counter = Counter(flat_menuitems)\n    \n    # Prepare data for plotting\n    labels, values = zip(*counter.items())\n    indices = np.arange(len(labels))\n    \n    # Plotting the histogram\n    fig, ax = plt.subplots()\n    ax.bar(indices, values, width=width, color=color)\n    ax.set_xticks(indices)\n    ax.set_xticklabels(labels, rotation=45)\n    ax.set_title(title)\n    ax.set_xlabel('Menu Items')\n    ax.set_ylabel('Frequency')\n    \n    return ax\n\n# Example usage:\n# ax = task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/541", "solution": "import os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\ndef task_func(package_name):\n    try:\n        # Import the package dynamically\n        package = importlib.import_module(package_name)\n    except ImportError as e:\n        raise ImportError(f\"Package '{package_name}' not found. Please install it using 'pip install {package_name}'\") from e\n\n    # Get the path of the package\n    package_path = package.__path__ if hasattr(package, '__path__') else None\n\n    # Add the package path to the system path if it's not already there\n    if package_path and package_path[0] not in sys.path:\n        sys.path.append(package_path[0])\n\n    # List all modules in the package\n    modules = [module.name for module in iter_modules(package_path)]\n\n    return modules", "raw_solution": "import os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\n\ndef task_func(package_name):\n    try:\n        # Import the package dynamically\n        package = importlib.import_module(package_name)\n    except ImportError as e:\n        raise ImportError(f\"Package '{package_name}' not found. Please install it using 'pip install {package_name}'\") from e\n\n    # Get the path of the package\n    package_path = package.__path__ if hasattr(package, '__path__') else None\n\n    # Add the package path to the system path if it's not already there\n    if package_path and package_path[0] not in sys.path:\n        sys.path.append(package_path[0])\n\n    # List all modules in the package\n    modules = [module.name for module in iter_modules(package_path)]\n\n    return modules\n\n# Test cases"}
{"task_id": "BigCodeBench/542", "solution": "import hashlib\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS, seed=42):\n    random.seed(seed)\n    selected_key = random.choice(hex_keys)\n    \n    try:\n        # Convert hexadecimal string to a floating-point number\n        float_num = struct.unpack('!f', bytes.fromhex(selected_key))[0]\n    except struct.error:\n        raise ValueError(\"Invalid hexadecimal string provided.\")\n    \n    # Calculate MD5 hash of the floating-point number\n    hash_object = hashlib.md5(struct.pack('!f', float_num))\n    return hash_object.hexdigest()", "raw_solution": "import hashlib\nimport random\nimport struct\n\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_keys=KEYS, seed=42):\n    random.seed(seed)\n    selected_key = random.choice(hex_keys)\n    \n    try:\n        # Convert hexadecimal string to a floating-point number\n        float_num = struct.unpack('!f', bytes.fromhex(selected_key))[0]\n    except struct.error:\n        raise ValueError(\"Invalid hexadecimal string provided.\")\n    \n    # Calculate MD5 hash of the floating-point number\n    hash_object = hashlib.md5(struct.pack('!f', float_num))\n    return hash_object.hexdigest()\n\n# Example usage:\n# print(task_func())\n```\n"}
{"task_id": "BigCodeBench/543", "solution": "import base64\nimport os\nimport random\ndef task_func():\n    # Generate a random float number\n    random_float = random.random()\n\n    # Convert the float to hexadecimal\n    hex_representation = hex(int(random_float * (2**32)))\n\n    # Encode the hexadecimal representation in base64\n    base64_encoded = base64.b64encode(hex_representation.encode('utf-8')).decode('utf-8')\n\n    return base64_encoded", "raw_solution": "import base64\nimport os\nimport random\n\ndef task_func():\n    # Generate a random float number\n    random_float = random.random()\n\n    # Convert the float to hexadecimal\n    hex_representation = hex(int(random_float * (2**32)))\n\n    # Encode the hexadecimal representation in base64\n    base64_encoded = base64.b64encode(hex_representation.encode('utf-8')).decode('utf-8')\n\n    return base64_encoded\n\n# Test cases"}
{"task_id": "BigCodeBench/544", "solution": "import struct\nimport zlib\nKEY = '470FC614'\ndef task_func(hex_string=KEY):\n    \"\"\"\n    Ek diya gaya hex string ko float number mein badalta hai aur phir binary32 float number ko compress karta hai.\n\n    Params:\n    - hex_string (str, optional): hex string jise badla jaana hai. Default 470FC614.\n\n    Returns:\n        bytes: Compressed float number.\n\n    Reqs:\n    - struct\n    - zlib\n\n    Examples:\n    - >>> task_func(\"470FC614\")\n    - b'x\\x9c\\xf3\\xeb\\x93\\xef\\x01\\x00\\x03\\xb0\\x01\\x88'\n    - >>> task_func(\"ABCD1234\")\n    - b'x\\x9c\\xf3\\xd7>+\\x04\\x00\\x03m\\x01Z'\n\n    \"\"\"\n    # Convert hex string to float\n    float_num = struct.unpack('!f', bytes.fromhex(hex_string))[0]\n\n    # Pack the float into bytes\n    packed_float = struct.pack('!f', float_num)\n\n    # Compress the packed float bytes\n    compressed_float = zlib.compress(packed_float)\n\n    return compressed_float", "raw_solution": "import struct\nimport zlib\n\n# Constants\nKEY = '470FC614'\n\ndef task_func(hex_string=KEY):\n    \"\"\"\n    Ek diya gaya hex string ko float number mein badalta hai aur phir binary32 float number ko compress karta hai.\n\n    Params:\n    - hex_string (str, optional): hex string jise badla jaana hai. Default 470FC614.\n\n    Returns:\n        bytes: Compressed float number.\n\n    Reqs:\n    - struct\n    - zlib\n\n    Examples:\n    - >>> task_func(\"470FC614\")\n    - b'x\\x9c\\xf3\\xeb\\x93\\xef\\x01\\x00\\x03\\xb0\\x01\\x88'\n    - >>> task_func(\"ABCD1234\")\n    - b'x\\x9c\\xf3\\xd7>+\\x04\\x00\\x03m\\x01Z'\n\n    \"\"\"\n    # Convert hex string to float\n    float_num = struct.unpack('!f', bytes.fromhex(hex_string))[0]\n\n    # Pack the float into bytes\n    packed_float = struct.pack('!f', float_num)\n\n    # Compress the packed float bytes\n    compressed_float = zlib.compress(packed_float)\n\n    return compressed_float\n```\n"}
{"task_id": "BigCodeBench/545", "solution": "import codecs\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS):\n    \"\"\"\n    Ek list se hex strings se ek random float number generate karein aur phir float number ko utf-8 mein encode karein.\n\n    Params:\n    - hex_keys (list of str): Ek hexadecimal strings ki list jis se chuna ja sake.\n\n    Returns:\n        bytes: utf-8 encoded float number.\n\n    Reqs:\n    - struct\n    - codecs\n    - random\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func()\n    - b'36806.078125'\n\n    \"\"\"\n    # Generate a random float number\n    random_float = random.uniform(0, 10000)\n    \n    # Convert the float number to a byte array\n    byte_array = struct.pack('f', random_float)\n    \n    # Encode the byte array to utf-8\n    utf8_encoded = codecs.encode(byte_array, 'utf-8')\n    \n    return utf8_encoded", "raw_solution": "import codecs\nimport random\nimport struct\n\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_keys=KEYS):\n    \"\"\"\n    Ek list se hex strings se ek random float number generate karein aur phir float number ko utf-8 mein encode karein.\n\n    Params:\n    - hex_keys (list of str): Ek hexadecimal strings ki list jis se chuna ja sake.\n\n    Returns:\n        bytes: utf-8 encoded float number.\n\n    Reqs:\n    - struct\n    - codecs\n    - random\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func()\n    - b'36806.078125'\n\n    \"\"\"\n    # Generate a random float number\n    random_float = random.uniform(0, 10000)\n    \n    # Convert the float number to a byte array\n    byte_array = struct.pack('f', random_float)\n    \n    # Encode the byte array to utf-8\n    utf8_encoded = codecs.encode(byte_array, 'utf-8')\n    \n    return utf8_encoded\n```\n"}
{"task_id": "BigCodeBench/546", "solution": "from collections import OrderedDict\nfrom prettytable import PrettyTable\ndef task_func(my_dict):\n    # Sort the dictionary by keys\n    sorted_dict = OrderedDict(sorted(my_dict.items()))\n\n    # Create a PrettyTable object\n    table = PrettyTable()\n    table.field_names = [\"Key\", \"Value\"]\n\n    # Populate the table with sorted dictionary items\n    for key, value in sorted_dict.items():\n        table.add_row([key, value])\n\n    return table", "raw_solution": "from collections import OrderedDict\nfrom prettytable import PrettyTable\n\ndef task_func(my_dict):\n    # Sort the dictionary by keys\n    sorted_dict = OrderedDict(sorted(my_dict.items()))\n\n    # Create a PrettyTable object\n    table = PrettyTable()\n    table.field_names = [\"Key\", \"Value\"]\n\n    # Populate the table with sorted dictionary items\n    for key, value in sorted_dict.items():\n        table.add_row([key, value])\n\n    return table\n\n# Test cases"}
{"task_id": "BigCodeBench/547", "solution": "import hashlib\nimport os\nimport base64\ndef task_func(password: str, salt_length: int = 8) -> str:\n    \"\"\"\n    password ko salt aur SHA-256 ka upyog kar ke encrypt karein, phir result ko base64 mein encode karein.\n\n    Params:\n    - password (str): encrypt kiye jaane wala password.\n    - salt_length (int, optional): optional salt ki int. default 8 hai.\n\n    Returns:\n        str: base64 format mein encrypted password.\n\n    Reqs:\n    - base64\n    - hashlib\n    - os\n\n    Examples:\n    - >>> isinstance(task_func('my_password'), str)\n    - True\n\n    \"\"\"\n    # Generate a random salt\n    salt = os.urandom(salt_length)\n    \n    # Create a new SHA-256 hash object\n    hash_obj = hashlib.sha256()\n    \n    # Update the hash object with the password and salt\n    hash_obj.update(password.encode('utf-8'))\n    hash_obj.update(salt)\n    \n    # Get the hashed password\n    hashed_password = hash_obj.digest()\n    \n    # Encode the hashed password with base64\n    encoded_password = base64.b64encode(hashed_password + salt)\n    \n    return encoded_password.decode('utf-8')", "raw_solution": "import hashlib\nimport os\nimport base64\n\ndef task_func(password: str, salt_length: int = 8) -> str:\n    \"\"\"\n    password ko salt aur SHA-256 ka upyog kar ke encrypt karein, phir result ko base64 mein encode karein.\n\n    Params:\n    - password (str): encrypt kiye jaane wala password.\n    - salt_length (int, optional): optional salt ki int. default 8 hai.\n\n    Returns:\n        str: base64 format mein encrypted password.\n\n    Reqs:\n    - base64\n    - hashlib\n    - os\n\n    Examples:\n    - >>> isinstance(task_func('my_password'), str)\n    - True\n\n    \"\"\"\n    # Generate a random salt\n    salt = os.urandom(salt_length)\n    \n    # Create a new SHA-256 hash object\n    hash_obj = hashlib.sha256()\n    \n    # Update the hash object with the password and salt\n    hash_obj.update(password.encode('utf-8'))\n    hash_obj.update(salt)\n    \n    # Get the hashed password\n    hashed_password = hash_obj.digest()\n    \n    # Encode the hashed password with base64\n    encoded_password = base64.b64encode(hashed_password + salt)\n    \n    return encoded_password.decode('utf-8')\n```\n"}
{"task_id": "BigCodeBench/548", "solution": "import random\nimport string\nimport base64\nimport zlib\ndef task_func(string_length=100):\n    # Generate a random string of specified length\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=string_length))\n    \n    # Compress the random string using zlib\n    compressed_data = zlib.compress(random_string.encode('utf-8'))\n    \n    # Encode the compressed data using base64\n    encoded_data = base64.b64encode(compressed_data).decode('utf-8')\n    \n    return encoded_data", "raw_solution": "import random\nimport string\nimport base64\nimport zlib\n\ndef task_func(string_length=100):\n    # Generate a random string of specified length\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=string_length))\n    \n    # Compress the random string using zlib\n    compressed_data = zlib.compress(random_string.encode('utf-8'))\n    \n    # Encode the compressed data using base64\n    encoded_data = base64.b64encode(compressed_data).decode('utf-8')\n    \n    return encoded_data\n\n# Example usage:\n# random.seed(1)\n# compressed_string = task_func(50)\n# print(compressed_string)\n```\n"}
{"task_id": "BigCodeBench/549", "solution": "import base64\nimport pandas as pd\ndef task_func(df):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Convert the DataFrame to a CSV format\n    csv_bytes = df.to_csv(index=False).encode('utf-8')\n    \n    # Encode the CSV bytes to a base64 string\n    base64_bytes = base64.b64encode(csv_bytes)\n    base64_string = base64_bytes.decode('utf-8')\n    \n    return base64_string\ndf = {'A': [1, 2, 3], 'B': [4, 5, 6]}", "raw_solution": "import base64\nimport pandas as pd\n\ndef task_func(df):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Convert the DataFrame to a CSV format\n    csv_bytes = df.to_csv(index=False).encode('utf-8')\n    \n    # Encode the CSV bytes to a base64 string\n    base64_bytes = base64.b64encode(csv_bytes)\n    base64_string = base64_bytes.decode('utf-8')\n    \n    return base64_string\n\n# Example usage:\ndf = {'A': [1, 2, 3], 'B': [4, 5, 6]}\nencoded_df = task_func(df)"}
{"task_id": "BigCodeBench/550", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(list_of_menuitems):\n    # Flatten the list of menu items\n    flat_menuitems = [item for sublist in list_of_menuitems for item in sublist]\n    \n    # Count the occurrences of each menu item\n    menuitem_counts = Counter(flat_menuitems)\n    \n    # Create a DataFrame from the counts\n    df = pd.DataFrame.from_dict(menuitem_counts, orient='index', columns=['Count'])\n    \n    return df", "raw_solution": "from collections import Counter\nimport pandas as pd\n\ndef task_func(list_of_menuitems):\n    # Flatten the list of menu items\n    flat_menuitems = [item for sublist in list_of_menuitems for item in sublist]\n    \n    # Count the occurrences of each menu item\n    menuitem_counts = Counter(flat_menuitems)\n    \n    # Create a DataFrame from the counts\n    df = pd.DataFrame.from_dict(menuitem_counts, orient='index', columns=['Count'])\n    \n    return df\n\n# Example usage:\n# result = task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n# print(result.loc['Pizza', 'Count'])  # Output: 2\n# print(result.loc['Coke', 'Count'])  # Output: 2\n```\n"}
{"task_id": "BigCodeBench/551", "solution": "from collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\ndef task_func(list_of_menuitems):\n    # Flatten the list of menu items\n    flat_menuitems = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_menuitems)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(counter.items(), columns=['Menu Item', 'Frequency'])\n\n    # Plot the frequencies using seaborn\n    if not df.empty:\n        plt.figure(figsize=(10, 6))\n        ax = sns.barplot(x='Menu Item', y='Frequency', data=df)\n        ax.set_title('Menu Item Frequency')\n        ax.set_ylabel('Frequency')\n        ax.set_xlabel('Menu Item')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return ax\n    else:\n        return None", "raw_solution": "from collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(list_of_menuitems):\n    # Flatten the list of menu items\n    flat_menuitems = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_menuitems)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(counter.items(), columns=['Menu Item', 'Frequency'])\n\n    # Plot the frequencies using seaborn\n    if not df.empty:\n        plt.figure(figsize=(10, 6))\n        ax = sns.barplot(x='Menu Item', y='Frequency', data=df)\n        ax.set_title('Menu Item Frequency')\n        ax.set_ylabel('Frequency')\n        ax.set_xlabel('Menu Item')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return ax\n    else:\n        return None\n```\n"}
{"task_id": "BigCodeBench/552", "solution": "import collections\nimport itertools\nimport matplotlib.pyplot as plt\nITEMS = ['apple', 'banana']\ndef task_func(a, b, items=ITEMS):\n    # Count frequency of each item in both lists\n    combined_items = a + b\n    frequency = collections.Counter(combined_items)\n\n    # Filter frequency based on provided items\n    filtered_frequency = {item: frequency.get(item, 0) for item in items}\n\n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(filtered_frequency.keys(), filtered_frequency.values())\n    ax.set_xlabel('Items')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Items in Lists')\n\n    return ax", "raw_solution": "import collections\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS = ['apple', 'banana']\n\ndef task_func(a, b, items=ITEMS):\n    # Count frequency of each item in both lists\n    combined_items = a + b\n    frequency = collections.Counter(combined_items)\n\n    # Filter frequency based on provided items\n    filtered_frequency = {item: frequency.get(item, 0) for item in items}\n\n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(filtered_frequency.keys(), filtered_frequency.values())\n    ax.set_xlabel('Items')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Items in Lists')\n\n    return ax\n\n# Example usage:\n# ax = task_func(['apple', 'banana', 'cherry'], ['date', 'elderberry', 'apple', 'banana', 'cherry'])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(a, b):\n    # Create a DataFrame with random values\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), columns=b)\n\n    # Set the index of the DataFrame to the list 'a'\n    df.index = a\n\n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n\n    # Set the title and labels\n    ax.set_title('Bar Chart')\n    ax.set_xlabel('Rows')\n    ax.set_ylabel('Values')\n\n    # Return the axes object\n    return ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Create a DataFrame with random values\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), columns=b)\n\n    # Set the index of the DataFrame to the list 'a'\n    df.index = a\n\n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n\n    # Set the title and labels\n    ax.set_title('Bar Chart')\n    ax.set_xlabel('Rows')\n    ax.set_ylabel('Values')\n\n    # Return the axes object\n    return ax\n```\n"}
{"task_id": "BigCodeBench/554", "solution": "import numpy as np\nimport random\ndef task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL):\n    \"\"\"\n    ek palindrome sentence str karta hai jo ek specified pool se random words ka upyog karta hai. sentence ki length random roop se\n    minimum (MIN_WORDS) and maximum (MAX_WORDS) words ki number ke beech chuni jaati hai. function yah sunishchit karta hai ki\n    sentence aage and peechhe same padha jae.\n    jaanch karein ki str sentence ki length specified range ke bheetar hai ya nahi.\n    >>> sentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)\n    >>> MIN_WORDS <= len(sentence.split()) <= MAX_WORDS\n    True\n\n    Params:\n    - MIN_WORDS (int): palindrome sentence mein words ki minimum number.\n    - MAX_WORDS (int): palindrome sentence mein words ki maximum number.\n    - WORDS_POOL (list): palindrome str karne ke liye chune jaane wale words ki list.\n\n    Returns:\n        str: str palindrome sentence.\n\n    Reqs:\n    - numpy\n    - random\n\n    Examples:\n    - udaaharan:\n    - ek palindrome sentence str karein and jaanchen ki yah indeed ek palindrome hai.\n    - >>> MIN_WORDS, MAX_WORDS, WORDS_POOL = 3, 10, ['apple', 'banana', 'racecar', 'world', 'level', 'madam', 'radar', 'rotor']\n    - >>> sentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)\n    - >>> re_sentence = \" \".join(sentence.split()[::-1])\n    - >>> sentence == re_sentence\n    - True\n\n    \"\"\"\n    # Generate a random number of words between MIN_WORDS and MAX_WORDS\n    num_words = random.randint(MIN_WORDS, MAX_WORDS)\n    \n    # Select random words from the WORDS_POOL\n    words = random.choices(WORDS_POOL, k=num_words)\n    \n    # Join the words to form a sentence\n    sentence = \" \".join(words)\n    \n    # Check if the sentence is a palindrome\n    reversed_sentence = \" \".join(sentence.split()[::-1])\n    assert sentence == reversed_sentence, \"The sentence is not a palindrome.\"\n    \n    return sentence", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL):\n    \"\"\"\n    ek palindrome sentence str karta hai jo ek specified pool se random words ka upyog karta hai. sentence ki length random roop se\n    minimum (MIN_WORDS) and maximum (MAX_WORDS) words ki number ke beech chuni jaati hai. function yah sunishchit karta hai ki\n    sentence aage and peechhe same padha jae.\n    jaanch karein ki str sentence ki length specified range ke bheetar hai ya nahi.\n    >>> sentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)\n    >>> MIN_WORDS <= len(sentence.split()) <= MAX_WORDS\n    True\n\n    Params:\n    - MIN_WORDS (int): palindrome sentence mein words ki minimum number.\n    - MAX_WORDS (int): palindrome sentence mein words ki maximum number.\n    - WORDS_POOL (list): palindrome str karne ke liye chune jaane wale words ki list.\n\n    Returns:\n        str: str palindrome sentence.\n\n    Reqs:\n    - numpy\n    - random\n\n    Examples:\n    - udaaharan:\n    - ek palindrome sentence str karein and jaanchen ki yah indeed ek palindrome hai.\n    - >>> MIN_WORDS, MAX_WORDS, WORDS_POOL = 3, 10, ['apple', 'banana', 'racecar', 'world', 'level', 'madam', 'radar', 'rotor']\n    - >>> sentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)\n    - >>> re_sentence = \" \".join(sentence.split()[::-1])\n    - >>> sentence == re_sentence\n    - True\n\n    \"\"\"\n    # Generate a random number of words between MIN_WORDS and MAX_WORDS\n    num_words = random.randint(MIN_WORDS, MAX_WORDS)\n    \n    # Select random words from the WORDS_POOL\n    words = random.choices(WORDS_POOL, k=num_words)\n    \n    # Join the words to form a sentence\n    sentence = \" \".join(words)\n    \n    # Check if the sentence is a palindrome\n    reversed_sentence = \" \".join(sentence.split()[::-1])\n    assert sentence == reversed_sentence, \"The sentence is not a palindrome.\"\n    \n    return sentence\n\n# Example usage:\nMIN_WORDS, MAX_WORDS, WORDS_POOL = 3, 10, ['apple', 'banana', 'racecar', 'world', 'level', 'madam', 'radar', 'rotor']\nsentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)"}
{"task_id": "BigCodeBench/555", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef task_func(a, b):\n    # Create a Pandas DataFrame from the two lists\n    df = pd.DataFrame({'a': a, 'b': b})\n\n    # Calculate the Pearson correlation coefficient\n    correlation, _ = stats.pearsonr(df['a'], df['b'])\n\n    # Create a scatter plot with a regression line\n    fig, ax = plt.subplots()\n    ax.scatter(df['a'], df['b'], label='Data points')\n    ax.plot(df['a'], df['a'], color='red', label='Regression line')  # Simple linear regression line\n    ax.set_xlabel('a')\n    ax.set_ylabel('b')\n    ax.set_title('Scatter plot with regression line')\n    ax.legend()\n\n    return correlation, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef task_func(a, b):\n    # Create a Pandas DataFrame from the two lists\n    df = pd.DataFrame({'a': a, 'b': b})\n\n    # Calculate the Pearson correlation coefficient\n    correlation, _ = stats.pearsonr(df['a'], df['b'])\n\n    # Create a scatter plot with a regression line\n    fig, ax = plt.subplots()\n    ax.scatter(df['a'], df['b'], label='Data points')\n    ax.plot(df['a'], df['a'], color='red', label='Regression line')  # Simple linear regression line\n    ax.set_xlabel('a')\n    ax.set_ylabel('b')\n    ax.set_title('Scatter plot with regression line')\n    ax.legend()\n\n    return correlation, ax\n```\n"}
{"task_id": "BigCodeBench/556", "solution": "import numpy as np\nimport random\nfrom difflib import SequenceMatcher\ndef task_func(s, min_length, max_length, letters):\n    # Generate a random string of length between min_length and max_length\n    length = random.randint(min_length, max_length)\n    generated_s = ''.join(random.choice(letters) for _ in range(length))\n\n    # Calculate similarity between the generated string and the provided string\n    similarity = SequenceMatcher(None, s, generated_s).ratio()\n\n    # Check if the similarity is above a certain threshold (0.5 in this case)\n    is_similar = similarity >= 0.5\n\n    return generated_s, is_similar\ns = 'apple'\nmin_length = 5\nmax_length = 10\nletters = 'abcdefghijklmnopqrstuvwxyz'", "raw_solution": "import numpy as np\nimport random\nfrom difflib import SequenceMatcher\n\ndef task_func(s, min_length, max_length, letters):\n    # Generate a random string of length between min_length and max_length\n    length = random.randint(min_length, max_length)\n    generated_s = ''.join(random.choice(letters) for _ in range(length))\n\n    # Calculate similarity between the generated string and the provided string\n    similarity = SequenceMatcher(None, s, generated_s).ratio()\n\n    # Check if the similarity is above a certain threshold (0.5 in this case)\n    is_similar = similarity >= 0.5\n\n    return generated_s, is_similar\n\n# Example usage:\ns = 'apple'\nmin_length = 5\nmax_length = 10\nletters = 'abcdefghijklmnopqrstuvwxyz'\ngenerated_s, is_similar = task_func(s, min_length, max_length, letters)\n\n# Test cases to verify the solution correctness\nassert len(generated_s) >= min_length and len(generated_s) <= max_length, \"Generated string length is out of range.\"\nassert isinstance(is_similar, bool), \"The result should be a boolean value.\""}
{"task_id": "BigCodeBench/557", "solution": "import numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\ndef task_func(s_list, plot_path=None):\n    if not isinstance(s_list, list) or not all(isinstance(s, str) for s in s_list):\n        raise ValueError(\"Input must be a list of strings.\")\n    \n    if len(s_list) == 1:\n        return [np.nan]\n\n    avg_scores = []\n    for i, s1 in enumerate(s_list):\n        total_similarity = 0\n        for s2 in s_list:\n            if i != s_list.index(s2):\n                total_similarity += SequenceMatcher(None, s1, s2).ratio()\n        avg_scores.append(total_similarity / (len(s_list) - 1))\n\n    if plot_path:\n        plt.figure(figsize=(10, 5))\n        plt.plot(avg_scores, marker='o')\n        plt.title('Average Similarity Scores')\n        plt.xlabel('String Index')\n        plt.ylabel('Average Similarity')\n        plt.grid(True)\n        plt.savefig(plot_path)\n        plt.close()\n\n    return avg_scores\ns_list = ['apple', 'apples', 'ape', 'app', 'april']", "raw_solution": "import numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\n\ndef task_func(s_list, plot_path=None):\n    if not isinstance(s_list, list) or not all(isinstance(s, str) for s in s_list):\n        raise ValueError(\"Input must be a list of strings.\")\n    \n    if len(s_list) == 1:\n        return [np.nan]\n\n    avg_scores = []\n    for i, s1 in enumerate(s_list):\n        total_similarity = 0\n        for s2 in s_list:\n            if i != s_list.index(s2):\n                total_similarity += SequenceMatcher(None, s1, s2).ratio()\n        avg_scores.append(total_similarity / (len(s_list) - 1))\n\n    if plot_path:\n        plt.figure(figsize=(10, 5))\n        plt.plot(avg_scores, marker='o')\n        plt.title('Average Similarity Scores')\n        plt.xlabel('String Index')\n        plt.ylabel('Average Similarity')\n        plt.grid(True)\n        plt.savefig(plot_path)\n        plt.close()\n\n    return avg_scores\n\n# Example usage:\ns_list = ['apple', 'apples', 'ape', 'app', 'april']\navg_scores = task_func(s_list, 'similarity_plot.png')"}
{"task_id": "BigCodeBench/558", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(a, b, columns=['A', 'B']):\n    # Create a DataFrame from the input lists\n    data = pd.DataFrame({columns[0]: a, columns[1]: b})\n\n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n\n    # Standardize the data\n    standardized_data = scaler.fit_transform(data)\n\n    # Create a DataFrame from the standardized data\n    standardized_df = pd.DataFrame(standardized_data, columns=columns)\n\n    # Plot the standardized data\n    ax = standardized_df.plot(kind='bar')\n\n    # Return the standardized DataFrame and the plot axes\n    return standardized_df, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(a, b, columns=['A', 'B']):\n    # Create a DataFrame from the input lists\n    data = pd.DataFrame({columns[0]: a, columns[1]: b})\n\n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n\n    # Standardize the data\n    standardized_data = scaler.fit_transform(data)\n\n    # Create a DataFrame from the standardized data\n    standardized_df = pd.DataFrame(standardized_data, columns=columns)\n\n    # Plot the standardized data\n    ax = standardized_df.plot(kind='bar')\n\n    # Return the standardized DataFrame and the plot axes\n    return standardized_df, ax\n```\n"}
{"task_id": "BigCodeBench/559", "solution": "import pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\ndef task_func(a, b):\n    # Calculate the euclidean distance\n    euclidean_distance = distance.euclidean(a, b)\n\n    # Create a Pandas DataFrame\n    df = pd.DataFrame({'A': a, 'B': b})\n\n    # Plot the values\n    ax = df.plot(kind='line')\n    ax.set_title('Line Plot of Lists A and B')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    return euclidean_distance, df, ax", "raw_solution": "import pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\n\ndef task_func(a, b):\n    # Calculate the euclidean distance\n    euclidean_distance = distance.euclidean(a, b)\n\n    # Create a Pandas DataFrame\n    df = pd.DataFrame({'A': a, 'B': b})\n\n    # Plot the values\n    ax = df.plot(kind='line')\n    ax.set_title('Line Plot of Lists A and B')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    return euclidean_distance, df, ax\n```\n"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    # Convert the data string into a pandas DataFrame\n    data_list = data.split(',')\n    data_dict = {'Date': [], 'Value': []}\n    for item in data_list:\n        date_str, value_str = item.split('-')\n        data_dict['Date'].append(datetime.strptime(date_str, '%Y-%m'))\n        data_dict['Value'].append(int(value_str))\n    df = pd.DataFrame(data_dict)\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Value'], marker='o')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    ax.set_title(f'{df[\"Date\"].iloc[0].year} ke liye monthly data')\n    ax.grid(True)\n\n    return ax\ndata = '2022-01-100,2022-02-200,2022-03-150,2022-04-300,2022-05-250,2022-06-350,2022-07-400,2022-08-450,2022-09-500,2022-10-550,2022-11-600,2022-12-650'", "raw_solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Convert the data string into a pandas DataFrame\n    data_list = data.split(',')\n    data_dict = {'Date': [], 'Value': []}\n    for item in data_list:\n        date_str, value_str = item.split('-')\n        data_dict['Date'].append(datetime.strptime(date_str, '%Y-%m'))\n        data_dict['Value'].append(int(value_str))\n    df = pd.DataFrame(data_dict)\n\n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Value'], marker='o')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    ax.set_title(f'{df[\"Date\"].iloc[0].year} ke liye monthly data')\n    ax.grid(True)\n\n    return ax\n\n# Example usage:\ndata = '2022-01-100,2022-02-200,2022-03-150,2022-04-300,2022-05-250,2022-06-350,2022-07-400,2022-08-450,2022-09-500,2022-10-550,2022-11-600,2022-12-650'\nax = task_func(data)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/561", "solution": "import pytz\nfrom dateutil import parser\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Convert a datetime from one timezone to another.\n\n    Params:\n    - date_str (str): A date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    - from_tz (str): The timezone of the input date string.\n    - to_tz (str): The timezone to convert the date to.\n\n    Returns:\n        str: The converted date string in \"yyyy-mm-dd hh:mm:ss\" format.\n\n    Reqs:\n    - pytz\n    - dateutil.parser\n\n    Examples:\n    - >>> task_func('2022-03-01 12:00:00', 'UTC', 'America/New_York')\n    - '2022-03-01 07:00:00'\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    naive_datetime = parser.parse(date_str)\n    \n    # Get the source and target timezones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_datetime = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the target timezone\n    converted_datetime = aware_datetime.astimezone(to_timezone)\n    \n    # Return the converted datetime as a string\n    return converted_datetime.strftime('%Y-%m-%d %H:%M:%S')", "raw_solution": "import pytz\nfrom dateutil import parser\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Convert a datetime from one timezone to another.\n\n    Params:\n    - date_str (str): A date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    - from_tz (str): The timezone of the input date string.\n    - to_tz (str): The timezone to convert the date to.\n\n    Returns:\n        str: The converted date string in \"yyyy-mm-dd hh:mm:ss\" format.\n\n    Reqs:\n    - pytz\n    - dateutil.parser\n\n    Examples:\n    - >>> task_func('2022-03-01 12:00:00', 'UTC', 'America/New_York')\n    - '2022-03-01 07:00:00'\n\n    \"\"\"\n    # Parse the date string into a datetime object\n    naive_datetime = parser.parse(date_str)\n    \n    # Get the source and target timezones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_datetime = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the target timezone\n    converted_datetime = aware_datetime.astimezone(to_timezone)\n    \n    # Return the converted datetime as a string\n    return converted_datetime.strftime('%Y-%m-%d %H:%M:%S')\n```\n"}
{"task_id": "BigCodeBench/562", "solution": "import os\nimport ctypes\nimport sys\nimport subprocess\ndef task_func(filepath):\n    if not isinstance(filepath, str):\n        raise TypeError(\"The filepath must be a string.\")\n    \n    if not os.path.isfile(filepath):\n        raise OSError(\"The filepath does not exist or is not a file.\")\n    \n    # Load the DLL file\n    try:\n        ctypes.CDLL(filepath)\n    except OSError as e:\n        raise OSError(f\"Failed to load the DLL file: {e}\")\n    \n    # Get system information\n    system_name = os.name\n    node_name = os.uname().nodename\n    release = os.uname().release\n    version = os.uname().version\n    machine = os.uname().machine\n    python_version = sys.version\n    pip_version = subprocess.check_output(['pip', '--version']).decode('utf-8').strip()\n    \n    # Print system information\n    print(f\"system: {system_name}\")\n    print(f\"node name: {node_name}\")\n    print(f\"release: {release}\")\n    print(f\"version: {version}\")\n    print(f\"machine: {machine}\")\n    print(f\"python version: {python_version}\")\n    print(f\"PIP version: {pip_version}\")\n    \n    return filepath", "raw_solution": "import os\nimport ctypes\nimport sys\nimport subprocess\n\ndef task_func(filepath):\n    if not isinstance(filepath, str):\n        raise TypeError(\"The filepath must be a string.\")\n    \n    if not os.path.isfile(filepath):\n        raise OSError(\"The filepath does not exist or is not a file.\")\n    \n    # Load the DLL file\n    try:\n        ctypes.CDLL(filepath)\n    except OSError as e:\n        raise OSError(f\"Failed to load the DLL file: {e}\")\n    \n    # Get system information\n    system_name = os.name\n    node_name = os.uname().nodename\n    release = os.uname().release\n    version = os.uname().version\n    machine = os.uname().machine\n    python_version = sys.version\n    pip_version = subprocess.check_output(['pip', '--version']).decode('utf-8').strip()\n    \n    # Print system information\n    print(f\"system: {system_name}\")\n    print(f\"node name: {node_name}\")\n    print(f\"release: {release}\")\n    print(f\"version: {version}\")\n    print(f\"machine: {machine}\")\n    print(f\"python version: {python_version}\")\n    print(f\"PIP version: {pip_version}\")\n    \n    return filepath\n\n# Example usage:\n# task_func('libc.so.6')\n```\n"}
{"task_id": "BigCodeBench/563", "solution": "import ctypes\nimport os\nimport shutil\nimport glob\ndef task_func(filepath, destination_dir):\n    # Load the DLL file\n    loaded_dll = ctypes.CDLL(filepath)\n    \n    # Move all DLL files from the same directory to the destination directory\n    for file in glob.glob(os.path.join(os.path.dirname(filepath), '*.dll')):\n        shutil.move(file, destination_dir)\n    \n    # Return the name of the loaded DLL file\n    return loaded_dll._name", "raw_solution": "import ctypes\nimport os\nimport shutil\nimport glob\n\ndef task_func(filepath, destination_dir):\n    # Load the DLL file\n    loaded_dll = ctypes.CDLL(filepath)\n    \n    # Move all DLL files from the same directory to the destination directory\n    for file in glob.glob(os.path.join(os.path.dirname(filepath), '*.dll')):\n        shutil.move(file, destination_dir)\n    \n    # Return the name of the loaded DLL file\n    return loaded_dll._name\n\n# Example usage:\n# destination = 'destination_dir'\n# print(task_func('example.dll', destination))\n# assert isinstance(task_func('example.dll', destination), str)\n```\nPlease note that the example usage and the assertion are for demonstration purposes and may not work in a real scenario without the actual 'example.dll' file and 'destination_dir' directory. The doctest will vary based on the system and the availability of the specified DLL file.\n"}
{"task_id": "BigCodeBench/564", "solution": "import os\nimport ctypes\nfrom datetime import datetime\nimport pytz\ndef task_func(filepath):\n    # Load the DLL file\n    lib = ctypes.CDLL(filepath)\n    \n    # Get the file's metadata\n    file_stats = os.stat(filepath)\n    modification_time = datetime.fromtimestamp(file_stats.st_mtime, pytz.utc)\n    creation_time = datetime.fromtimestamp(file_stats.st_ctime, pytz.utc)\n    \n    # Prepare the metadata dictionary\n    metadata = {\n        'nirmaan time': modification_time,\n        'modification time': modification_time,\n        'aakaar': creation_time\n    }\n    \n    # Return the name of the loaded library and the metadata\n    return lib._name, metadata", "raw_solution": "import os\nimport ctypes\nfrom datetime import datetime\nimport pytz\n\ndef task_func(filepath):\n    # Load the DLL file\n    lib = ctypes.CDLL(filepath)\n    \n    # Get the file's metadata\n    file_stats = os.stat(filepath)\n    modification_time = datetime.fromtimestamp(file_stats.st_mtime, pytz.utc)\n    creation_time = datetime.fromtimestamp(file_stats.st_ctime, pytz.utc)\n    \n    # Prepare the metadata dictionary\n    metadata = {\n        'nirmaan time': modification_time,\n        'modification time': modification_time,\n        'aakaar': creation_time\n    }\n    \n    # Return the name of the loaded library and the metadata\n    return lib._name, metadata\n\n# Test cases"}
{"task_id": "BigCodeBench/565", "solution": "import ctypes\nimport hashlib\nimport binascii\ndef task_func(filepath):\n    # Load the DLL file\n    lib = ctypes.CDLL(filepath)\n    \n    # Calculate MD5 hash\n    md5_hash = hashlib.md5()\n    with open(filepath, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(chunk)\n    md5_hex = binascii.hexlify(md5_hash.digest()).decode('utf-8')\n    \n    # Calculate SHA256 hash\n    sha256_hash = hashlib.sha256()\n    with open(filepath, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(chunk)\n    sha256_hex = binascii.hexlify(sha256_hash.digest()).decode('utf-8')\n    \n    # Print the hashes\n    print(f\"MD5 hash: {md5_hex}\")\n    print(f\"SHA256 hash: {sha256_hex}\")\n    \n    # Return the name of the loaded DLL\n    return lib._name", "raw_solution": "import ctypes\nimport hashlib\nimport binascii\n\ndef task_func(filepath):\n    # Load the DLL file\n    lib = ctypes.CDLL(filepath)\n    \n    # Calculate MD5 hash\n    md5_hash = hashlib.md5()\n    with open(filepath, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(chunk)\n    md5_hex = binascii.hexlify(md5_hash.digest()).decode('utf-8')\n    \n    # Calculate SHA256 hash\n    sha256_hash = hashlib.sha256()\n    with open(filepath, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(chunk)\n    sha256_hex = binascii.hexlify(sha256_hash.digest()).decode('utf-8')\n    \n    # Print the hashes\n    print(f\"MD5 hash: {md5_hex}\")\n    print(f\"SHA256 hash: {sha256_hex}\")\n    \n    # Return the name of the loaded DLL\n    return lib._name\n\n# Example usage:\n# result = task_func('path_to_dll_file')\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/566", "solution": "import inspect\nimport types\ndef task_func(f):\n    \"\"\"\n    This function takes a function 'f' as input and returns a dictionary containing the function's name,\n    whether it is a lambda function, its arguments, default values, and annotations.\n\n    Parameters:\n    - f (function): The function to inspect.\n\n    Returns:\n    - dict: A dictionary containing the function's name, whether it is a lambda function, its arguments,\n            default values, and annotations.\n\n    Requirements:\n    - inspect\n    - types\n\n    Examples:\n    - >>> def sample_function(x, y=5): return x + y\n    - >>> result = task_func(sample_function)\n    - >>> 'sample_function' == result['function_name'] and len(result['args']) == 2\n    - True\n    - >>> lambda_func = lambda x: x * 2\n    - >>> task_func(lambda_func)['is_lambda']\n    - True\n    \"\"\"\n\n    # Initialize the result dictionary\n    result = {\n        'function_name': f.__name__,\n        'is_lambda': isinstance(f, types.LambdaType),\n        'args': [],\n        'defaults': [],\n        'annotations': {}\n    }\n\n    # Get the signature of the function\n    sig = inspect.signature(f)\n    for name, param in sig.parameters.items():\n        result['args'].append(name)\n        if param.default is not inspect.Parameter.empty:\n            result['defaults'].append(param.default)\n        if param.annotation is not inspect.Parameter.empty:\n            result['annotations'][name] = param.annotation\n\n    return result\ndef sample_function(x, y=5): return x + y", "raw_solution": "import inspect\nimport types\n\ndef task_func(f):\n    \"\"\"\n    This function takes a function 'f' as input and returns a dictionary containing the function's name,\n    whether it is a lambda function, its arguments, default values, and annotations.\n\n    Parameters:\n    - f (function): The function to inspect.\n\n    Returns:\n    - dict: A dictionary containing the function's name, whether it is a lambda function, its arguments,\n            default values, and annotations.\n\n    Requirements:\n    - inspect\n    - types\n\n    Examples:\n    - >>> def sample_function(x, y=5): return x + y\n    - >>> result = task_func(sample_function)\n    - >>> 'sample_function' == result['function_name'] and len(result['args']) == 2\n    - True\n    - >>> lambda_func = lambda x: x * 2\n    - >>> task_func(lambda_func)['is_lambda']\n    - True\n    \"\"\"\n\n    # Initialize the result dictionary\n    result = {\n        'function_name': f.__name__,\n        'is_lambda': isinstance(f, types.LambdaType),\n        'args': [],\n        'defaults': [],\n        'annotations': {}\n    }\n\n    # Get the signature of the function\n    sig = inspect.signature(f)\n    for name, param in sig.parameters.items():\n        result['args'].append(name)\n        if param.default is not inspect.Parameter.empty:\n            result['defaults'].append(param.default)\n        if param.annotation is not inspect.Parameter.empty:\n            result['annotations'][name] = param.annotation\n\n    return result\n\n# Test cases\ndef sample_function(x, y=5): return x + y\ndef lambda_func(x): return x * 2\n\n# Test for sample_function\nresult = task_func(sample_function)\nassert 'sample_function' == result['function_name'] and len(result['args']) == 2\n\n# Test for lambda_func\nresult = task_func(lambda_func)\nassert result['is_lambda'] == True\n```\n"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the string data into a pandas Series\n    data_series = pd.Series(data.split('-')).astype(int)\n\n    # Create a histogram\n    ax = data_series.plot.hist(bins=np.arange(data_series.min(), data_series.max()+2) - 0.5, rwidth=0.8)\n\n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Values Histogram')\n\n    return ax\ndata = '1-2-3-4-5-6-7-8-9-10'", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the string data into a pandas Series\n    data_series = pd.Series(data.split('-')).astype(int)\n\n    # Create a histogram\n    ax = data_series.plot.hist(bins=np.arange(data_series.min(), data_series.max()+2) - 0.5, rwidth=0.8)\n\n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Values Histogram')\n\n    return ax\n\n# Example usage:\ndata = '1-2-3-4-5-6-7-8-9-10'\nax = task_func(data)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/568", "solution": "import inspect\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(f_list):\n    if any(inspect.islambda(f) for f in f_list):\n        raise ValueError(\"The function should not be a lambda function.\")\n    \n    data = []\n    for f in f_list:\n        num_args = len(inspect.signature(f).parameters)\n        data.append({'Function Name': f.__name__, 'Number of Arguments': num_args})\n    \n    df = pd.DataFrame(data)\n    \n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Function Name'], df['Number of Arguments'], color='skyblue')\n    plt.xlabel('Function Name')\n    plt.ylabel('Number of Arguments')\n    plt.title('Number of Arguments per Function')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n    \n    return df\ndef f(x):\n    return x * x", "raw_solution": "import inspect\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(f_list):\n    if any(inspect.islambda(f) for f in f_list):\n        raise ValueError(\"The function should not be a lambda function.\")\n    \n    data = []\n    for f in f_list:\n        num_args = len(inspect.signature(f).parameters)\n        data.append({'Function Name': f.__name__, 'Number of Arguments': num_args})\n    \n    df = pd.DataFrame(data)\n    \n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Function Name'], df['Number of Arguments'], color='skyblue')\n    plt.xlabel('Function Name')\n    plt.ylabel('Number of Arguments')\n    plt.title('Number of Arguments per Function')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n    \n    return df\n\n# Example usage:\ndef f(x):\n    return x * x\n\ndef g(x, y=2):\n    return x * y\n\n# task_func([f, g])\n```\nThis script first checks if any of the functions in the list are lambda functions, and if so, it raises a ValueError. It then inspects each function to count the number of arguments and stores this information in a pandas DataFrame. Finally, it uses matplotlib to create a bar chart of the number of arguments per function.\n"}
{"task_id": "BigCodeBench/569", "solution": "import inspect\nimport types\nimport math\ndef task_func(f):\n    \"\"\"\n    This function inspects a given function 'f' and returns a dictionary containing the function's name, the square root of the number of arguments, and whether there are any lambda functions in the default values.\n\n    Parameters:\n    - f (function): The function to inspect.\n\n    Returns:\n    - dict: A dictionary with the function's name, the square root of the number of arguments, and whether there are any lambda functions in the default values.\n\n    Requirements:\n    - inspect\n    - types\n    - math\n\n    Examples:\n    - >>> def sample_function(x, y=2): return x + y\n    - >>> result = task_func(sample_function)\n    - >>> 'sample_function' == result['function_name'] and result['sqrt_args'] == math.sqrt(2)\n    - True\n    - >>> lambda_func = lambda x: x * 2\n    - >>> task_func(lambda_func)['lambda_in_defaults'] == 0\n    - True\n    \"\"\"\n\n    # Initialize the result dictionary\n    result = {\n        'function_name': f.__name__,\n        'sqrt_args': math.sqrt(len(inspect.signature(f).parameters)),\n        'lambda_in_defaults': 0\n    }\n\n    # Check for lambda functions in default values\n    for name, param in inspect.signature(f).parameters.items():\n        if param.default is inspect.Parameter.empty:\n            continue\n        if isinstance(param.default, types.LambdaType):\n            result['lambda_in_defaults'] = 1\n            break\n\n    return result", "raw_solution": "import inspect\nimport types\nimport math\n\ndef task_func(f):\n    \"\"\"\n    This function inspects a given function 'f' and returns a dictionary containing the function's name, the square root of the number of arguments, and whether there are any lambda functions in the default values.\n\n    Parameters:\n    - f (function): The function to inspect.\n\n    Returns:\n    - dict: A dictionary with the function's name, the square root of the number of arguments, and whether there are any lambda functions in the default values.\n\n    Requirements:\n    - inspect\n    - types\n    - math\n\n    Examples:\n    - >>> def sample_function(x, y=2): return x + y\n    - >>> result = task_func(sample_function)\n    - >>> 'sample_function' == result['function_name'] and result['sqrt_args'] == math.sqrt(2)\n    - True\n    - >>> lambda_func = lambda x: x * 2\n    - >>> task_func(lambda_func)['lambda_in_defaults'] == 0\n    - True\n    \"\"\"\n\n    # Initialize the result dictionary\n    result = {\n        'function_name': f.__name__,\n        'sqrt_args': math.sqrt(len(inspect.signature(f).parameters)),\n        'lambda_in_defaults': 0\n    }\n\n    # Check for lambda functions in default values\n    for name, param in inspect.signature(f).parameters.items():\n        if param.default is inspect.Parameter.empty:\n            continue\n        if isinstance(param.default, types.LambdaType):\n            result['lambda_in_defaults'] = 1\n            break\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/570", "solution": "import inspect\nimport types\nimport json\ndef task_func(f):\n    \"\"\"\n    Dee gayee  function kaa nirikshan kartaa hai aur JSON string ke roop mein iskee specifications detaa hai. Ismein\n    function kaa naam, arguments, default values, ek string format mein annotations, aur yah bataane vaalaa ek boolean\n    return hai ki yah ek lambda function hai.\n\n    Params:\n    - f (function): The function to inspect.\n\n    Returns:\n        str: Ek JSON string jismein function kee specifications return hain.\n\n    Reqs:\n    - inspect\n    - types\n    - json\n\n    Examples:\n    - Examples:\n    - >>> def sample_function(x, y=2): return x + y\n    - >>> 'sample_function' in task_func(sample_function)\n    - True\n    - >>> def sample_function2(x, y=2): return x * y\n    - >>> 'sample_function2' in task_func(sample_function2)\n    - True\n\n    \"\"\"\n    if not isinstance(f, types.FunctionType):\n        raise TypeError(\"Provided input is not a function\")\n\n    # Inspect the function\n    func_name = f.__name__\n    func_args = inspect.signature(f).parameters\n    func_defaults = {k: v.default for k, v in func_args.items() if v.default is not inspect.Parameter.empty}\n    func_annotations = f.__annotations__\n    is_lambda = isinstance(f, types.LambdaType)\n\n    # Create a dictionary of the function's details\n    func_details = {\n        \"name\": func_name,\n        \"arguments\": func_args,\n        \"defaults\": func_defaults,\n        \"annotations\": func_annotations,\n        \"is_lambda\": is_lambda\n    }\n\n    # Convert the dictionary to a JSON string\n    func_json = json.dumps(func_details, default=str)\n\n    return func_json", "raw_solution": "import inspect\nimport types\nimport json\n\ndef task_func(f):\n    \"\"\"\n    Dee gayee  function kaa nirikshan kartaa hai aur JSON string ke roop mein iskee specifications detaa hai. Ismein\n    function kaa naam, arguments, default values, ek string format mein annotations, aur yah bataane vaalaa ek boolean\n    return hai ki yah ek lambda function hai.\n\n    Params:\n    - f (function): The function to inspect.\n\n    Returns:\n        str: Ek JSON string jismein function kee specifications return hain.\n\n    Reqs:\n    - inspect\n    - types\n    - json\n\n    Examples:\n    - Examples:\n    - >>> def sample_function(x, y=2): return x + y\n    - >>> 'sample_function' in task_func(sample_function)\n    - True\n    - >>> def sample_function2(x, y=2): return x * y\n    - >>> 'sample_function2' in task_func(sample_function2)\n    - True\n\n    \"\"\"\n    if not isinstance(f, types.FunctionType):\n        raise TypeError(\"Provided input is not a function\")\n\n    # Inspect the function\n    func_name = f.__name__\n    func_args = inspect.signature(f).parameters\n    func_defaults = {k: v.default for k, v in func_args.items() if v.default is not inspect.Parameter.empty}\n    func_annotations = f.__annotations__\n    is_lambda = isinstance(f, types.LambdaType)\n\n    # Create a dictionary of the function's details\n    func_details = {\n        \"name\": func_name,\n        \"arguments\": func_args,\n        \"defaults\": func_defaults,\n        \"annotations\": func_annotations,\n        \"is_lambda\": is_lambda\n    }\n\n    # Convert the dictionary to a JSON string\n    func_json = json.dumps(func_details, default=str)\n\n    return func_json\n```\n"}
{"task_id": "BigCodeBench/571", "solution": "import inspect\nimport pandas as pd\ndef task_func(f_list, file_path):\n    if not isinstance(f_list, list) or not all(callable(f) for f in f_list):\n        raise ValueError(\"f_list must be a list of functions.\")\n    if not isinstance(file_path, str):\n        raise ValueError(\"file_path must be a string.\")\n\n    data = []\n    for f in f_list:\n        func_name = f.__name__\n        num_args = len(inspect.signature(f).parameters)\n        defaults = [param.default for param in inspect.signature(f).parameters.values() if param.default is not inspect.Parameter.empty]\n        annotations = {k: v for k, v in f.__annotations__.items() if k != 'return'}", "raw_solution": "import inspect\nimport pandas as pd\n\ndef task_func(f_list, file_path):\n    if not isinstance(f_list, list) or not all(callable(f) for f in f_list):\n        raise ValueError(\"f_list must be a list of functions.\")\n    if not isinstance(file_path, str):\n        raise ValueError(\"file_path must be a string.\")\n\n    data = []\n    for f in f_list:\n        func_name = f.__name__\n        num_args = len(inspect.signature(f).parameters)\n        defaults = [param.default for param in inspect.signature(f).parameters.values() if param.default is not inspect.Parameter.empty]\n        annotations = {k: v for k, v in f.__annotations__.items() if k != 'return'}\n        is_lambda = isinstance(f, lambda)\n        \n        data.append({\n            'Function Name': func_name,\n            'Number of Arguments': num_args,\n            'Defaults': defaults,\n            'Annotations': annotations,\n            'Is Lambda': is_lambda\n        })\n\n    df = pd.DataFrame(data)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError as e:\n        raise IOError(f\"Error writing to file: {e}\")\n\n# Example usage:\n# def f(x): return 2 * x\n# def g(x, y=2): return x * y\n# task_func([f, g], './function_info.csv')\n# os.remove('./function_info.csv')\n```\n"}
{"task_id": "BigCodeBench/572", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100):\n    # Generate two random arrays of integers\n    array1 = np.array([randint(0, 100) for _ in range(array_length)])\n    array2 = np.array([randint(0, 100) for _ in range(array_length)])\n\n    # Calculate the maximum values of the respective elements\n    max_values = np.maximum(array1, array2)\n\n    # Plot the line diagram\n    fig, ax = plt.subplots()\n    ax.plot(max_values, label='Maximum Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Maximum Value')\n    ax.set_title('Line Diagram of Maximum Values')\n    ax.legend()\n\n    return ax", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100):\n    # Generate two random arrays of integers\n    array1 = np.array([randint(0, 100) for _ in range(array_length)])\n    array2 = np.array([randint(0, 100) for _ in range(array_length)])\n\n    # Calculate the maximum values of the respective elements\n    max_values = np.maximum(array1, array2)\n\n    # Plot the line diagram\n    fig, ax = plt.subplots()\n    ax.plot(max_values, label='Maximum Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Maximum Value')\n    ax.set_title('Line Diagram of Maximum Values')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/573", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array_length=100):\n    # Generate two arrays of random numbers\n    array1 = np.random.rand(array_length)\n    array2 = np.random.rand(array_length)\n\n    # Calculate statistics\n    mean1 = np.mean(array1)\n    mean2 = np.mean(array2)\n    median1 = np.median(array1)\n    median2 = np.median(array2)\n    std1 = np.std(array1)\n    std2 = np.std(array2)\n\n    # Create a DataFrame to store the statistics\n    statistics = pd.DataFrame({\n        'Array1': [mean1, median1, std1],\n        'Array2': [mean2, median2, std2]\n    }, index=['Mean', 'Median', 'Standard Deviation'])\n\n    # Plot the statistics\n    ax = statistics.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Comparison of Statistics between Array1 and Array2')\n    ax.set_ylabel('Value')\n    ax.set_xlabel('Statistic')\n    plt.xticks(rotation=0)\n    plt.tight_layout()\n\n    return statistics, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(array_length=100):\n    # Generate two arrays of random numbers\n    array1 = np.random.rand(array_length)\n    array2 = np.random.rand(array_length)\n\n    # Calculate statistics\n    mean1 = np.mean(array1)\n    mean2 = np.mean(array2)\n    median1 = np.median(array1)\n    median2 = np.median(array2)\n    std1 = np.std(array1)\n    std2 = np.std(array2)\n\n    # Create a DataFrame to store the statistics\n    statistics = pd.DataFrame({\n        'Array1': [mean1, median1, std1],\n        'Array2': [mean2, median2, std2]\n    }, index=['Mean', 'Median', 'Standard Deviation'])\n\n    # Plot the statistics\n    ax = statistics.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Comparison of Statistics between Array1 and Array2')\n    ax.set_ylabel('Value')\n    ax.set_xlabel('Statistic')\n    plt.xticks(rotation=0)\n    plt.tight_layout()\n\n    return statistics, ax\n```\n"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate a noisy sign wave\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sign(np.sin(x)) + noise_level * np.random.randn(array_length)\n\n    # Define a simple linear function to fit the curve\n    def linear_func(x, a, b):\n        return a * x + b\n\n    # Fit the curve\n    popt, _ = curve_fit(linear_func, x, y)\n\n    # Plot the noisy data and the fitted curve\n    plt.figure()\n    plt.plot(x, y, 'b-', label='Noisy Data')\n    plt.plot(x, linear_func(x, *popt), 'r--', label='Fitted Curve')\n    plt.legend()\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.title('Noisy Sign Wave and Fitted Curve')\n    ax = plt.gca()\n    plt.show()\n\n    return ax", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate a noisy sign wave\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sign(np.sin(x)) + noise_level * np.random.randn(array_length)\n\n    # Define a simple linear function to fit the curve\n    def linear_func(x, a, b):\n        return a * x + b\n\n    # Fit the curve\n    popt, _ = curve_fit(linear_func, x, y)\n\n    # Plot the noisy data and the fitted curve\n    plt.figure()\n    plt.plot(x, y, 'b-', label='Noisy Data')\n    plt.plot(x, linear_func(x, *popt), 'r--', label='Fitted Curve')\n    plt.legend()\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.title('Noisy Sign Wave and Fitted Curve')\n    ax = plt.gca()\n    plt.show()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/575", "solution": "from random import shuffle\nimport pandas as pd\nimport numpy as np\nDEFAULT_GROUPS = 5\ndef task_func(l, n_groups=DEFAULT_GROUPS):\n    \"\"\"\n    Shuffles the list `l` and creates a DataFrame from the shuffled list.\n    Each row in the DataFrame takes the first `n_groups` elements from the list.\n\n    Params:\n    - l (list): The list of elements.\n    - n_groups (int): The number of groups. Default is 5.\n\n    Returns:\n    - pd.DataFrame: A shuffled DataFrame with the first `n_groups` elements in each row.\n\n    Reqs:\n    - pandas\n    - numpy\n    - random\n\n    Examples:\n    - df = task_func(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])\n    - df.shape == (5, 10)\n    - set(df.iloc[0]) == set(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])\n    \"\"\"\n    # Shuffle the list\n    shuffle(l)\n    \n    # Create a DataFrame from the shuffled list\n    df = pd.DataFrame([l[i:i+n_groups] for i in range(0, len(l), n_groups)])\n    \n    return df", "raw_solution": "from random import shuffle\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDEFAULT_GROUPS = 5\n\ndef task_func(l, n_groups=DEFAULT_GROUPS):\n    \"\"\"\n    Shuffles the list `l` and creates a DataFrame from the shuffled list.\n    Each row in the DataFrame takes the first `n_groups` elements from the list.\n\n    Params:\n    - l (list): The list of elements.\n    - n_groups (int): The number of groups. Default is 5.\n\n    Returns:\n    - pd.DataFrame: A shuffled DataFrame with the first `n_groups` elements in each row.\n\n    Reqs:\n    - pandas\n    - numpy\n    - random\n\n    Examples:\n    - df = task_func(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])\n    - df.shape == (5, 10)\n    - set(df.iloc[0]) == set(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])\n    \"\"\"\n    # Shuffle the list\n    shuffle(l)\n    \n    # Create a DataFrame from the shuffled list\n    df = pd.DataFrame([l[i:i+n_groups] for i in range(0, len(l), n_groups)])\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/576", "solution": "from random import shuffle, randint\nimport pandas as pd\ndef task_func(l, n_groups=5):\n    # Shuffle the list\n    shuffle(l)\n    \n    # Create a longer series by cycling through the list\n    series = pd.Series(l * n_groups)\n    \n    # Modify each element by taking a random number of characters from the start to the end\n    modified_series = series.apply(lambda x: x[randint(0, len(x) - 1):])\n    \n    return modified_series", "raw_solution": "from random import shuffle, randint\nimport pandas as pd\n\ndef task_func(l, n_groups=5):\n    # Shuffle the list\n    shuffle(l)\n    \n    # Create a longer series by cycling through the list\n    series = pd.Series(l * n_groups)\n    \n    # Modify each element by taking a random number of characters from the start to the end\n    modified_series = series.apply(lambda x: x[randint(0, len(x) - 1):])\n    \n    return modified_series\n\n# Example usage:\n# result = task_func(['ABC', 'DEF', 'GHI'])\n# print(isinstance(result, pd.Series))  # Check if the output is a pandas Series\n# print(len(result) == 15)  # Check if the length of the result is as expected\n```\n"}
{"task_id": "BigCodeBench/577", "solution": "import os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\ndef task_func(directory):\n    \"\"\"\n    This function processes all files in the given directory, normalizes their filenames to ASCII,\n    calculates their MD5 hashes, and records their sizes. It returns a dictionary with filenames as keys\n    and dictionaries containing their sizes and MD5 hashes as values.\n\n    Params:\n    - directory (str): The directory path to process.\n\n    Returns:\n        dict: A dictionary with filenames as keys and dictionaries as values, containing 'size' and 'MD5 hash'.\n\n    Reqs:\n    - os\n    - pathlib\n    - hashlib.md5\n    - unicodedata\n\n    Examples:\n    - >>> info = task_func('test')\n    - >>> type(info) == dict\n    - True\n    - >>> 'test.txt' in info\n    - True\n    \"\"\"\n    result = {}\n    for file_path in pathlib.Path(directory).iterdir():\n        if file_path.is_file():\n            # Normalize filename to ASCII\n            file_name = unicodedata.normalize('NFKD', file_path.name).encode('ascii', 'ignore').decode('ascii')\n            \n            # Calculate MD5 hash\n            with open(file_path, 'rb') as f:\n                file_hash = md5(f.read()).hexdigest()\n            \n            # Get file size\n            file_size = file_path.stat().st_size\n            \n            # Store in result dictionary\n            result[file_name] = {'size': file_size, 'MD5 hash': file_hash}\n    \n    return result", "raw_solution": "import os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\n\ndef task_func(directory):\n    \"\"\"\n    This function processes all files in the given directory, normalizes their filenames to ASCII,\n    calculates their MD5 hashes, and records their sizes. It returns a dictionary with filenames as keys\n    and dictionaries containing their sizes and MD5 hashes as values.\n\n    Params:\n    - directory (str): The directory path to process.\n\n    Returns:\n        dict: A dictionary with filenames as keys and dictionaries as values, containing 'size' and 'MD5 hash'.\n\n    Reqs:\n    - os\n    - pathlib\n    - hashlib.md5\n    - unicodedata\n\n    Examples:\n    - >>> info = task_func('test')\n    - >>> type(info) == dict\n    - True\n    - >>> 'test.txt' in info\n    - True\n    \"\"\"\n    result = {}\n    for file_path in pathlib.Path(directory).iterdir():\n        if file_path.is_file():\n            # Normalize filename to ASCII\n            file_name = unicodedata.normalize('NFKD', file_path.name).encode('ascii', 'ignore').decode('ascii')\n            \n            # Calculate MD5 hash\n            with open(file_path, 'rb') as f:\n                file_hash = md5(f.read()).hexdigest()\n            \n            # Get file size\n            file_size = file_path.stat().st_size\n            \n            # Store in result dictionary\n            result[file_name] = {'size': file_size, 'MD5 hash': file_hash}\n    \n    return result\n```\n"}
{"task_id": "BigCodeBench/578", "solution": "import unicodedata\nimport requests\nURL = 'https://api.github.com/users/'\ndef task_func(username):\n    response = requests.get(URL + username)\n    response.raise_for_status()  # Raises an HTTPError for bad responses\n\n    user_data = response.json()\n\n    normalized_data = {}\n    for key, value in user_data.items():\n        if isinstance(value, str):\n            normalized_data[key] = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n        else:\n            normalized_data[key] = value\n\n    return normalized_data", "raw_solution": "import unicodedata\nimport requests\n\nURL = 'https://api.github.com/users/'\n\ndef task_func(username):\n    response = requests.get(URL + username)\n    response.raise_for_status()  # Raises an HTTPError for bad responses\n\n    user_data = response.json()\n\n    normalized_data = {}\n    for key, value in user_data.items():\n        if isinstance(value, str):\n            normalized_data[key] = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n        else:\n            normalized_data[key] = value\n\n    return normalized_data\n```\n"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        with open(csv_file, newline='', encoding='utf-8') as csvfile:\n            reader = csv.reader(csvfile)\n            text = ' '.join(row[0] for row in reader)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    except IOError:\n        raise IOError(f\"There was an error reading the file {csv_file}.\")\n\n    # Normalize text to ASCII\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n\n    # Tokenize text into words\n    words = text.split()\n\n    # Count word frequencies\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Plot the frequencies\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Top 10 most common words')\n\n    return ax, most_common_words", "raw_solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        with open(csv_file, newline='', encoding='utf-8') as csvfile:\n            reader = csv.reader(csvfile)\n            text = ' '.join(row[0] for row in reader)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    except IOError:\n        raise IOError(f\"There was an error reading the file {csv_file}.\")\n\n    # Normalize text to ASCII\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n\n    # Tokenize text into words\n    words = text.split()\n\n    # Count word frequencies\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Plot the frequencies\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Top 10 most common words')\n\n    return ax, most_common_words\n```\n"}
{"task_id": "BigCodeBench/580", "solution": "import pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\nRANGE = 10000\nSIZE = 1000\nBIN_WIDTH = 100\ndef task_func():\n    # Generate random numbers\n    random_numbers = [random.randint(0, RANGE) for _ in range(SIZE)]\n\n    # Calculate moving average\n    moving_averages = [\n        statistics.mean(random_numbers[i-5:i+1]) for i in range(5, SIZE+1)\n    ]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'Random Numbers': random_numbers,\n        'Moving Average': moving_averages\n    })\n\n    # Plot histogram\n    plt.hist(df['Random Numbers'], bins=np.arange(0, RANGE+BIN_WIDTH, BIN_WIDTH), alpha=0.7, color='blue')\n    plt.title('Histogram of Random Numbers')\n    plt.xlabel('Random Number')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nRANGE = 10000  # The range within which random numbers are generated\nSIZE = 1000  # The number of random numbers to generate\nBIN_WIDTH = 100  # The width of bins for the histogram\n\n\ndef task_func():\n    # Generate random numbers\n    random_numbers = [random.randint(0, RANGE) for _ in range(SIZE)]\n\n    # Calculate moving average\n    moving_averages = [\n        statistics.mean(random_numbers[i-5:i+1]) for i in range(5, SIZE+1)\n    ]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'Random Numbers': random_numbers,\n        'Moving Average': moving_averages\n    })\n\n    # Plot histogram\n    plt.hist(df['Random Numbers'], bins=np.arange(0, RANGE+BIN_WIDTH, BIN_WIDTH), alpha=0.7, color='blue')\n    plt.title('Histogram of Random Numbers')\n    plt.xlabel('Random Number')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/581", "solution": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nSIZE = 1000\nPI = np.pi\ndef task_func(size=SIZE, frequency=1):\n    '''\n    Create a list of random sinusoidal values and plot them in a graph.\n    \n    Parameters:\n    - size (int): The number of points for the sinusoidal wave. Default is 1000.\n    - frequency (float): The frequency of the sinusoidal wave. Default is 1.\n    \n    Returns:\n    - Axes object: The plot of the sinusoidal wave.\n    \n    Requirements:\n    - random\n    - math\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> import matplotlib\n    >>> ax = task_func(size=1000, frequency=1)\n    >>> isinstance(ax, matplotlib.axes.Axes)\n    True\n    >>> len(ax.lines[0].get_ydata()) == 1000  # Verify the number of data points in the sinusoidal wave\n    True\n    >>> isinstance(ax.lines[0].get_ydata()[0], float)  # Check if y-values are floating-point numbers\n    True\n    '''\n    # Generate random values\n    random.seed(42)\n    x_values = np.linspace(0, 2 * PI, size)\n    y_values = [math.sin(frequency * x) + random.uniform(-0.1, 0.1) for x in x_values]\n\n    # Plot the values\n    fig, ax = plt.subplots()\n    ax.plot(x_values, y_values, label='Sinusoidal Wave')\n    ax.set_title('Random Sinusoidal Wave')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.legend()\n\n    return ax", "raw_solution": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n# Constants\nRANGE = 10000\nSIZE = 1000\nPI = np.pi\n\n\ndef task_func(size=SIZE, frequency=1):\n    '''\n    Create a list of random sinusoidal values and plot them in a graph.\n    \n    Parameters:\n    - size (int): The number of points for the sinusoidal wave. Default is 1000.\n    - frequency (float): The frequency of the sinusoidal wave. Default is 1.\n    \n    Returns:\n    - Axes object: The plot of the sinusoidal wave.\n    \n    Requirements:\n    - random\n    - math\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> import matplotlib\n    >>> ax = task_func(size=1000, frequency=1)\n    >>> isinstance(ax, matplotlib.axes.Axes)\n    True\n    >>> len(ax.lines[0].get_ydata()) == 1000  # Verify the number of data points in the sinusoidal wave\n    True\n    >>> isinstance(ax.lines[0].get_ydata()[0], float)  # Check if y-values are floating-point numbers\n    True\n    '''\n    # Generate random values\n    random.seed(42)\n    x_values = np.linspace(0, 2 * PI, size)\n    y_values = [math.sin(frequency * x) + random.uniform(-0.1, 0.1) for x in x_values]\n\n    # Plot the values\n    fig, ax = plt.subplots()\n    ax.plot(x_values, y_values, label='Sinusoidal Wave')\n    ax.set_title('Random Sinusoidal Wave')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a histogram of the data\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the PDF\n    pdf = stats.norm.pdf(bins, loc=0, scale=1)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n    \n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a histogram of the data\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the PDF\n    pdf = stats.norm.pdf(bins, loc=0, scale=1)\n    ax.plot(bins, pdf, 'k', linewidth=2)\n    \n    return fig\n```\n"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA keys\n    public_key, private_key = rsa.newkeys(512)\n\n    # Generate random password and nonce for AES encryption\n    password = get_random_bytes(16)\n    nonce = get_random_bytes(16)\n\n    # Encrypt the private key with AES\n    cipher = AES.new(password, AES.MODE_EAX, nonce)\n    encrypted_private_key = cipher.encrypt(private_key.export_key())\n\n    # Encode the encrypted private key and nonce to base64\n    encoded_encrypted_private_key = b64encode(encrypted_private_key)\n    encoded_nonce = b64encode(nonce)\n\n    # Generate a random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Write the encoded encrypted private key and nonce to a file\n    with open(filename, 'wb') as f:\n        f.write(encoded_encrypted_private_key)\n        f.write(b\"\\n\")\n        f.write(encoded_nonce)\n\n    return public_key, filename, password, nonce", "raw_solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA keys\n    public_key, private_key = rsa.newkeys(512)\n\n    # Generate random password and nonce for AES encryption\n    password = get_random_bytes(16)\n    nonce = get_random_bytes(16)\n\n    # Encrypt the private key with AES\n    cipher = AES.new(password, AES.MODE_EAX, nonce)\n    encrypted_private_key = cipher.encrypt(private_key.export_key())\n\n    # Encode the encrypted private key and nonce to base64\n    encoded_encrypted_private_key = b64encode(encrypted_private_key)\n    encoded_nonce = b64encode(nonce)\n\n    # Generate a random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Write the encoded encrypted private key and nonce to a file\n    with open(filename, 'wb') as f:\n        f.write(encoded_encrypted_private_key)\n        f.write(b\"\\n\")\n        f.write(encoded_nonce)\n\n    return public_key, filename, password, nonce\n\n# Example usage:\n# pub_key, filename, password, nonce = task_func()\n# print(isinstance(pub_key, rsa.PublicKey))  # Should print True\n# print(isinstance(filename, str))  # Should print True\n# print(isinstance(password, bytes))  # Should print True\n# print(isinstance(nonce, bytes))  # Should print True\n```\n"}
{"task_id": "BigCodeBench/584", "solution": "import rsa\nimport urllib.request\nfrom hashlib import sha256\ndef task_func(url):\n    try:\n        # Fetch the content from the URL\n        with urllib.request.urlopen(url) as response:\n            content = response.read()\n\n        # Generate SHA256 hash of the content\n        hash_value = sha256(content).hexdigest()\n\n        # Generate RSA keys\n        pub_key, priv_key = rsa.newkeys(512)\n\n        # Sign the hash using the private key\n        signed_hash = rsa.sign(content, priv_key, 'SHA-256')\n\n        # Verify the signature using the public key\n        rsa.verify(content, signed_hash, pub_key)\n\n        return pub_key, hash_value, signed_hash\n\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Failed to fetch data from URL: {e}\")\n    except rsa.pkcs1.VerificationError as e:\n        raise ValueError(f\"Verification failed: {e}\")", "raw_solution": "import rsa\nimport urllib.request\nfrom hashlib import sha256\n\ndef task_func(url):\n    try:\n        # Fetch the content from the URL\n        with urllib.request.urlopen(url) as response:\n            content = response.read()\n\n        # Generate SHA256 hash of the content\n        hash_value = sha256(content).hexdigest()\n\n        # Generate RSA keys\n        pub_key, priv_key = rsa.newkeys(512)\n\n        # Sign the hash using the private key\n        signed_hash = rsa.sign(content, priv_key, 'SHA-256')\n\n        # Verify the signature using the public key\n        rsa.verify(content, signed_hash, pub_key)\n\n        return pub_key, hash_value, signed_hash\n\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Failed to fetch data from URL: {e}\")\n    except rsa.pkcs1.VerificationError as e:\n        raise ValueError(f\"Verification failed: {e}\")\n\n# Example usage:\n# pub_key, hash_value, signed_hash = task_func('https://www.example.com')\n# print(isinstance(pub_key, rsa.PublicKey))  # Should print True\n# print(isinstance(hash_value, str))  # Should print True\n# print(isinstance(signed_hash, bytes))  # Should print True\n```\n"}
{"task_id": "BigCodeBench/585", "solution": "import rsa\nimport os\nimport zipfile\nfrom base64 import b64encode\ndef task_func(directory):\n    # Generate RSA keys\n    public_key, private_key = rsa.newkeys(512)\n\n    # Create a zip file name from the directory name\n    zipfile_name = f\"{directory.split('/')[-1]}.zip\"\n\n    # Create a zip file in write mode\n    with zipfile.ZipFile(zipfile_name, 'w') as zipf:\n        # Iterate over all files in the directory\n        for file in os.listdir(directory):\n            file_path = os.path.join(directory, file)\n\n            # Check if it is a file\n            if os.path.isfile(file_path):\n                # Read the file content\n                with open(file_path, 'rb') as f:\n                    file_data = f.read()\n\n                # Encrypt the file content using the public key\n                encrypted_data = rsa.encrypt(file_data, public_key)\n\n                # Encode the encrypted data to base64\n                encoded_data = b64encode(encrypted_data)\n\n                # Write the encoded data to the zip file\n                zipf.writestr(file, encoded_data)\n\n    return public_key, zipfile_name", "raw_solution": "import rsa\nimport os\nimport zipfile\nfrom base64 import b64encode\n\ndef task_func(directory):\n    # Generate RSA keys\n    public_key, private_key = rsa.newkeys(512)\n\n    # Create a zip file name from the directory name\n    zipfile_name = f\"{directory.split('/')[-1]}.zip\"\n\n    # Create a zip file in write mode\n    with zipfile.ZipFile(zipfile_name, 'w') as zipf:\n        # Iterate over all files in the directory\n        for file in os.listdir(directory):\n            file_path = os.path.join(directory, file)\n\n            # Check if it is a file\n            if os.path.isfile(file_path):\n                # Read the file content\n                with open(file_path, 'rb') as f:\n                    file_data = f.read()\n\n                # Encrypt the file content using the public key\n                encrypted_data = rsa.encrypt(file_data, public_key)\n\n                # Encode the encrypted data to base64\n                encoded_data = b64encode(encrypted_data)\n\n                # Write the encoded data to the zip file\n                zipf.writestr(file, encoded_data)\n\n    return public_key, zipfile_name\n```\n"}
{"task_id": "BigCodeBench/586", "solution": "import rsa\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # Generate Fernet key\n    key = Fernet.generate_key()\n    cipher = Fernet(key)\n\n    # Encrypt file content\n    encrypted_content = cipher.encrypt(file_content)\n\n    # Encrypt Fernet key with RSA public key\n    encrypted_key = rsa.encrypt(key, pub_key)\n\n    # Save encrypted content to a new file\n    encrypted_file = file_path + '.encrypted'\n    with open(encrypted_file, 'wb') as f:\n        f.write(encrypted_content)\n\n    # Save encrypted key to a new file\n    encrypted_key_file = file_path + '_key.encrypted'\n    with open(encrypted_key_file, 'wb') as f:\n        f.write(b64encode(encrypted_key))\n\n    return pub_key, encrypted_file, encrypted_key_file", "raw_solution": "import rsa\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # Generate Fernet key\n    key = Fernet.generate_key()\n    cipher = Fernet(key)\n\n    # Encrypt file content\n    encrypted_content = cipher.encrypt(file_content)\n\n    # Encrypt Fernet key with RSA public key\n    encrypted_key = rsa.encrypt(key, pub_key)\n\n    # Save encrypted content to a new file\n    encrypted_file = file_path + '.encrypted'\n    with open(encrypted_file, 'wb') as f:\n        f.write(encrypted_content)\n\n    # Save encrypted key to a new file\n    encrypted_key_file = file_path + '_key.encrypted'\n    with open(encrypted_key_file, 'wb') as f:\n        f.write(b64encode(encrypted_key))\n\n    return pub_key, encrypted_file, encrypted_key_file\n```\n"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Read the file to encrypt\n    with open(file_path, 'rb') as f:\n        data = f.read()\n\n    # Generate AES key and IV\n    aes_key = os.urandom(32)\n    iv = os.urandom(16)\n\n    # Create AES cipher\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Pad and encrypt the data\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pub_key)\n\n    # Save the encrypted data and encrypted AES key to files\n    encrypted_file_path = file_path + '.encrypted'\n    encrypted_key_file_path = file_path + '_key.encrypted'\n    \n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_data)\n\n    with open(encrypted_key_file_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pub_key, encrypted_file_path, encrypted_key_file_path", "raw_solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    pub_key, priv_key = rsa.newkeys(512)\n\n    # Read the file to encrypt\n    with open(file_path, 'rb') as f:\n        data = f.read()\n\n    # Generate AES key and IV\n    aes_key = os.urandom(32)\n    iv = os.urandom(16)\n\n    # Create AES cipher\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Pad and encrypt the data\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pub_key)\n\n    # Save the encrypted data and encrypted AES key to files\n    encrypted_file_path = file_path + '.encrypted'\n    encrypted_key_file_path = file_path + '_key.encrypted'\n    \n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_data)\n\n    with open(encrypted_key_file_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pub_key, encrypted_file_path, encrypted_key_file_path\n```\n"}
{"task_id": "BigCodeBench/588", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nRANGE = 100\nSIZE = 1000\ndef task_func():\n    \"\"\"\n    Ek dataframe do 'X' aur 'Y' columns ke saath banata hai, each one with a specified range, ke bheetar random integers se bhara hota hai,\n    aur in points ko ek scatter plot ka upyog karke plot karta hai. visualization ko Matplotlib ke upar Seaborn ka upyog karke banaya gaya hai.\n    Function ko saralta ke liye parameter-free design kiya gaya hai, jo configuration ke liye sthirank ka upyog karta hai.\n    No Parameters.\n\n    Returns:\n        pd.dataframe: Ek dataframe jismein 'X' aur 'Y' columns hote hain jismein utpann random integers hote hain.\n\n    Reqs:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> df = task_func()\n    - >>> isinstance(df, pd.dataframe)\n    - True\n    - >>> 'X' in df.columns and 'Y' in df.columns\n    - True\n    - >>> len(df)\n    - 1000\n    - >>> all(df['X'].between(0, RANGE - 1)) and all(df['Y'].between(0, RANGE - 1))\n    - True\n\n    \"\"\"\n    # Generating random integers for 'X' and 'Y' columns\n    data = {\n        'X': np.random.randint(0, RANGE, SIZE),\n        'Y': np.random.randint(0, RANGE, SIZE)\n    }\n    \n    # Creating a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Plotting the data\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='X', y='Y')\n    plt.title('Scatter Plot of Random Points')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n    \n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants defining the range of random integers and the size of the DataFrame\nRANGE = 100\nSIZE = 1000\n\n\ndef task_func():\n    \"\"\"\n    Ek dataframe do 'X' aur 'Y' columns ke saath banata hai, each one with a specified range, ke bheetar random integers se bhara hota hai,\n    aur in points ko ek scatter plot ka upyog karke plot karta hai. visualization ko Matplotlib ke upar Seaborn ka upyog karke banaya gaya hai.\n    Function ko saralta ke liye parameter-free design kiya gaya hai, jo configuration ke liye sthirank ka upyog karta hai.\n    No Parameters.\n\n    Returns:\n        pd.dataframe: Ek dataframe jismein 'X' aur 'Y' columns hote hain jismein utpann random integers hote hain.\n\n    Reqs:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> df = task_func()\n    - >>> isinstance(df, pd.dataframe)\n    - True\n    - >>> 'X' in df.columns and 'Y' in df.columns\n    - True\n    - >>> len(df)\n    - 1000\n    - >>> all(df['X'].between(0, RANGE - 1)) and all(df['Y'].between(0, RANGE - 1))\n    - True\n\n    \"\"\"\n    # Generating random integers for 'X' and 'Y' columns\n    data = {\n        'X': np.random.randint(0, RANGE, SIZE),\n        'Y': np.random.randint(0, RANGE, SIZE)\n    }\n    \n    # Creating a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Plotting the data\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='X', y='Y')\n    plt.title('Scatter Plot of Random Points')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/589", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\ndef task_func():\n    # Generating random 2D data points\n    data = np.random.rand(SIZE, 2) * RANGE\n\n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=CLUSTERS)\n    kmeans.fit(data)\n\n    # Plotting the clusters and centroids\n    plt.figure(figsize=(8, 6))\n    for i, color in zip(range(CLUSTERS), ['r', 'g', 'b', 'c', 'm']):\n        cluster_data = data[kmeans.labels_ == i]\n        plt.scatter(cluster_data[:, 0], cluster_data[:, 1], c=color, label=f'Cluster {i+1}')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', marker='X', label='Centroids')\n    plt.title('KMeans Clustering')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return data, kmeans", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\n# Constants for configuration\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\n\n\ndef task_func():\n    # Generating random 2D data points\n    data = np.random.rand(SIZE, 2) * RANGE\n\n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=CLUSTERS)\n    kmeans.fit(data)\n\n    # Plotting the clusters and centroids\n    plt.figure(figsize=(8, 6))\n    for i, color in zip(range(CLUSTERS), ['r', 'g', 'b', 'c', 'm']):\n        cluster_data = data[kmeans.labels_ == i]\n        plt.scatter(cluster_data[:, 0], cluster_data[:, 1], c=color, label=f'Cluster {i+1}')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', marker='X', label='Centroids')\n    plt.title('KMeans Clustering')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return data, kmeans\n```\n"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL cannot be empty or invalid.\")\n    \n    try:\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"Failed to retrieve the URL: {e}\")\n\n    doc = pq(html)\n    anchors = doc('a')\n    \n    data = []\n    for anchor in anchors:\n        text = anchor.text\n        href = anchor.get('href')\n        data.append({'text': text, 'href': href})\n\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    return df", "raw_solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL cannot be empty or invalid.\")\n    \n    try:\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"Failed to retrieve the URL: {e}\")\n\n    doc = pq(html)\n    anchors = doc('a')\n    \n    data = []\n    for anchor in anchors:\n        text = anchor.text\n        href = anchor.get('href')\n        data.append({'text': text, 'href': href})\n\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    return df\n\n# Example usage:\n# df = task_func('https://en.wikibooks.org/wiki/main_page')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/591", "solution": "from datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\nFILE_PATH = 'custom_data.csv'\ndef task_func(hours, file_path=FILE_PATH):\n    data = []\n    for hour in range(hours):\n        time = datetime.now() + pd.Timedelta(hours=hour)\n        temperature = randint(15, 35)\n        category = TEMP_CATEGORIES[randint(0, 2)]\n        data.append({'Time': time, 'Temperature': temperature, 'Category': category})\n\n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)\n\n    ax = df.plot(x='Time', y='Temperature', title='Temperature over Time', legend=True)\n    plt.xlabel('Time')\n    plt.ylabel('Temperature')\n    plt.show()\n\n    return file_path, ax", "raw_solution": "from datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\nFILE_PATH = 'custom_data.csv'\n\ndef task_func(hours, file_path=FILE_PATH):\n    data = []\n    for hour in range(hours):\n        time = datetime.now() + pd.Timedelta(hours=hour)\n        temperature = randint(15, 35)\n        category = TEMP_CATEGORIES[randint(0, 2)]\n        data.append({'Time': time, 'Temperature': temperature, 'Category': category})\n\n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)\n\n    ax = df.plot(x='Time', y='Temperature', title='Temperature over Time', legend=True)\n    plt.xlabel('Time')\n    plt.ylabel('Temperature')\n    plt.show()\n\n    return file_path, ax\n```\n"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nSENSORS = ['Time', 'Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(SENSORS)\n\n        for _ in range(hours):\n            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            temperature = randint(20, 100)\n            humidity = randint(30, 90)\n            pressure = randint(1000, 1100)\n            writer.writerow([time, temperature, humidity, pressure])\n\n    return filename", "raw_solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nSENSORS = ['Time', 'Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(SENSORS)\n\n        for _ in range(hours):\n            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            temperature = randint(20, 100)\n            humidity = randint(30, 90)\n            pressure = randint(1000, 1100)\n            writer.writerow([time, temperature, humidity, pressure])\n\n    return filename\n\n# Example usage:\n# file_path = task_func(1)\n# print(os.path.exists(file_path))\n# print(isinstance(file_path, str))\n# print('sensor_data.csv' in file_path)\n```\nThis script generates a CSV file with sensor data for the specified number of hours. The data includes the current time, a random temperature, humidity, and pressure. The file is saved in the specified output directory, which is created if it does not exist. The function returns the path to the generated CSV file.\n"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    data = {'samay': [], 'car': [], 'bus': [], 'truck': [], 'bike': []}\n    for hour in range(hours):\n        data['samay'].append(datetime.now() + timedelta(hours=hour))\n        for vehicle in VEHICLE_TYPES:\n            data[vehicle].append(randint(0, 100))\n\n    file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['samay'] + VEHICLE_TYPES)\n        writer.writeheader()\n        for i in range(hours):\n            row = {key: value[i] for key, value in data.items()}\n            writer.writerow(row)\n\n    df = pd.DataFrame(data)\n    ax = df.plot(x='samay', y=VEHICLE_TYPES, kind='line')\n    plt.xlabel('Time')\n    plt.ylabel('Vehicle Count')\n    plt.title('Traffic Data Over Time')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return file_path, ax", "raw_solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    data = {'samay': [], 'car': [], 'bus': [], 'truck': [], 'bike': []}\n    for hour in range(hours):\n        data['samay'].append(datetime.now() + timedelta(hours=hour))\n        for vehicle in VEHICLE_TYPES:\n            data[vehicle].append(randint(0, 100))\n\n    file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['samay'] + VEHICLE_TYPES)\n        writer.writeheader()\n        for i in range(hours):\n            row = {key: value[i] for key, value in data.items()}\n            writer.writerow(row)\n\n    df = pd.DataFrame(data)\n    ax = df.plot(x='samay', y=VEHICLE_TYPES, kind='line')\n    plt.xlabel('Time')\n    plt.ylabel('Vehicle Count')\n    plt.title('Traffic Data Over Time')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return file_path, ax\n```\n"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, 'weather_data.csv')\n\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n\n        for _ in range(hours):\n            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n            writer.writerow([time, condition])\n\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    shutil.copy(file_path, backup_dir)\n\n    return file_path", "raw_solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, 'weather_data.csv')\n\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n\n        for _ in range(hours):\n            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n            writer.writerow([time, condition])\n\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    shutil.copy(file_path, backup_dir)\n\n    return file_path\n```\n"}
{"task_id": "BigCodeBench/595", "solution": "import random\nimport bisect\nfrom array import array\ndef task_func(n=10, total=100):\n    # Generate 'n' random integers\n    random_nums = [random.randint(0, total) for _ in range(n)]\n    \n    # Sort the random numbers\n    sorted_nums = sorted(random_nums)\n    \n    # Generate a new random number to be inserted\n    new_num = random.randint(0, total)\n    \n    # Find the position where the new number should be inserted\n    pos = bisect.bisect_left(sorted_nums, new_num)\n    \n    # Insert the new number into the sorted list\n    sorted_nums.insert(pos, new_num)\n    \n    # Convert the list to an array\n    sorted_array = array('i', sorted_nums)\n    \n    return sorted_array, pos", "raw_solution": "import random\nimport bisect\nfrom array import array\n\ndef task_func(n=10, total=100):\n    # Generate 'n' random integers\n    random_nums = [random.randint(0, total) for _ in range(n)]\n    \n    # Sort the random numbers\n    sorted_nums = sorted(random_nums)\n    \n    # Generate a new random number to be inserted\n    new_num = random.randint(0, total)\n    \n    # Find the position where the new number should be inserted\n    pos = bisect.bisect_left(sorted_nums, new_num)\n    \n    # Insert the new number into the sorted list\n    sorted_nums.insert(pos, new_num)\n    \n    # Convert the list to an array\n    sorted_array = array('i', sorted_nums)\n    \n    return sorted_array, pos\n\n# Example usage:\nsorted_nums, pos = task_func(5, 50)"}
{"task_id": "BigCodeBench/596", "solution": "import time\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\ndef task_func(duration):\n    timestamps = []\n    random_values = []\n\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=duration)\n\n    while datetime.now() < end_time:\n        current_time = datetime.now()\n        timestamps.append(current_time.strftime('%H:%M:%S.%f'))\n        random_values.append(randint(0, 100))\n        time.sleep(0.01)  # Sleep for a short duration to reduce CPU usage\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(timestamps, random_values, marker='o')\n    plt.xlabel('Timestamp')\n    plt.ylabel('Random Value')\n    plt.title('Random Value over Time')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return timestamps, random_values", "raw_solution": "import time\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef task_func(duration):\n    timestamps = []\n    random_values = []\n\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=duration)\n\n    while datetime.now() < end_time:\n        current_time = datetime.now()\n        timestamps.append(current_time.strftime('%H:%M:%S.%f'))\n        random_values.append(randint(0, 100))\n        time.sleep(0.01)  # Sleep for a short duration to reduce CPU usage\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(timestamps, random_values, marker='o')\n    plt.xlabel('Timestamp')\n    plt.ylabel('Random Value')\n    plt.title('Random Value over Time')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return timestamps, random_values\n```\nThis script generates random values at regular intervals and plots them against timestamps. The timestamps are in the format '%H:%M:%S.%f' which represents the current time in hours, minutes, seconds, and microseconds. The function returns a tuple containing two lists: one for timestamps and one for random values.\n"}
{"task_id": "BigCodeBench/597", "solution": "import pandas as pd\nimport time\ndef task_func(data, letter):\n    \"\"\"\n    Ek dictionary mein rows ko filter karta hai jahan 'name' column mein maujood maan ek nirdisht letter se shuru hote hain.\n    First, dictionary ko ek DataFrame mein badlen aur phir is DataFrame mein rows ko filter karein.\n\n    Params:\n    - df (dic of list): input dictionary. Ismein ek 'name' ki honi chahiye.\n    - letter (str): 'name' column ko filter karne ke liye letter.\n\n    Returns:\n        pd.series: Filter kiye gaye 'name' column ki ek series.\n\n    Reqs:\n    - pandas\n    - time\n\n    Examples:\n    - >>> data = {'name': ['Alice', 'Bob', 'Charlie', 'David', 'eva', 'Fiona']}\n    - >>> filtered_names = task_func(data, 'a')\n    - >>> filtered_names.index[0].startswith('A')\n    - true\n    - >>> len(filtered_names)\n    - 1\n\n    \"\"\"\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Filter rows where 'name' starts with the given letter\n    filtered_names = df[df['name'].str.startswith(letter)]\n    \n    return filtered_names['name']", "raw_solution": "import pandas as pd\nimport time\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\n\ndef task_func(data, letter):\n    \"\"\"\n    Ek dictionary mein rows ko filter karta hai jahan 'name' column mein maujood maan ek nirdisht letter se shuru hote hain.\n    First, dictionary ko ek DataFrame mein badlen aur phir is DataFrame mein rows ko filter karein.\n\n    Params:\n    - df (dic of list): input dictionary. Ismein ek 'name' ki honi chahiye.\n    - letter (str): 'name' column ko filter karne ke liye letter.\n\n    Returns:\n        pd.series: Filter kiye gaye 'name' column ki ek series.\n\n    Reqs:\n    - pandas\n    - time\n\n    Examples:\n    - >>> data = {'name': ['Alice', 'Bob', 'Charlie', 'David', 'eva', 'Fiona']}\n    - >>> filtered_names = task_func(data, 'a')\n    - >>> filtered_names.index[0].startswith('A')\n    - true\n    - >>> len(filtered_names)\n    - 1\n\n    \"\"\"\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Filter rows where 'name' starts with the given letter\n    filtered_names = df[df['name'].str.startswith(letter)]\n    \n    return filtered_names['name']\n```\n"}
{"task_id": "BigCodeBench/598", "solution": "import pandas as pd\nimport time\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Start timing the process\n    start_time = time.time()\n    \n    # Filter the DataFrame to only include words that start with the given letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the length of each word and count the occurrences\n    word_lengths = filtered_df['Word'].apply(len)\n    length_counts = word_lengths.value_counts().to_dict()\n    \n    # End timing the process\n    end_time = time.time()\n    \n    # Print the time taken for the operation\n    print(f\"Time taken: {end_time - start_time} seconds\")\n    \n    return length_counts\ndf = {'Word': ['apple', 'banana', 'cherry', 'date', 'fig', 'grape', 'kiwi']}", "raw_solution": "import pandas as pd\nimport time\n\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Start timing the process\n    start_time = time.time()\n    \n    # Filter the DataFrame to only include words that start with the given letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the length of each word and count the occurrences\n    word_lengths = filtered_df['Word'].apply(len)\n    length_counts = word_lengths.value_counts().to_dict()\n    \n    # End timing the process\n    end_time = time.time()\n    \n    # Print the time taken for the operation\n    print(f\"Time taken: {end_time - start_time} seconds\")\n    \n    return length_counts\n\n# Example usage\ndf = {'Word': ['apple', 'banana', 'cherry', 'date', 'fig', 'grape', 'kiwi']}"}
{"task_id": "BigCodeBench/599", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport time\ndef task_func(df, letter):\n    \"\"\"\n    This function converts an input dictionary to a DataFrame, filters rows where the 'Word' column starts with a specific letter,\n    counts the occurrences of words starting with that letter, and plots a histogram of the word counts.\n\n    Params:\n    - df (dict of list): A dictionary mapping a string key to a list of strings.\n    - letter (str): A letter to filter the 'Word' column. This should be a lowercase letter.\n\n    Returns:\n        Axes: A histogram plot of words starting with the specified letter.\n\n    Reqs:\n    - pandas\n    - matplotlib\n    - time\n\n    Examples:\n    - >>> df = {'Word': ['apple', 'banana', 'cherry', 'date', 'fig', 'grape', 'avocado']}\n    - >>> ax = task_func(df, 'a')\n    \"\"\"\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(df)\n    \n    # Filter words starting with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Count occurrences of each word\n    word_counts = filtered_df['Word'].value_counts()\n    \n    # Plot histogram\n    ax = word_counts.plot(kind='bar', figsize=(10, 6))\n    ax.set_title(f'Words starting with \"{letter}\"')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    \n    # Show plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\ndef task_func(df, letter):\n    \"\"\"\n    This function converts an input dictionary to a DataFrame, filters rows where the 'Word' column starts with a specific letter,\n    counts the occurrences of words starting with that letter, and plots a histogram of the word counts.\n\n    Params:\n    - df (dict of list): A dictionary mapping a string key to a list of strings.\n    - letter (str): A letter to filter the 'Word' column. This should be a lowercase letter.\n\n    Returns:\n        Axes: A histogram plot of words starting with the specified letter.\n\n    Reqs:\n    - pandas\n    - matplotlib\n    - time\n\n    Examples:\n    - >>> df = {'Word': ['apple', 'banana', 'cherry', 'date', 'fig', 'grape', 'avocado']}\n    - >>> ax = task_func(df, 'a')\n    \"\"\"\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(df)\n    \n    # Filter words starting with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Count occurrences of each word\n    word_counts = filtered_df['Word'].value_counts()\n    \n    # Plot histogram\n    ax = word_counts.plot(kind='bar', figsize=(10, 6))\n    ax.set_title(f'Words starting with \"{letter}\"')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    \n    # Show plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax\n```\n"}
{"task_id": "BigCodeBench/600", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Filter the DataFrame to only include rows where the 'Word' starts with the given letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the mean, median, and mode of the 'Word' lengths\n    mean_length = filtered_df['Word'].apply(len).mean()\n    median_length = filtered_df['Word'].apply(len).median()\n    mode_length = filtered_df['Word'].apply(len).mode()[0]\n    \n    # Return the results as a dictionary\n    return {\n        'mean': mean_length,\n        'median': median_length,\n        'mode': mode_length\n    }\ndf = {'Word': ['apple', 'banana', 'apricot', 'blueberry', 'cherry', 'avocado']}", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(df, letter):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n    \n    # Filter the DataFrame to only include rows where the 'Word' starts with the given letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the mean, median, and mode of the 'Word' lengths\n    mean_length = filtered_df['Word'].apply(len).mean()\n    median_length = filtered_df['Word'].apply(len).median()\n    mode_length = filtered_df['Word'].apply(len).mode()[0]\n    \n    # Return the results as a dictionary\n    return {\n        'mean': mean_length,\n        'median': median_length,\n        'mode': mode_length\n    }\n\n# Test cases\ndf = {'Word': ['apple', 'banana', 'apricot', 'blueberry', 'cherry', 'avocado']}\nstats = task_func(df, 'a')"}
{"task_id": "BigCodeBench/601", "solution": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\ndef task_func(df, letter):\n    \"\"\"\n    This function filters the DataFrame to include only words that start with the specified letter,\n    then generates a box plot of the lengths of these words.\n\n    Parameters:\n    - df (pd.DataFrame): The input DataFrame containing a 'Word' column.\n    - letter (str): The starting letter to filter the words.\n\n    Returns:\n    - ax (matplotlib.axes._subplots.AxesSubplot): The axes object of the box plot. Returns None if the DataFrame is empty or the 'Word' column is missing.\n    \"\"\"\n\n    # Check if the DataFrame is empty or if the 'Word' column is missing\n    if df.empty or 'Word' not in df.columns:\n        return None\n\n    # Filter the DataFrame\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Check if the filtered DataFrame is empty\n    if filtered_df.empty:\n        return None\n\n    # Calculate the lengths of the words\n    filtered_df['Length'] = filtered_df['Word'].str.len()\n\n    # Create a box plot\n    ax = sns.boxplot(x='Length', data=filtered_df)\n    plt.title(f'Length distribution of words starting with \"{letter}\"')\n    plt.show()\n\n    return ax", "raw_solution": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\ndef task_func(df, letter):\n    \"\"\"\n    This function filters the DataFrame to include only words that start with the specified letter,\n    then generates a box plot of the lengths of these words.\n\n    Parameters:\n    - df (pd.DataFrame): The input DataFrame containing a 'Word' column.\n    - letter (str): The starting letter to filter the words.\n\n    Returns:\n    - ax (matplotlib.axes._subplots.AxesSubplot): The axes object of the box plot. Returns None if the DataFrame is empty or the 'Word' column is missing.\n    \"\"\"\n\n    # Check if the DataFrame is empty or if the 'Word' column is missing\n    if df.empty or 'Word' not in df.columns:\n        return None\n\n    # Filter the DataFrame\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Check if the filtered DataFrame is empty\n    if filtered_df.empty:\n        return None\n\n    # Calculate the lengths of the words\n    filtered_df['Length'] = filtered_df['Word'].str.len()\n\n    # Create a box plot\n    ax = sns.boxplot(x='Length', data=filtered_df)\n    plt.title(f'Length distribution of words starting with \"{letter}\"')\n    plt.show()\n\n    return ax\n\n# Example usage"}
{"task_id": "BigCodeBench/602", "solution": "import numpy as np\nimport pandas as pd\nimport os\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nOUTPUT_DIR = './output'\ndef task_func(file_path, output_dir=OUTPUT_DIR):\n    \"\"\"\n    This function generates a random 2D matrix using lowercase letters and saves it to a CSV file.\n\n    Parameters:\n    - file_path (str): The path where the CSV file will be saved.\n    - output_dir (str, optional): The directory where the CSV file will be saved.\n\n    Returns:\n    - None: The function saves the CSV file to the specified path.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    - >>> task_func(os.path.join(OUTPUT_DIR, 'random_matrix.csv'))\n    \"\"\"\n    # Create a random 5x5 matrix using lowercase letters\n    matrix = np.random.choice(LETTERS, size=(5, 5))\n\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(file_path, index=False)\n\n    return None", "raw_solution": "import numpy as np\nimport pandas as pd\nimport os\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nOUTPUT_DIR = './output'\n\ndef task_func(file_path, output_dir=OUTPUT_DIR):\n    \"\"\"\n    This function generates a random 2D matrix using lowercase letters and saves it to a CSV file.\n\n    Parameters:\n    - file_path (str): The path where the CSV file will be saved.\n    - output_dir (str, optional): The directory where the CSV file will be saved.\n\n    Returns:\n    - None: The function saves the CSV file to the specified path.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    - >>> task_func(os.path.join(OUTPUT_DIR, 'random_matrix.csv'))\n    \"\"\"\n    # Create a random 5x5 matrix using lowercase letters\n    matrix = np.random.choice(LETTERS, size=(5, 5))\n\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(file_path, index=False)\n\n    return None\n```\n"}
{"task_id": "BigCodeBench/603", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(matrix1, matrix2):\n    \"\"\"\n    Do 2D numeric arrays (matrices) ko second axis (columns) ke saath jodata hai,\n    Unhe ek Pandas DataFrame mein badalta hai, aur DataFrame ka ek string representation lautaata hai.\n\n    Params:\n    - matrix1 (np.ndarray): first 2D numpy array.\n    - matrix2 (np.ndarray): second 2D numpy array.\n\n    Returns:\n        str: index aur header ke bina DataFrame ka string representation.\n\n    Reqs:\n    - pandas\n    - numpy\n\n    Examples:\n    - >>> matrix1 = np.array([[1, 2, 3], [4, 5, 6]])\n    - >>> matrix2 = np.array([[7, 8, 9], [10, 11, 12]])\n    - >>> result = task_func(matrix1, matrix2)\n    - >>> all(x in result.replace(' ', '') for x in ['123789', '456101112'])\n    - True\n\n    \"\"\"\n    # Convert the numpy arrays to pandas DataFrame\n    df = pd.DataFrame(np.concatenate((matrix1, matrix2), axis=1))\n    \n    # Convert DataFrame to string representation without index and header\n    result = df.to_string(index=False, header=False)\n    \n    return result\nmatrix1 = np.array([[1, 2, 3], [4, 5, 6]])\nmatrix2 = np.array([[7, 8, 9], [10, 11, 12]])", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(matrix1, matrix2):\n    \"\"\"\n    Do 2D numeric arrays (matrices) ko second axis (columns) ke saath jodata hai,\n    Unhe ek Pandas DataFrame mein badalta hai, aur DataFrame ka ek string representation lautaata hai.\n\n    Params:\n    - matrix1 (np.ndarray): first 2D numpy array.\n    - matrix2 (np.ndarray): second 2D numpy array.\n\n    Returns:\n        str: index aur header ke bina DataFrame ka string representation.\n\n    Reqs:\n    - pandas\n    - numpy\n\n    Examples:\n    - >>> matrix1 = np.array([[1, 2, 3], [4, 5, 6]])\n    - >>> matrix2 = np.array([[7, 8, 9], [10, 11, 12]])\n    - >>> result = task_func(matrix1, matrix2)\n    - >>> all(x in result.replace(' ', '') for x in ['123789', '456101112'])\n    - True\n\n    \"\"\"\n    # Convert the numpy arrays to pandas DataFrame\n    df = pd.DataFrame(np.concatenate((matrix1, matrix2), axis=1))\n    \n    # Convert DataFrame to string representation without index and header\n    result = df.to_string(index=False, header=False)\n    \n    return result\n\n# Test cases\nmatrix1 = np.array([[1, 2, 3], [4, 5, 6]])\nmatrix2 = np.array([[7, 8, 9], [10, 11, 12]])\nresult = task_func(matrix1, matrix2)"}
{"task_id": "BigCodeBench/604", "solution": "import subprocess\nimport logging\ndef task_func(filepath):\n    # Set up logging\n    logging.basicConfig(filename='compile.log', level=logging.DEBUG, format='%(asctime)s - %(message)s')\n\n    try:\n        # Compile the C++ file\n        result = subprocess.run(['g++', filepath, '-o', filepath.split('.')[0]], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        logging.info(f\"Compilation successful for {filepath}\")\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"Compilation failed for {filepath}: {e.stderr.decode()}\")\n        raise\n    except FileNotFoundError as e:\n        logging.error(f\"File not found: {filepath}\")\n        raise\n\n    # Check if the output file exists\n    output_file = filepath.split('.')[0]\n    if not os.path.exists(output_file):\n        logging.error(f\"Output file {output_file} does not exist.\")\n        raise FileNotFoundError(f\"Output file {output_file} does not exist.\")\n\n    # Return None as the function does not return any value\n    return None", "raw_solution": "import subprocess\nimport logging\n\ndef task_func(filepath):\n    # Set up logging\n    logging.basicConfig(filename='compile.log', level=logging.DEBUG, format='%(asctime)s - %(message)s')\n\n    try:\n        # Compile the C++ file\n        result = subprocess.run(['g++', filepath, '-o', filepath.split('.')[0]], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        logging.info(f\"Compilation successful for {filepath}\")\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"Compilation failed for {filepath}: {e.stderr.decode()}\")\n        raise\n    except FileNotFoundError as e:\n        logging.error(f\"File not found: {filepath}\")\n        raise\n\n    # Check if the output file exists\n    output_file = filepath.split('.')[0]\n    if not os.path.exists(output_file):\n        logging.error(f\"Output file {output_file} does not exist.\")\n        raise FileNotFoundError(f\"Output file {output_file} does not exist.\")\n\n    # Return None as the function does not return any value\n    return None\n\n# Example usage"}
{"task_id": "BigCodeBench/605", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(matrix):\n    \"\"\"\n    Use matplotlib.pyplot to create a heatmap of a 2D numeric array (matrix).\n    Use cmap to specify the color mapping and interpolation to control the pixel rendering.\n\n    Params:\n    - matrix (array): 2D numpy array.\n\n    Returns:\n        ax (matplotlib.pyplot.axes._axes.Axes): the axis object with the heatmap.\n\n    Reqs:\n    - pandas\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> import numpy as np\n    - >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - >>> ax = task_func(matrix)\n\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    ax = plt.axes()\n    cax = ax.matshow(df, cmap='viridis', interpolation='nearest')\n    plt.colorbar(cax)\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    \"\"\"\n    Use matplotlib.pyplot to create a heatmap of a 2D numeric array (matrix).\n    Use cmap to specify the color mapping and interpolation to control the pixel rendering.\n\n    Params:\n    - matrix (array): 2D numpy array.\n\n    Returns:\n        ax (matplotlib.pyplot.axes._axes.Axes): the axis object with the heatmap.\n\n    Reqs:\n    - pandas\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> import numpy as np\n    - >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - >>> ax = task_func(matrix)\n\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    ax = plt.axes()\n    cax = ax.matshow(df, cmap='viridis', interpolation='nearest')\n    plt.colorbar(cax)\n    return ax\n```\n"}
{"task_id": "BigCodeBench/606", "solution": "import pandas as pd\nfrom scipy import stats\nimport numpy as np\ndef task_func(matrix):\n    \"\"\"\n    Z score ka upayog karke 2D numeric sarani (matrix) ko samanya karta hai.\n\n    Params:\n    - matrix (array): 2D numpy sarani.\n\n    Returns:\n        DataFrame: samanyikrit DataFrame.\n\n    Reqs:\n    - pandas\n    - numpy\n    - scipy\n\n    Examples:\n    - >>> import numpy as np\n    - >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - >>> normalized_df = task_func(matrix)\n    - >>> isinstance(normalized_df, pd.DataFrame)\n    - True\n    - >>> np.allclose(normalized_df.mean(), 0)\n    - True\n    - >>> np.allclose(normalized_df.std(ddof=0), 1)\n    - True\n\n    \"\"\"\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Apply z-score normalization\n    normalized_df = (df - df.mean()) / df.std(ddof=0)\n\n    return normalized_df", "raw_solution": "import pandas as pd\nfrom scipy import stats\nimport numpy as np\n\ndef task_func(matrix):\n    \"\"\"\n    Z score ka upayog karke 2D numeric sarani (matrix) ko samanya karta hai.\n\n    Params:\n    - matrix (array): 2D numpy sarani.\n\n    Returns:\n        DataFrame: samanyikrit DataFrame.\n\n    Reqs:\n    - pandas\n    - numpy\n    - scipy\n\n    Examples:\n    - >>> import numpy as np\n    - >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - >>> normalized_df = task_func(matrix)\n    - >>> isinstance(normalized_df, pd.DataFrame)\n    - True\n    - >>> np.allclose(normalized_df.mean(), 0)\n    - True\n    - >>> np.allclose(normalized_df.std(ddof=0), 1)\n    - True\n\n    \"\"\"\n    # Convert the matrix to a pandas DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Apply z-score normalization\n    normalized_df = (df - df.mean()) / df.std(ddof=0)\n\n    return normalized_df\n\n# Test cases"}
{"task_id": "BigCodeBench/607", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    '''\n    Remove rows from a dataframe based on column values and generate random scatter plots.\n\n    Parameters:\n    - df (pd.DataFrame): The input DataFrame to be modified.\n    - tuples (list): A list of tuples, each representing a row's values for removal.\n    - n_plots (int): Number of scatter plots to generate from random pairs of columns.\n\n    Returns:\n    - pd.DataFrame: The DataFrame after removal of specified rows.\n    - list: A list containing matplotlib Axes objects of the generated plots.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=COLUMNS)\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> modified_df, plots = task_func(df, tuples, 3)\n    '''\n    # Remove rows based on the provided tuples\n    for tup in tuples:\n        df = df[~df.eq(tup).any(axis=1)]\n\n    # Generate random scatter plots\n    plots = []\n    for _ in range(n_plots):\n        # Randomly select two columns\n        columns = sample(COLUMNS, 2)\n        ax = df.plot.scatter(x=columns[0], y=columns[1], alpha=0.5)\n        plots.append(ax)\n        plt.show()\n\n    return df, plots", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants for column names to use in plots\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    '''\n    Remove rows from a dataframe based on column values and generate random scatter plots.\n\n    Parameters:\n    - df (pd.DataFrame): The input DataFrame to be modified.\n    - tuples (list): A list of tuples, each representing a row's values for removal.\n    - n_plots (int): Number of scatter plots to generate from random pairs of columns.\n\n    Returns:\n    - pd.DataFrame: The DataFrame after removal of specified rows.\n    - list: A list containing matplotlib Axes objects of the generated plots.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=COLUMNS)\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> modified_df, plots = task_func(df, tuples, 3)\n    '''\n    # Remove rows based on the provided tuples\n    for tup in tuples:\n        df = df[~df.eq(tup).any(axis=1)]\n\n    # Generate random scatter plots\n    plots = []\n    for _ in range(n_plots):\n        # Randomly select two columns\n        columns = sample(COLUMNS, 2)\n        ax = df.plot.scatter(x=columns[0], y=columns[1], alpha=0.5)\n        plots.append(ax)\n        plt.show()\n\n    return df, plots\n```\n"}
{"task_id": "BigCodeBench/608", "solution": "import seaborn as sns\nfrom random import sample\nimport pandas as pd\nimport numpy as np\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on the provided tuples\n    for t in tuples:\n        df = df[~df.isin(t).any(axis=1)]\n    \n    # Generate n_plots pairplots using randomly selected column pairs\n    plots = []\n    for _ in range(n_plots):\n        columns = sample(COLUMNS, 2)\n        plot = sns.pairplot(df, vars=columns)\n        plots.append(plot)\n    \n    return df, plots", "raw_solution": "import seaborn as sns\nfrom random import sample\nimport pandas as pd\nimport numpy as np\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on the provided tuples\n    for t in tuples:\n        df = df[~df.isin(t).any(axis=1)]\n    \n    # Generate n_plots pairplots using randomly selected column pairs\n    plots = []\n    for _ in range(n_plots):\n        columns = sample(COLUMNS, 2)\n        plot = sns.pairplot(df, vars=columns)\n        plots.append(plot)\n    \n    return df, plots\n\n# Example usage"}
{"task_id": "BigCodeBench/609", "solution": "import pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom random import sample\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on the tuples\n    for t in tuples:\n        df = df[~df.isin(t).any(axis=1)]\n    \n    # Get all column combinations\n    columns = df.columns\n    column_combinations = list(combinations(columns, 2))\n    \n    # Select random combinations for plotting\n    selected_combinations = sample(column_combinations, min(n_plots, len(column_combinations)))\n    \n    # Create scatter plots for selected combinations\n    plots = []\n    for col1, col2 in selected_combinations:\n        plot = df.plot.scatter(x=col1, y=col2)\n        plots.append((col1, col2, plot))\n    \n    return df, plots", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom random import sample\n\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on the tuples\n    for t in tuples:\n        df = df[~df.isin(t).any(axis=1)]\n    \n    # Get all column combinations\n    columns = df.columns\n    column_combinations = list(combinations(columns, 2))\n    \n    # Select random combinations for plotting\n    selected_combinations = sample(column_combinations, min(n_plots, len(column_combinations)))\n    \n    # Create scatter plots for selected combinations\n    plots = []\n    for col1, col2 in selected_combinations:\n        plot = df.plot.scatter(x=col1, y=col2)\n        plots.append((col1, col2, plot))\n    \n    return df, plots\n\n# Example usage"}
{"task_id": "BigCodeBench/610", "solution": "from random import sample\nimport seaborn as sns\nimport pandas as pd\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    '''\n    Remove rows from a dataframe based on values of multiple columns, \n    and then create n random joint plots of two columns against each other if the DataFrame is not empty.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    tuples (list): A list of tuples, where each tuple represents a row to be removed.\n    n_plots (int): The number of jointplots to be generated.\n    \n    Returns:\n    tuple: A tuple containing:\n        - DataFrame: The modified DataFrame.\n        - list: A list of generated joint plots (sns.JointGrid objects) if the DataFrame is not empty, otherwise an empty list.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - random\n    \n    Example:\n    >>> import numpy as np\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> modified_df, plots = task_func(df, tuples, 3)\n    '''\n    # Remove rows based on the provided tuples\n    for t in tuples:\n        df = df[~df.eq(t).any(axis=1)]\n    \n    plots = []\n    if not df.empty and n_plots > 0:\n        # Generate n random joint plots\n        for _ in range(n_plots):\n            # Randomly select two columns\n            columns = sample(COLUMNS, 2)\n            plot = sns.jointplot(x=columns[0], y=columns[1], data=df)\n            plots.append(plot)\n    \n    return df, plots", "raw_solution": "from random import sample\nimport seaborn as sns\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    '''\n    Remove rows from a dataframe based on values of multiple columns, \n    and then create n random joint plots of two columns against each other if the DataFrame is not empty.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    tuples (list): A list of tuples, where each tuple represents a row to be removed.\n    n_plots (int): The number of jointplots to be generated.\n    \n    Returns:\n    tuple: A tuple containing:\n        - DataFrame: The modified DataFrame.\n        - list: A list of generated joint plots (sns.JointGrid objects) if the DataFrame is not empty, otherwise an empty list.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - random\n    \n    Example:\n    >>> import numpy as np\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> modified_df, plots = task_func(df, tuples, 3)\n    '''\n    # Remove rows based on the provided tuples\n    for t in tuples:\n        df = df[~df.eq(t).any(axis=1)]\n    \n    plots = []\n    if not df.empty and n_plots > 0:\n        # Generate n random joint plots\n        for _ in range(n_plots):\n            # Randomly select two columns\n            columns = sample(COLUMNS, 2)\n            plot = sns.jointplot(x=columns[0], y=columns[1], data=df)\n            plots.append(plot)\n    \n    return df, plots\n```\n"}
{"task_id": "BigCodeBench/611", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    # Check if the DataFrame has the required columns\n    if not all(column in df.columns for column in COLUMNS):\n        raise ValueError(\"DataFrame must contain columns A, B, C, D, E\")\n\n    # Sample tuples from the list of tuples\n    sampled_tuples = sample(tuples, n_plots)\n\n    # Create a new DataFrame with sampled tuples\n    sampled_df = pd.DataFrame(sampled_tuples, columns=COLUMNS)\n\n    # Plot the sampled data\n    plot_details = []\n    for i in range(n_plots):\n        plt.figure()\n        plt.plot(sampled_df.iloc[i], marker='o')\n        plt.title(f\"Line Plot for Tuple {i+1}\")\n        plt.xlabel(\"Columns\")\n        plt.ylabel(\"Values\")\n        plt.grid(True)\n        plt.show()\n        plot_details.append((COLUMNS, sampled_df.iloc[i]))\n\n    return sampled_df, plot_details", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df, tuples, n_plots):\n    # Check if the DataFrame has the required columns\n    if not all(column in df.columns for column in COLUMNS):\n        raise ValueError(\"DataFrame must contain columns A, B, C, D, E\")\n\n    # Sample tuples from the list of tuples\n    sampled_tuples = sample(tuples, n_plots)\n\n    # Create a new DataFrame with sampled tuples\n    sampled_df = pd.DataFrame(sampled_tuples, columns=COLUMNS)\n\n    # Plot the sampled data\n    plot_details = []\n    for i in range(n_plots):\n        plt.figure()\n        plt.plot(sampled_df.iloc[i], marker='o')\n        plt.title(f\"Line Plot for Tuple {i+1}\")\n        plt.xlabel(\"Columns\")\n        plt.ylabel(\"Values\")\n        plt.grid(True)\n        plt.show()\n        plot_details.append((COLUMNS, sampled_df.iloc[i]))\n\n    return sampled_df, plot_details\n```\n"}
{"task_id": "BigCodeBench/612", "solution": "from random import choice\nimport numpy as np\nimport pandas as pd\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COSTS = [100, 200, 300, 400, 500]\ndef task_func(goals, penalties, teams=TEAMS, penalties_costs=PENALTIES_COSTS):\n    report_data = []\n\n    for team in teams:\n        goal_count = goals.get(team, 0)\n        penalty_count = penalties.get(team, 0)\n        penalty_cost = choice(penalties_costs)\n        performance_score = np.abs(goal_count - penalty_count)\n\n        report_data.append([team, goal_count, penalty_count, penalty_cost, performance_score])\n\n    report_df = pd.DataFrame(report_data, columns=['Team', 'Goals', 'Penalties', 'Penalty Cost', 'Performance Score'])\n\n    return report_df\ngoals = {'Team A': 3, 'Team B': 2}\npenalties = {'Team A': 1, 'Team B': 0}", "raw_solution": "from random import choice\nimport numpy as np\nimport pandas as pd\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COSTS = [100, 200, 300, 400, 500]\n\ndef task_func(goals, penalties, teams=TEAMS, penalties_costs=PENALTIES_COSTS):\n    report_data = []\n\n    for team in teams:\n        goal_count = goals.get(team, 0)\n        penalty_count = penalties.get(team, 0)\n        penalty_cost = choice(penalties_costs)\n        performance_score = np.abs(goal_count - penalty_count)\n\n        report_data.append([team, goal_count, penalty_count, penalty_cost, performance_score])\n\n    report_df = pd.DataFrame(report_data, columns=['Team', 'Goals', 'Penalties', 'Penalty Cost', 'Performance Score'])\n\n    return report_df\n\n# Example usage:\ngoals = {'Team A': 3, 'Team B': 2}\npenalties = {'Team A': 1, 'Team B': 0}\nreport = task_func(goals, penalties)"}
{"task_id": "BigCodeBench/613", "solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\ndef task_func(goals, penalties):\n    # Initialize scores\n    scores = {team: 0 for team in TEAMS}\n\n    # Calculate scores\n    for team in TEAMS:\n        if team in goals and team in penalties:\n            net_score = goals[team] - penalties[team]\n            scores[team] = max(min(net_score, GOALS_RANGE[1]), GOALS_RANGE[0])\n\n    # Create DataFrame\n    df = pd.DataFrame(list(scores.items()), columns=['Team', 'Score'])\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Team'], df['Score'], color='skyblue')\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Net Score for Each Team')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return df\ngoals = {'Team A': 5, 'Team B': 3, 'Team C': 1, 'Team D': 0, 'Team E': 4}\npenalties = {'Team A': 1, 'Team B': 1, 'Team C': 1, 'Team D': 0, 'Team E': 2}", "raw_solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\n\n\ndef task_func(goals, penalties):\n    # Initialize scores\n    scores = {team: 0 for team in TEAMS}\n\n    # Calculate scores\n    for team in TEAMS:\n        if team in goals and team in penalties:\n            net_score = goals[team] - penalties[team]\n            scores[team] = max(min(net_score, GOALS_RANGE[1]), GOALS_RANGE[0])\n\n    # Create DataFrame\n    df = pd.DataFrame(list(scores.items()), columns=['Team', 'Score'])\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Team'], df['Score'], color='skyblue')\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Net Score for Each Team')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return df\n\n# Example usage\ngoals = {'Team A': 5, 'Team B': 3, 'Team C': 1, 'Team D': 0, 'Team E': 4}\npenalties = {'Team A': 1, 'Team B': 1, 'Team C': 1, 'Team D': 0, 'Team E': 2}\ndf = task_func(goals, penalties)"}
{"task_id": "BigCodeBench/614", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(goals, penalties):\n    # Create a DataFrame from the input dictionaries\n    df = pd.DataFrame(list(goals.items()), columns=['Team', 'Goals'])\n    df['Penalties'] = df['Team'].map(penalties)\n\n    # Create a seaborn pairplot to visualize the distribution of goals and penalties\n    sns.set(style=\"whitegrid\")\n    pairplot = sns.pairplot(df, hue='Team', palette='Set2')\n\n    # Show the plot\n    plt.show()\n\n    return df, pairplot", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(goals, penalties):\n    # Create a DataFrame from the input dictionaries\n    df = pd.DataFrame(list(goals.items()), columns=['Team', 'Goals'])\n    df['Penalties'] = df['Team'].map(penalties)\n\n    # Create a seaborn pairplot to visualize the distribution of goals and penalties\n    sns.set(style=\"whitegrid\")\n    pairplot = sns.pairplot(df, hue='Team', palette='Set2')\n\n    # Show the plot\n    plt.show()\n\n    return df, pairplot\n```\n"}
{"task_id": "BigCodeBench/615", "solution": "from random import randint, seed\nimport pandas as pd\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for i in range(5):  # Assuming we have 5 teams\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = team_penalties * 1000  # Assuming penalty cost is $1000 per penalty\n        data.append([f'Team {chr(65+i)}', f'{team_goals} goals, ${penalty_cost}'])\n\n    df = pd.DataFrame(data, columns=['team', 'match result'])\n    return df", "raw_solution": "from random import randint, seed\nimport pandas as pd\n\n# Method\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for i in range(5):  # Assuming we have 5 teams\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = team_penalties * 1000  # Assuming penalty cost is $1000 per penalty\n        data.append([f'Team {chr(65+i)}', f'{team_goals} goals, ${penalty_cost}'])\n\n    df = pd.DataFrame(data, columns=['team', 'match result'])\n    return df\n\n# Example usage:\nseed(42)  # Setting the seed for reproducibility\nresults = task_func(5, 3, 42)"}
{"task_id": "BigCodeBench/616", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalty_cost = randint(0, penalties) * penalty_cost\n        data.append([team, team_goals, team_penalty_cost])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    ax = df.plot(x='Team', y=['Goals', 'Penalty Cost'], kind='bar')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return df, ax", "raw_solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants (they can be overridden with default parameters)\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalty_cost = randint(0, penalties) * penalty_cost\n        data.append([team, team_goals, team_penalty_cost])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    ax = df.plot(x='Team', y=['Goals', 'Penalty Cost'], kind='bar')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/617", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, rng_seed=None, teams=TEAMS):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = team_penalties * PENALTY_COST\n        data.append([team, team_goals, penalty_cost])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Team'], df['Goals'], label='Goals', color='blue')\n    plt.bar(df['Team'], df['Penalty Cost'], label='Penalty Cost', color='red', bottom=df['Goals'])\n    plt.xlabel('Teams')\n    plt.ylabel('Goals and Penalty Cost')\n    plt.title('Match Results Analysis')\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    return df", "raw_solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, rng_seed=None, teams=TEAMS):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = team_penalties * PENALTY_COST\n        data.append([team, team_goals, penalty_cost])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Team'], df['Goals'], label='Goals', color='blue')\n    plt.bar(df['Team'], df['Penalty Cost'], label='Penalty Cost', color='red', bottom=df['Goals'])\n    plt.xlabel('Teams')\n    plt.ylabel('Goals and Penalty Cost')\n    plt.title('Match Results Analysis')\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    return df\n\n# Example usage:\n# analyzed_data = task_func(5, 3, rng_seed=42)\n# print(analyzed_data[['Team', 'Goals', 'Penalty Cost']])\n```\nThis script generates a DataFrame with random goals and penalty costs for each team, then plots a bar chart showing the total goals and penalty costs for each team. The `task_func` function is self-contained and includes all necessary imports and constants.\n"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    \"\"\"\n    Generate and visualize a Pandas pd of the results of football matches for multiple teams 'Team' with\n    random goals 'Goals' and penalties 'Penalty Cost'. Penalties are converted into fines according to penalty costs.\n\n    Params:\n    - goals (int): The maximum number of goals a team can score in a match.\n    - penalties (int): The maximum number of penalties a team can receive in a match.\n\n    Returns:\n        pd.pd: A dataframe containing match results.\n        list: A list containing two seaborn plot objects (Axes) for goals and penalty costs.\n\n    Reqs:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - random\n\n    Examples:\n    - >>> df, plots = task_func(5, 3)\n\n    \"\"\"\n    data = []\n    for team in TEAMS:\n        for _ in range(5):  # Simulating 5 matches for each team\n            goals_scored = randint(0, goals)\n            penalties_committed = randint(0, penalties)\n            penalty_fines = penalties_committed * PENALTY_COST\n            data.append([team, goals_scored, penalties_committed, penalty_fines])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalties', 'Penalty Fines'])\n\n    # Plotting\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0])\n    axes[0].set_title('Goals per Team')\n    axes[0].set_ylabel('Goals')\n\n    sns.barplot(x='Team', y='Penalty Fines', data=df, ax=axes[1])\n    axes[1].set_title('Penalty Fines per Team')\n    axes[1].set_ylabel('Penalty Fines')\n\n    plt.tight_layout()\n\n    return df, axes", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Generate and visualize a Pandas pd of the results of football matches for multiple teams 'Team' with\n    random goals 'Goals' and penalties 'Penalty Cost'. Penalties are converted into fines according to penalty costs.\n\n    Params:\n    - goals (int): The maximum number of goals a team can score in a match.\n    - penalties (int): The maximum number of penalties a team can receive in a match.\n\n    Returns:\n        pd.pd: A dataframe containing match results.\n        list: A list containing two seaborn plot objects (Axes) for goals and penalty costs.\n\n    Reqs:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - random\n\n    Examples:\n    - >>> df, plots = task_func(5, 3)\n\n    \"\"\"\n    data = []\n    for team in TEAMS:\n        for _ in range(5):  # Simulating 5 matches for each team\n            goals_scored = randint(0, goals)\n            penalties_committed = randint(0, penalties)\n            penalty_fines = penalties_committed * PENALTY_COST\n            data.append([team, goals_scored, penalties_committed, penalty_fines])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalties', 'Penalty Fines'])\n\n    # Plotting\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0])\n    axes[0].set_title('Goals per Team')\n    axes[0].set_ylabel('Goals')\n\n    sns.barplot(x='Team', y='Penalty Fines', data=df, ax=axes[1])\n    axes[1].set_title('Penalty Fines per Team')\n    axes[1].set_ylabel('Penalty Fines')\n\n    plt.tight_layout()\n\n    return df, axes\n```\n"}
{"task_id": "BigCodeBench/619", "solution": "from random import randint, seed\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in TEAMS:\n        for goal in range(goals + 1):\n            for penalty in range(penalties + 1):\n                cost = goal * PENALTY_COST + penalty * PENALTY_COST\n                data.append([team, goal, penalty, cost])\n\n    df = pd.DataFrame(data, columns=['team', 'goal', 'penalty', 'cost'])\n\n    X = df[['goal', 'penalty']]\n    y = df['cost']\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return df, model", "raw_solution": "from random import randint, seed\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    data = []\n    for team in TEAMS:\n        for goal in range(goals + 1):\n            for penalty in range(penalties + 1):\n                cost = goal * PENALTY_COST + penalty * PENALTY_COST\n                data.append([team, goal, penalty, cost])\n\n    df = pd.DataFrame(data, columns=['team', 'goal', 'penalty', 'cost'])\n\n    X = df[['goal', 'penalty']]\n    y = df['cost']\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return df, model\n```\n"}
{"task_id": "BigCodeBench/620", "solution": "import numpy as np\nimport pandas as pd\nRANGE = (1, 100)\ndef task_func(L):\n    '''\n    Generates a DataFrame filled with random integers. The dimensions of the DataFrame (number of rows and columns)\n    are determined by multiplying pairs of integers from nested lists within the input list of lists 'L'.\n    \n    Requirements:\n    - numpy\n    - pandas\n\n    Parameters:\n    L (list of lists): A list of lists where each sublist contains two integers.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n    \n    Example:\n    >>> df = task_func([[2, 3], [5, 6]])\n    >>> type(df)\n    <class 'pandas.core.frame.DataFrame'>\n    '''\n    # Initialize an empty list to store data\n    data = []\n\n    # Iterate over each pair of integers in the input list\n    for pair in L:\n        # Generate a random 2D array using numpy\n        array = np.random.randint(RANGE[0], RANGE[1], size=(pair[0], pair[1]))\n        # Append the array to the data list\n        data.append(array)\n\n    # Create a pandas DataFrame from the data\n    df = pd.DataFrame(data)\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\n# Constants\nRANGE = (1, 100)\n\ndef task_func(L):\n    '''\n    Generates a DataFrame filled with random integers. The dimensions of the DataFrame (number of rows and columns)\n    are determined by multiplying pairs of integers from nested lists within the input list of lists 'L'.\n    \n    Requirements:\n    - numpy\n    - pandas\n\n    Parameters:\n    L (list of lists): A list of lists where each sublist contains two integers.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n    \n    Example:\n    >>> df = task_func([[2, 3], [5, 6]])\n    >>> type(df)\n    <class 'pandas.core.frame.DataFrame'>\n    '''\n    # Initialize an empty list to store data\n    data = []\n\n    # Iterate over each pair of integers in the input list\n    for pair in L:\n        # Generate a random 2D array using numpy\n        array = np.random.randint(RANGE[0], RANGE[1], size=(pair[0], pair[1]))\n        # Append the array to the data list\n        data.append(array)\n\n    # Create a pandas DataFrame from the data\n    df = pd.DataFrame(data)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/621", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Flatten the list of lists into a single list of integers\n    flat_list = list(chain(*L))\n    \n    # Convert the list to a numpy array\n    array = np.array(flat_list)\n    \n    # Standardize the array\n    scaler = StandardScaler()\n    standardized_array = scaler.fit_transform(array.reshape(-1, 1)).flatten()\n    \n    # Plot the standardized values\n    fig, ax = plt.subplots()\n    ax.plot(standardized_array, marker='o')\n    ax.set_title('Standardized Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n    \n    return ax", "raw_solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    # Flatten the list of lists into a single list of integers\n    flat_list = list(chain(*L))\n    \n    # Convert the list to a numpy array\n    array = np.array(flat_list)\n    \n    # Standardize the array\n    scaler = StandardScaler()\n    standardized_array = scaler.fit_transform(array.reshape(-1, 1)).flatten()\n    \n    # Plot the standardized values\n    fig, ax = plt.subplots()\n    ax.plot(standardized_array, marker='o')\n    ax.set_title('Standardized Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n    \n    return ax\n\n# Example usage:\n# ax = task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/622", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import chain\ndef task_func(L):\n    # Flatten the list of lists into a single list of integers\n    flat_list = list(chain.from_iterable(L))\n    \n    # Convert the list to a numpy array\n    data = np.array(flat_list)\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=20, density=True, alpha=0.6, color='g')\n    \n    # Plot the normal distribution overlay\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import chain\n\ndef task_func(L):\n    # Flatten the list of lists into a single list of integers\n    flat_list = list(chain.from_iterable(L))\n    \n    # Convert the list to a numpy array\n    data = np.array(flat_list)\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=20, density=True, alpha=0.6, color='g')\n    \n    # Plot the normal distribution overlay\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    return ax\n\n# Example usage:\n# ax = task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# plt.show()\n```\n"}
{"task_id": "BigCodeBench/623", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Flatten the list of lists into a single list\n    flat_list = list(chain(*L))\n    # Convert the list to a numpy array\n    data = np.array(flat_list).reshape(-1, 1)\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0).fit(data)\n    # Get the cluster centers\n    centers = kmeans.cluster_centers_\n    # Get the labels for each data point\n    labels = kmeans.labels_\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data, np.zeros_like(data), c=labels, cmap='viridis')\n    ax.scatter(centers, np.zeros_like(centers), c='red', marker='x')\n    ax.set_title('KMeans Clustering')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Values')\n    return ax", "raw_solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    # Flatten the list of lists into a single list\n    flat_list = list(chain(*L))\n    # Convert the list to a numpy array\n    data = np.array(flat_list).reshape(-1, 1)\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0).fit(data)\n    # Get the cluster centers\n    centers = kmeans.cluster_centers_\n    # Get the labels for each data point\n    labels = kmeans.labels_\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data, np.zeros_like(data), c=labels, cmap='viridis')\n    ax.scatter(centers, np.zeros_like(centers), c='red', marker='x')\n    ax.set_title('KMeans Clustering')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Values')\n    return ax\n```\n"}
{"task_id": "BigCodeBench/624", "solution": "from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\nN_COMPONENTS = 2\ndef task_func(L):\n    \"\"\"\n    L as a list of lists to a 2D numeric array, apply PCA on it and return the PCA result along with a scatter plot.\n\n    Params:\n    - L (list of lists): A list of lists where each sublist contains integers.\n\n    Returns:\n        tuple: A tuple containing the PCA result (numpy array) and the scatter plot (matplotlib Axes object).\n\n    Reqs:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> pca_result, plot = task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - >>> type(pca_result)\n    - <class 'numpy.ndarray'>\n    \"\"\"\n    # Convert list of lists to numpy array\n    data = np.array(L)\n\n    # Apply PCA\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(data)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_title('PCA Result Scatter Plot')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return pca_result, ax", "raw_solution": "from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nN_COMPONENTS = 2\n\n\ndef task_func(L):\n    \"\"\"\n    L as a list of lists to a 2D numeric array, apply PCA on it and return the PCA result along with a scatter plot.\n\n    Params:\n    - L (list of lists): A list of lists where each sublist contains integers.\n\n    Returns:\n        tuple: A tuple containing the PCA result (numpy array) and the scatter plot (matplotlib Axes object).\n\n    Reqs:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> pca_result, plot = task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - >>> type(pca_result)\n    - <class 'numpy.ndarray'>\n    \"\"\"\n    # Convert list of lists to numpy array\n    data = np.array(L)\n\n    # Apply PCA\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(data)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_title('PCA Result Scatter Plot')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return pca_result, ax\n```\n"}
{"task_id": "BigCodeBench/625", "solution": "import math\nfrom random import randint\nimport pandas as pd\ndef task_func(cities_list):\n    data = {'City': cities_list, 'Population': []}\n    for _ in range(len(cities_list)):\n        # Generate a random population between 1 and 10^6\n        population = randint(1, 10**6)\n        data['Population'].append(population)\n    \n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import math\nfrom random import randint\nimport pandas as pd\n\ndef task_func(cities_list):\n    data = {'City': cities_list, 'Population': []}\n    for _ in range(len(cities_list)):\n        # Generate a random population between 1 and 10^6\n        population = randint(1, 10**6)\n        data['Population'].append(population)\n    \n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\ncities = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\npop_data = task_func(cities)"}
{"task_id": "BigCodeBench/626", "solution": "from random import choice\nimport pytz\nfrom dateutil.parser import parse\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\ndef task_func(date_str, from_tz):\n    \"\"\"\n    Converts a datetime string from one timezone to another randomly chosen timezone.\n\n    Params:\n    - date_str (str): datetime string in \"yyyy-mm-dd hh:mm:ss\" format.\n    - from_tz (str): the timezone of the input datetime string.\n\n    Returns:\n        tuple: a tuple containing the converted datetime string and the randomly chosen timezone.\n\n    Reqs:\n    - pytz\n    - dateutil.parser\n    - random\n\n    Examples:\n    - >>> date_str, from_tz = '2023-06-15 12:00:00', 'UTC'\n    - >>> converted_date, to_tz = task_func(date_str, from_tz)\n    - >>> to_tz in TIMEZONES\n    - True\n    \"\"\"\n    # Parse the input date string\n    naive_dt = parse(date_str)\n    \n    # Get the source timezone\n    from_timezone = pytz.timezone(from_tz)\n    local_dt = from_timezone.localize(naive_dt)\n    \n    # Choose a random timezone\n    to_tz = choice(TIMEZONES)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Convert the datetime to the new timezone\n    converted_dt = local_dt.astimezone(to_timezone)\n    \n    return converted_dt.strftime('%Y-%m-%d %H:%M:%S'), to_tz", "raw_solution": "from random import choice\nimport pytz\nfrom dateutil.parser import parse\n\n# Constants\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef task_func(date_str, from_tz):\n    \"\"\"\n    Converts a datetime string from one timezone to another randomly chosen timezone.\n\n    Params:\n    - date_str (str): datetime string in \"yyyy-mm-dd hh:mm:ss\" format.\n    - from_tz (str): the timezone of the input datetime string.\n\n    Returns:\n        tuple: a tuple containing the converted datetime string and the randomly chosen timezone.\n\n    Reqs:\n    - pytz\n    - dateutil.parser\n    - random\n\n    Examples:\n    - >>> date_str, from_tz = '2023-06-15 12:00:00', 'UTC'\n    - >>> converted_date, to_tz = task_func(date_str, from_tz)\n    - >>> to_tz in TIMEZONES\n    - True\n    \"\"\"\n    # Parse the input date string\n    naive_dt = parse(date_str)\n    \n    # Get the source timezone\n    from_timezone = pytz.timezone(from_tz)\n    local_dt = from_timezone.localize(naive_dt)\n    \n    # Choose a random timezone\n    to_tz = choice(TIMEZONES)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Convert the datetime to the new timezone\n    converted_dt = local_dt.astimezone(to_timezone)\n    \n    return converted_dt.strftime('%Y-%m-%d %H:%M:%S'), to_tz\n```\n"}
{"task_id": "BigCodeBench/627", "solution": "from random import randint\nfrom statistics import mean\nimport pandas as pd\ndef task_func(products_list):\n    data = []\n    for product in products_list:\n        sales = [randint(50, 200) for _ in range(12)]\n        avg_sales = mean(sales)\n        data.append([product] + sales + [avg_sales])\n\n    columns = ['product'] + [f'mahina {i}' for i in range(1, 13)] + ['average bikri']\n    df = pd.DataFrame(data, columns=columns)\n\n    return df", "raw_solution": "from random import randint\nfrom statistics import mean\nimport pandas as pd\n\ndef task_func(products_list):\n    data = []\n    for product in products_list:\n        sales = [randint(50, 200) for _ in range(12)]\n        avg_sales = mean(sales)\n        data.append([product] + sales + [avg_sales])\n\n    columns = ['product'] + [f'mahina {i}' for i in range(1, 13)] + ['average bikri']\n    df = pd.DataFrame(data, columns=columns)\n\n    return df\n\n# Example usage:\nproducts = ['Apples', 'Bananas', 'Grapes', 'Oranges', 'Pineapples']\nsales_data = task_func(products)"}
{"task_id": "BigCodeBench/628", "solution": "import math\nfrom random import randint\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate random frequency and phase shift\n    frequency = randint(1, 5)\n    phase_shift = randint(0, 2 * math.pi)\n\n    # Generate time values\n    time = [i * 0.1 for i in range(100)]\n\n    # Generate random sine wave\n    amplitude = [math.sin(2 * math.pi * frequency * t + phase_shift) for t in time]\n\n    # Plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(time, amplitude)\n    ax.set_title('Random Sine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n\n    return ax", "raw_solution": "import math\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    # Generate random frequency and phase shift\n    frequency = randint(1, 5)\n    phase_shift = randint(0, 2 * math.pi)\n\n    # Generate time values\n    time = [i * 0.1 for i in range(100)]\n\n    # Generate random sine wave\n    amplitude = [math.sin(2 * math.pi * frequency * t + phase_shift) for t in time]\n\n    # Plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(time, amplitude)\n    ax.set_title('Random Sine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n\n    return ax\n\n# Example usage"}
{"task_id": "BigCodeBench/629", "solution": "import os\nimport time\nimport pandas as pd\nOUTPUT_DIR = './output'\ndef task_func(dataset, filename, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filepath = os.path.join(output_dir, filename)\n    with open(filepath, 'w') as file:\n        for df in dataset:\n            file.write(df.to_csv(index=False))\n            file.write(\"\\n\" + \"-\"*10 + \"\\n\")", "raw_solution": "import os\nimport time\nimport pandas as pd\n\nOUTPUT_DIR = './output'\n\ndef task_func(dataset, filename, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filepath = os.path.join(output_dir, filename)\n    with open(filepath, 'w') as file:\n        for df in dataset:\n            file.write(df.to_csv(index=False))\n            file.write(\"\\n\" + \"-\"*10 + \"\\n\")\n\n# Example usage:"}
{"task_id": "BigCodeBench/630", "solution": "import pandas as pd\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Pandas DataFrame ko ek specified directory mein ek JSON file mein save karein.\n\n    Notes:\n        function Pandas DataFrame ko badalta hai aur use JSON file ke roop mein save karta hai.\n\n    Params:\n    - df (DataFrame): Ek Pandas DataFrame jise save kiya jaana hai.\n    - filename (str): JSON file ka filename jahaan DataFrame ko save kiya jaayega.\n    - output_dir (str, optional): output directory.\n\n    Returns:\n        str: Poora file path jahaan DataFrame ko save kiya gaya hai.\n\n    Reqs:\n    - os\n    - pandas\n\n    Examples:\n    - >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    - >>> 'data.json' in task_func(df, 'data.json')\n    - True\n\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, filename)\n    df.to_json(file_path, orient='records', lines=True)\n\n    return file_path", "raw_solution": "import pandas as pd\nimport os\nOUTPUT_DIR = './output'\n\n\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Pandas DataFrame ko ek specified directory mein ek JSON file mein save karein.\n\n    Notes:\n        function Pandas DataFrame ko badalta hai aur use JSON file ke roop mein save karta hai.\n\n    Params:\n    - df (DataFrame): Ek Pandas DataFrame jise save kiya jaana hai.\n    - filename (str): JSON file ka filename jahaan DataFrame ko save kiya jaayega.\n    - output_dir (str, optional): output directory.\n\n    Returns:\n        str: Poora file path jahaan DataFrame ko save kiya gaya hai.\n\n    Reqs:\n    - os\n    - pandas\n\n    Examples:\n    - >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    - >>> 'data.json' in task_func(df, 'data.json')\n    - True\n\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, filename)\n    df.to_json(file_path, orient='records', lines=True)\n\n    return file_path\n```\n"}
{"task_id": "BigCodeBench/631", "solution": "import csv\nimport os\nimport pandas as pd\nOUTPUT_DIR = './output'\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, filename)\n    \n    df.to_csv(file_path, index=False)\n    \n    return os.path.abspath(file_path)", "raw_solution": "import csv\nimport os\nimport pandas as pd\n\nOUTPUT_DIR = './output'\n\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, filename)\n    \n    df.to_csv(file_path, index=False)\n    \n    return os.path.abspath(file_path)\n\n# Example usage:\n# df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n# file_path = task_func(df, 'data.csv')\n# print(file_path)\n```\n"}
{"task_id": "BigCodeBench/632", "solution": "import pandas as pd\nimport time\nOUTPUT_DIR = './output'\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    \"\"\"\n    Ek Pandas DataFrame ko ek JSON Lines file mein likhen aur ise ek specified directory mein save karen.\n\n    Params:\n    - df (pd.DataFrame): Ek Pandas DataFrame jise save kiya jaana hai.\n    - filename (str): JSON Lines file ka filename jise save kiya jaana hai.\n\n    Returns:\n        str: **Full path** jahaan **JSON Lines** **file** ko save kiya gaya tha.\n\n    Reqs:\n    - pandas\n    - time\n\n    Examples:\n    - >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    - >>> 'data.jsonl' in task_func(df, 'data.jsonl')\n    - True\n\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # Full path to the file\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Save the DataFrame to a JSON Lines file\n    df.to_json(file_path, orient='records', lines=True)\n\n    # Return the full path to the file\n    return file_path", "raw_solution": "import pandas as pd\nimport time\nOUTPUT_DIR = './output'\n\n\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    \"\"\"\n    Ek Pandas DataFrame ko ek JSON Lines file mein likhen aur ise ek specified directory mein save karen.\n\n    Params:\n    - df (pd.DataFrame): Ek Pandas DataFrame jise save kiya jaana hai.\n    - filename (str): JSON Lines file ka filename jise save kiya jaana hai.\n\n    Returns:\n        str: **Full path** jahaan **JSON Lines** **file** ko save kiya gaya tha.\n\n    Reqs:\n    - pandas\n    - time\n\n    Examples:\n    - >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    - >>> 'data.jsonl' in task_func(df, 'data.jsonl')\n    - True\n\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # Full path to the file\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Save the DataFrame to a JSON Lines file\n    df.to_json(file_path, orient='records', lines=True)\n\n    # Return the full path to the file\n    return file_path\n```\n"}
{"task_id": "BigCodeBench/633", "solution": "import re\nfrom nltk.corpus import stopwords\ndef task_func(text: str) -> dict:\n    # Ensure stopwords are downloaded\n    nltk.download('stopwords')\n    \n    # Define a set of common English stopwords\n    stop_words = set(stopwords.words('english'))\n    \n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Filter out stopwords\n    filtered_words = [word for word in words if word not in stop_words]\n    \n    # Create a frequency distribution\n    freq_dist = {}\n    for word in filtered_words:\n        if word in freq_dist:\n            freq_dist[word] += 1\n        else:\n            freq_dist[word] = 1\n    \n    return freq_dist", "raw_solution": "import re\nfrom nltk.corpus import stopwords\n\ndef task_func(text: str) -> dict:\n    # Ensure stopwords are downloaded\n    nltk.download('stopwords')\n    \n    # Define a set of common English stopwords\n    stop_words = set(stopwords.words('english'))\n    \n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Filter out stopwords\n    filtered_words = [word for word in words if word not in stop_words]\n    \n    # Create a frequency distribution\n    freq_dist = {}\n    for word in filtered_words:\n        if word in freq_dist:\n            freq_dist[word] += 1\n        else:\n            freq_dist[word] = 1\n    \n    return freq_dist\n```\n"}
{"task_id": "BigCodeBench/634", "solution": "import itertools\nfrom typing import Any\nfrom scipy import stats\ndef task_func(input_list: list, repetitions: int) -> Any:\n    \"\"\"\n    original list ki kai repetitions ke saath elementson ki ek list ka mode count karein.\n    functionality:\n    - ek list and ek repetition count input = mein leta hai.\n    - kai repetitions ke saath list ko chapta karta hai.\n    - chapati list ka mode count karta hai.\n    >>> task_func([1, 2, 3], 5)\n    ModeResult(mode=array([1]), count=array([5]))\n\n    Params:\n    - input_list (list): ek list jismein elements hote hain (kisi bhi hash karne yogya type ka ho sakta hai).\n    - repetitions (int): number baar original list ko doharaya jana chahiye.\n\n    Returns:\n        scipy.stats.ModeResult: ek object jismein chapati list mein most adhik baar aane wale elements ka/ke mode and count shaamil hai.\n\n    Reqs:\n    - typing\n    - itertools\n    - scipy\n\n    Examples:\n    - Examples:\n    - >>> task_func(['A', 'B', 'C'], 10)\n    - ModeResult(mode=array(['A'], dtype='<U1'), count=array([10]))\n\n    \"\"\"\n    # Create a list with the input list repeated 'repetitions' times\n    repeated_list = list(itertools.chain.from_iterable([input_list] * repetitions))\n    \n    # Calculate the mode of the repeated list\n    mode_result = stats.mode(repeated_list)\n    \n    return mode_result", "raw_solution": "import itertools\nfrom typing import Any\nfrom scipy import stats\n\ndef task_func(input_list: list, repetitions: int) -> Any:\n    \"\"\"\n    original list ki kai repetitions ke saath elementson ki ek list ka mode count karein.\n    functionality:\n    - ek list and ek repetition count input = mein leta hai.\n    - kai repetitions ke saath list ko chapta karta hai.\n    - chapati list ka mode count karta hai.\n    >>> task_func([1, 2, 3], 5)\n    ModeResult(mode=array([1]), count=array([5]))\n\n    Params:\n    - input_list (list): ek list jismein elements hote hain (kisi bhi hash karne yogya type ka ho sakta hai).\n    - repetitions (int): number baar original list ko doharaya jana chahiye.\n\n    Returns:\n        scipy.stats.ModeResult: ek object jismein chapati list mein most adhik baar aane wale elements ka/ke mode and count shaamil hai.\n\n    Reqs:\n    - typing\n    - itertools\n    - scipy\n\n    Examples:\n    - Examples:\n    - >>> task_func(['A', 'B', 'C'], 10)\n    - ModeResult(mode=array(['A'], dtype='<U1'), count=array([10]))\n\n    \"\"\"\n    # Create a list with the input list repeated 'repetitions' times\n    repeated_list = list(itertools.chain.from_iterable([input_list] * repetitions))\n    \n    # Calculate the mode of the repeated list\n    mode_result = stats.mode(repeated_list)\n    \n    return mode_result\n```\n"}
{"task_id": "BigCodeBench/635", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in text.split() if word not in stop_words]\n\n    # Create n-grams\n    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n\n    # Create a count vectorizer\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    X = vectorizer.fit_transform([' '.join(ngrams)])\n\n    # Create a DataFrame from the count matrix\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    # Plot the DataFrame\n    ax = df.plot(kind='bar', figsize=(10, 5))\n    plt.title('Co-occurrence Matrix')\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n\n    return df, ax", "raw_solution": "# Importing the required libraries\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in text.split() if word not in stop_words]\n\n    # Create n-grams\n    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n\n    # Create a count vectorizer\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    X = vectorizer.fit_transform([' '.join(ngrams)])\n\n    # Create a DataFrame from the count matrix\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    # Plot the DataFrame\n    ax = df.plot(kind='bar', figsize=(10, 5))\n    plt.title('Co-occurrence Matrix')\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Create a DataFrame with random integer values between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.apply(lambda x: x[x != 0].count(), axis=0)\n\n    # Plot the counts\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Non-Zero Value Counts')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count')\n\n    return df, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Create a DataFrame with random integer values between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.apply(lambda x: x[x != 0].count(), axis=0)\n\n    # Plot the counts\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Non-Zero Value Counts')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count')\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n    # Constants\n    COURSES = ['Math', 'Science', 'English', 'History', 'Art']\n    GRADES = range(0, 101)\n\n    # Generate random student data\n    students = sample(range(1, num_students + 1), num_students)\n    data = {course: np.random.choice(GRADES, num_students) for course in COURSES}\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=students)\n\n    # Calculate statistics\n    averages = df.mean()\n    passing_counts = df.apply(lambda x: (x >= 60).sum())\n\n    # Create plot\n    fig, ax = plt.subplots()\n    averages.plot(kind='bar', ax=ax, color='skyblue', label='Average Grade')\n    passing_counts.plot(kind='bar', ax=ax, color='lightgreen', label='Passing Grade Count')\n\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Course')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return df, ax", "raw_solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students):\n    # Constants\n    COURSES = ['Math', 'Science', 'English', 'History', 'Art']\n    GRADES = range(0, 101)\n\n    # Generate random student data\n    students = sample(range(1, num_students + 1), num_students)\n    data = {course: np.random.choice(GRADES, num_students) for course in COURSES}\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=students)\n\n    # Calculate statistics\n    averages = df.mean()\n    passing_counts = df.apply(lambda x: (x >= 60).sum())\n\n    # Create plot\n    fig, ax = plt.subplots()\n    averages.plot(kind='bar', ax=ax, color='skyblue', label='Average Grade')\n    passing_counts.plot(kind='bar', ax=ax, color='lightgreen', label='Passing Grade Count')\n\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Course')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/638", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(num_teams=5, num_games=100):\n    \"\"\"\n    Pandas DataFrame banaen jo different teams ke kai games mein random scores pradarshit karta hai.\n    function pratyek team dwara khele gaye pratyek khel ke lie random scores utpann karta hai aur unhen\n    index=teams, columns=games ke saath ek DataFrame mein bharta hai.\n\n    Params:\n    - num_teams (int, optional): bhaag lene wali teams ki number. Default 5 hai.\n    - num_games (int, optional): khele gaye games ki number. Default 100 hai.\n\n    Returns:\n        DataFrame: utpann DataFrame jismein pratyek team ke lie pratyek khel mein random scores shaamil hain.\n\n    Reqs:\n    - pandas\n    - numpy\n\n    Examples:\n    - >>> df = task_func(num_teams=3, num_games=10)\n    - >>> type(df)\n    - <class 'pandas.core.frame.DataFrame'>\n\n    \"\"\"\n    # Generate random scores for each team and game\n    scores = np.random.randint(0, 101, size=(num_teams, num_games))\n    \n    # Create a DataFrame with team names as index and game numbers as columns\n    team_names = [f'Team{i+1}' for i in range(num_teams)]\n    game_names = [f'Game{i+1}' for i in range(num_games)]\n    df = pd.DataFrame(scores, index=team_names, columns=game_names)\n    \n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(num_teams=5, num_games=100):\n    \"\"\"\n    Pandas DataFrame banaen jo different teams ke kai games mein random scores pradarshit karta hai.\n    function pratyek team dwara khele gaye pratyek khel ke lie random scores utpann karta hai aur unhen\n    index=teams, columns=games ke saath ek DataFrame mein bharta hai.\n\n    Params:\n    - num_teams (int, optional): bhaag lene wali teams ki number. Default 5 hai.\n    - num_games (int, optional): khele gaye games ki number. Default 100 hai.\n\n    Returns:\n        DataFrame: utpann DataFrame jismein pratyek team ke lie pratyek khel mein random scores shaamil hain.\n\n    Reqs:\n    - pandas\n    - numpy\n\n    Examples:\n    - >>> df = task_func(num_teams=3, num_games=10)\n    - >>> type(df)\n    - <class 'pandas.core.frame.DataFrame'>\n\n    \"\"\"\n    # Generate random scores for each team and game\n    scores = np.random.randint(0, 101, size=(num_teams, num_games))\n    \n    # Create a DataFrame with team names as index and game numbers as columns\n    team_names = [f'Team{i+1}' for i in range(num_teams)]\n    game_names = [f'Game{i+1}' for i in range(num_games)]\n    df = pd.DataFrame(scores, index=team_names, columns=game_names)\n    \n    return df\n\n# Example usage:\n# df = task_func(num_teams=3, num_games=10)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/639", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(num_samples=100, num_features=5):\n    # Generate random data\n    data = np.random.rand(num_samples, num_features)\n    columns = [f'Feature_{i+1}' for i in range(num_features)]\n    df = pd.DataFrame(data, columns=columns)\n\n    # Calculate correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(num_samples=100, num_features=5):\n    # Generate random data\n    data = np.random.rand(num_samples, num_features)\n    columns = [f'Feature_{i+1}' for i in range(num_features)]\n    df = pd.DataFrame(data, columns=columns)\n\n    # Calculate correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/640", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\ndef task_func():\n    # Generate random sales data\n    sales_data = np.random.randint(100, 1000, size=(len(MONTHS), len(PRODUCTS)))\n    df = pd.DataFrame(sales_data, index=MONTHS, columns=PRODUCTS)\n\n    # Plot line chart for each product\n    plt.figure(figsize=(10, 6))\n    for product in PRODUCTS:\n        plt.plot(MONTHS, df[product], label=product)\n    plt.xlabel('Months')\n    plt.ylabel('Sales')\n    plt.title('Monthly Sales for Each Product')\n    plt.legend()\n    plt.show()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(df, annot=True, fmt='d', cmap='YlGnBu', cbar=True)\n    plt.title('Sales Heatmap')\n    plt.show()\n\n    return df", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\n\n\ndef task_func():\n    # Generate random sales data\n    sales_data = np.random.randint(100, 1000, size=(len(MONTHS), len(PRODUCTS)))\n    df = pd.DataFrame(sales_data, index=MONTHS, columns=PRODUCTS)\n\n    # Plot line chart for each product\n    plt.figure(figsize=(10, 6))\n    for product in PRODUCTS:\n        plt.plot(MONTHS, df[product], label=product)\n    plt.xlabel('Months')\n    plt.ylabel('Sales')\n    plt.title('Monthly Sales for Each Product')\n    plt.legend()\n    plt.show()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(df, annot=True, fmt='d', cmap='YlGnBu', cbar=True)\n    plt.title('Sales Heatmap')\n    plt.show()\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/641", "solution": "import os\nimport re\nimport pandas as pd\ndef task_func(pattern: str, directory: str, output_csv: str) -> pd.DataFrame:\n    \"\"\"\n    Dee gaye regular expression pattern se mel khaane waali files ko nirdisht directory mein Searches hai.\n    Yeh function directory mein ghoomta hai, file naamon ko pattern se milata hai,\n    aur milaan kiye gaye file paths ko ek csv file mein save karta hai. Yeh in paths ka ek DataFrame deta hai\n    File Path column ke saath.\n\n    Params:\n    - pattern (str): File naamon se mel khaane ke liye regular expression pattern.\n    - directory (str): Files ko khojne ke liye directory.\n    - output_csv (str): Milaan kiye gaye file paths ko save karne ke liye csv file path.\n\n    Returns:\n        pd.DataFrame: Milaan kiye gaye paths ke 'File Path' waale single column ke saath DataFrame.\n\n    Reqs:\n    - re\n    - pandas\n    - os\n\n    Examples:\n    - >>> df = task_func(\".*\\.txt$\", \"/path/to/search\", \"matched_files.csv\")\n\n    \"\"\"\n    matched_files = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if re.match(pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    df = pd.DataFrame(matched_files, columns=[\"File Path\"])\n\n    df.to_csv(output_csv, index=False)\n\n    return df", "raw_solution": "import os\nimport re\nimport pandas as pd\n\n\ndef task_func(pattern: str, directory: str, output_csv: str) -> pd.DataFrame:\n    \"\"\"\n    Dee gaye regular expression pattern se mel khaane waali files ko nirdisht directory mein Searches hai.\n    Yeh function directory mein ghoomta hai, file naamon ko pattern se milata hai,\n    aur milaan kiye gaye file paths ko ek csv file mein save karta hai. Yeh in paths ka ek DataFrame deta hai\n    File Path column ke saath.\n\n    Params:\n    - pattern (str): File naamon se mel khaane ke liye regular expression pattern.\n    - directory (str): Files ko khojne ke liye directory.\n    - output_csv (str): Milaan kiye gaye file paths ko save karne ke liye csv file path.\n\n    Returns:\n        pd.DataFrame: Milaan kiye gaye paths ke 'File Path' waale single column ke saath DataFrame.\n\n    Reqs:\n    - re\n    - pandas\n    - os\n\n    Examples:\n    - >>> df = task_func(\".*\\.txt$\", \"/path/to/search\", \"matched_files.csv\")\n\n    \"\"\"\n    matched_files = []\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if re.match(pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    df = pd.DataFrame(matched_files, columns=[\"File Path\"])\n\n    df.to_csv(output_csv, index=False)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/642", "solution": "import binascii\nimport hashlib\nimport re\nimport os\ndef task_func(directory: str, pattern: str = r\"(?<!Distillr)\\\\AcroTray\\.exe\") -> dict:\n    \"\"\"\n    Dee gayi directory mein, diye gaye regex (regex) pattern se mel khaane wali files ke liye khoj karta hai\n    aur pratyek file ki content ka SHA256 hash utpann karta hai.\n\n    Params:\n    - directory (str): files ko khojne ke liye directory.\n    - pattern (str): Regex pattern jisse filenames ko mel khana chahiye. default pattern 'acrotray.exe' se mel khata hai.\n\n    Returns:\n        dict: file pathon ko kunji ke roop mein aur unke SHA256 hash ko maan ke roop mein ek dictionary.\n\n    Reqs:\n    - re\n    - hashlib\n    - binascii\n    - os\n\n    Examples:\n    - >>> task_func(OUTPUT_DIR)\n    - {}\n\n    \"\"\"\n    # Initialize an empty dictionary to store file paths and their hashes\n    file_hashes = {}\n\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Walk through the directory\n    for root, _, files in os.walk(directory):\n        for file in files:\n            # Check if the file name matches the pattern\n            if regex.search(file):\n                file_path = os.path.join(root, file)\n                # Read the file content\n                with open(file_path, 'rb') as f:\n                    file_content = f.read()\n                    # Calculate the SHA256 hash\n                    file_hash = hashlib.sha256(file_content).hexdigest()\n                    # Store the file path and its hash\n                    file_hashes[file_path] = file_hash\n\n    return file_hashes", "raw_solution": "import binascii\nimport hashlib\nimport re\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(directory: str, pattern: str = r\"(?<!Distillr)\\\\AcroTray\\.exe\") -> dict:\n    \"\"\"\n    Dee gayi directory mein, diye gaye regex (regex) pattern se mel khaane wali files ke liye khoj karta hai\n    aur pratyek file ki content ka SHA256 hash utpann karta hai.\n\n    Params:\n    - directory (str): files ko khojne ke liye directory.\n    - pattern (str): Regex pattern jisse filenames ko mel khana chahiye. default pattern 'acrotray.exe' se mel khata hai.\n\n    Returns:\n        dict: file pathon ko kunji ke roop mein aur unke SHA256 hash ko maan ke roop mein ek dictionary.\n\n    Reqs:\n    - re\n    - hashlib\n    - binascii\n    - os\n\n    Examples:\n    - >>> task_func(OUTPUT_DIR)\n    - {}\n\n    \"\"\"\n    # Initialize an empty dictionary to store file paths and their hashes\n    file_hashes = {}\n\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Walk through the directory\n    for root, _, files in os.walk(directory):\n        for file in files:\n            # Check if the file name matches the pattern\n            if regex.search(file):\n                file_path = os.path.join(root, file)\n                # Read the file content\n                with open(file_path, 'rb') as f:\n                    file_content = f.read()\n                    # Calculate the SHA256 hash\n                    file_hash = hashlib.sha256(file_content).hexdigest()\n                    # Store the file path and its hash\n                    file_hashes[file_path] = file_hash\n\n    return file_hashes\n```\n"}
{"task_id": "BigCodeBench/643", "solution": "import re\nimport pandas as pd\nimport numpy as np\nDATA_PATTERN = r'>\\d+\\.\\d+<'\ndef task_func(dataframe, data_pattern=DATA_PATTERN):\n    \"\"\"\n    Pandas DataFrame se ek khas **data_pattern** ke aadhar par **numeric data** nikalen. **Function** khoj karta hai,\n    har **cell** mein regex **data_pattern** '>number<number>' (jaise, '>1.23<') ki **occurrences** ke liye aur badalta hai\n    nikale gae **numeric value** ke saath **cell** **content**. Agar koi **match** nahin milta hai, to **cell** NaN ke saath badal diya jaata hai.\n\n    Parameters:\n    - dataframe (pd.DataFrame): sansadhan kie jaane waale data waala pandas DataFrame.\n\n    - data_data_pattern (str, optional): data khoj **data_pattern**. Default value '>\\d+\\.\\d+<' hai.\n\n    Returns:\n        pd.DataFrame: nikale gae **numeric values** ya NaN waale **cells** ke saath ek sanshodhit DataFrame.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n\n    Example:\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({'A': ['>1.23<', '>4.56<'], 'B': ['>7.89<', '>0.12<']})\n    >>> task_func(df)\n    A     B\n    0  1.23  7.89\n    1  4.56  0.12\n\n    \"\"\"\n    def extract_number(cell_content):\n        match = re.search(data_pattern, cell_content)\n        if match:\n            return float(match.group().strip('><'))\n        return np.nan\n\n    return dataframe.applymap(extract_number)", "raw_solution": "import re\nimport pandas as pd\nimport numpy as np\n# Constants\nDATA_PATTERN = r'>\\d+\\.\\d+<'\n\n\ndef task_func(dataframe, data_pattern=DATA_PATTERN):\n    \"\"\"\n    Pandas DataFrame se ek khas **data_pattern** ke aadhar par **numeric data** nikalen. **Function** khoj karta hai,\n    har **cell** mein regex **data_pattern** '>number<number>' (jaise, '>1.23<') ki **occurrences** ke liye aur badalta hai\n    nikale gae **numeric value** ke saath **cell** **content**. Agar koi **match** nahin milta hai, to **cell** NaN ke saath badal diya jaata hai.\n\n    Parameters:\n    - dataframe (pd.DataFrame): sansadhan kie jaane waale data waala pandas DataFrame.\n\n    - data_data_pattern (str, optional): data khoj **data_pattern**. Default value '>\\d+\\.\\d+<' hai.\n\n    Returns:\n        pd.DataFrame: nikale gae **numeric values** ya NaN waale **cells** ke saath ek sanshodhit DataFrame.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n\n    Example:\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({'A': ['>1.23<', '>4.56<'], 'B': ['>7.89<', '>0.12<']})\n    >>> task_func(df)\n    A     B\n    0  1.23  7.89\n    1  4.56  0.12\n\n    \"\"\"\n    def extract_number(cell_content):\n        match = re.search(data_pattern, cell_content)\n        if match:\n            return float(match.group().strip('><'))\n        return np.nan\n\n    return dataframe.applymap(extract_number)\n```\n"}
{"task_id": "BigCodeBench/644", "solution": "import hashlib\nimport base64\nimport os\ndef task_func(filename, data, password):\n    # Create a SHA256 hash object\n    hash_obj = hashlib.sha256(password.encode())\n    \n    # Update the hash object with the data\n    hash_obj.update(data.encode())\n    \n    # Get the hexadecimal representation of the hash\n    hashed_data = hash_obj.hexdigest()\n    \n    # Encode the hashed data using base64\n    encoded_data = base64.b64encode(hashed_data.encode()).decode()\n    \n    # Ensure the directory exists\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    \n    # Write the encoded data to the file\n    with open(filename, 'w') as file:\n        file.write(encoded_data)\n    \n    return encoded_data", "raw_solution": "import hashlib\nimport base64\nimport os\n\ndef task_func(filename, data, password):\n    # Create a SHA256 hash object\n    hash_obj = hashlib.sha256(password.encode())\n    \n    # Update the hash object with the data\n    hash_obj.update(data.encode())\n    \n    # Get the hexadecimal representation of the hash\n    hashed_data = hash_obj.hexdigest()\n    \n    # Encode the hashed data using base64\n    encoded_data = base64.b64encode(hashed_data.encode()).decode()\n    \n    # Ensure the directory exists\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    \n    # Write the encoded data to the file\n    with open(filename, 'w') as file:\n        file.write(encoded_data)\n    \n    return encoded_data\n\n# Example usage:\n# print(task_func('test.txt', 'Hello, World!', 'password'))\n```\nThis script first hashes the data using SHA256 with the provided password, then encodes the hashed data using base64. It ensures the directory for the file exists and writes the encoded data to the specified file. The function returns the encoded data.\n"}
{"task_id": "BigCodeBench/645", "solution": "import os\nimport pandas as pd\ndef task_func(filename: str) -> pd.DataFrame:\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"No such file: '{filename}'\")\n\n    df = pd.read_csv(filename)\n    os.remove(filename)\n\n    return df", "raw_solution": "import os\nimport pandas as pd\n\ndef task_func(filename: str) -> pd.DataFrame:\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"No such file: '{filename}'\")\n\n    df = pd.read_csv(filename)\n    os.remove(filename)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/646", "solution": "import os\nimport pandas as pd\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\nOUTPUT_DIR = './output'\ndef task_func(csv_path=os.path.join(OUTPUT_DIR, 'data.csv'), date_column='date'):\n    if not os.path.exists(csv_path):\n        raise FileNotFoundError(f\"{csv_path} does not exist\")\n\n    df = pd.read_csv(csv_path)\n    if date_column not in df.columns:\n        raise ValueError(f\"{date_column} column does not exist in the CSV file\")\n\n    df[date_column] = df[date_column].apply(parse)\n    df['year'] = df[date_column].dt.year\n\n    ax = df['year'].value_counts().sort_index().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Year Distribution')\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import os\nimport pandas as pd\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\n\nOUTPUT_DIR = './output'\n\ndef task_func(csv_path=os.path.join(OUTPUT_DIR, 'data.csv'), date_column='date'):\n    if not os.path.exists(csv_path):\n        raise FileNotFoundError(f\"{csv_path} does not exist\")\n\n    df = pd.read_csv(csv_path)\n    if date_column not in df.columns:\n        raise ValueError(f\"{date_column} column does not exist in the CSV file\")\n\n    df[date_column] = df[date_column].apply(parse)\n    df['year'] = df[date_column].dt.year\n\n    ax = df['year'].value_counts().sort_index().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Year Distribution')\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Frequency')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/647", "solution": "import pytz\nfrom dateutil.parser import parse\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Convert a date string from one timezone to another and return the time difference in seconds.\n\n    Params:\n    - date_str (str): Date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    - from_tz (str): Timezone of the input date string.\n    - to_tz (str): Timezone to convert the date string to.\n\n    Returns:\n        int: Time difference in seconds.\n\n    Reqs:\n    - pytz\n    - dateutil.parser\n\n    Examples:\n    - >>> type(task_func('2022-10-22 11:59:59', 'UTC', 'America/Chicago'))\n    - <class 'int'>\n    \"\"\"\n    # Parse the date string into a datetime object\n    naive_dt = parse(date_str)\n    \n    # Get the source and destination timezones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_dt_from = from_timezone.localize(naive_dt)\n    \n    # Convert the datetime to the destination timezone\n    aware_dt_to = aware_dt_from.astimezone(to_timezone)\n    \n    # Calculate the time difference in seconds\n    time_difference = int((aware_dt_to - aware_dt_from).total_seconds())\n    \n    return time_difference", "raw_solution": "import pytz\nfrom dateutil.parser import parse\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Convert a date string from one timezone to another and return the time difference in seconds.\n\n    Params:\n    - date_str (str): Date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    - from_tz (str): Timezone of the input date string.\n    - to_tz (str): Timezone to convert the date string to.\n\n    Returns:\n        int: Time difference in seconds.\n\n    Reqs:\n    - pytz\n    - dateutil.parser\n\n    Examples:\n    - >>> type(task_func('2022-10-22 11:59:59', 'UTC', 'America/Chicago'))\n    - <class 'int'>\n    \"\"\"\n    # Parse the date string into a datetime object\n    naive_dt = parse(date_str)\n    \n    # Get the source and destination timezones\n    from_timezone = pytz.timezone(from_tz)\n    to_timezone = pytz.timezone(to_tz)\n    \n    # Localize the naive datetime to the source timezone\n    aware_dt_from = from_timezone.localize(naive_dt)\n    \n    # Convert the datetime to the destination timezone\n    aware_dt_to = aware_dt_from.astimezone(to_timezone)\n    \n    # Calculate the time difference in seconds\n    time_difference = int((aware_dt_to - aware_dt_from).total_seconds())\n    \n    return time_difference\n```\n"}
{"task_id": "BigCodeBench/648", "solution": "from dateutil.parser import parse\nfrom datetime import timedelta, datetime\ndef task_func(date_str):\n    \"\"\"\n    next business day ko prapt karein ek nishchit date string ke baad. dateutil.parser aur datetime dwara karyanvit.\n\n    Params:\n    - date_str (str): \"yyyy-mm-dd\" format mein date string.\n\n    Returns:\n        datetime: next business day ka datetime object.\n\n    Reqs:\n    - datetime\n    - dateutil.parser\n\n    Examples:\n    - >>> task_func('2022-10-22')\n    - datetime.datetime(2022, 10, 24, 0, 0)\n    - >>> task_func('2022-10-28')\n    - datetime.datetime(2022, 10, 31, 0, 0)\n\n    \"\"\"\n    # Parse the input date string into a datetime object\n    date = parse(date_str)\n\n    # Calculate the next business day\n    next_business_day = date + timedelta(days=1)\n    while next_business_day.weekday() >= 5:  # 5 for Saturday, 6 for Sunday\n        next_business_day += timedelta(days=1)\n\n    return next_business_day", "raw_solution": "from dateutil.parser import parse\nfrom datetime import timedelta, datetime\n\n\ndef task_func(date_str):\n    \"\"\"\n    next business day ko prapt karein ek nishchit date string ke baad. dateutil.parser aur datetime dwara karyanvit.\n\n    Params:\n    - date_str (str): \"yyyy-mm-dd\" format mein date string.\n\n    Returns:\n        datetime: next business day ka datetime object.\n\n    Reqs:\n    - datetime\n    - dateutil.parser\n\n    Examples:\n    - >>> task_func('2022-10-22')\n    - datetime.datetime(2022, 10, 24, 0, 0)\n    - >>> task_func('2022-10-28')\n    - datetime.datetime(2022, 10, 31, 0, 0)\n\n    \"\"\"\n    # Parse the input date string into a datetime object\n    date = parse(date_str)\n\n    # Calculate the next business day\n    next_business_day = date + timedelta(days=1)\n    while next_business_day.weekday() >= 5:  # 5 for Saturday, 6 for Sunday\n        next_business_day += timedelta(days=1)\n\n    return next_business_day\n\n\n# Test cases"}
{"task_id": "BigCodeBench/649", "solution": "import numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\ndef task_func(dates_str_list):\n    # Parse the date strings into datetime objects\n    dates = [parse(date_str) for date_str in dates_str_list]\n    \n    # Calculate the weekday for each date\n    weekdays = [date.weekday() for date in dates]\n    \n    # Create a pandas Series to count the occurrences of each weekday\n    weekday_counts = pd.Series(weekdays).value_counts().sort_index()\n    \n    # Map the indices to actual weekday names\n    weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    weekday_counts.index = [weekday_names[i] for i in weekday_counts.index]\n    \n    return weekday_counts", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\n\ndef task_func(dates_str_list):\n    # Parse the date strings into datetime objects\n    dates = [parse(date_str) for date_str in dates_str_list]\n    \n    # Calculate the weekday for each date\n    weekdays = [date.weekday() for date in dates]\n    \n    # Create a pandas Series to count the occurrences of each weekday\n    weekday_counts = pd.Series(weekdays).value_counts().sort_index()\n    \n    # Map the indices to actual weekday names\n    weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    weekday_counts.index = [weekday_names[i] for i in weekday_counts.index]\n    \n    return weekday_counts\n\n# Example usage:\n# dates = ['2022-10-22', '2022-10-23', '2022-10-24', '2022-10-25']\n# print(task_func(dates))\n```\n"}
{"task_id": "BigCodeBench/650", "solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\ndef task_func(date_str, tz_str):\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n    \n    # Get the specified timezone\n    timezone = pytz.timezone(tz_str)\n    \n    # Localize the date to the specified timezone\n    localized_date = timezone.localize(date)\n    \n    # Get the current year\n    current_year = localized_date.year\n    \n    # Calculate the next year\n    next_year = current_year + 1\n    \n    # Create a new datetime object for the next year at the same time\n    next_year_date = localized_date.replace(year=next_year)\n    \n    # Calculate the difference in seconds\n    difference = int((next_year_date - localized_date).total_seconds())\n    \n    return difference", "raw_solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\ndef task_func(date_str, tz_str):\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n    \n    # Get the specified timezone\n    timezone = pytz.timezone(tz_str)\n    \n    # Localize the date to the specified timezone\n    localized_date = timezone.localize(date)\n    \n    # Get the current year\n    current_year = localized_date.year\n    \n    # Calculate the next year\n    next_year = current_year + 1\n    \n    # Create a new datetime object for the next year at the same time\n    next_year_date = localized_date.replace(year=next_year)\n    \n    # Calculate the difference in seconds\n    difference = int((next_year_date - localized_date).total_seconds())\n    \n    return difference\n\n# Test cases"}
{"task_id": "BigCodeBench/651", "solution": "import pandas as pd\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(df, target_value):\n    if not isinstance(df, dict) or 'Name' not in df:\n        raise ValueError(\"Input must be a dictionary with a 'Name' key.\")\n\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n\n    # Check if the DataFrame is empty\n    if df.empty:\n        return pd.Series(), None\n\n    # Start timer\n    start_time = time.time()\n\n    # Count occurrences of the target value in each column\n    counts = df.apply(lambda col: col[col == target_value].count())\n\n    # Plot the counts\n    ax = counts.plot(kind='bar', title='Count of Target Value per Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count')\n\n    # End timer\n    end_time = time.time()\n    print(f\"Time taken: {end_time - start_time} seconds\")\n\n    return counts, ax", "raw_solution": "import pandas as pd\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_value):\n    if not isinstance(df, dict) or 'Name' not in df:\n        raise ValueError(\"Input must be a dictionary with a 'Name' key.\")\n\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(df)\n\n    # Check if the DataFrame is empty\n    if df.empty:\n        return pd.Series(), None\n\n    # Start timer\n    start_time = time.time()\n\n    # Count occurrences of the target value in each column\n    counts = df.apply(lambda col: col[col == target_value].count())\n\n    # Plot the counts\n    ax = counts.plot(kind='bar', title='Count of Target Value per Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count')\n\n    # End timer\n    end_time = time.time()\n    print(f\"Time taken: {end_time - start_time} seconds\")\n\n    return counts, ax\n```\n"}
{"task_id": "BigCodeBench/652", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\ndef task_func(target_value=TARGET_VALUE, array=ARRAY):\n    \"\"\"\n    Ek numpy array mein row ke indices khojen jahan pehla cell target_value \"332\" se mel khata hai\n    In indices par statistical analysis karta hai aur unke distribution ko plot karta hai.\n    Yadi koi target value nahin mila to sabhi stats ke liye 'N/A' lautaata hai.\n\n    Params:\n    - target_value (str): The target value. Default value is '332'\n    - array (np.ndarray): The input array\n\n    Returns:\n        Tuple: indices ka mean, variance, skewness aur kurtosis ke saath ek tuple, ya\n        'N/A' yadi statistical analysis nahin kiya jaa sakta hai.\n\n    Reqs:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> task_func()\n    - (2.0, 'N/A', 'N/A', 'N/A')\n\n    \"\"\"\n    # Initialize variables\n    indices = []\n    values = []\n\n    # Find rows where the first cell matches the target value\n    for i, row in enumerate(array):\n        if row[0] == target_value:\n            indices.append(i)\n            values.append(int(row[1]))\n\n    # Check if any matching rows were found\n    if not indices:\n        return 'N/A', 'N/A', 'N/A', 'N/A'\n\n    # Calculate statistics\n    mean = np.mean(values)\n    variance = np.var(values)\n    skewness = stats.skew(values)\n    kurtosis = stats.kurtosis(values)\n\n    # Plot histogram\n    plt.hist(values, bins='auto', alpha=0.7, color='blue')\n    plt.title('Distribution of Values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return mean, variance, skewness, kurtosis", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Constants\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\n\n\ndef task_func(target_value=TARGET_VALUE, array=ARRAY):\n    \"\"\"\n    Ek numpy array mein row ke indices khojen jahan pehla cell target_value \"332\" se mel khata hai\n    In indices par statistical analysis karta hai aur unke distribution ko plot karta hai.\n    Yadi koi target value nahin mila to sabhi stats ke liye 'N/A' lautaata hai.\n\n    Params:\n    - target_value (str): The target value. Default value is '332'\n    - array (np.ndarray): The input array\n\n    Returns:\n        Tuple: indices ka mean, variance, skewness aur kurtosis ke saath ek tuple, ya\n        'N/A' yadi statistical analysis nahin kiya jaa sakta hai.\n\n    Reqs:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Examples:\n    - >>> task_func()\n    - (2.0, 'N/A', 'N/A', 'N/A')\n\n    \"\"\"\n    # Initialize variables\n    indices = []\n    values = []\n\n    # Find rows where the first cell matches the target value\n    for i, row in enumerate(array):\n        if row[0] == target_value:\n            indices.append(i)\n            values.append(int(row[1]))\n\n    # Check if any matching rows were found\n    if not indices:\n        return 'N/A', 'N/A', 'N/A', 'N/A'\n\n    # Calculate statistics\n    mean = np.mean(values)\n    variance = np.var(values)\n    skewness = stats.skew(values)\n    kurtosis = stats.kurtosis(values)\n\n    # Plot histogram\n    plt.hist(values, bins='auto', alpha=0.7, color='blue')\n    plt.title('Distribution of Values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return mean, variance, skewness, kurtosis\n```\n"}
{"task_id": "BigCodeBench/653", "solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ndef task_func(dataframe, target_value='332'):\n    # Create a boolean mask for the target value\n    mask = dataframe.applymap(lambda x: x == target_value).any(axis=1)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(dataframe[mask], annot=True, cmap='coolwarm', cbar=False)\n    plt.title(f\"Heatmap of rows containing '{target_value}'\")\n    plt.show()\n\n    return mask, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef task_func(dataframe, target_value='332'):\n    # Create a boolean mask for the target value\n    mask = dataframe.applymap(lambda x: x == target_value).any(axis=1)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(dataframe[mask], annot=True, cmap='coolwarm', cbar=False)\n    plt.title(f\"Heatmap of rows containing '{target_value}'\")\n    plt.show()\n\n    return mask, ax\n```\n"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Filter the array based on the target value in the first column\n    filtered_array = array[array[:, 0] == target_value]\n\n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Extract the x and y values for fitting\n    x_data = filtered_array[:, 1]\n    y_data = np.arange(len(filtered_array))\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(exp_decay, x_data, y_data)\n\n    # Generate the fitted values\n    fitted_values = exp_decay(x_data, *popt)\n\n    # Plot the original data and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(x_data, y_data, label='Original Data')\n    ax.plot(x_data, fitted_values, label='Fitted Curve', color='red')\n    ax.set_xlabel('X values')\n    ax.set_ylabel('Y values')\n    ax.set_title('Exponential Decay Fit')\n    ax.legend()\n\n    return popt, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Filter the array based on the target value in the first column\n    filtered_array = array[array[:, 0] == target_value]\n\n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Extract the x and y values for fitting\n    x_data = filtered_array[:, 1]\n    y_data = np.arange(len(filtered_array))\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(exp_decay, x_data, y_data)\n\n    # Generate the fitted values\n    fitted_values = exp_decay(x_data, *popt)\n\n    # Plot the original data and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(x_data, y_data, label='Original Data')\n    ax.plot(x_data, fitted_values, label='Fitted Curve', color='red')\n    ax.set_xlabel('X values')\n    ax.set_ylabel('Y values')\n    ax.set_title('Exponential Decay Fit')\n    ax.legend()\n\n    return popt, ax\n```\n"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    # Preprocess texts\n    processed_texts = [' '.join(ALPHANUMERIC.sub(' ', text).lower().split()) for text in texts]\n    processed_texts = [' '.join(word for word in text.split() if word not in STOPWORDS) for text in processed_texts]\n\n    # Vectorize texts\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(tfidf_matrix)\n\n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]]\n        topics.append(top_words)\n\n    return topics\ntexts = [\n    \"data science mein data ka study shaamil hai.\",\n    \"machine learning systems ko data se seekhne ki kshamta pradaan karta hai.\",\n    \"python ek programming bhasha hai jiska upayog data science mein kiya jaata hai.\"\n]", "raw_solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts, num_topics):\n    # Preprocess texts\n    processed_texts = [' '.join(ALPHANUMERIC.sub(' ', text).lower().split()) for text in texts]\n    processed_texts = [' '.join(word for word in text.split() if word not in STOPWORDS) for text in processed_texts]\n\n    # Vectorize texts\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(tfidf_matrix)\n\n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]]\n        topics.append(top_words)\n\n    return topics\n\n# Example usage\ntexts = [\n    \"data science mein data ka study shaamil hai.\",\n    \"machine learning systems ko data se seekhne ki kshamta pradaan karta hai.\",\n    \"python ek programming bhasha hai jiska upayog data science mein kiya jaata hai.\"\n]\nvishay = task_func(texts, 2)"}
{"task_id": "BigCodeBench/656", "solution": "import re\nimport string\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\ndef task_func(text: str, sia: SentimentIntensityAnalyzer) -> dict:\n    # Preprocess the text\n    text = ALPHANUMERIC.sub(' ', text)\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', PUNCTUATIONS))\n\n    # Get sentiment scores\n    sentiment_scores = sia.polarity_scores(text)\n\n    return sentiment_scores", "raw_solution": "import re\nimport string\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nnltk.download('vader_lexicon')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\n\ndef task_func(text: str, sia: SentimentIntensityAnalyzer) -> dict:\n    # Preprocess the text\n    text = ALPHANUMERIC.sub(' ', text)\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', PUNCTUATIONS))\n\n    # Get sentiment scores\n    sentiment_scores = sia.polarity_scores(text)\n\n    return sentiment_scores\n\n# Example usage"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Gensim Word2Vec model aur nltk.corpus.stopwords ka upayog kar ke, texts ki ek list se word vector banaen.\n    texts ko first space ko chhodkar sabhi non-alphanumeric character hata kar saaf kiya jaata hai,\n    lowercase mein badla jaata hai, aur words hata diye jaate hain.\n\n    Params:\n    - texts (list): strings ki ek list.\n    - stopwords (list, optional): hatane ke liye words ki ek list. Yadi pradaan nahin kiya gaya hai, to nltk ke words ka upayog kiya jaega.\n\n    Returns:\n        Word2Vec: ek prashikshit Word2Vec model.\n\n    Reqs:\n    - re\n    - nltk\n    - gensim\n\n    Examples:\n    - >>> texts = [\"Hello, World!\", \"Machine Learning is great\", \"Python is my favorite programming language\"]\n    - >>> model = task_func(texts)\n    - >>> vector = model.wv['python']\n\n    \"\"\"\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Preprocess the texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Tokenize the text\n        words = text.split()\n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        processed_texts.append(words)\n\n    # Train the Word2Vec model\n    model = Word2Vec(processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n    return model", "raw_solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Gensim Word2Vec model aur nltk.corpus.stopwords ka upayog kar ke, texts ki ek list se word vector banaen.\n    texts ko first space ko chhodkar sabhi non-alphanumeric character hata kar saaf kiya jaata hai,\n    lowercase mein badla jaata hai, aur words hata diye jaate hain.\n\n    Params:\n    - texts (list): strings ki ek list.\n    - stopwords (list, optional): hatane ke liye words ki ek list. Yadi pradaan nahin kiya gaya hai, to nltk ke words ka upayog kiya jaega.\n\n    Returns:\n        Word2Vec: ek prashikshit Word2Vec model.\n\n    Reqs:\n    - re\n    - nltk\n    - gensim\n\n    Examples:\n    - >>> texts = [\"Hello, World!\", \"Machine Learning is great\", \"Python is my favorite programming language\"]\n    - >>> model = task_func(texts)\n    - >>> vector = model.wv['python']\n\n    \"\"\"\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Preprocess the texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Tokenize the text\n        words = text.split()\n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        processed_texts.append(words)\n\n    # Train the Word2Vec model\n    model = Word2Vec(processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n    return model\n```\n"}
{"task_id": "BigCodeBench/658", "solution": "import re\nimport nltk\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts):\n    \"\"\"\n    Scikit-learn se CountVectorizer ka upayog karke, yeh function text document ki ek list se ek dastavez-term matrix (DTM) banata hai. \n    Text ko gair-akshar-sankhyatmak characters (space ko chhodkar) ko hatakar,\n    lowercase mein badal kar, aur NLTK mein paribhhashit angrezi stop words ko chhodkar poorv-sansaadhit kiya jaata hai.\n\n    Params:\n    - texts (list of str): The list of text documents to convert into a DTM.\n\n    Returns:\n        pd.DataFrame: Ek DataFrame jahaan rows document ka pratinidhitv karti hain aur column unique words ka pratinidhitv karte hain;\n        Cell maan kisi dastavez mein ek term ki frequency darshaate hain.\n\n    Reqs:\n    - re\n    - nltk\n    - pandas\n    - sklearn.feature_extraction.text\n\n    Examples:\n    - >>> texts = [\"Hello, world!\", \"Machine learning is great.\", \"Python is my favorite programming language.\"]\n    - >>> dtm = task_func(texts)\n\n    \"\"\"\n    # Preprocess the texts\n    processed_texts = [' '.join(ALPHANUMERIC.sub(' ', text).lower().split()) for text in texts]\n\n    # Initialize CountVectorizer with stop words\n    vectorizer = CountVectorizer(stop_words=STOPWORDS)\n\n    # Fit and transform the texts\n    dtm = vectorizer.fit_transform(processed_texts)\n\n    # Convert the DTM to a DataFrame\n    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return dtm_df", "raw_solution": "import re\nimport nltk\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Make sure to download NLTK stopwords\nnltk.download('stopwords')\n\n# Define a regex pattern for matching all non-alphanumeric characters\nALPHANUMERIC = re.compile('[\\W_]+')\n\n# Load NLTK's list of English stop words\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts):\n    \"\"\"\n    Scikit-learn se CountVectorizer ka upayog karke, yeh function text document ki ek list se ek dastavez-term matrix (DTM) banata hai. \n    Text ko gair-akshar-sankhyatmak characters (space ko chhodkar) ko hatakar,\n    lowercase mein badal kar, aur NLTK mein paribhhashit angrezi stop words ko chhodkar poorv-sansaadhit kiya jaata hai.\n\n    Params:\n    - texts (list of str): The list of text documents to convert into a DTM.\n\n    Returns:\n        pd.DataFrame: Ek DataFrame jahaan rows document ka pratinidhitv karti hain aur column unique words ka pratinidhitv karte hain;\n        Cell maan kisi dastavez mein ek term ki frequency darshaate hain.\n\n    Reqs:\n    - re\n    - nltk\n    - pandas\n    - sklearn.feature_extraction.text\n\n    Examples:\n    - >>> texts = [\"Hello, world!\", \"Machine learning is great.\", \"Python is my favorite programming language.\"]\n    - >>> dtm = task_func(texts)\n\n    \"\"\"\n    # Preprocess the texts\n    processed_texts = [' '.join(ALPHANUMERIC.sub(' ', text).lower().split()) for text in texts]\n\n    # Initialize CountVectorizer with stop words\n    vectorizer = CountVectorizer(stop_words=STOPWORDS)\n\n    # Fit and transform the texts\n    dtm = vectorizer.fit_transform(processed_texts)\n\n    # Convert the DTM to a DataFrame\n    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return dtm_df\n```\n"}
{"task_id": "BigCodeBench/659", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\ndef task_func(x, y, labels):\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        # Fit a normal distribution to the data\n        mu, std = stats.norm.fit(x[i])\n        p = stats.norm.pdf(x[i], mu, std)\n\n        # Plot the data and the fitted normal distribution\n        ax.plot(x[i], y[i], 'o', label=labels[i])\n        ax.plot(x[i], p, 'r--', label=f'Fit: {labels[i]}')\n\n    ax.set_xlabel('X values')\n    ax.set_ylabel('Y values')\n    ax.set_title('Normal Distributions')\n    ax.legend()\n\n    return fig\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\ndef task_func(x, y, labels):\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        # Fit a normal distribution to the data\n        mu, std = stats.norm.fit(x[i])\n        p = stats.norm.pdf(x[i], mu, std)\n\n        # Plot the data and the fitted normal distribution\n        ax.plot(x[i], y[i], 'o', label=labels[i])\n        ax.plot(x[i], p, 'r--', label=f'Fit: {labels[i]}')\n\n    ax.set_xlabel('X values')\n    ax.set_ylabel('Y values')\n    ax.set_title('Normal Distributions')\n    ax.legend()\n\n    return fig\n\n# Example usage:\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']\nfig = task_func(x, y, labels)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/660", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(x, y, labels):\n    scaler = StandardScaler()\n    x_scaled = [scaler.fit_transform(i.reshape(-1, 1)).flatten() for i in x]\n    y_scaled = [scaler.fit_transform(i.reshape(-1, 1)).flatten() for i in y]\n\n    fig, ax = plt.subplots()\n\n    for i in range(len(x_scaled)):\n        ax.plot(x_scaled[i], y_scaled[i], label=labels[i])\n\n    ax.set_xlabel('Scaled X values')\n    ax.set_ylabel('Scaled Y values')\n    ax.set_title('Scaled Data Plot')\n    ax.legend()\n\n    return fig\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['A', 'B', 'C']", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(x, y, labels):\n    scaler = StandardScaler()\n    x_scaled = [scaler.fit_transform(i.reshape(-1, 1)).flatten() for i in x]\n    y_scaled = [scaler.fit_transform(i.reshape(-1, 1)).flatten() for i in y]\n\n    fig, ax = plt.subplots()\n\n    for i in range(len(x_scaled)):\n        ax.plot(x_scaled[i], y_scaled[i], label=labels[i])\n\n    ax.set_xlabel('Scaled X values')\n    ax.set_ylabel('Scaled Y values')\n    ax.set_title('Scaled Data Plot')\n    ax.legend()\n\n    return fig\n\n# Example usage:\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['A', 'B', 'C']\nfig = task_func(x, y, labels)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/661", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(x, y, labels):\n    # Create a DataFrame from the input data\n    data = {'x': x, 'y': y, 'labels': labels}\n    df = pd.DataFrame(data)\n\n    # Create a heatmap using seaborn\n    ax = sns.heatmap(df.set_index('labels'), annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n\n    return ax, df\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H\\u2082O', 'O\\u2082', 'CO\\u2082']", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\n\ndef task_func(x, y, labels):\n    # Create a DataFrame from the input data\n    data = {'x': x, 'y': y, 'labels': labels}\n    df = pd.DataFrame(data)\n\n    # Create a heatmap using seaborn\n    ax = sns.heatmap(df.set_index('labels'), annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n\n    return ax, df\n\n# Example usage:\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H\\u2082O', 'O\\u2082', 'CO\\u2082']\nax = task_func(x, y, labels)\n```\n"}
{"task_id": "BigCodeBench/662", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(x, y, labels):\n    # Convert lists to numpy arrays\n    x = np.array(x)\n    y = np.array(y)\n\n    # Concatenate x and y into a single dataset\n    data = np.column_stack((x, y))\n\n    # Apply PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(data)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot each set of points with a different color\n    for i, (principal_x, principal_y) in enumerate(zip(*principal_components)):\n        ax.scatter(principal_x, principal_y, label=labels[i])\n\n    # Add legend\n    ax.legend()\n\n    # Return the figure object\n    return fig\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(x, y, labels):\n    # Convert lists to numpy arrays\n    x = np.array(x)\n    y = np.array(y)\n\n    # Concatenate x and y into a single dataset\n    data = np.column_stack((x, y))\n\n    # Apply PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(data)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot each set of points with a different color\n    for i, (principal_x, principal_y) in enumerate(zip(*principal_components)):\n        ax.scatter(principal_x, principal_y, label=labels[i])\n\n    # Add legend\n    ax.legend()\n\n    # Return the figure object\n    return fig\n\n# Example usage:\nx = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']\nfig = task_func(x, y, labels)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/663", "solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\ndef task_func(x, y, labels):\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    fig, ax = plt.subplots()\n\n    for x_data, y_data, label in zip(x, y, labels):\n        popt, _ = curve_fit(func, x_data, y_data)\n        ax.plot(x_data, func(x_data, *popt), label=label)\n\n    ax.set_xlabel('X values')\n    ax.set_ylabel('Y values')\n    ax.set_title('Exponential Curve Fits')\n    ax.legend()\n\n    return fig\nx_data = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny_data = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']", "raw_solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef task_func(x, y, labels):\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    fig, ax = plt.subplots()\n\n    for x_data, y_data, label in zip(x, y, labels):\n        popt, _ = curve_fit(func, x_data, y_data)\n        ax.plot(x_data, func(x_data, *popt), label=label)\n\n    ax.set_xlabel('X values')\n    ax.set_ylabel('Y values')\n    ax.set_title('Exponential Curve Fits')\n    ax.legend()\n\n    return fig\n\n# Example usage:\nx_data = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\ny_data = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\nlabels = ['H2O', 'O2', 'CO2']\n\nfig = task_func(x_data, y_data, labels)\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/664", "solution": "import statistics\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(sales_data):\n    \"\"\"\n    a ek saal mein paanch products ke liye sales trends ko plot karein, standard deviation shading ke saath variability par prakaash daalte hue\n    'month' ko x-aksh par aur 'sales' ko y-aksh par rakhein.\n\n    Params:\n    - sales_data (pd.DataFrame): dataframe jismein sales data hai, apekshit columns: 'month', 'product a' se 'product ee'.\n\n    Returns:\n        ax (matplotlib.axes.Axes): sales ruzhaan plot ke saath aksh object.\n\n    Reqs:\n    - matplotlib.pyplot\n    - statistics\n\n    Examples:\n    - >>> import pandas as pd, numpy as np\n    - >>> sales_data = pd.DataFrame({\n    - ...     'Month': range(1, 13),\n    - ...     'Product A': np.random.randint(100, 200, size=12),\n    - ...     'Product B': np.random.randint(150, 250, size=12),\n    - ...     'Product C': np.random.randint(120, 220, size=12),\n    - ...     'Product D': np.random.randint(130, 230, size=12),\n    - ...     'Product E': np.random.randint(140, 240, size=12)\n    - ... })\n    - >>> ax = task_func(sales_data)\n    - >>> plt.show()  # Displays the plot\n\n    \"\"\"\n    # Extract months and sales data\n    months = sales_data['Month']\n    sales = sales_data[['Product A', 'Product B', 'Product C', 'Product D', 'Product E']]\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot each product's sales data\n    for product in sales.columns:\n        ax.plot(months, sales[product], label=product)\n\n    # Calculate standard deviation for each product\n    std_devs = sales.apply(lambda x: statistics.stdev(x))\n\n    # Shade the area representing standard deviation\n    for product in sales.columns:\n        ax.fill_between(months, sales[product] - std_devs[product], sales[product] + std_devs[product], alpha=0.2)\n\n    # Set labels and title\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Trends with Standard Deviation Shading')\n    ax.legend()\n\n    return ax", "raw_solution": "import statistics\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(sales_data):\n    \"\"\"\n    a ek saal mein paanch products ke liye sales trends ko plot karein, standard deviation shading ke saath variability par prakaash daalte hue\n    'month' ko x-aksh par aur 'sales' ko y-aksh par rakhein.\n\n    Params:\n    - sales_data (pd.DataFrame): dataframe jismein sales data hai, apekshit columns: 'month', 'product a' se 'product ee'.\n\n    Returns:\n        ax (matplotlib.axes.Axes): sales ruzhaan plot ke saath aksh object.\n\n    Reqs:\n    - matplotlib.pyplot\n    - statistics\n\n    Examples:\n    - >>> import pandas as pd, numpy as np\n    - >>> sales_data = pd.DataFrame({\n    - ...     'Month': range(1, 13),\n    - ...     'Product A': np.random.randint(100, 200, size=12),\n    - ...     'Product B': np.random.randint(150, 250, size=12),\n    - ...     'Product C': np.random.randint(120, 220, size=12),\n    - ...     'Product D': np.random.randint(130, 230, size=12),\n    - ...     'Product E': np.random.randint(140, 240, size=12)\n    - ... })\n    - >>> ax = task_func(sales_data)\n    - >>> plt.show()  # Displays the plot\n\n    \"\"\"\n    # Extract months and sales data\n    months = sales_data['Month']\n    sales = sales_data[['Product A', 'Product B', 'Product C', 'Product D', 'Product E']]\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot each product's sales data\n    for product in sales.columns:\n        ax.plot(months, sales[product], label=product)\n\n    # Calculate standard deviation for each product\n    std_devs = sales.apply(lambda x: statistics.stdev(x))\n\n    # Shade the area representing standard deviation\n    for product in sales.columns:\n        ax.fill_between(months, sales[product] - std_devs[product], sales[product] + std_devs[product], alpha=0.2)\n\n    # Set labels and title\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Trends with Standard Deviation Shading')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/665", "solution": "import shutil\nimport os\nimport fnmatch\nimport itertools\ndef task_func(src_dir, dst_dir):\n    \"\"\"\n    Copy all the files from 'src_dir' to 'dst_dir' that match any of the patterns in ['*.txt', '*.docx'].\n\n    Params:\n    - src_dir (str): source directory.\n    - dst_dir (str): destination directory.\n\n    Returns:\n        str: destination directory.\n\n    Reqs:\n    - shutil\n    - os\n    - fnmatch\n    - itertools\n\n    Examples:\n    - >>> task_func('./source', './destination')\n    - >>> './destination'\n\n    \"\"\"\n    # Ensure the destination directory exists\n    os.makedirs(dst_dir, exist_ok=True)\n\n    # List of file patterns to match\n    patterns = ['*.txt', '*.docx']\n\n    # Iterate over all files in the source directory\n    for root, _, files in os.walk(src_dir):\n        for filename in files:\n            # Check if the file matches any of the patterns\n            if any(fnmatch.fnmatch(filename, pattern) for pattern in patterns):\n                # Construct full file paths\n                src_file = os.path.join(root, filename)\n                dst_file = os.path.join(dst_dir, filename)\n                # Copy the file\n                shutil.copy2(src_file, dst_file)\n\n    return dst_dir", "raw_solution": "import shutil\nimport os\nimport fnmatch\nimport itertools\n\ndef task_func(src_dir, dst_dir):\n    \"\"\"\n    Copy all the files from 'src_dir' to 'dst_dir' that match any of the patterns in ['*.txt', '*.docx'].\n\n    Params:\n    - src_dir (str): source directory.\n    - dst_dir (str): destination directory.\n\n    Returns:\n        str: destination directory.\n\n    Reqs:\n    - shutil\n    - os\n    - fnmatch\n    - itertools\n\n    Examples:\n    - >>> task_func('./source', './destination')\n    - >>> './destination'\n\n    \"\"\"\n    # Ensure the destination directory exists\n    os.makedirs(dst_dir, exist_ok=True)\n\n    # List of file patterns to match\n    patterns = ['*.txt', '*.docx']\n\n    # Iterate over all files in the source directory\n    for root, _, files in os.walk(src_dir):\n        for filename in files:\n            # Check if the file matches any of the patterns\n            if any(fnmatch.fnmatch(filename, pattern) for pattern in patterns):\n                # Construct full file paths\n                src_file = os.path.join(root, filename)\n                dst_file = os.path.join(dst_dir, filename)\n                # Copy the file\n                shutil.copy2(src_file, dst_file)\n\n    return dst_dir\n```\n"}
{"task_id": "BigCodeBench/666", "solution": "from itertools import combinations\nimport math\ndef task_func(seq, letter_weight_dict):\n    max_weight = -math.inf\n    max_subsequence = \"\"\n\n    for r in range(1, len(seq) + 1):\n        for combination in combinations(seq, r):\n            weight_sum = sum(letter_weight_dict[char] for char in combination)\n            if weight_sum > max_weight:\n                max_weight = weight_sum\n                max_subsequence = combination\n\n    return \"\".join(max_subsequence)", "raw_solution": "from itertools import combinations\nimport math\n\ndef task_func(seq, letter_weight_dict):\n    max_weight = -math.inf\n    max_subsequence = \"\"\n\n    for r in range(1, len(seq) + 1):\n        for combination in combinations(seq, r):\n            weight_sum = sum(letter_weight_dict[char] for char in combination)\n            if weight_sum > max_weight:\n                max_weight = weight_sum\n                max_subsequence = combination\n\n    return \"\".join(max_subsequence)\n\n# Test cases"}
{"task_id": "BigCodeBench/667", "solution": "import heapq\nimport collections\ndef task_func(x, n):\n    \"\"\"\n    dictionary x mein most aam n letters ka pata lagaen, jahan key letter aur maan unki frequencies hain.\n\n    Params:\n    - x (dict): letter aavrityon ka dictionary.\n    - n (int): vaapas kiye jaane vaale most adhik baar aane vaale letters ki number.\n\n    Returns:\n        list: most adhik baar aane vaale n letter.\n\n    Reqs:\n    - heapq\n    - collections\n\n    Examples:\n    - >>> task_func({'a': 1, 'b': 2, 'c': 3}, 2)\n    - ['c', 'b']\n\n    \"\"\"\n    # Convert dictionary to a list of tuples\n    items = list(x.items())\n    \n    # Use heapq to find the n most common elements\n    most_common = heapq.nlargest(n, items, key=lambda item: item[1])\n    \n    # Extract the letters from the most common elements\n    letters = [item[0] for item in most_common]\n    \n    return letters", "raw_solution": "import heapq\nimport collections\n\ndef task_func(x, n):\n    \"\"\"\n    dictionary x mein most aam n letters ka pata lagaen, jahan key letter aur maan unki frequencies hain.\n\n    Params:\n    - x (dict): letter aavrityon ka dictionary.\n    - n (int): vaapas kiye jaane vaale most adhik baar aane vaale letters ki number.\n\n    Returns:\n        list: most adhik baar aane vaale n letter.\n\n    Reqs:\n    - heapq\n    - collections\n\n    Examples:\n    - >>> task_func({'a': 1, 'b': 2, 'c': 3}, 2)\n    - ['c', 'b']\n\n    \"\"\"\n    # Convert dictionary to a list of tuples\n    items = list(x.items())\n    \n    # Use heapq to find the n most common elements\n    most_common = heapq.nlargest(n, items, key=lambda item: item[1])\n    \n    # Extract the letters from the most common elements\n    letters = [item[0] for item in most_common]\n    \n    return letters\n\n# Test cases"}
{"task_id": "BigCodeBench/668", "solution": "import itertools\nimport math\ndef task_func(x):\n    \"\"\"\n    Ek dictionary, x, ke up-sequence ka pata lagaen jiski kul length minimum ho, jahan keys letter hain aur maan unki length hain.\n\n    Params:\n    - x (dict): The dictionary of letter lengths.\n\n    Returns:\n        list: The subsequence with the minimum total length.\n\n    Reqs:\n    - itertools\n    - math\n\n    Examples:\n    - >>> task_func({'a': 1, 'b': 2, 'c': 3})\n    - ['a']\n    - >>> task_func({'a': 1, 'b': -2, 'c': -5, 'd': 4})\n    - ['b', 'c']\n\n    \"\"\"\n    # Sort the dictionary by values\n    sorted_x = sorted(x.items(), key=lambda item: item[1])\n    \n    # Initialize the minimum length to a large number\n    min_length = math.inf\n    min_sequence = []\n\n    # Iterate over all possible subsequences\n    for r in range(1, len(sorted_x) + 1):\n        for combination in itertools.combinations(sorted_x, r):\n            total_length = sum(item[1] for item in combination)\n            if total_length < min_length:\n                min_length = total_length\n                min_sequence = [item[0] for item in combination]\n\n    return min_sequence", "raw_solution": "import itertools\nimport math\n\ndef task_func(x):\n    \"\"\"\n    Ek dictionary, x, ke up-sequence ka pata lagaen jiski kul length minimum ho, jahan keys letter hain aur maan unki length hain.\n\n    Params:\n    - x (dict): The dictionary of letter lengths.\n\n    Returns:\n        list: The subsequence with the minimum total length.\n\n    Reqs:\n    - itertools\n    - math\n\n    Examples:\n    - >>> task_func({'a': 1, 'b': 2, 'c': 3})\n    - ['a']\n    - >>> task_func({'a': 1, 'b': -2, 'c': -5, 'd': 4})\n    - ['b', 'c']\n\n    \"\"\"\n    # Sort the dictionary by values\n    sorted_x = sorted(x.items(), key=lambda item: item[1])\n    \n    # Initialize the minimum length to a large number\n    min_length = math.inf\n    min_sequence = []\n\n    # Iterate over all possible subsequences\n    for r in range(1, len(sorted_x) + 1):\n        for combination in itertools.combinations(sorted_x, r):\n            total_length = sum(item[1] for item in combination)\n            if total_length < min_length:\n                min_length = total_length\n                min_sequence = [item[0] for item in combination]\n\n    return min_sequence\n```\n"}
{"task_id": "BigCodeBench/669", "solution": "import itertools\nimport math\ndef task_func(x):\n    \"\"\"\n    Dictionary, in x find the key pair jiska value ke cosine ka sum sabse highest ho.\n\n    Params:\n    - x (dict): key-value pairs ka dictionary.\n\n    Returns:\n        tuple: un keys ka pair jinka value ke cosine ka sum sabse highest ho.\n\n    Reqs:\n    - itertools\n    - math\n\n    Examples:\n    - >>> task_func({'a': 1, 'b': 2, 'c': 3})\n    - ('a', 'b')\n    - ('a', 'b')\n    - >>> task_func({'a': 1, 'b': 2, 'c': 3, 'd': 4})\n    - ('a', 'b')\n    - ('a', 'b')\n\n    \"\"\"\n    if not x or len(x) < 2:\n        return None\n\n    max_sum = -math.inf\n    max_pair = None\n\n    for a, b in itertools.combinations(x.keys(), 2):\n        sum_cosine = x[a] * x[b]\n        if sum_cosine > max_sum:\n            max_sum = sum_cosine\n            max_pair = (a, b)\n\n    return max_pair", "raw_solution": "import itertools\nimport math\n\ndef task_func(x):\n    \"\"\"\n    Dictionary, in x find the key pair jiska value ke cosine ka sum sabse highest ho.\n\n    Params:\n    - x (dict): key-value pairs ka dictionary.\n\n    Returns:\n        tuple: un keys ka pair jinka value ke cosine ka sum sabse highest ho.\n\n    Reqs:\n    - itertools\n    - math\n\n    Examples:\n    - >>> task_func({'a': 1, 'b': 2, 'c': 3})\n    - ('a', 'b')\n    - ('a', 'b')\n    - >>> task_func({'a': 1, 'b': 2, 'c': 3, 'd': 4})\n    - ('a', 'b')\n    - ('a', 'b')\n\n    \"\"\"\n    if not x or len(x) < 2:\n        return None\n\n    max_sum = -math.inf\n    max_pair = None\n\n    for a, b in itertools.combinations(x.keys(), 2):\n        sum_cosine = x[a] * x[b]\n        if sum_cosine > max_sum:\n            max_sum = sum_cosine\n            max_pair = (a, b)\n\n    return max_pair\n```\n"}
{"task_id": "BigCodeBench/670", "solution": "from itertools import combinations\nimport math\ndef task_func(x, w):\n    max_weight = -math.inf\n    max_substr = \"\"\n\n    for i in range(1, len(x) + 1):\n        for combo in combinations(x, i):\n            weight = sum(w[char] for char in combo)\n            if weight > max_weight:\n                max_weight = weight\n                max_substr = combo\n\n    return \"\".join(max_substr)", "raw_solution": "from itertools import combinations\nimport math\n\ndef task_func(x, w):\n    max_weight = -math.inf\n    max_substr = \"\"\n\n    for i in range(1, len(x) + 1):\n        for combo in combinations(x, i):\n            weight = sum(w[char] for char in combo)\n            if weight > max_weight:\n                max_weight = weight\n                max_substr = combo\n\n    return \"\".join(max_substr)\n```\n"}
{"task_id": "BigCodeBench/671", "solution": "import os\nimport random\nimport json\ndef task_func(directory, n):\n    \"\"\"\n    Ek directory mein 'n' yaadrichchhik files banaen, jismein 'number' key aur 1 se 100 ke beech ek yaadrichchhik number maan ke saath json content ho, aur phir pratyek file ki shuruaat mein cursor ko reset karen.\n\n    Params:\n    - directory (str): Vah directory jismein files utpann ki jaani hain.\n    - n (int): Utpann ki jaane vaali files ki sankhya.\n\n    Returns:\n        directory (str): Vah directory jismein files utpann hueen.\n\n    Reqs:\n    - os\n    - random\n    - json\n\n    Examples:\n    - >>> task_func('/path/to/directory', 1)\n    - '/path/to/directory'\n\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n):\n        file_name = f\"file_{i+1}.json\"\n        file_path = os.path.join(directory, file_name)\n        \n        with open(file_path, 'w') as f:\n            data = {\"number\": random.randint(1, 100)}\n            json.dump(data, f)\n    \n    return directory", "raw_solution": "import os\nimport random\nimport json\n\ndef task_func(directory, n):\n    \"\"\"\n    Ek directory mein 'n' yaadrichchhik files banaen, jismein 'number' key aur 1 se 100 ke beech ek yaadrichchhik number maan ke saath json content ho, aur phir pratyek file ki shuruaat mein cursor ko reset karen.\n\n    Params:\n    - directory (str): Vah directory jismein files utpann ki jaani hain.\n    - n (int): Utpann ki jaane vaali files ki sankhya.\n\n    Returns:\n        directory (str): Vah directory jismein files utpann hueen.\n\n    Reqs:\n    - os\n    - random\n    - json\n\n    Examples:\n    - >>> task_func('/path/to/directory', 1)\n    - '/path/to/directory'\n\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n):\n        file_name = f\"file_{i+1}.json\"\n        file_path = os.path.join(directory, file_name)\n        \n        with open(file_path, 'w') as f:\n            data = {\"number\": random.randint(1, 100)}\n            json.dump(data, f)\n    \n    return directory\n```\n"}
{"task_id": "BigCodeBench/672", "solution": "import csv\nimport sys\ndef task_func(filename):\n    \"\"\"\n    Ek CSV file padho, lines ke order ko ulta karo and ulti lines ko back file mein likho. Fir cursor ko file ki beginning mein reset karo.\n\n    Params:\n    - filename (str): CSV file ka name.\n\n    Returns:\n        filename (str): CSV file ka name.\n\n    Reqs:\n    - csv\n    - sys\n\n    Examples:\n    - >>> task_func('file.csv')\n    - 'file.csv'\n\n    \"\"\"\n    # Check if file exists\n    try:\n        with open(filename, 'r') as file:\n            pass\n    except FileNotFoundError:\n        print(f\"File {filename} does not exist.\")\n        sys.exit(1)\n\n    # Read the file and store lines in a list\n    with open(filename, 'r') as file:\n        lines = file.readlines()\n\n    # Reverse the order of lines\n    lines = lines[::-1]\n\n    # Write the reversed lines back to the file\n    with open(filename, 'w') as file:\n        file.writelines(lines)\n\n    # Reset cursor to the beginning of the file\n    with open(filename, 'r') as file:\n        pass\n\n    return filename", "raw_solution": "import csv\nimport sys\n\ndef task_func(filename):\n    \"\"\"\n    Ek CSV file padho, lines ke order ko ulta karo and ulti lines ko back file mein likho. Fir cursor ko file ki beginning mein reset karo.\n\n    Params:\n    - filename (str): CSV file ka name.\n\n    Returns:\n        filename (str): CSV file ka name.\n\n    Reqs:\n    - csv\n    - sys\n\n    Examples:\n    - >>> task_func('file.csv')\n    - 'file.csv'\n\n    \"\"\"\n    # Check if file exists\n    try:\n        with open(filename, 'r') as file:\n            pass\n    except FileNotFoundError:\n        print(f\"File {filename} does not exist.\")\n        sys.exit(1)\n\n    # Read the file and store lines in a list\n    with open(filename, 'r') as file:\n        lines = file.readlines()\n\n    # Reverse the order of lines\n    lines = lines[::-1]\n\n    # Write the reversed lines back to the file\n    with open(filename, 'w') as file:\n        file.writelines(lines)\n\n    # Reset cursor to the beginning of the file\n    with open(filename, 'r') as file:\n        pass\n\n    return filename\n```\n"}
{"task_id": "BigCodeBench/673", "solution": "import os\nimport random\ndef task_func(directory, n_files):\n    \"\"\"\n    Ek specific directory mein n random txt files banaen, pratyek file mein only ek ank ka random integer likhen, and phir pratyek file ki beginning mein cursor ko reset karen.\n    File naam 'file_1.txt' se shuru hote hain and pratyek file ke liye 1 se badhte hain.\n\n    Params:\n    - directory (str): Vah directory jismein files utpann karni hain.\n    - n_files (int): Utpann ki jaane waali n_files ki number.\n\n    Returns:\n        n_files (int): Utpann n_files ki number.\n\n    Reqs:\n    - os\n    - random\n\n    Examples:\n    - >>> random.seed(2)\n    - >>> task_func('/path/to/directory', 5)\n    - 5\n\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(1, n_files + 1):\n        file_name = os.path.join(directory, f\"file_{i}.txt\")\n        with open(file_name, 'w') as f:\n            random_number = random.randint(1, 100)\n            f.write(str(random_number))\n            f.seek(0)\n\n    return n_files", "raw_solution": "import os\nimport random\n\ndef task_func(directory, n_files):\n    \"\"\"\n    Ek specific directory mein n random txt files banaen, pratyek file mein only ek ank ka random integer likhen, and phir pratyek file ki beginning mein cursor ko reset karen.\n    File naam 'file_1.txt' se shuru hote hain and pratyek file ke liye 1 se badhte hain.\n\n    Params:\n    - directory (str): Vah directory jismein files utpann karni hain.\n    - n_files (int): Utpann ki jaane waali n_files ki number.\n\n    Returns:\n        n_files (int): Utpann n_files ki number.\n\n    Reqs:\n    - os\n    - random\n\n    Examples:\n    - >>> random.seed(2)\n    - >>> task_func('/path/to/directory', 5)\n    - 5\n\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(1, n_files + 1):\n        file_name = os.path.join(directory, f\"file_{i}.txt\")\n        with open(file_name, 'w') as f:\n            random_number = random.randint(1, 100)\n            f.write(str(random_number))\n            f.seek(0)\n\n    return n_files\n```\n"}
{"task_id": "BigCodeBench/674", "solution": "import pandas as pd\nimport os\ndef task_func(filename):\n    \"\"\"\n    Read the CSV file of Pandas, reverse the order of lines and write the reversed lines to the back file. Then, le jao the cursor back to the beginning of the file.\n    Do not reverse the header and the file can also be khali.\n\n    Params:\n    - filename (str): Name of the CSV file ka.\n\n    Returns:\n        filename (str): Name of the CSV file ka.\n\n    Reqs:\n    - os\n    - pandas\n\n    Examples:\n    - >>> task_func('file.csv')\n    - 'file.csv'\n\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n\n    # Read the CSV file\n    df = pd.read_csv(filename)\n\n    # Reverse the DataFrame\n    df_reversed = df.iloc[::-1]\n\n    # Write the reversed DataFrame to a new file\n    reversed_filename = filename.split('.')[0] + '_reversed.csv'\n    df_reversed.to_csv(reversed_filename, index=False)\n\n    # Return the new filename\n    return reversed_filename", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(filename):\n    \"\"\"\n    Read the CSV file of Pandas, reverse the order of lines and write the reversed lines to the back file. Then, le jao the cursor back to the beginning of the file.\n    Do not reverse the header and the file can also be khali.\n\n    Params:\n    - filename (str): Name of the CSV file ka.\n\n    Returns:\n        filename (str): Name of the CSV file ka.\n\n    Reqs:\n    - os\n    - pandas\n\n    Examples:\n    - >>> task_func('file.csv')\n    - 'file.csv'\n\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n\n    # Read the CSV file\n    df = pd.read_csv(filename)\n\n    # Reverse the DataFrame\n    df_reversed = df.iloc[::-1]\n\n    # Write the reversed DataFrame to a new file\n    reversed_filename = filename.split('.')[0] + '_reversed.csv'\n    df_reversed.to_csv(reversed_filename, index=False)\n\n    # Return the new filename\n    return reversed_filename\n```\n"}
{"task_id": "BigCodeBench/675", "solution": "import os\nimport random\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        filename = f\"file_{i+1}.txt\"\n        filepath = os.path.join(directory, filename)\n        \n        with open(filepath, 'w') as file:\n            random_string = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=10))\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")", "raw_solution": "import os\nimport random\n\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        filename = f\"file_{i+1}.txt\"\n        filepath = os.path.join(directory, filename)\n        \n        with open(filepath, 'w') as file:\n            random_string = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=10))\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n            file.write(random_string)\n            file.write(\"\\n\")\n           "}
{"task_id": "BigCodeBench/676", "solution": "import pandas as pd\nimport random\ndef task_func(df):\n    \"\"\"\n    Ek df banaen jis mein kai teams ke beech different games ke lie savegames shaamil hon.\n    Input df ki pratyek pankti ek match ka pratinidhitv karti hai, aur is mein do teamen aur unke sambandhit score shaamil hain.\n    Function df mein ek 'winner' column jodta hai, jo pratyek match mein sabase adhik score wali team hai.\n    Yadi score barabar hain, to winner ko randomly roop se tay kiya jaana chaahie.\n\n    Params:\n    - df (pandas.df): input df jis mein 'team1', 'team2', 'score1', 'score2' column hain.\n\n    Returns:\n        df (pandas.df): 'winner' column ke saath df.\n\n    Reqs:\n    - pandas\n    - random\n\n    Examples:\n    - >>> import numpy as np\n    - >>> import pandas as pd\n    - >>> df = pd.df({'team1': np.random.choice(['Team A', 'Team B', 'Team C', 'Team D', 'Team E'], 20),\n    - ...                    'team2': np.random.choice(['Team A', 'Team B', 'Team C', 'Team D', 'Team E'], 20),\n    - ...                    'score1': np.random.randint(0, 10, 20),\n    - ...                    'score2': np.random.randint(0, 10, 20)})\n    - >>> df = task_func(df)\n    - >>> assert 'winner' in df.columns\n    - >>> assert df['winner'].dtype == object\n    - >>> assert all(winner in ['Team A', 'Team B', 'Team C', 'Team D', 'Team E'] for winner in df['winner'])\n\n    \"\"\"\n    # Ensure the dataframe has the required columns\n    if not {'team1', 'team2', 'score1', 'score2'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'team1', 'team2', 'score1', and 'score2' columns.\")\n\n    # Determine the winner for each match\n    df['winner'] = df.apply(\n        lambda row: random.choice([row['team1'], row['team2']]) if row['score1'] == row['score2']\n        else row['team1'] if row['score1'] > row['score2'] else row['team2'], axis=1\n    )\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(df):\n    \"\"\"\n    Ek df banaen jis mein kai teams ke beech different games ke lie savegames shaamil hon.\n    Input df ki pratyek pankti ek match ka pratinidhitv karti hai, aur is mein do teamen aur unke sambandhit score shaamil hain.\n    Function df mein ek 'winner' column jodta hai, jo pratyek match mein sabase adhik score wali team hai.\n    Yadi score barabar hain, to winner ko randomly roop se tay kiya jaana chaahie.\n\n    Params:\n    - df (pandas.df): input df jis mein 'team1', 'team2', 'score1', 'score2' column hain.\n\n    Returns:\n        df (pandas.df): 'winner' column ke saath df.\n\n    Reqs:\n    - pandas\n    - random\n\n    Examples:\n    - >>> import numpy as np\n    - >>> import pandas as pd\n    - >>> df = pd.df({'team1': np.random.choice(['Team A', 'Team B', 'Team C', 'Team D', 'Team E'], 20),\n    - ...                    'team2': np.random.choice(['Team A', 'Team B', 'Team C', 'Team D', 'Team E'], 20),\n    - ...                    'score1': np.random.randint(0, 10, 20),\n    - ...                    'score2': np.random.randint(0, 10, 20)})\n    - >>> df = task_func(df)\n    - >>> assert 'winner' in df.columns\n    - >>> assert df['winner'].dtype == object\n    - >>> assert all(winner in ['Team A', 'Team B', 'Team C', 'Team D', 'Team E'] for winner in df['winner'])\n\n    \"\"\"\n    # Ensure the dataframe has the required columns\n    if not {'team1', 'team2', 'score1', 'score2'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'team1', 'team2', 'score1', and 'score2' columns.\")\n\n    # Determine the winner for each match\n    df['winner'] = df.apply(\n        lambda row: random.choice([row['team1'], row['team2']]) if row['score1'] == row['score2']\n        else row['team1'] if row['score1'] > row['score2'] else row['team2'], axis=1\n    )\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/677", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\ndef task_func(df):\n    \"\"\"\n    Analyze the relationship between two variables in df.\n    function karta hai linear regression on two variables and adds a 'predicted' column in df.\n\n    Params:\n    - df (pandas.df): input df with 'var1', 'var2' columns.\n\n    Returns:\n        df (pandas.df): df with 'predicted' column.\n\n    Reqs:\n    - numpy\n    - pandas\n    - scipy\n\n    Examples:\n    - >>> df = pd.DataFrame({'var1': np.random.randn(10),\n    - ...                    'var2': np.random.randn(10)})\n    - >>> df = task_func(df)\n    - >>> assert 'predicted' in df.columns\n    - >>> assert len(df) == 10\n    - >>> assert len(df.columns) == 3\n\n    \"\"\"\n    # Ensure the required columns exist\n    if 'var1' not in df.columns or 'var2' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'var1' and 'var2' columns.\")\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(df['var1'], df['var2'])\n\n    # Add 'predicted' column\n    df['predicted'] = slope * df['var1'] + intercept\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\n\ndef task_func(df):\n    \"\"\"\n    Analyze the relationship between two variables in df.\n    function karta hai linear regression on two variables and adds a 'predicted' column in df.\n\n    Params:\n    - df (pandas.df): input df with 'var1', 'var2' columns.\n\n    Returns:\n        df (pandas.df): df with 'predicted' column.\n\n    Reqs:\n    - numpy\n    - pandas\n    - scipy\n\n    Examples:\n    - >>> df = pd.DataFrame({'var1': np.random.randn(10),\n    - ...                    'var2': np.random.randn(10)})\n    - >>> df = task_func(df)\n    - >>> assert 'predicted' in df.columns\n    - >>> assert len(df) == 10\n    - >>> assert len(df.columns) == 3\n\n    \"\"\"\n    # Ensure the required columns exist\n    if 'var1' not in df.columns or 'var2' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'var1' and 'var2' columns.\")\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(df['var1'], df['var2'])\n\n    # Add 'predicted' column\n    df['predicted'] = slope * df['var1'] + intercept\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Create a list to store dataframes\n    dataframes = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            # Read the JSON file\n            with open(os.path.join(path, filename), 'r') as f:\n                data = json.load(f)\n\n            # Convert the JSON data to a pandas DataFrame\n            df = pd.DataFrame(data)\n\n            # Add a 'source' column with the filename\n            df['source'] = filename\n\n            # Append the dataframe to the list\n            dataframes.append(df)\n\n    # Concatenate all dataframes into a single dataframe\n    combined_df = pd.concat(dataframes, ignore_index=True)\n\n    # Create a 'processed' directory if it doesn't exist\n    if not os.path.exists(os.path.join(path, 'processed')):\n        os.mkdir(os.path.join(path, 'processed'))\n\n    # Move all JSON files to the 'processed' directory\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            shutil.move(os.path.join(path, filename), os.path.join(path, 'processed', filename))\n\n    return combined_df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Create a list to store dataframes\n    dataframes = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            # Read the JSON file\n            with open(os.path.join(path, filename), 'r') as f:\n                data = json.load(f)\n\n            # Convert the JSON data to a pandas DataFrame\n            df = pd.DataFrame(data)\n\n            # Add a 'source' column with the filename\n            df['source'] = filename\n\n            # Append the dataframe to the list\n            dataframes.append(df)\n\n    # Concatenate all dataframes into a single dataframe\n    combined_df = pd.concat(dataframes, ignore_index=True)\n\n    # Create a 'processed' directory if it doesn't exist\n    if not os.path.exists(os.path.join(path, 'processed')):\n        os.mkdir(os.path.join(path, 'processed'))\n\n    # Move all JSON files to the 'processed' directory\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            shutil.move(os.path.join(path, filename), os.path.join(path, 'processed', filename))\n\n    return combined_df\n```\n"}
{"task_id": "BigCodeBench/679", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(df):\n    \"\"\"\n    Dataframe mein elements ke combinations ki frequency ki gnnnaa karein.\n    Function dataframe mein ek 'combination' column jodd'ta hai, jo pratyek row mein items ka combination hai.\n    Phir yeh pratyek combination ki frequency ki gnnnaa karta hai.\n\n    Params:\n    - df (pandas.df): input df jismein 'item1', 'item2', 'item3', 'item4', 'item5' columns shaamil hain.\n\n    Returns:\n        dict: Ek dictionary jismein sabhi combination ki frequency shaamil hai.\n\n    Reqs:\n    - pandas\n    - collections\n\n    Examples:\n    - >>> df = pd.DataFrame({'item1': ['a', 'b', 'a'], 'item2': ['b', 'c', 'b'], 'item3': ['c', 'd', 'c'], 'item4': ['d', 'e', 'd'], 'item5': ['e', 'f', 'e']})\n    - >>> task_func(df)\n    - {('a', 'b', 'c', 'd', 'e'): 2, ('b', 'c', 'd', 'e', 'f'): 1}\n\n    \"\"\"\n    # Create a new column 'combination' that contains tuples of all items\n    df['combination'] = df.apply(lambda row: tuple(row), axis=1)\n    \n    # Count the frequency of each combination\n    combination_counts = Counter(df['combination'])\n    \n    return dict(combination_counts)", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(df):\n    \"\"\"\n    Dataframe mein elements ke combinations ki frequency ki gnnnaa karein.\n    Function dataframe mein ek 'combination' column jodd'ta hai, jo pratyek row mein items ka combination hai.\n    Phir yeh pratyek combination ki frequency ki gnnnaa karta hai.\n\n    Params:\n    - df (pandas.df): input df jismein 'item1', 'item2', 'item3', 'item4', 'item5' columns shaamil hain.\n\n    Returns:\n        dict: Ek dictionary jismein sabhi combination ki frequency shaamil hai.\n\n    Reqs:\n    - pandas\n    - collections\n\n    Examples:\n    - >>> df = pd.DataFrame({'item1': ['a', 'b', 'a'], 'item2': ['b', 'c', 'b'], 'item3': ['c', 'd', 'c'], 'item4': ['d', 'e', 'd'], 'item5': ['e', 'f', 'e']})\n    - >>> task_func(df)\n    - {('a', 'b', 'c', 'd', 'e'): 2, ('b', 'c', 'd', 'e', 'f'): 1}\n\n    \"\"\"\n    # Create a new column 'combination' that contains tuples of all items\n    df['combination'] = df.apply(lambda row: tuple(row), axis=1)\n    \n    # Count the frequency of each combination\n    combination_counts = Counter(df['combination'])\n    \n    return dict(combination_counts)\n```\n"}
{"task_id": "BigCodeBench/680", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, features):\n    \"\"\"\n    DataFrame mein functions ko standard karein.\n    Yeh function features par standard scaling laagu karta hai.\n\n    Params:\n    - df (pandas.DataFrame): input DataFrame.\n    - features (list): Standardize karne ke liye features ki list. Khali ho sakti hai.\n\n    Returns:\n        df (pandas.DataFrame): standardized features wala DataFrame.\n\n    Reqs:\n    - pandas\n    - numpy\n    - scikit-learn\n\n    Examples:\n    - >>> np.random.seed(42)\n    - >>> df = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])\n    - >>> df = task_func(df, ['a', 'b'])\n    - >>> df.head(2)\n    - a         b         c\n    - 0  0.608932  0.127900  0.647689\n    - 1  2.025355  0.031682 -0.234137\n\n    \"\"\"\n    scaler = StandardScaler()\n    df[features] = scaler.fit_transform(df[features])\n    return df\ndf = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, features):\n    \"\"\"\n    DataFrame mein functions ko standard karein.\n    Yeh function features par standard scaling laagu karta hai.\n\n    Params:\n    - df (pandas.DataFrame): input DataFrame.\n    - features (list): Standardize karne ke liye features ki list. Khali ho sakti hai.\n\n    Returns:\n        df (pandas.DataFrame): standardized features wala DataFrame.\n\n    Reqs:\n    - pandas\n    - numpy\n    - scikit-learn\n\n    Examples:\n    - >>> np.random.seed(42)\n    - >>> df = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])\n    - >>> df = task_func(df, ['a', 'b'])\n    - >>> df.head(2)\n    - a         b         c\n    - 0  0.608932  0.127900  0.647689\n    - 1  2.025355  0.031682 -0.234137\n\n    \"\"\"\n    scaler = StandardScaler()\n    df[features] = scaler.fit_transform(df[features])\n    return df\n\n# Example usage:\nnp.random.seed(42)\ndf = pd.DataFrame(np.random.randn(20, 3), columns=['a', 'b', 'c'])\ndf = task_func(df, ['a', 'b'])"}
{"task_id": "BigCodeBench/681", "solution": "import pandas as pd\nimport json\ndef task_func(file_path, key):\n    \"\"\"\n    Pandas df mein ek JSON file load karein, pratyek object se ek specific key hataein aur sansadhit df ko records dwara unmukhi JSON file mein vapas likhein.\n\n    Params:\n    - file_path (str): JSON file ka path.\n    - key (str): Pratyek object se hatane ki key.\n\n    Returns:\n        df (df): Sansadhit JSON data ka ek pandas df representation.\n\n    Reqs:\n    - pandas\n    - json\n\n    Examples:\n    - >>> df = task_func('data.json', 'ele')\n\n    \"\"\"\n    # Load JSON data from file\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(data)\n\n    # Extract the specified key from each record\n    df[key] = df[key]\n\n    # Save the modified DataFrame back to the JSON file\n    df.to_json(file_path, orient='records', lines=True)\n\n    return df", "raw_solution": "import pandas as pd\nimport json\n\ndef task_func(file_path, key):\n    \"\"\"\n    Pandas df mein ek JSON file load karein, pratyek object se ek specific key hataein aur sansadhit df ko records dwara unmukhi JSON file mein vapas likhein.\n\n    Params:\n    - file_path (str): JSON file ka path.\n    - key (str): Pratyek object se hatane ki key.\n\n    Returns:\n        df (df): Sansadhit JSON data ka ek pandas df representation.\n\n    Reqs:\n    - pandas\n    - json\n\n    Examples:\n    - >>> df = task_func('data.json', 'ele')\n\n    \"\"\"\n    # Load JSON data from file\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(data)\n\n    # Extract the specified key from each record\n    df[key] = df[key]\n\n    # Save the modified DataFrame back to the JSON file\n    df.to_json(file_path, orient='records', lines=True)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/682", "solution": "from collections import Counter\nimport math\ndef task_func(nested_dict):\n    aggregated_values = Counter()\n\n    for key, values in nested_dict.items():\n        if 'ele' in values:\n            del values['ele']\n        aggregated_values.update(values)\n\n    result = {}\n    for key, count in aggregated_values.items():\n        result[key] = math.sin(count)\n\n    return result\nnested_dict = {\n    'dict1': {'ale': 1, 'ele': 2, 'ile': 3},\n    'dict2': {'ele': 4, 'ole': 5, 'ule': 6},\n    'dict3': {'ile': 7, 'ale': 8, 'ele': 9}\n}", "raw_solution": "from collections import Counter\nimport math\n\ndef task_func(nested_dict):\n    aggregated_values = Counter()\n\n    for key, values in nested_dict.items():\n        if 'ele' in values:\n            del values['ele']\n        aggregated_values.update(values)\n\n    result = {}\n    for key, count in aggregated_values.items():\n        result[key] = math.sin(count)\n\n    return result\n\n# Example usage:\nnested_dict = {\n    'dict1': {'ale': 1, 'ele': 2, 'ile': 3},\n    'dict2': {'ele': 4, 'ole': 5, 'ule': 6},\n    'dict3': {'ile': 7, 'ale': 8, 'ele': 9}\n}\n"}
{"task_id": "BigCodeBench/683", "solution": "import math\nimport yaml\ndef task_func(yaml_path, key):\n    \"\"\"\n    Ek YAML file padho, data se ek specific key par cosine lagu karo, and phir sanshodhit data ko wapas YAML file mein likho.\n\n    Params:\n    - yaml_path (str): YAML file ka path.\n    - key (str): cosine lene ke liye key.\n\n    Returns:\n        data (dict): sanshodhit YAML data ka dictionary representation.\n\n    Reqs:\n    - math\n    - yaml\n\n    Examples:\n    - >>> yaml_data = task_func('data.yaml', 'ele')\n\n    \"\"\"\n    # Read the YAML file\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n\n    # Calculate cosine of the value associated with the key\n    if key in data:\n        value = data[key]\n        data[key] = math.cos(value)\n    else:\n        raise KeyError(f\"Key '{key}' not found in the YAML file.\")\n\n    # Write the updated data back to the YAML file\n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n\n    return data", "raw_solution": "import math\nimport yaml\n\ndef task_func(yaml_path, key):\n    \"\"\"\n    Ek YAML file padho, data se ek specific key par cosine lagu karo, and phir sanshodhit data ko wapas YAML file mein likho.\n\n    Params:\n    - yaml_path (str): YAML file ka path.\n    - key (str): cosine lene ke liye key.\n\n    Returns:\n        data (dict): sanshodhit YAML data ka dictionary representation.\n\n    Reqs:\n    - math\n    - yaml\n\n    Examples:\n    - >>> yaml_data = task_func('data.yaml', 'ele')\n\n    \"\"\"\n    # Read the YAML file\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n\n    # Calculate cosine of the value associated with the key\n    if key in data:\n        value = data[key]\n        data[key] = math.cos(value)\n    else:\n        raise KeyError(f\"Key '{key}' not found in the YAML file.\")\n\n    # Write the updated data back to the YAML file\n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n\n    return data\n```\n"}
{"task_id": "BigCodeBench/684", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(df, col):\n    \"\"\"\n    pandas df ko process karein, ek specific column ko hatakar aur ek 'isevenindex' column jodein.\n    'isevenindex' column ek boolean flag hai jo darshata hai ki pratyek row ka index even hai.\n\n    Params:\n    - df (pd.DataFrame): process karne ke liye pandas df.\n    - col (str): hatane ke liye column.\n\n    Returns:\n        df (pd.DataFrame): process kiya gaya pandas df jismein nirdisht column hata diya gaya hai aur ek naya 'isevenindex' column joda gaya hai.\n\n    Reqs:\n    - pandas\n    - numpy\n\n    Examples:\n    - >>> np.random.seed(42)\n    - >>> df = pd.DataFrame(np.random.randint(0,100,size=(5, 4)), columns=list('ABCD'))\n    - >>> df = task_func(df, 'C')\n    - >>> print(df)\n    - A   B   D  isevenindex\n    - 0  51  92  71         true\n    - 1  60  20  86        false\n    - 2  74  74  99         true\n    - 3  23   2  52        false\n    - 4   1  87  37         true\n\n    \"\"\"\n    df = df.drop(columns=[col])\n    df['isevenindex'] = df.index % 2 == 0\n    return df\ndf = pd.DataFrame(np.random.randint(0,100,size=(5, 4)), columns=list('ABCD'))", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(df, col):\n    \"\"\"\n    pandas df ko process karein, ek specific column ko hatakar aur ek 'isevenindex' column jodein.\n    'isevenindex' column ek boolean flag hai jo darshata hai ki pratyek row ka index even hai.\n\n    Params:\n    - df (pd.DataFrame): process karne ke liye pandas df.\n    - col (str): hatane ke liye column.\n\n    Returns:\n        df (pd.DataFrame): process kiya gaya pandas df jismein nirdisht column hata diya gaya hai aur ek naya 'isevenindex' column joda gaya hai.\n\n    Reqs:\n    - pandas\n    - numpy\n\n    Examples:\n    - >>> np.random.seed(42)\n    - >>> df = pd.DataFrame(np.random.randint(0,100,size=(5, 4)), columns=list('ABCD'))\n    - >>> df = task_func(df, 'C')\n    - >>> print(df)\n    - A   B   D  isevenindex\n    - 0  51  92  71         true\n    - 1  60  20  86        false\n    - 2  74  74  99         true\n    - 3  23   2  52        false\n    - 4   1  87  37         true\n\n    \"\"\"\n    df = df.drop(columns=[col])\n    df['isevenindex'] = df.index % 2 == 0\n    return df\n\n# Test cases\nnp.random.seed(42)\ndf = pd.DataFrame(np.random.randint(0,100,size=(5, 4)), columns=list('ABCD'))\ndf = task_func(df, 'C')"}
{"task_id": "BigCodeBench/685", "solution": "from collections import Counter\nfrom itertools import chain\ndef task_func(list_of_lists):\n    \"\"\"\n    Ek list kee lists se sabhee sublists ko ek list mein milaen aur elements kee count vaapas karen.\n\n    Params:\n    - list_of_lists (list): Sansaadhit kee jaane vaalee list.\n\n    Returns:\n        collections.Counter: collections kee gaee list mein elements kee count ke saath Counter object.\n\n    Reqs:\n    - itertools\n    - collections\n\n    Examples:\n    - >>> task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - Counter({1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1})\n\n    \"\"\"\n    # Flatten the list of lists into a single list\n    flat_list = list(chain.from_iterable(list_of_lists))\n    \n    # Count the frequency of each element in the flattened list\n    element_count = Counter(flat_list)\n    \n    return element_count", "raw_solution": "from collections import Counter\nfrom itertools import chain\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Ek list kee lists se sabhee sublists ko ek list mein milaen aur elements kee count vaapas karen.\n\n    Params:\n    - list_of_lists (list): Sansaadhit kee jaane vaalee list.\n\n    Returns:\n        collections.Counter: collections kee gaee list mein elements kee count ke saath Counter object.\n\n    Reqs:\n    - itertools\n    - collections\n\n    Examples:\n    - >>> task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - Counter({1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1})\n\n    \"\"\"\n    # Flatten the list of lists into a single list\n    flat_list = list(chain.from_iterable(list_of_lists))\n    \n    # Count the frequency of each element in the flattened list\n    element_count = Counter(flat_list)\n    \n    return element_count\n```\n"}
{"task_id": "BigCodeBench/686", "solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\ndef task_func(list_of_lists):\n    \"\"\"\n    poorv-nirdhaarit lists ke ek set ko ek soochi mein milaata hai aur soochi ke tatvon ko one_hot encodes.\n\n    Params:\n    - list_of_lists (list): sansaadhit ki jaane vaalee soochi.\n\n    Returns:\n        one_hot (numpy.array): marj kee gaee soochi ka one_hot encoding.\n\n    Reqs:\n    - numpy\n    - scikit-learn\n\n    Examples:\n    - >>> task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n    - [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n    - [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n    - [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n    - [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n    - [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n    - [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n    - [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n    - [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n    \"\"\"\n    # Convert the list of lists into a numpy array\n    array = np.array(list_of_lists)\n    \n    # Initialize the OneHotEncoder\n    encoder = OneHotEncoder(sparse=False)\n    \n    # Fit and transform the array\n    one_hot = encoder.fit_transform(array)\n    \n    return one_hot", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef task_func(list_of_lists):\n    \"\"\"\n    poorv-nirdhaarit lists ke ek set ko ek soochi mein milaata hai aur soochi ke tatvon ko one_hot encodes.\n\n    Params:\n    - list_of_lists (list): sansaadhit ki jaane vaalee soochi.\n\n    Returns:\n        one_hot (numpy.array): marj kee gaee soochi ka one_hot encoding.\n\n    Reqs:\n    - numpy\n    - scikit-learn\n\n    Examples:\n    - >>> task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    - array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n    - [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n    - [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n    - [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n    - [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n    - [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n    - [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n    - [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n    - [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n    \"\"\"\n    # Convert the list of lists into a numpy array\n    array = np.array(list_of_lists)\n    \n    # Initialize the OneHotEncoder\n    encoder = OneHotEncoder(sparse=False)\n    \n    # Fit and transform the array\n    one_hot = encoder.fit_transform(array)\n    \n    return one_hot\n```\n"}
{"task_id": "BigCodeBench/687", "solution": "import numpy as np\nfrom scipy.stats import mode\ndef task_func(list_of_lists):\n    # Flatten the list of lists into a single list\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Calculate the mode of the merged list\n    mode_value, mode_count = mode(merged_list)\n    \n    return mode_value, mode_count", "raw_solution": "import numpy as np\nfrom scipy.stats import mode\n\ndef task_func(list_of_lists):\n    # Flatten the list of lists into a single list\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Calculate the mode of the merged list\n    mode_value, mode_count = mode(merged_list)\n    \n    return mode_value, mode_count\n\n# Test cases"}
{"task_id": "BigCodeBench/688", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    \"\"\"\n    Ek Pandas df diya gaya hai jismein random numeric maan hain, ise sklearn se standard scaler ka upayog karke standard karein.\n\n    Params:\n    - df (df): standard kiye jaane wala df.\n\n    Returns:\n        df_standardized (df): standard kiya gaya df.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    - >>> task_func(df)\n    - a         b\n    - 0 -1.224745 -1.224745\n    - 1  0.000000  0.000000\n    - 2  1.224745  1.224745\n\n    \"\"\"\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    \"\"\"\n    Ek Pandas df diya gaya hai jismein random numeric maan hain, ise sklearn se standard scaler ka upayog karke standard karein.\n\n    Params:\n    - df (df): standard kiye jaane wala df.\n\n    Returns:\n        df_standardized (df): standard kiya gaya df.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    - >>> task_func(df)\n    - a         b\n    - 0 -1.224745 -1.224745\n    - 1  0.000000  0.000000\n    - 2  1.224745  1.224745\n\n    \"\"\"\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized\n\n# Test cases"}
{"task_id": "BigCodeBench/689", "solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\ndef task_func(df):\n    p_values = {}\n    for column in df.columns:\n        data = df[column].dropna()\n        if len(data) > 0:\n            stat, p_value = stats.shapiro(data)\n            p_values[column] = p_value\n    return p_values\ndf = pd.DataFrame(np.random.normal(size=(100, 5)))", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\n\ndef task_func(df):\n    p_values = {}\n    for column in df.columns:\n        data = df[column].dropna()\n        if len(data) > 0:\n            stat, p_value = stats.shapiro(data)\n            p_values[column] = p_value\n    return p_values\n\n# Example usage:\nnp.random.seed(42)\ndf = pd.DataFrame(np.random.normal(size=(100, 5)))\np_values = task_func(df)"}
{"task_id": "BigCodeBench/690", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    \"\"\"\n    Ek Pandas df diya gaya hai jismein random numeric maan and column X and Y hain, sklearn ke linear regression ka upyog karke data ko ek linear model se milayen.\n\n    Params:\n    - df (df): Upyog karne ke liye df.\n\n    Returns:\n        model (LinearRegression): Fit kiya gaya linear model.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> import numpy as np\n    - >>> np.random.seed(42)\n    - >>> df = pd.df(np.random.normal(size=(100, 2)), columns=['X', 'Y'])\n    - >>> model = task_func(df)\n    - >>> print(model)\n    - LinearRegression()\n\n    \"\"\"\n    # Extracting the features and target from the DataFrame\n    X = df[['X']].values\n    Y = df['Y'].values\n\n    # Creating and training the Linear Regression model\n    model = LinearRegression()\n    model.fit(X, Y)\n\n    return model", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nROWS = 100\nCOLUMNS = ['X', 'Y']\n\ndef task_func(df):\n    \"\"\"\n    Ek Pandas df diya gaya hai jismein random numeric maan and column X and Y hain, sklearn ke linear regression ka upyog karke data ko ek linear model se milayen.\n\n    Params:\n    - df (df): Upyog karne ke liye df.\n\n    Returns:\n        model (LinearRegression): Fit kiya gaya linear model.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> import numpy as np\n    - >>> np.random.seed(42)\n    - >>> df = pd.df(np.random.normal(size=(100, 2)), columns=['X', 'Y'])\n    - >>> model = task_func(df)\n    - >>> print(model)\n    - LinearRegression()\n\n    \"\"\"\n    # Extracting the features and target from the DataFrame\n    X = df[['X']].values\n    Y = df['Y'].values\n\n    # Creating and training the Linear Regression model\n    model = LinearRegression()\n    model.fit(X, Y)\n\n    return model\n```\n"}
{"task_id": "BigCodeBench/691", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    # Standardizing the data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n\n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    kmeans.fit(scaled_data)\n    labels = kmeans.labels_\n\n    return labels", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    # Standardizing the data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n\n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    kmeans.fit(scaled_data)\n    labels = kmeans.labels_\n\n    return labels\n\n# Example usage:"}
{"task_id": "BigCodeBench/692", "solution": "import math\nimport pandas as pd\ndef task_func(tuples_list):\n    \"\"\"\n    Dee gayee tuples kee ek list ko math.sin ko har number par laagoo karake, unhen ek Pandas DataFrame mein badlen.\n\n    Params:\n    - tuples_list (list): tuples_list.\n\n    Returns:\n        df (DataFrame): Ek pandas DataFrame. df kee har row tuples_list se ek tuple kee anuroop hai, jisamen maan tuple ke original values ke sine hain.\n\n    Reqs:\n    - math\n    - pandas\n\n    Examples:\n    - >>> df = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)])\n    - >>> print(df)\n    - 0         1         2         3\n    - 0  0.841471  0.909297  0.141120 -0.756802\n    - 1 -0.958924 -0.279415  0.656987  0.989358\n    - 2  0.412118 -0.544021 -0.999990 -0.536573\n\n    \"\"\"\n    data = []\n    for tup in tuples_list:\n        sine_values = [math.sin(x) for x in tup]\n        data.append(sine_values)\n    \n    df = pd.DataFrame(data, columns=[f'col_{i}' for i in range(len(tup))])\n    return df", "raw_solution": "import math\nimport pandas as pd\n\ndef task_func(tuples_list):\n    \"\"\"\n    Dee gayee tuples kee ek list ko math.sin ko har number par laagoo karake, unhen ek Pandas DataFrame mein badlen.\n\n    Params:\n    - tuples_list (list): tuples_list.\n\n    Returns:\n        df (DataFrame): Ek pandas DataFrame. df kee har row tuples_list se ek tuple kee anuroop hai, jisamen maan tuple ke original values ke sine hain.\n\n    Reqs:\n    - math\n    - pandas\n\n    Examples:\n    - >>> df = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)])\n    - >>> print(df)\n    - 0         1         2         3\n    - 0  0.841471  0.909297  0.141120 -0.756802\n    - 1 -0.958924 -0.279415  0.656987  0.989358\n    - 2  0.412118 -0.544021 -0.999990 -0.536573\n\n    \"\"\"\n    data = []\n    for tup in tuples_list:\n        sine_values = [math.sin(x) for x in tup]\n        data.append(sine_values)\n    \n    df = pd.DataFrame(data, columns=[f'col_{i}' for i in range(len(tup))])\n    return df\n```\n"}
{"task_id": "BigCodeBench/693", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(tuples_list, columns):\n    df = pd.DataFrame(tuples_list, columns=columns)\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n    return df_scaled", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(tuples_list, columns):\n    df = pd.DataFrame(tuples_list, columns=columns)\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n    return df_scaled\n```\n"}
{"task_id": "BigCodeBench/694", "solution": "import itertools\nimport random\ndef task_func(t, n):\n    \"\"\"\n    Ek tuple se length *n* ke sabhi combinations ko banaen aur length *n* ka ek random combination lautaaen.\n\n    Params:\n    - t (tuple): The tuple.\n    - n (int): The length of the combinations.\n\n    Returns:\n        tuple: Input tuple ka ek combination.\n\n    Reqs:\n    - itertools\n    - random\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func((1, 2, 3, 4), 2)\n    - (3, 4)\n\n    \"\"\"\n    # Generate all combinations of length n\n    combinations = list(itertools.combinations(t, n))\n    \n    # Select a random combination\n    random_combination = random.choice(combinations)\n    \n    return random_combination", "raw_solution": "import itertools\nimport random\n\ndef task_func(t, n):\n    \"\"\"\n    Ek tuple se length *n* ke sabhi combinations ko banaen aur length *n* ka ek random combination lautaaen.\n\n    Params:\n    - t (tuple): The tuple.\n    - n (int): The length of the combinations.\n\n    Returns:\n        tuple: Input tuple ka ek combination.\n\n    Reqs:\n    - itertools\n    - random\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func((1, 2, 3, 4), 2)\n    - (3, 4)\n\n    \"\"\"\n    # Generate all combinations of length n\n    combinations = list(itertools.combinations(t, n))\n    \n    # Select a random combination\n    random_combination = random.choice(combinations)\n    \n    return random_combination\n```\n"}
{"task_id": "BigCodeBench/695", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(tuples_list, n_components):\n    \"\"\"\n    Ek list of tuples par Principal Component Analysis (PCA) karein.\n\n    Params:\n    - tuples_list (list): tuples ki list.\n\n    Returns:\n        transformed_data (ndarray): transformed_data.\n\n    Reqs:\n    - numpy\n    - sklearn\n\n    Examples:\n    - >>> data = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)], 2)\n    - >>> print(data)\n    - [[ 8.00000000e+00  3.84592537e-16]\n    - [ 0.00000000e+00  0.00000000e+00]\n    - [-8.00000000e+00  3.84592537e-16]]\n\n    \"\"\"\n    # Convert list of tuples to numpy array\n    data = np.array(tuples_list)\n    \n    # Initialize PCA model\n    pca = PCA(n_components=n_components)\n    \n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n    \n    return transformed_data", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\n\ndef task_func(tuples_list, n_components):\n    \"\"\"\n    Ek list of tuples par Principal Component Analysis (PCA) karein.\n\n    Params:\n    - tuples_list (list): tuples ki list.\n\n    Returns:\n        transformed_data (ndarray): transformed_data.\n\n    Reqs:\n    - numpy\n    - sklearn\n\n    Examples:\n    - >>> data = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)], 2)\n    - >>> print(data)\n    - [[ 8.00000000e+00  3.84592537e-16]\n    - [ 0.00000000e+00  0.00000000e+00]\n    - [-8.00000000e+00  3.84592537e-16]]\n\n    \"\"\"\n    # Convert list of tuples to numpy array\n    data = np.array(tuples_list)\n    \n    # Initialize PCA model\n    pca = PCA(n_components=n_components)\n    \n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n    \n    return transformed_data\n\n# Test cases\ndata = task_func([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)], 2)"}
{"task_id": "BigCodeBench/696", "solution": "import numpy as np\nimport math\nimport random\nfrom random import uniform\ndef task_func(radius, num_points):\n    points = []\n    for _ in range(num_points):\n        angle = uniform(0, 2 * math.pi)\n        r = uniform(0, radius)\n        x = r * math.cos(angle)\n        y = r * math.sin(angle)\n        points.append((x, y))\n    return points", "raw_solution": "import numpy as np\nimport math\nimport random\nfrom random import uniform\n\ndef task_func(radius, num_points):\n    points = []\n    for _ in range(num_points):\n        angle = uniform(0, 2 * math.pi)\n        r = uniform(0, radius)\n        x = r * math.cos(angle)\n        y = r * math.sin(angle)\n        points.append((x, y))\n    return points\n\n# Example usage:\nrandom.seed(42)"}
{"task_id": "BigCodeBench/697", "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\ndef task_func(df):\n    # Extracting the feature and value columns\n    X = df[['feature']].values\n    y = df['value'].values\n\n    # Creating and training the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Getting the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n\n    # Returning the results in a dictionary\n    result = {\n        'coefficients': coefficients.tolist(),\n        'intercept': [intercept]\n    }\n\n    return result", "raw_solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\ndef task_func(df):\n    # Extracting the feature and value columns\n    X = df[['feature']].values\n    y = df['value'].values\n\n    # Creating and training the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Getting the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n\n    # Returning the results in a dictionary\n    result = {\n        'coefficients': coefficients.tolist(),\n        'intercept': [intercept]\n    }\n\n    return result\n\n# Example usage"}
{"task_id": "BigCodeBench/698", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df):\n    # Splitting the data into features and target\n    X = df.drop(columns=['target'])\n    y = df['target']\n    \n    # Splitting the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    return X_train, X_test, y_train, y_test", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df):\n    # Splitting the data into features and target\n    X = df.drop(columns=['target'])\n    y = df['target']\n    \n    # Splitting the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    return X_train, X_test, y_train, y_test\n\n# Example usage:"}
{"task_id": "BigCodeBench/699", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    # Create a DataFrame from the input lists\n    df = pd.DataFrame({'x': x_list, 'y': y_list})\n\n    # Initialize and fit the KMeans model\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(df)\n\n    # Return the labels and centroids\n    return kmeans.labels_, kmeans.cluster_centers_", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    # Create a DataFrame from the input lists\n    df = pd.DataFrame({'x': x_list, 'y': y_list})\n\n    # Initialize and fit the KMeans model\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(df)\n\n    # Return the labels and centroids\n    return kmeans.labels_, kmeans.cluster_centers_\n\n# Example usage:\n# df = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6], 'y': [2, 3, 4, 5, 6, 7]})\n# labels, centroids = task_func([1, 2, 3, 4, 5, 6], [2, 3, 4, 5, 6, 7], 2, 0)\n```\n"}
{"task_id": "BigCodeBench/700", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, cols):\n    # Create a DataFrame from the data and columns\n    df = pd.DataFrame(data, columns=cols)\n    \n    # Calculate the correlation matrix\n    correlation_matrix = df.corr()\n    \n    return correlation_matrix", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data, cols):\n    # Create a DataFrame from the data and columns\n    df = pd.DataFrame(data, columns=cols)\n    \n    # Calculate the correlation matrix\n    correlation_matrix = df.corr()\n    \n    return correlation_matrix\n\n# Example usage:\ncorrelation_matrix = task_func([[5.1, 3.5, 1.4], [4.9, 3.0, 1.4], [4.7, 3.2, 1.3]], ['x', 'y', 'z'])"}
{"task_id": "BigCodeBench/701", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, target):\n    \"\"\"\n    Dye gaye DataFrame par ek linear regression analysis karein.\n\n    Params:\n    - df (pd.DataFrame): The pandas DataFrame.\n    - target (str): The target variable.\n\n    Returns:\n        score (float): The R-squared score of the model.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> import numpy as np\n    - >>> np.random.seed(42)\n    - >>> df = pd.DataFrame({'feature': np.random.rand(100), 'target': np.random.rand(100)})  # Explicitly using pd\n    - >>> r_squared = task_func(df, 'target')\n    - >>> print(r_squared)\n    - 0.0011582111228732872\n\n    \"\"\"\n    # Separate the features and the target\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    # Initialize and fit the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Calculate the R-squared score\n    score = model.score(X, y)\n\n    return score", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, target):\n    \"\"\"\n    Dye gaye DataFrame par ek linear regression analysis karein.\n\n    Params:\n    - df (pd.DataFrame): The pandas DataFrame.\n    - target (str): The target variable.\n\n    Returns:\n        score (float): The R-squared score of the model.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> import numpy as np\n    - >>> np.random.seed(42)\n    - >>> df = pd.DataFrame({'feature': np.random.rand(100), 'target': np.random.rand(100)})  # Explicitly using pd\n    - >>> r_squared = task_func(df, 'target')\n    - >>> print(r_squared)\n    - 0.0011582111228732872\n\n    \"\"\"\n    # Separate the features and the target\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    # Initialize and fit the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Calculate the R-squared score\n    score = model.score(X, y)\n\n    return score\n```\n"}
{"task_id": "BigCodeBench/702", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(df):\n    \"\"\"\n    df par Principal Component Analysis (PCA) karein aur first do main components ko record karein.\n\n    Params:\n    - df (df): The pandas df.\n\n    Returns:\n        df_pca (df): df pehla do principal components 'PC1' aur 'PC2' naam ke columns ke roop mein.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> df = pd.DataFrame([[5.1, 3.5, 1.4], [4.9, 3.0, 1.4], [4.7, 3.2, 1.3]], columns = ['x', 'y', 'z'])\n    - >>> df_pca = task_func(df)\n    - >>> print(df_pca)\n    - PC1       PC2\n    - 0  0.334781 -0.011992\n    - 1 -0.187649 -0.142630\n    - 2 -0.147132  0.154622\n\n    \"\"\"\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df)\n    df_pca = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])\n    return df_pca", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\n\ndef task_func(df):\n    \"\"\"\n    df par Principal Component Analysis (PCA) karein aur first do main components ko record karein.\n\n    Params:\n    - df (df): The pandas df.\n\n    Returns:\n        df_pca (df): df pehla do principal components 'PC1' aur 'PC2' naam ke columns ke roop mein.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> df = pd.DataFrame([[5.1, 3.5, 1.4], [4.9, 3.0, 1.4], [4.7, 3.2, 1.3]], columns = ['x', 'y', 'z'])\n    - >>> df_pca = task_func(df)\n    - >>> print(df_pca)\n    - PC1       PC2\n    - 0  0.334781 -0.011992\n    - 1 -0.187649 -0.142630\n    - 2 -0.147132  0.154622\n\n    \"\"\"\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df)\n    df_pca = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])\n    return df_pca\n```\n"}
{"task_id": "BigCodeBench/703", "solution": "import pandas as pd\nfrom sklearn.cluster import DBSCAN\ndef task_func(data, cols):\n    df = pd.DataFrame(data, columns=cols)\n    dbscan = DBSCAN(eps=3, min_samples=2)\n    df['Cluster'] = dbscan.fit_predict(df)\n    return df\ndata = [[5.1, 3.5], [4.9, 3.0], [4.7, 3.2]]\ncols = ['x', 'y']", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import DBSCAN\n\ndef task_func(data, cols):\n    df = pd.DataFrame(data, columns=cols)\n    dbscan = DBSCAN(eps=3, min_samples=2)\n    df['Cluster'] = dbscan.fit_predict(df)\n    return df\n\n# Example usage:\ndata = [[5.1, 3.5], [4.9, 3.0], [4.7, 3.2]]\ncols = ['x', 'y']\ndf = task_func(data, cols)"}
{"task_id": "BigCodeBench/704", "solution": "import pandas as pd\nfrom itertools import combinations\ndef task_func(data, cols, percentage):\n    df = pd.DataFrame(data, columns=cols)\n    corr_combinations = []\n\n    for col1, col2 in combinations(cols, 2):\n        corr = df[col1].corr(df[col2])\n        if abs(corr) >= percentage:\n            corr_combinations.append((col1, col2))\n\n    return corr_combinations", "raw_solution": "import pandas as pd\nfrom itertools import combinations\n\n# Constants\nMIN_PERCENTAGE = 0.75\n\ndef task_func(data, cols, percentage):\n    df = pd.DataFrame(data, columns=cols)\n    corr_combinations = []\n\n    for col1, col2 in combinations(cols, 2):\n        corr = df[col1].corr(df[col2])\n        if abs(corr) >= percentage:\n            corr_combinations.append((col1, col2))\n\n    return corr_combinations\n\n# Example usage:\nresult = task_func([[5.1, 5.0, 1.4], [4.9, 4.8, 1.4], [4.7, 4.6, 2.0]], ['x', 'y', 'z'], 0.9)"}
{"task_id": "BigCodeBench/705", "solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\ndef task_func(df, column, alpha):\n    \"\"\"\n    Shapiro-Wilk test ke saath df se ek vishisht aankik *column* ki *normality* ka test karen,\n    np ka explicitly upayog karne ke lie ek *artificial step* sahit.\n\n    Params:\n    - df (pd.df): input df.\n    - *column* (str): *column* ka naam.\n    - *alpha* (float): *significance level*.\n\n    Returns:\n        *bool*: yadi *column normality test* mein paas hoti hai, to *True*, otherwise *False*.\n\n    Reqs:\n    - numpy\n    - scipy.stats\n\n    Examples:\n    - >>> import pandas as pd\n    - >>> np.random.seed(0)\n    - >>> df = pd.df({'Value': np.random.normal(0, 1, 1000)})\n    - >>> print(task_func(df, 'Value', 0.05))\n    - True\n\n    \"\"\"\n    # Extract the column data\n    data = df[column].dropna()\n\n    # Perform Shapiro-Wilk test\n    stat, p_value = stats.shapiro(data)\n\n    # Check if the p-value is greater than the significance level\n    return p_value > alpha", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\n\ndef task_func(df, column, alpha):\n    \"\"\"\n    Shapiro-Wilk test ke saath df se ek vishisht aankik *column* ki *normality* ka test karen,\n    np ka explicitly upayog karne ke lie ek *artificial step* sahit.\n\n    Params:\n    - df (pd.df): input df.\n    - *column* (str): *column* ka naam.\n    - *alpha* (float): *significance level*.\n\n    Returns:\n        *bool*: yadi *column normality test* mein paas hoti hai, to *True*, otherwise *False*.\n\n    Reqs:\n    - numpy\n    - scipy.stats\n\n    Examples:\n    - >>> import pandas as pd\n    - >>> np.random.seed(0)\n    - >>> df = pd.df({'Value': np.random.normal(0, 1, 1000)})\n    - >>> print(task_func(df, 'Value', 0.05))\n    - True\n\n    \"\"\"\n    # Extract the column data\n    data = df[column].dropna()\n\n    # Perform Shapiro-Wilk test\n    stat, p_value = stats.shapiro(data)\n\n    # Check if the p-value is greater than the significance level\n    return p_value > alpha\n```\n"}
{"task_id": "BigCodeBench/706", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\ndef task_func(data, columns, target_column):\n    # Convert the numpy array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=columns)\n    \n    # Split the data into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Initialize and train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Predict the target for the test set\n    y_pred = model.predict(X_test)\n    \n    # Calculate the accuracy of the model\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef task_func(data, columns, target_column):\n    # Convert the numpy array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=columns)\n    \n    # Split the data into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Initialize and train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Predict the target for the test set\n    y_pred = model.predict(X_test)\n    \n    # Calculate the accuracy of the model\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy\n\n# Example usage:\n# np.random.seed(42)\n# data = np.random.randint(0, 100, size=(100, 4))  # Using np to generate random data\n# columns = ['A', 'B', 'C', 'target']\n# print(task_func(data, columns, 'target'))\n```\n"}
{"task_id": "BigCodeBench/707", "solution": "import json\nimport numpy as np\nimport pandas as pd\nimport os\ndef task_func(df):\n    # Check if 'intcol' column exists in the dataframe\n    if 'intcol' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'intcol' column\")\n\n    # Apply logarithm transformation on 'intcol' column\n    df['intcol'] = np.log10(df['intcol'])\n\n    # Convert the transformed column to a list\n    transformed_list = df['intcol'].tolist()\n\n    # Write the list to a JSON file\n    with open('intcol.json', 'w') as f:\n        json.dump(transformed_list, f)\n\n    # Return the transformed dataframe\n    return df", "raw_solution": "import json\nimport numpy as np\nimport pandas as pd\nimport os\n\ndef task_func(df):\n    # Check if 'intcol' column exists in the dataframe\n    if 'intcol' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'intcol' column\")\n\n    # Apply logarithm transformation on 'intcol' column\n    df['intcol'] = np.log10(df['intcol'])\n\n    # Convert the transformed column to a list\n    transformed_list = df['intcol'].tolist()\n\n    # Write the list to a JSON file\n    with open('intcol.json', 'w') as f:\n        json.dump(transformed_list, f)\n\n    # Return the transformed dataframe\n    return df\n\n# Example usage:\n# df = pd.DataFrame({'intcol': [10, 100, 1000, 10000, 100000]})\n# df_transformed = task_func(df)\n# print(df_transformed)\n```\nThis script first checks if the required 'intcol' column exists in the dataframe. If it does, it applies a logarithm transformation to the column using numpy, converts the transformed column to a list, and writes the list to a JSON file. Finally, it returns the transformed dataframe.\n"}
{"task_id": "BigCodeBench/708", "solution": "import json\nimport csv\nimport os\nimport base64\ndef task_func(raw_string, filename, output_dir):\n    # Decode the base64 string to JSON\n    json_data = base64.b64decode(raw_string).decode('utf-8')\n    data = json.loads(json_data)\n\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Define the file path\n    file_path = os.path.join(output_dir, f\"{filename}.csv\")\n\n    # Write the data to a CSV file\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(data.keys())\n        writer.writerow(data.values())\n\n    return file_path", "raw_solution": "import json\nimport csv\nimport os\nimport base64\n\ndef task_func(raw_string, filename, output_dir):\n    # Decode the base64 string to JSON\n    json_data = base64.b64decode(raw_string).decode('utf-8')\n    data = json.loads(json_data)\n\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Define the file path\n    file_path = os.path.join(output_dir, f\"{filename}.csv\")\n\n    # Write the data to a CSV file\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(data.keys())\n        writer.writerow(data.values())\n\n    return file_path\n\n# Example usage:\n# print(task_func('eyJrZXkiOiAiVmFsdWUifQ==', 'data', './output'))\n# Output: './output/data.csv'\n```\n"}
{"task_id": "BigCodeBench/709", "solution": "import base64\nimport re\nfrom html import unescape\nimport textwrap\ndef task_func(raw_string, line_length):\n    # Decode the base64 encoded string\n    decoded_bytes = base64.b64decode(raw_string)\n    decoded_string = decoded_bytes.decode('utf-8')\n\n    # Unescape HTML entities\n    unescaped_string = unescape(decoded_string)\n\n    # Replace multiple spaces with a single space\n    cleaned_string = re.sub(r'\\s+', ' ', unescaped_string)\n\n    # Remove leading and trailing spaces\n    trimmed_string = cleaned_string.strip()\n\n    # Wrap the text to the specified line length\n    wrapped_text = textwrap.fill(trimmed_string, width=line_length)\n\n    return wrapped_text", "raw_solution": "import base64\nimport re\nfrom html import unescape\nimport textwrap\n\ndef task_func(raw_string, line_length):\n    # Decode the base64 encoded string\n    decoded_bytes = base64.b64decode(raw_string)\n    decoded_string = decoded_bytes.decode('utf-8')\n\n    # Unescape HTML entities\n    unescaped_string = unescape(decoded_string)\n\n    # Replace multiple spaces with a single space\n    cleaned_string = re.sub(r'\\s+', ' ', unescaped_string)\n\n    # Remove leading and trailing spaces\n    trimmed_string = cleaned_string.strip()\n\n    # Wrap the text to the specified line length\n    wrapped_text = textwrap.fill(trimmed_string, width=line_length)\n\n    return wrapped_text\n\n# Test cases"}
{"task_id": "BigCodeBench/710", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data_path):\n    \"\"\"\n    Ek .csv file se ek dataset ko samanya karta hai.\n\n    Params:\n    - data_path (str): .csv data file ka path.\n\n    Returns:\n        df (DataFrame): Samanyikrit dataset.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> df = task_func('path_to_data_file.csv')\n\n    \"\"\"\n    # Load the dataset\n    df = pd.read_csv(data_path)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the scaler to the dataframe\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_scaled", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data_path):\n    \"\"\"\n    Ek .csv file se ek dataset ko samanya karta hai.\n\n    Params:\n    - data_path (str): .csv data file ka path.\n\n    Returns:\n        df (DataFrame): Samanyikrit dataset.\n\n    Reqs:\n    - pandas\n    - sklearn\n\n    Examples:\n    - >>> df = task_func('path_to_data_file.csv')\n\n    \"\"\"\n    # Load the dataset\n    df = pd.read_csv(data_path)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the scaler to the dataframe\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_scaled\n```\n"}
{"task_id": "BigCodeBench/711", "solution": "import json\nimport csv\ndef task_func(json_file, csv_file):\n    \"\"\"\n    json file ko CSV mein badlen.\n\n    Params:\n    - json_file (str): json file ka path.\n    - csv_file (str): CSV file ka path.\n\n    Returns:\n        csv_file: function CSV file ke likhe gaye path ko return karta hai.\n\n    Reqs:\n    - json\n    - csv\n\n    Examples:\n    - >>> task_func('path_to_json_file.json', 'path_to_csv_file.csv')\n    - 'path_to_csv_file.csv'\n\n    \"\"\"\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data[0].keys())\n        for row in data:\n            writer.writerow(row.values())\n\n    return csv_file", "raw_solution": "import json\nimport csv\n\ndef task_func(json_file, csv_file):\n    \"\"\"\n    json file ko CSV mein badlen.\n\n    Params:\n    - json_file (str): json file ka path.\n    - csv_file (str): CSV file ka path.\n\n    Returns:\n        csv_file: function CSV file ke likhe gaye path ko return karta hai.\n\n    Reqs:\n    - json\n    - csv\n\n    Examples:\n    - >>> task_func('path_to_json_file.json', 'path_to_csv_file.csv')\n    - 'path_to_csv_file.csv'\n\n    \"\"\"\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data[0].keys())\n        for row in data:\n            writer.writerow(row.values())\n\n    return csv_file\n```\n"}
{"task_id": "BigCodeBench/712", "solution": "import os\nimport shutil\nimport glob\ndef task_func(source_dir, dest_dir, extension):\n    \"\"\"\n    Ek nishchit extension wali sabhi files ko ek directory se doosri directory mein le jaayen.\n\n    Params:\n    - source_dir (str): The source directory.\n    - dest_dir (str): The destination directory.\n    - extension (str): The file extension.\n\n    Returns:\n        result (int): Sthaanantarit ki gayi files ki sankhya.\n\n    Reqs:\n    - os\n    - shutil\n    - glob\n\n    Examples:\n    - >>> task_func('path_to_source_dir', 'path_to_dest_dir', '.txt')\n    - 10\n\n    \"\"\"\n    # Ensure the destination directory exists\n    os.makedirs(dest_dir, exist_ok=True)\n\n    # Find all files with the specified extension in the source directory\n    files = glob.glob(os.path.join(source_dir, f'*{extension}'))\n\n    # Move each file to the destination directory\n    for file in files:\n        shutil.move(file, dest_dir)\n\n    # Return the number of files moved\n    return len(files)", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(source_dir, dest_dir, extension):\n    \"\"\"\n    Ek nishchit extension wali sabhi files ko ek directory se doosri directory mein le jaayen.\n\n    Params:\n    - source_dir (str): The source directory.\n    - dest_dir (str): The destination directory.\n    - extension (str): The file extension.\n\n    Returns:\n        result (int): Sthaanantarit ki gayi files ki sankhya.\n\n    Reqs:\n    - os\n    - shutil\n    - glob\n\n    Examples:\n    - >>> task_func('path_to_source_dir', 'path_to_dest_dir', '.txt')\n    - 10\n\n    \"\"\"\n    # Ensure the destination directory exists\n    os.makedirs(dest_dir, exist_ok=True)\n\n    # Find all files with the specified extension in the source directory\n    files = glob.glob(os.path.join(source_dir, f'*{extension}'))\n\n    # Move each file to the destination directory\n    for file in files:\n        shutil.move(file, dest_dir)\n\n    # Return the number of files moved\n    return len(files)\n```\n"}
{"task_id": "BigCodeBench/713", "solution": "import os\nimport re\ndef task_func(log_file_path: str, keywords: list):\n    formatted_lines = []\n    pattern = '|'.join(map(re.escape, keywords))\n    \n    with open(log_file_path, 'r') as file:\n        for line in file:\n            for keyword in keywords:\n                if keyword in line:\n                    timestamp = re.search(r'\\d{2}:\\d{2}:\\d{2}', line)\n                    message = re.search(r':\\s*(.*)', line)\n                    if timestamp and message:\n                        formatted_line = f\"    {keyword} :    {timestamp.group()} : {message.group(1)}\"\n                        formatted_lines.append(formatted_line)\n    \n    return formatted_lines", "raw_solution": "import os\nimport re\n\ndef task_func(log_file_path: str, keywords: list):\n    formatted_lines = []\n    pattern = '|'.join(map(re.escape, keywords))\n    \n    with open(log_file_path, 'r') as file:\n        for line in file:\n            for keyword in keywords:\n                if keyword in line:\n                    timestamp = re.search(r'\\d{2}:\\d{2}:\\d{2}', line)\n                    message = re.search(r':\\s*(.*)', line)\n                    if timestamp and message:\n                        formatted_line = f\"    {keyword} :    {timestamp.group()} : {message.group(1)}\"\n                        formatted_lines.append(formatted_line)\n    \n    return formatted_lines\n\n# Example usage:\n# print(task_func('/path/to/log_file.log', ['ERROR', 'WARNING']))\n```\n"}
{"task_id": "BigCodeBench/714", "solution": "import sys\nfrom pathlib import Path\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    sys.path mein ek vishisht path jodein aur us path mein ek directory banaen yadi vah maujood nahi hai.\n    >>> task_func()\n    \"/path/to/whatever\"\n\n    Notes:\n        Yeh function ek constant PATH_TO_APPEND ka upayog karta hai jo default roop se '/path/to/whatever' hai.\n\n    Params:\n    - path_to_append (str): sys.path mein jodne aur ek directory banane ka path. Default '/path/to/whatever' hai.\n\n    Returns:\n        path_to_append (str): Vah path jise joda gaya tha aur jahan directory banai gayi thi.\n\n    Reqs:\n    - sys\n    - pathlib\n\n    Examples:\n    - Examples:\n    - >>> task_func(\"/new/path/to/append\")\n    - \"/new/path/to/append\"\n\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Check if the path exists\n    path = Path(path_to_append)\n    if not path.exists():\n        # Create the directory if it does not exist\n        path.mkdir(parents=True)\n\n    return path_to_append", "raw_solution": "import sys\nfrom pathlib import Path\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef task_func(path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    sys.path mein ek vishisht path jodein aur us path mein ek directory banaen yadi vah maujood nahi hai.\n    >>> task_func()\n    \"/path/to/whatever\"\n\n    Notes:\n        Yeh function ek constant PATH_TO_APPEND ka upayog karta hai jo default roop se '/path/to/whatever' hai.\n\n    Params:\n    - path_to_append (str): sys.path mein jodne aur ek directory banane ka path. Default '/path/to/whatever' hai.\n\n    Returns:\n        path_to_append (str): Vah path jise joda gaya tha aur jahan directory banai gayi thi.\n\n    Reqs:\n    - sys\n    - pathlib\n\n    Examples:\n    - Examples:\n    - >>> task_func(\"/new/path/to/append\")\n    - \"/new/path/to/append\"\n\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Check if the path exists\n    path = Path(path_to_append)\n    if not path.exists():\n        # Create the directory if it does not exist\n        path.mkdir(parents=True)\n\n    return path_to_append\n```\n"}
{"task_id": "BigCodeBench/715", "solution": "import sys\nimport subprocess\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Switch to a specific version of Python and append a specific path to sys.path.\n\n    Notes:\n        This function changes the global Python version and should be used carefully.\n\n    Params:\n    - python_version (str): The version of Python to switch to. Default is '3.8'.\n    - path_to_append (str): The path to append to sys.path. Default is '/path/to/whatever'.\n\n    Returns:\n        python_version (str): The Python version that was switched to.\n\n    Reqs:\n    - sys\n    - subprocess\n\n    Examples:\n    - >>> task_func('3.7', '/path/to/new_directory')\n    - '3.7'\n\n    \"\"\"\n    # Check if the provided Python version is installed\n    try:\n        subprocess.run([f'python{python_version}', '--version'], check=True)\n    except subprocess.CalledProcessError:\n        print(f\"Python version {python_version} is not installed.\")\n        return None\n\n    # Switch to the specified Python version\n    subprocess.run(['sudo', 'update-alternatives', '--set', 'python', f'/usr/bin/python{python_version}'])\n\n    # Append the specified path to sys.path\n    sys.path.append(path_to_append)\n\n    return python_version", "raw_solution": "import sys\nimport subprocess\n\n# Constants\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Switch to a specific version of Python and append a specific path to sys.path.\n\n    Notes:\n        This function changes the global Python version and should be used carefully.\n\n    Params:\n    - python_version (str): The version of Python to switch to. Default is '3.8'.\n    - path_to_append (str): The path to append to sys.path. Default is '/path/to/whatever'.\n\n    Returns:\n        python_version (str): The Python version that was switched to.\n\n    Reqs:\n    - sys\n    - subprocess\n\n    Examples:\n    - >>> task_func('3.7', '/path/to/new_directory')\n    - '3.7'\n\n    \"\"\"\n    # Check if the provided Python version is installed\n    try:\n        subprocess.run([f'python{python_version}', '--version'], check=True)\n    except subprocess.CalledProcessError:\n        print(f\"Python version {python_version} is not installed.\")\n        return None\n\n    # Switch to the specified Python version\n    subprocess.run(['sudo', 'update-alternatives', '--set', 'python', f'/usr/bin/python{python_version}'])\n\n    # Append the specified path to sys.path\n    sys.path.append(path_to_append)\n\n    return python_version\n```\n"}
{"task_id": "BigCodeBench/716", "solution": "import sys\nimport json\nfrom datetime import datetime\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\ndef task_func(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Get the current date and time\n    current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    # Load the JSON file\n    with open(json_file, 'r') as file:\n        json_data = json.load(file)\n\n    # Update the JSON data with the current date and time\n    json_data['last_updated'] = current_datetime\n\n    # Write the updated JSON data back to the file\n    with open(json_file, 'w') as file:\n        json.dump(json_data, file, indent=4)\n\n    return json_data", "raw_solution": "import sys\nimport json\nfrom datetime import datetime\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\n\ndef task_func(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Get the current date and time\n    current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    # Load the JSON file\n    with open(json_file, 'r') as file:\n        json_data = json.load(file)\n\n    # Update the JSON data with the current date and time\n    json_data['last_updated'] = current_datetime\n\n    # Write the updated JSON data back to the file\n    with open(json_file, 'w') as file:\n        json.dump(json_data, file, indent=4)\n\n    return json_data\n\n# Example usage:\n# print(task_func('/path/to/new_directory', '/path/to/new_json_file.json'))\n```\nThis script appends a given path to `sys.path`, updates a JSON file with the current date and time, and returns the updated JSON data. The example usage is commented out and can be uncommented to test the function.\n"}
{"task_id": "BigCodeBench/717", "solution": "import sys\nfrom configparser import ConfigParser\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\ndef task_func(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    \"\"\"\n    sys.path mein ek specific path jodein aur is path ke saath ek configuration file ko update karein.\n\n    Params:\n    - path_to_append (str): sys.path mein jodne ka path. Default '/path/to/whatever' hai.\n    - config_file (str): Update karne ke liye configparser ka path. Default '/path/to/config.ini' hai.\n\n    Returns:\n        config (object): Vah object jismein update ki gayi configuration shaamil hai.\n        config_file (str): Us configuration file ka path jise abhi sanshodhit kiya gaya tha.\n\n    Reqs:\n    - sys\n    - configparser.ConfigParser\n\n    Examples:\n    - >>> config = task_func('/path/to/new_directory', '/path/to/new_config.ini')\n    - >>> 'path_to_append' in config['DEFAULT']\n    - True\n\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Read and update the configuration file\n    config = ConfigParser()\n    config.read(config_file)\n\n    # Update the configuration file\n    config['DEFAULT']['path_to_append'] = path_to_append\n\n    # Write the updated configuration file\n    with open(config_file, 'w') as configfile:\n        config.write(configfile)\n\n    return config, config_file", "raw_solution": "import sys\nfrom configparser import ConfigParser\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\n\ndef task_func(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    \"\"\"\n    sys.path mein ek specific path jodein aur is path ke saath ek configuration file ko update karein.\n\n    Params:\n    - path_to_append (str): sys.path mein jodne ka path. Default '/path/to/whatever' hai.\n    - config_file (str): Update karne ke liye configparser ka path. Default '/path/to/config.ini' hai.\n\n    Returns:\n        config (object): Vah object jismein update ki gayi configuration shaamil hai.\n        config_file (str): Us configuration file ka path jise abhi sanshodhit kiya gaya tha.\n\n    Reqs:\n    - sys\n    - configparser.ConfigParser\n\n    Examples:\n    - >>> config = task_func('/path/to/new_directory', '/path/to/new_config.ini')\n    - >>> 'path_to_append' in config['DEFAULT']\n    - True\n\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Read and update the configuration file\n    config = ConfigParser()\n    config.read(config_file)\n\n    # Update the configuration file\n    config['DEFAULT']['path_to_append'] = path_to_append\n\n    # Write the updated configuration file\n    with open(config_file, 'w') as configfile:\n        config.write(configfile)\n\n    return config, config_file\n```\n"}
{"task_id": "BigCodeBench/718", "solution": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\ndef task_func(text1, text2):\n    \"\"\"\n    Do t-test for the number of words in two strings, **only** if the strings produce words in **same** number.\n\n    Params:\n    - text1 (str), text2 (str): The two text strings.\n\n    Returns:\n        t_statistic (float): t-statistic, ya NaN agar a**same** **lambaee** ke kaaran **tests** nahi kiye ja sakte.\n        p_value (float): p-value, ya NaN agar a**same** **lambaee** ke kaaran **tests** nahi kiye ja sakte.\n\n    Reqs:\n    - re\n    - numpy\n    - scipy\n\n    Examples:\n    - >>> task_func('Words, words, words.', 'And more words!')\n    - (1.7320508075688774, 0.22540333075851657)\n\n    \"\"\"\n    # Extract words using regex\n    words1 = re.findall(r'\\b\\w+\\b', text1)\n    words2 = re.findall(r'\\b\\w+\\b', text2)\n\n    # Check if the number of words is the same\n    if len(words1) != len(words2):\n        return np.nan, np.nan\n\n    # Perform t-test\n    t_statistic, p_value = ttest_rel(words1, words2)\n\n    return t_statistic, p_value", "raw_solution": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\n\ndef task_func(text1, text2):\n    \"\"\"\n    Do t-test for the number of words in two strings, **only** if the strings produce words in **same** number.\n\n    Params:\n    - text1 (str), text2 (str): The two text strings.\n\n    Returns:\n        t_statistic (float): t-statistic, ya NaN agar a**same** **lambaee** ke kaaran **tests** nahi kiye ja sakte.\n        p_value (float): p-value, ya NaN agar a**same** **lambaee** ke kaaran **tests** nahi kiye ja sakte.\n\n    Reqs:\n    - re\n    - numpy\n    - scipy\n\n    Examples:\n    - >>> task_func('Words, words, words.', 'And more words!')\n    - (1.7320508075688774, 0.22540333075851657)\n\n    \"\"\"\n    # Extract words using regex\n    words1 = re.findall(r'\\b\\w+\\b', text1)\n    words2 = re.findall(r'\\b\\w+\\b', text2)\n\n    # Check if the number of words is the same\n    if len(words1) != len(words2):\n        return np.nan, np.nan\n\n    # Perform t-test\n    t_statistic, p_value = ttest_rel(words1, words2)\n\n    return t_statistic, p_value\n```\n"}
{"task_id": "BigCodeBench/719", "solution": "import re\nimport os\nimport glob\ndef task_func(directory, word):\n    \"\"\"\n    This function counts the number of files in a given directory that contain a specific word.\n\n    Params:\n    - directory (str): The directory path.\n    - word (str): The word to search for.\n\n    Returns:\n        count (int): The number of files containing the specific word.\n\n    Reqs:\n    - re\n    - os\n    - glob\n\n    Examples:\n    - >>> task_func('./documents', 'word')\n    - 2\n    - >>> task_func('./documents', 'apple')\n    - 3\n\n    \"\"\"\n    # Get list of all files in the directory\n    files = glob.glob(os.path.join(directory, '*'))\n\n    # Initialize count\n    count = 0\n\n    # Loop through each file\n    for file in files:\n        # Open file and read content\n        with open(file, 'r') as f:\n            content = f.read()\n\n            # Use regex to find occurrences of the word\n            if re.search(r'\\b' + re.escape(word) + r'\\b', content):\n                count += 1\n\n    return count", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(directory, word):\n    \"\"\"\n    This function counts the number of files in a given directory that contain a specific word.\n\n    Params:\n    - directory (str): The directory path.\n    - word (str): The word to search for.\n\n    Returns:\n        count (int): The number of files containing the specific word.\n\n    Reqs:\n    - re\n    - os\n    - glob\n\n    Examples:\n    - >>> task_func('./documents', 'word')\n    - 2\n    - >>> task_func('./documents', 'apple')\n    - 3\n\n    \"\"\"\n    # Get list of all files in the directory\n    files = glob.glob(os.path.join(directory, '*'))\n\n    # Initialize count\n    count = 0\n\n    # Loop through each file\n    for file in files:\n        # Open file and read content\n        with open(file, 'r') as f:\n            content = f.read()\n\n            # Use regex to find occurrences of the word\n            if re.search(r'\\b' + re.escape(word) + r'\\b', content):\n                count += 1\n\n    return count\n```\n"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the path for the output file\n    output_file_path = 'task_func_data/Output.txt'\n    \n    # Create a list of random temperature and humidity values\n    data = [{'Temperature': random.uniform(20.0, 30.0), 'Humidity': random.uniform(40.0, 60.0)} for _ in range(10)]\n    \n    # Append the data to the CSV file\n    with open(output_file_path, mode='a', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Temperature', 'Humidity'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    # Return the path of the output file\n    return output_file_path", "raw_solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Define the path for the output file\n    output_file_path = 'task_func_data/Output.txt'\n    \n    # Create a list of random temperature and humidity values\n    data = [{'Temperature': random.uniform(20.0, 30.0), 'Humidity': random.uniform(40.0, 60.0)} for _ in range(10)]\n    \n    # Append the data to the CSV file\n    with open(output_file_path, mode='a', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Temperature', 'Humidity'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    # Return the path of the output file\n    return output_file_path\n\n# Example usage"}
{"task_id": "BigCodeBench/721", "solution": "import os\nimport csv\nfrom collections import Counter\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return None\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        words = []\n        for row in reader:\n            words.extend(row)\n\n    word_counts = Counter(words)\n    if not word_counts:\n        return None\n\n    most_common_word, frequency = word_counts.most_common(1)[0]\n    return most_common_word, frequency", "raw_solution": "import os\nimport csv\nfrom collections import Counter\n\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return None\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        words = []\n        for row in reader:\n            words.extend(row)\n\n    word_counts = Counter(words)\n    if not word_counts:\n        return None\n\n    most_common_word, frequency = word_counts.most_common(1)[0]\n    return most_common_word, frequency\n\n# Example usage:\n# Assuming 'example.csv' contains multiple repetitions of the word 'example'\n# print(task_func('example.csv'))  # Output: ('example', <some_positive_integer>)\n```\n"}
{"task_id": "BigCodeBench/722", "solution": "import urllib.request\nimport os\nimport re\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\ndef task_func(url):\n    \"\"\"\n    Download a text file from a given URL and count the occurrences of the word 'ERROR'.\n\n    Params:\n    - url (str): The URL of the text file to download.\n\n    Returns:\n    - occurrences (int): The number of occurrences of the word 'ERROR'.\n\n    Reqs:\n    - urllib\n    - os\n    - re\n\n    Examples:\n    - >>> task_func('http://example.com/log.txt')\n    - 5 # This means that in the file, 'ERROR' occurred 5 times.\n    \"\"\"\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    # Read the file\n    with open(TARGET_FILE, 'r') as file:\n        content = file.read()\n\n    # Count occurrences of 'ERROR'\n    occurrences = len(re.findall(SEARCH_PATTERN, content, re.IGNORECASE))\n\n    # Clean up by removing the downloaded file\n    os.remove(TARGET_FILE)\n\n    return occurrences", "raw_solution": "import urllib.request\nimport os\nimport re\n\n# Constants\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\n\ndef task_func(url):\n    \"\"\"\n    Download a text file from a given URL and count the occurrences of the word 'ERROR'.\n\n    Params:\n    - url (str): The URL of the text file to download.\n\n    Returns:\n    - occurrences (int): The number of occurrences of the word 'ERROR'.\n\n    Reqs:\n    - urllib\n    - os\n    - re\n\n    Examples:\n    - >>> task_func('http://example.com/log.txt')\n    - 5 # This means that in the file, 'ERROR' occurred 5 times.\n    \"\"\"\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    # Read the file\n    with open(TARGET_FILE, 'r') as file:\n        content = file.read()\n\n    # Count occurrences of 'ERROR'\n    occurrences = len(re.findall(SEARCH_PATTERN, content, re.IGNORECASE))\n\n    # Clean up by removing the downloaded file\n    os.remove(TARGET_FILE)\n\n    return occurrences\n```\n"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Dee gaye url se data ekatr karen aur ekatr kiye gaye data ko CSV file mein save karen.\n\n    Params:\n    - url (str): Data ekatr karne ke liye url.\n\n    Returns:\n        CSV_FILE_PATH (str): CSV file ka path jahan ekatr kiya gaya data save kiya gaya hai.\n\n    Reqs:\n    - urllib\n    - bs4\n    - csv\n    - os\n\n    Examples:\n    - >>> task_func('http://www.example.com/')\n    - 'scraped_data.csv'\n\n    \"\"\"\n    # Fetch the webpage\n    response = urllib.request.urlopen(url)\n    web_content = response.read()\n\n    # Parse the webpage content\n    soup = BeautifulSoup(web_content, 'html.parser')\n\n    # Extract data from the webpage (for example, all paragraph texts)\n    data = []\n    for paragraph in soup.find_all('p'):\n        data.append(paragraph.get_text())\n\n    # Write the data to a CSV file\n    with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        for row in data:\n            writer.writerow([row])\n\n    return CSV_FILE_PATH", "raw_solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Dee gaye url se data ekatr karen aur ekatr kiye gaye data ko CSV file mein save karen.\n\n    Params:\n    - url (str): Data ekatr karne ke liye url.\n\n    Returns:\n        CSV_FILE_PATH (str): CSV file ka path jahan ekatr kiya gaya data save kiya gaya hai.\n\n    Reqs:\n    - urllib\n    - bs4\n    - csv\n    - os\n\n    Examples:\n    - >>> task_func('http://www.example.com/')\n    - 'scraped_data.csv'\n\n    \"\"\"\n    # Fetch the webpage\n    response = urllib.request.urlopen(url)\n    web_content = response.read()\n\n    # Parse the webpage content\n    soup = BeautifulSoup(web_content, 'html.parser')\n\n    # Extract data from the webpage (for example, all paragraph texts)\n    data = []\n    for paragraph in soup.find_all('p'):\n        data.append(paragraph.get_text())\n\n    # Write the data to a CSV file\n    with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        for row in data:\n            writer.writerow([row])\n\n    return CSV_FILE_PATH\n```\n"}
{"task_id": "BigCodeBench/724", "solution": "import os\nimport json\ndef task_func(config_path: str) -> dict:\n    \"\"\"\n    Ek json configuration file load karein aur configuration dictionary lautayein.\n\n    Params:\n    - config_path (str): configuration file ka path.\n\n    Returns:\n        config (dict): file se load kiya gaya configuration dictionary.\n\n    Reqs:\n    - os\n    - json\n\n    Raises:\n    - FileNotFoundError: Yadi pradaan ki gayi configuration file maujood nahi hai.\n\n    Examples:\n    - >>> task_func(\"config.json\")\n    - {'key': 'value', 'setting': true}\n\n    \"\"\"\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file at {config_path} does not exist.\")\n    \n    with open(config_path, 'r') as file:\n        config = json.load(file)\n    \n    return config", "raw_solution": "import os\nimport json\n\ndef task_func(config_path: str) -> dict:\n    \"\"\"\n    Ek json configuration file load karein aur configuration dictionary lautayein.\n\n    Params:\n    - config_path (str): configuration file ka path.\n\n    Returns:\n        config (dict): file se load kiya gaya configuration dictionary.\n\n    Reqs:\n    - os\n    - json\n\n    Raises:\n    - FileNotFoundError: Yadi pradaan ki gayi configuration file maujood nahi hai.\n\n    Examples:\n    - >>> task_func(\"config.json\")\n    - {'key': 'value', 'setting': true}\n\n    \"\"\"\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file at {config_path} does not exist.\")\n    \n    with open(config_path, 'r') as file:\n        config = json.load(file)\n    \n    return config\n```\n"}
{"task_id": "BigCodeBench/725", "solution": "import codecs\nimport os\nimport glob\nDIRECTORY_PATH = './files/'\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    This function changes the encoding of all text files in a given directory from one encoding to another.\n\n    Params:\n    - directory (str): The directory where text files are located. Default './files/'.\n    - from_encoding (str): The current encoding of the text files. Default 'cp1251'.\n    - to_encoding (str): The desired encoding to convert the text files to. Default 'utf8'.\n\n    Returns:\n        None\n\n    Requirements:\n    - codecs\n    - os\n    - glob\n\n    Examples:\n    - >>> task_func('./files/', 'cp1251', 'utf8')  # Converts all .txt files in './files/' from 'cp1251' to 'utf8'.\n    - >>> task_func('./other_files/', 'utf8', 'ascii')  # Converts all .txt files in './other_files/' from 'utf8' to 'ascii'.\n    \"\"\"\n    # Get all .txt files in the directory\n    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n        # Read the file with the original encoding\n        with codecs.open(file_path, 'r', encoding=from_encoding) as f:\n            content = f.read()\n\n        # Write the content to the same file with the new encoding\n        with codecs.open(file_path, 'w', encoding=to_encoding) as f:\n            f.write(content)", "raw_solution": "import codecs\nimport os\nimport glob\n\n# Constants\nDIRECTORY_PATH = './files/'\n\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    This function changes the encoding of all text files in a given directory from one encoding to another.\n\n    Params:\n    - directory (str): The directory where text files are located. Default './files/'.\n    - from_encoding (str): The current encoding of the text files. Default 'cp1251'.\n    - to_encoding (str): The desired encoding to convert the text files to. Default 'utf8'.\n\n    Returns:\n        None\n\n    Requirements:\n    - codecs\n    - os\n    - glob\n\n    Examples:\n    - >>> task_func('./files/', 'cp1251', 'utf8')  # Converts all .txt files in './files/' from 'cp1251' to 'utf8'.\n    - >>> task_func('./other_files/', 'utf8', 'ascii')  # Converts all .txt files in './other_files/' from 'utf8' to 'ascii'.\n    \"\"\"\n    # Get all .txt files in the directory\n    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n        # Read the file with the original encoding\n        with codecs.open(file_path, 'r', encoding=from_encoding) as f:\n            content = f.read()\n\n        # Write the content to the same file with the new encoding\n        with codecs.open(file_path, 'w', encoding=to_encoding) as f:\n            f.write(content)\n```\n"}
{"task_id": "BigCodeBench/726", "solution": "import re\nimport random\nfrom nltk.corpus import words\nfrom random import sample\nimport nltk\nSAMPLE_ENGLISH_WORDS = set(words.words())\ndef task_func(s, n):\n    # Extract words from the string\n    extracted_words = re.findall(r'\\b\\w+\\b', s.lower())\n    \n    # Filter out only English words\n    english_words = set(word for word in extracted_words if word in SAMPLE_ENGLISH_WORDS)\n    \n    # Randomly sample n words from the set of English words\n    sampled_words = sample(english_words, min(n, len(english_words)))\n    \n    return sampled_words\ns = 'This is an example string with some random words: Apple, banana, Test, hello, world'", "raw_solution": "import re\nimport random\nfrom nltk.corpus import words\nfrom random import sample\n\n# Ensure the words corpus is downloaded\nimport nltk\nnltk.download('words')\n\n# Constants\nSAMPLE_ENGLISH_WORDS = set(words.words())  # Correct initialization\n\ndef task_func(s, n):\n    # Extract words from the string\n    extracted_words = re.findall(r'\\b\\w+\\b', s.lower())\n    \n    # Filter out only English words\n    english_words = set(word for word in extracted_words if word in SAMPLE_ENGLISH_WORDS)\n    \n    # Randomly sample n words from the set of English words\n    sampled_words = sample(english_words, min(n, len(english_words)))\n    \n    return sampled_words\n\n# Test cases\ns = 'This is an example string with some random words: Apple, banana, Test, hello, world'"}
{"task_id": "BigCodeBench/727", "solution": "import re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nSENTENCES = ['This is a sentence', 'Another sentence here', 'More sentences']\ndef task_func(s: str) -> np.ndarray:\n    \"\"\"\n    Bag-of-words model ka upayog kar ke string ko vectorize karen. string ko words mein vibhajit kiya gaya hai aur pratyek shabd ko ek attribute ke roop mein mana jaata hai. Pratyek attribute ka maan string mein shabd ki number hai. Function vectorization ke liye kuchh purnirdharit sentences (sentence constant) ka bhi upayog karta hai.\n\n    Params:\n    - s (str): The string to vectorize.\n\n    Returns:\n        np.ndarray: Vectorized string ke saath ek numpy array.\n\n    Reqs:\n    - re\n    - sklearn.feature_extraction.text.CountVectorizer\n    - numpy\n\n    Examples:\n    - >>> s = 'This is a test string.'\n    - >>> vec = task_func(s)\n    - >>> print(vec)\n    - [0 0 1 0 0 0 1 1 1]\n\n    \"\"\"\n    # Preprocess the string to remove punctuation and make lowercase\n    processed_s = re.sub(r'[^\\w\\s]', '', s).lower()\n\n    # Initialize the CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    sentence_vectors = vectorizer.fit_transform(SENTENCES)\n\n    # Transform the input string\n    input_vector = vectorizer.transform([processed_s])\n\n    # Return the vectorized string as a numpy array\n    return input_vector.toarray()[0]", "raw_solution": "import re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Constants\nSENTENCES = ['This is a sentence', 'Another sentence here', 'More sentences']\n\ndef task_func(s: str) -> np.ndarray:\n    \"\"\"\n    Bag-of-words model ka upayog kar ke string ko vectorize karen. string ko words mein vibhajit kiya gaya hai aur pratyek shabd ko ek attribute ke roop mein mana jaata hai. Pratyek attribute ka maan string mein shabd ki number hai. Function vectorization ke liye kuchh purnirdharit sentences (sentence constant) ka bhi upayog karta hai.\n\n    Params:\n    - s (str): The string to vectorize.\n\n    Returns:\n        np.ndarray: Vectorized string ke saath ek numpy array.\n\n    Reqs:\n    - re\n    - sklearn.feature_extraction.text.CountVectorizer\n    - numpy\n\n    Examples:\n    - >>> s = 'This is a test string.'\n    - >>> vec = task_func(s)\n    - >>> print(vec)\n    - [0 0 1 0 0 0 1 1 1]\n\n    \"\"\"\n    # Preprocess the string to remove punctuation and make lowercase\n    processed_s = re.sub(r'[^\\w\\s]', '', s).lower()\n\n    # Initialize the CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    sentence_vectors = vectorizer.fit_transform(SENTENCES)\n\n    # Transform the input string\n    input_vector = vectorizer.transform([processed_s])\n\n    # Return the vectorized string as a numpy array\n    return input_vector.toarray()[0]\n```\n"}
{"task_id": "BigCodeBench/728", "solution": "import csv\nimport io\ndef task_func(filename, from_encoding='cp1251', to_encoding='utf8', delimiter=','):\n    # Read the CSV file with the specified encoding\n    with open(filename, 'r', encoding=from_encoding) as f:\n        reader = csv.DictReader(f, delimiter=delimiter)\n        data = [row for row in reader]\n\n    # Convert the data to a string with the specified encoding\n    output = io.StringIO()\n    writer = csv.DictWriter(output, fieldnames=reader.fieldnames, delimiter=delimiter)\n    writer.writeheader()\n    writer.writerows(data)\n    converted_csv = output.getvalue().encode(to_encoding).decode(to_encoding)\n\n    return data, converted_csv", "raw_solution": "import csv\nimport io\n\ndef task_func(filename, from_encoding='cp1251', to_encoding='utf8', delimiter=','):\n    # Read the CSV file with the specified encoding\n    with open(filename, 'r', encoding=from_encoding) as f:\n        reader = csv.DictReader(f, delimiter=delimiter)\n        data = [row for row in reader]\n\n    # Convert the data to a string with the specified encoding\n    output = io.StringIO()\n    writer = csv.DictWriter(output, fieldnames=reader.fieldnames, delimiter=delimiter)\n    writer.writeheader()\n    writer.writerows(data)\n    converted_csv = output.getvalue().encode(to_encoding).decode(to_encoding)\n\n    return data, converted_csv\n\n# Example usage:\n# data, converted_csv = task_func('sample.csv', 'cp1251', 'utf8')\n# print(data)\n# print(converted_csv)\n```\n"}
{"task_id": "BigCodeBench/729", "solution": "import pickle\nimport os\nimport random\nimport string\ndef task_func(strings, filename=None):\n    \n    if filename is None:\n        filename = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10)) + '.pkl'\n    \n    # Save the list of strings to a pickle file\n    with open(filename, 'wb') as file:\n        pickle.dump(strings, file)\n    \n    # Load the list of strings from the pickle file\n    with open(filename, 'rb') as file:\n        loaded_strings = pickle.load(file)\n    \n    # Remove the pickle file after loading\n    os.remove(filename)\n    \n    return loaded_strings", "raw_solution": "import pickle\nimport os\nimport random\nimport string\n\ndef task_func(strings, filename=None):\n    \n    if filename is None:\n        filename = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10)) + '.pkl'\n    \n    # Save the list of strings to a pickle file\n    with open(filename, 'wb') as file:\n        pickle.dump(strings, file)\n    \n    # Load the list of strings from the pickle file\n    with open(filename, 'rb') as file:\n        loaded_strings = pickle.load(file)\n    \n    # Remove the pickle file after loading\n    os.remove(filename)\n    \n    return loaded_strings\n\n# Example usage:"}
{"task_id": "BigCodeBench/730", "solution": "import pickle\nimport os\nfrom datetime import datetime\nimport pytz\nFILE_NAME = 'save.pkl'\ndef task_func(dt):\n    \"\"\"\n    'save.pkl' mein 'dt' date time object ko sahejain aur phir ise validation ke liye back padhein.\n\n    Params:\n    - dt (datetime): saheje jaane wala datetime object.\n\n    Returns:\n        loaded_dt (datetime): 'save.pkl' se load kiya gaya datetime object.\n\n    Reqs:\n    - pickle\n    - os\n\n    Examples:\n    - >>> dt = datetime.now(pytz.UTC)\n    - >>> loaded_dt = task_func(dt)\n    - >>> assert dt == loaded_dt\n\n    \"\"\"\n    # Serialize the datetime object to a file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump(dt, file)\n\n    # Deserialize the datetime object from the file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_dt = pickle.load(file)\n\n    # Clean up the file\n    if os.path.exists(FILE_NAME):\n        os.remove(FILE_NAME)\n\n    return loaded_dt", "raw_solution": "import pickle\nimport os\nfrom datetime import datetime\nimport pytz\n\n# Constants\nFILE_NAME = 'save.pkl'\n\ndef task_func(dt):\n    \"\"\"\n    'save.pkl' mein 'dt' date time object ko sahejain aur phir ise validation ke liye back padhein.\n\n    Params:\n    - dt (datetime): saheje jaane wala datetime object.\n\n    Returns:\n        loaded_dt (datetime): 'save.pkl' se load kiya gaya datetime object.\n\n    Reqs:\n    - pickle\n    - os\n\n    Examples:\n    - >>> dt = datetime.now(pytz.UTC)\n    - >>> loaded_dt = task_func(dt)\n    - >>> assert dt == loaded_dt\n\n    \"\"\"\n    # Serialize the datetime object to a file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump(dt, file)\n\n    # Deserialize the datetime object from the file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_dt = pickle.load(file)\n\n    # Clean up the file\n    if os.path.exists(FILE_NAME):\n        os.remove(FILE_NAME)\n\n    return loaded_dt\n\n# Example usage"}
{"task_id": "BigCodeBench/731", "solution": "import pickle\nimport os\nfrom sklearn.datasets import make_classification\nimport numpy as np\nFILE_NAME = 'save.pkl'\ndef task_func(data, target):\n    \"\"\"\n    Sklearn dataset (\"Data\" aur \"destination\") ko pickle file \"save.pkl\" mein save karein aur phir validation ke liye ise back padhein.\n\n    Params:\n    - data (numpy array): Sklearn dataset ka data part.\n    - target (numpy array): Sklearn dataset ka target part.\n\n    Returns:\n        tuple: 'save.pkl' se load kiya gaya tuple (data, target).\n\n    Reqs:\n    - pickle\n    - os\n    - sklearn.datasets\n\n    Examples:\n    - >>> data, target = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n    - >>> loaded_data, loaded_target = task_func(data, target)\n    - >>> assert np.array_equal(data, loaded_data) and np.array_equal(target, loaded_target)\n\n    \"\"\"\n    # Save the data and target to a pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n\n    # Load the data and target from the pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_data, loaded_target", "raw_solution": "import pickle\nimport os\nfrom sklearn.datasets import make_classification\nimport numpy as np\n\n# Constants\nFILE_NAME = 'save.pkl'\nDATA, TARGET = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n\ndef task_func(data, target):\n    \"\"\"\n    Sklearn dataset (\"Data\" aur \"destination\") ko pickle file \"save.pkl\" mein save karein aur phir validation ke liye ise back padhein.\n\n    Params:\n    - data (numpy array): Sklearn dataset ka data part.\n    - target (numpy array): Sklearn dataset ka target part.\n\n    Returns:\n        tuple: 'save.pkl' se load kiya gaya tuple (data, target).\n\n    Reqs:\n    - pickle\n    - os\n    - sklearn.datasets\n\n    Examples:\n    - >>> data, target = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n    - >>> loaded_data, loaded_target = task_func(data, target)\n    - >>> assert np.array_equal(data, loaded_data) and np.array_equal(target, loaded_target)\n\n    \"\"\"\n    # Save the data and target to a pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n\n    # Load the data and target from the pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    # Clean up the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_data, loaded_target\n\n# Test the function\nloaded_data, loaded_target = task_func(DATA, TARGET)\nassert np.array_equal(DATA, loaded_data) and np.array_equal(TARGET, loaded_target)\n```\n"}
{"task_id": "BigCodeBench/732", "solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nSTEMMER = PorterStemmer()\ndef task_func(content):\n    # Remove punctuation and convert to lowercase\n    content = content.lower()\n    content = re.sub(f'[{re.escape(string.punctuation)}]', '', content)\n\n    # Tokenize the content\n    words = content.split()\n\n    # Stem the words\n    stemmed_words = [STEMMER.stem(word) for word in words]\n\n    # Count the frequency of each stemmed word\n    word_count = Counter(stemmed_words)\n\n    return dict(word_count)", "raw_solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\n\nSTEMMER = PorterStemmer()\n\ndef task_func(content):\n    # Remove punctuation and convert to lowercase\n    content = content.lower()\n    content = re.sub(f'[{re.escape(string.punctuation)}]', '', content)\n\n    # Tokenize the content\n    words = content.split()\n\n    # Stem the words\n    stemmed_words = [STEMMER.stem(word) for word in words]\n\n    # Count the frequency of each stemmed word\n    word_count = Counter(stemmed_words)\n\n    return dict(word_count)\n\n# Example usage:\n# print(task_func('running runner run'))\n# Output: {'run': 1, 'runner': 1}\n```\n"}
{"task_id": "BigCodeBench/733", "solution": "import re\nimport string\ndef task_func(content):\n    \"\"\"\n    Antim shabda ko chhodkar, ek content mein non-stop ki ginti karen.\n\n    Params:\n    - content (str): Ginti karne ke liye content.\n\n    Returns:\n        count (int): non-stop ki ginti.\n\n    Reqs:\n    - re\n    - string\n\n    Examples:\n    - >>> task_func('this is an example content')\n    - 1\n\n    \"\"\"\n    # Define the pattern to match non-stop words\n    pattern = r'\\b(non-stop)\\b'\n    \n    # Find all matches in the content\n    matches = re.findall(pattern, content, re.IGNORECASE)\n    \n    # Return the count of matches\n    return len(matches)", "raw_solution": "import re\nimport string\n\ndef task_func(content):\n    \"\"\"\n    Antim shabda ko chhodkar, ek content mein non-stop ki ginti karen.\n\n    Params:\n    - content (str): Ginti karne ke liye content.\n\n    Returns:\n        count (int): non-stop ki ginti.\n\n    Reqs:\n    - re\n    - string\n\n    Examples:\n    - >>> task_func('this is an example content')\n    - 1\n\n    \"\"\"\n    # Define the pattern to match non-stop words\n    pattern = r'\\b(non-stop)\\b'\n    \n    # Find all matches in the content\n    matches = re.findall(pattern, content, re.IGNORECASE)\n    \n    # Return the count of matches\n    return len(matches)\n\n# Test cases"}
{"task_id": "BigCodeBench/734", "solution": "import nltk\nfrom collections import Counter\ndef task_func(content):\n    \"\"\"\n    Antim shabd ke bina ek content mein parts-of-speech (POS) tags ki ginti karein.\n\n    Params:\n    - content (str): POS tags ki ginti karne ke liye content.\n\n    Returns:\n        dict: Ek dict jisme POS tags ko kunji ke roop mein aur unki ginti ko maan ke roop mein rakha gaya hai.\n\n    Reqs:\n    - nltk\n    - collections.Counter\n\n    Examples:\n    - >>> task_func('this is an example content')\n    - {'DT': 2, 'VBZ': 1, 'NN': 1}\n\n    \"\"\"\n    # Tokenize the content into words\n    words = nltk.word_tokenize(content)\n\n    # Get the POS tags for each word\n    pos_tags = nltk.pos_tag(words)\n\n    # Count the occurrences of each POS tag\n    pos_counts = Counter(tag for _, tag in pos_tags)\n\n    return dict(pos_counts)", "raw_solution": "import nltk\n\n# Download necessary NLTK data (if not already present)\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\nfrom collections import Counter\n\ndef task_func(content):\n    \"\"\"\n    Antim shabd ke bina ek content mein parts-of-speech (POS) tags ki ginti karein.\n\n    Params:\n    - content (str): POS tags ki ginti karne ke liye content.\n\n    Returns:\n        dict: Ek dict jisme POS tags ko kunji ke roop mein aur unki ginti ko maan ke roop mein rakha gaya hai.\n\n    Reqs:\n    - nltk\n    - collections.Counter\n\n    Examples:\n    - >>> task_func('this is an example content')\n    - {'DT': 2, 'VBZ': 1, 'NN': 1}\n\n    \"\"\"\n    # Tokenize the content into words\n    words = nltk.word_tokenize(content)\n\n    # Get the POS tags for each word\n    pos_tags = nltk.pos_tag(words)\n\n    # Count the occurrences of each POS tag\n    pos_counts = Counter(tag for _, tag in pos_tags)\n\n    return dict(pos_counts)\n```\n"}
{"task_id": "BigCodeBench/735", "solution": "import numpy as np\nfrom itertools import chain\ndef task_func(L):\n    \"\"\"\n    Ek nested list 'L' ke sabhi elements ka mean aur variance calculate karein.\n\n    Params:\n    - L (list): The nested list.\n\n    Returns:\n        dict: Ek dict jismein mean aur variance shaamil hain.\n\n    Reqs:\n    - numpy\n    - itertools.chain\n\n    Examples:\n    - >>> task_func([[1,2,3],[4,5,6]])\n    - {'mean': 3.5, 'variance': 2.9166666666666665}\n\n    \"\"\"\n    # Flatten the list using itertools.chain\n    flat_list = list(chain.from_iterable(L))\n    \n    # Calculate mean using numpy\n    mean_value = np.mean(flat_list)\n    \n    # Calculate variance using numpy\n    variance_value = np.var(flat_list)\n    \n    return {'mean': mean_value, 'variance': variance_value}", "raw_solution": "import numpy as np\nfrom itertools import chain\n\ndef task_func(L):\n    \"\"\"\n    Ek nested list 'L' ke sabhi elements ka mean aur variance calculate karein.\n\n    Params:\n    - L (list): The nested list.\n\n    Returns:\n        dict: Ek dict jismein mean aur variance shaamil hain.\n\n    Reqs:\n    - numpy\n    - itertools.chain\n\n    Examples:\n    - >>> task_func([[1,2,3],[4,5,6]])\n    - {'mean': 3.5, 'variance': 2.9166666666666665}\n\n    \"\"\"\n    # Flatten the list using itertools.chain\n    flat_list = list(chain.from_iterable(L))\n    \n    # Calculate mean using numpy\n    mean_value = np.mean(flat_list)\n    \n    # Calculate variance using numpy\n    variance_value = np.var(flat_list)\n    \n    return {'mean': mean_value, 'variance': variance_value}\n```\n"}
{"task_id": "BigCodeBench/736", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(L):\n    '''\n    Calculate the mode of all elements in a nested list 'L'.\n    \n    Parameters:\n    L (list): The nested list.\n    \n    Returns:\n    - mode (int): The mode.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    1\n    '''\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the mode\n    mode_result = stats.mode(flat_list)\n    \n    return mode_result.mode[0]", "raw_solution": "import numpy as np\nfrom scipy import stats\n\ndef task_func(L):\n    '''\n    Calculate the mode of all elements in a nested list 'L'.\n    \n    Parameters:\n    L (list): The nested list.\n    \n    Returns:\n    - mode (int): The mode.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> task_func([[1,2,3],[4,5,6]])\n    1\n    '''\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the mode\n    mode_result = stats.mode(flat_list)\n    \n    return mode_result.mode[0]\n\n# Test cases"}
{"task_id": "BigCodeBench/737", "solution": "import numpy as np\nimport math\ndef task_func(L):\n    \"\"\"\n    L is a nested list, we are finding the median of all elements in the nested list.\n\n    Params:\n    - L (list): The nested list.\n\n    Returns:\n        median (float): The median.\n\n    Reqs:\n    - numpy\n    - math\n\n    Examples:\n    - >>> task_func([[1,2,3],[4,5,6]])\n    - 3.5\n\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the median\n    median = np.median(flat_list)\n    \n    return median", "raw_solution": "import numpy as np\nimport math\n\ndef task_func(L):\n    \"\"\"\n    L is a nested list, we are finding the median of all elements in the nested list.\n\n    Params:\n    - L (list): The nested list.\n\n    Returns:\n        median (float): The median.\n\n    Reqs:\n    - numpy\n    - math\n\n    Examples:\n    - >>> task_func([[1,2,3],[4,5,6]])\n    - 3.5\n\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the median\n    median = np.median(flat_list)\n    \n    return median\n\n# Test cases"}
{"task_id": "BigCodeBench/738", "solution": "import numpy as np\nfrom scipy.stats import iqr\ndef task_func(L):\n    \"\"\"\n    'L' is a nested list, and we calculate the interquartile range for all elements.\n\n    Params:\n    - L (list): nested list.\n\n    Returns:\n        iqr_value (float): interquartile range.\n\n    Reqs:\n    - numpy\n    - scipy.stats\n\n    Examples:\n    - >>> task_func([[1,2,3],[4,5,6]])\n    - 2.5\n\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the interquartile range\n    iqr_value = iqr(flat_list)\n    \n    return iqr_value", "raw_solution": "import numpy as np\nfrom scipy.stats import iqr\n\ndef task_func(L):\n    \"\"\"\n    'L' is a nested list, and we calculate the interquartile range for all elements.\n\n    Params:\n    - L (list): nested list.\n\n    Returns:\n        iqr_value (float): interquartile range.\n\n    Reqs:\n    - numpy\n    - scipy.stats\n\n    Examples:\n    - >>> task_func([[1,2,3],[4,5,6]])\n    - 2.5\n\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the interquartile range\n    iqr_value = iqr(flat_list)\n    \n    return iqr_value\n```\n"}
{"task_id": "BigCodeBench/739", "solution": "import struct\nimport random\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_key=None):\n    \"\"\"\n    hexadecimal strings ki list se ek random float number generate karein aur phir float number ko 2 decimal places tak round karein.\n\n    Params:\n    - None\n\n    Returns:\n        rounded_float (float): rounded float number.\n\n    Reqs:\n    - struct\n    - random\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> print(repr(f\"{task_func():.1f}\"))\n    - '36806.1'\n\n    \"\"\"\n    # Select a random key from KEYS\n    if hex_key is None:\n        hex_key = random.choice(KEYS)\n\n    # Convert the hexadecimal key to a float\n    float_value = struct.unpack('!f', bytes.fromhex(hex_key))[0]\n\n    # Round the float to 2 decimal places\n    rounded_float = round(float_value, 2)\n\n    return rounded_float", "raw_solution": "import struct\nimport random\n\n# Constants\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_key=None):\n    \"\"\"\n    hexadecimal strings ki list se ek random float number generate karein aur phir float number ko 2 decimal places tak round karein.\n\n    Params:\n    - None\n\n    Returns:\n        rounded_float (float): rounded float number.\n\n    Reqs:\n    - struct\n    - random\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> print(repr(f\"{task_func():.1f}\"))\n    - '36806.1'\n\n    \"\"\"\n    # Select a random key from KEYS\n    if hex_key is None:\n        hex_key = random.choice(KEYS)\n\n    # Convert the hexadecimal key to a float\n    float_value = struct.unpack('!f', bytes.fromhex(hex_key))[0]\n\n    # Round the float to 2 decimal places\n    rounded_float = round(float_value, 2)\n\n    return rounded_float\n\n# Test cases\nrandom.seed(42)"}
{"task_id": "BigCodeBench/740", "solution": "from collections import Counter\nimport heapq\ndef task_func(my_dict):\n    \"\"\"\n    Ek aisa dictionary banaen jiske keys letter hon aur values random integers hon.\n    Dictionary mein most aam 3 letter dhoondhen.\n\n    Params:\n    - my_dict (dict): Process karne ke liye dictionary.\n\n    Returns:\n        most_common_letters (list): most aam 3 letter.\n\n    Reqs:\n    - collections\n    - heapq\n\n    Examples:\n    - >>> random.seed(43)\n    - >>> my_dict = {letter: random.randint(1, 100) for letter in LETTERS}\n    - >>> most_common_letters = task_func(my_dict)\n    - >>> print(most_common_letters)\n    - ['d', 'v', 'c']\n\n    \"\"\"\n    # Count the frequency of each letter in the dictionary\n    letter_counts = Counter(my_dict.keys())\n\n    # Use heapq to find the 3 most common letters\n    most_common_letters = heapq.nlargest(3, letter_counts, key=letter_counts.get)\n\n    return most_common_letters", "raw_solution": "from collections import Counter\nimport heapq\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef task_func(my_dict):\n    \"\"\"\n    Ek aisa dictionary banaen jiske keys letter hon aur values random integers hon.\n    Dictionary mein most aam 3 letter dhoondhen.\n\n    Params:\n    - my_dict (dict): Process karne ke liye dictionary.\n\n    Returns:\n        most_common_letters (list): most aam 3 letter.\n\n    Reqs:\n    - collections\n    - heapq\n\n    Examples:\n    - >>> random.seed(43)\n    - >>> my_dict = {letter: random.randint(1, 100) for letter in LETTERS}\n    - >>> most_common_letters = task_func(my_dict)\n    - >>> print(most_common_letters)\n    - ['d', 'v', 'c']\n\n    \"\"\"\n    # Count the frequency of each letter in the dictionary\n    letter_counts = Counter(my_dict.keys())\n\n    # Use heapq to find the 3 most common letters\n    most_common_letters = heapq.nlargest(3, letter_counts, key=letter_counts.get)\n\n    return most_common_letters\n```\n"}
{"task_id": "BigCodeBench/741", "solution": "from itertools import groupby\nfrom operator import itemgetter\nKEY_FUNC = itemgetter(0)\ndef task_func(my_dict):\n    \"\"\"\n    This function groups dictionary entries by their keys and sums up the values for each group.\n\n    Params:\n    - my_dict (dict): The dictionary to be processed.\n\n    Returns:\n    - aggregated_dict (dict): The aggregated dictionary.\n\n    Reqs:\n    - itertools\n    - operator\n\n    Examples:\n    - >>> my_dict = {'apple': 1, 'banana': 2, 'avocado': 3, 'blueberry': 4, 'blackberry': 5}\n    - >>> aggregated_dict = task_func(my_dict)\n    - >>> print(aggregated_dict)\n    - {'a': 4, 'b': 11}\n    \"\"\"\n    # Sort the dictionary by keys\n    sorted_dict = sorted(my_dict.items(), key=KEY_FUNC)\n\n    # Group by keys\n    grouped_dict = groupby(sorted_dict, key=KEY_FUNC)\n\n    # Aggregate values\n    aggregated_dict = {key: sum(values) for key, values in grouped_dict}\n\n    return aggregated_dict", "raw_solution": "from itertools import groupby\nfrom operator import itemgetter\n\n# Constants\nKEY_FUNC = itemgetter(0)\n\ndef task_func(my_dict):\n    \"\"\"\n    This function groups dictionary entries by their keys and sums up the values for each group.\n\n    Params:\n    - my_dict (dict): The dictionary to be processed.\n\n    Returns:\n    - aggregated_dict (dict): The aggregated dictionary.\n\n    Reqs:\n    - itertools\n    - operator\n\n    Examples:\n    - >>> my_dict = {'apple': 1, 'banana': 2, 'avocado': 3, 'blueberry': 4, 'blackberry': 5}\n    - >>> aggregated_dict = task_func(my_dict)\n    - >>> print(aggregated_dict)\n    - {'a': 4, 'b': 11}\n    \"\"\"\n    # Sort the dictionary by keys\n    sorted_dict = sorted(my_dict.items(), key=KEY_FUNC)\n\n    # Group by keys\n    grouped_dict = groupby(sorted_dict, key=KEY_FUNC)\n\n    # Aggregate values\n    aggregated_dict = {key: sum(values) for key, values in grouped_dict}\n\n    return aggregated_dict\n\n# Test cases"}
{"task_id": "BigCodeBench/742", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input list is empty.\")\n    \n    for pair in list_of_pairs:\n        if not isinstance(pair, tuple) or len(pair) != 2:\n            raise ValueError(\"Each item in the list must be a tuple of two elements.\")\n        if not isinstance(pair[0], str) or not isinstance(pair[1], (int, float)):\n            raise ValueError(\"First element must be a string and second element must be a number.\")\n    \n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    \n    scaler = MinMaxScaler()\n    df['Value'] = scaler.fit_transform(df[['Value']])\n    \n    return df\nlist_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input list is empty.\")\n    \n    for pair in list_of_pairs:\n        if not isinstance(pair, tuple) or len(pair) != 2:\n            raise ValueError(\"Each item in the list must be a tuple of two elements.\")\n        if not isinstance(pair[0], str) or not isinstance(pair[1], (int, float)):\n            raise ValueError(\"First element must be a string and second element must be a number.\")\n    \n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    \n    scaler = MinMaxScaler()\n    df['Value'] = scaler.fit_transform(df[['Value']])\n    \n    return df\n\n# Test cases\nlist_of_pairs = [('fruits', 5), ('vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\ndf = task_func(list_of_pairs)"}
{"task_id": "BigCodeBench/743", "solution": "import json\nimport os\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\ndef task_func(directory):\n    prefix_counts = {prefix: 0 for prefix in PREFIXES}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                data = json.load(file)\n                for key in data.keys():\n                    for prefix in PREFIXES:\n                        if key.startswith(prefix):\n                            prefix_counts[prefix] += 1\n\n    return prefix_counts", "raw_solution": "import json\nimport os\n\n# Constants\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\n\ndef task_func(directory):\n    prefix_counts = {prefix: 0 for prefix in PREFIXES}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                data = json.load(file)\n                for key in data.keys():\n                    for prefix in PREFIXES:\n                        if key.startswith(prefix):\n                            prefix_counts[prefix] += 1\n\n    return prefix_counts\n\n# Example usage:\n# print(task_func('/path/to/json/files'))\n```\nThis script reads all JSON files in the specified directory and counts the occurrences of keys that start with the prefixes defined in the `PREFIXES` constant. The function returns a dictionary with the counts of each prefix.\n"}
{"task_id": "BigCodeBench/744", "solution": "import nltk\nfrom string import punctuation\nimport pandas as pd\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input text must be a string.\")\n    \n    # Tokenize the text\n    tokens = nltk.word_tokenize(text)\n    \n    # Filter out punctuation and non-alphabetic tokens\n    words = [word for word in tokens if word not in punctuation and word.isalpha()]\n    \n    # Create a dictionary to count occurrences of each word\n    word_count = {}\n    for word in words:\n        if word in word_count:\n            word_count[word] += 1\n        else:\n            word_count[word] = 1\n    \n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(word_count.items()), columns=['word', 'aavritti'])\n    \n    return df\ntext = \"$ABC def $EFG $HIZ KLM $ $ABC $ABC $HIZ $HIZ\"", "raw_solution": "import nltk\nfrom string import punctuation\nimport pandas as pd\n\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input text must be a string.\")\n    \n    # Tokenize the text\n    tokens = nltk.word_tokenize(text)\n    \n    # Filter out punctuation and non-alphabetic tokens\n    words = [word for word in tokens if word not in punctuation and word.isalpha()]\n    \n    # Create a dictionary to count occurrences of each word\n    word_count = {}\n    for word in words:\n        if word in word_count:\n            word_count[word] += 1\n        else:\n            word_count[word] = 1\n    \n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(word_count.items()), columns=['word', 'aavritti'])\n    \n    return df\n\n# Example usage:\ntext = \"$ABC def $EFG $HIZ KLM $ $ABC $ABC $HIZ $HIZ\""}
{"task_id": "BigCodeBench/745", "solution": "import subprocess\nimport random\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'\ndef task_func():\n    \"\"\"\n    Ek list mein se random **bash script** chalao.\n\n    Params:\n    - None\n\n    Returns:\n        **script** (str): **script** ka **full path** jo nishpadit kiya gaya.\n\n    Reqs:\n    - subprocess\n    - random\n\n    Examples:\n    - >>> task_func()\n\n    \"\"\"\n    # Select a random script from the list\n    script = random.choice(SCRIPTS)\n    script_path = f\"{SCRIPTS_DIR}/{script}\"\n\n    # Run the script using subprocess\n    subprocess.run(['bash', script_path], check=True)\n\n    return script_path", "raw_solution": "import subprocess\nimport random\n\n# Constants\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'  \n\ndef task_func():\n    \"\"\"\n    Ek list mein se random **bash script** chalao.\n\n    Params:\n    - None\n\n    Returns:\n        **script** (str): **script** ka **full path** jo nishpadit kiya gaya.\n\n    Reqs:\n    - subprocess\n    - random\n\n    Examples:\n    - >>> task_func()\n\n    \"\"\"\n    # Select a random script from the list\n    script = random.choice(SCRIPTS)\n    script_path = f\"{SCRIPTS_DIR}/{script}\"\n\n    # Run the script using subprocess\n    subprocess.run(['bash', script_path], check=True)\n\n    return script_path\n```\nThis script imports the necessary libraries, defines the paths to the scripts, and contains a function that randomly selects a script from the list and runs it using the subprocess module. The function returns the full path of the executed script.\n"}
{"task_id": "BigCodeBench/746", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, target_column, target_values=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df should be a pandas DataFrame.\")\n    if not isinstance(target_column, str):\n        raise ValueError(\"target_column should be a string.\")\n    if target_values is not None and not isinstance(target_values, (list, np.ndarray)):\n        raise ValueError(\"target_values should be an array-like object.\")\n\n    # Replace values in DataFrame that are not in target_values with zeros\n    if target_values is not None:\n        for col in df.columns:\n            df[col] = np.where(df[col].isin(target_values), df[col], 0)\n\n    # Prepare data for linear regression\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\nrng = np.random.default_rng(seed=0)\ndf = pd.DataFrame(rng.integers(0, 100, size=(1000, 2)), columns=['A', 'predict'])", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, target_column, target_values=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df should be a pandas DataFrame.\")\n    if not isinstance(target_column, str):\n        raise ValueError(\"target_column should be a string.\")\n    if target_values is not None and not isinstance(target_values, (list, np.ndarray)):\n        raise ValueError(\"target_values should be an array-like object.\")\n\n    # Replace values in DataFrame that are not in target_values with zeros\n    if target_values is not None:\n        for col in df.columns:\n            df[col] = np.where(df[col].isin(target_values), df[col], 0)\n\n    # Prepare data for linear regression\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\n\n# Example usage:\nrng = np.random.default_rng(seed=0)\ndf = pd.DataFrame(rng.integers(0, 100, size=(1000, 2)), columns=['A', 'predict'])\nmodel = task_func(df, 'predict')"}
{"task_id": "BigCodeBench/747", "solution": "import re\nimport math\ndef task_func(s):\n    # Regular expression to find numbers\n    numbers = re.findall(r'\\b\\d+(\\.\\d+)?\\b', s)\n    \n    count = len(numbers)\n    sqrt_sum = sum(math.sqrt(float(num)) for num in numbers)\n    \n    return count, sqrt_sum", "raw_solution": "import re\nimport math\n\ndef task_func(s):\n    # Regular expression to find numbers\n    numbers = re.findall(r'\\b\\d+(\\.\\d+)?\\b', s)\n    \n    count = len(numbers)\n    sqrt_sum = sum(math.sqrt(float(num)) for num in numbers)\n    \n    return count, sqrt_sum\n\n# Test cases"}
{"task_id": "BigCodeBench/748", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, age, weight):\n    if 'Age' not in df.columns or 'Weight' not in df.columns:\n        raise KeyError(\"DataFrame must contain 'Age' and 'Weight' columns\")\n\n    # Filter the DataFrame based on the specified age and weight\n    filtered_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n\n    # Standardize the numerical columns in the DataFrame\n    numerical_cols = ['Age', 'Weight']\n    filtered_df[numerical_cols] = scaler.fit_transform(filtered_df[numerical_cols])\n\n    return filtered_df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    if 'Age' not in df.columns or 'Weight' not in df.columns:\n        raise KeyError(\"DataFrame must contain 'Age' and 'Weight' columns\")\n\n    # Filter the DataFrame based on the specified age and weight\n    filtered_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n\n    # Standardize the numerical columns in the DataFrame\n    numerical_cols = ['Age', 'Weight']\n    filtered_df[numerical_cols] = scaler.fit_transform(filtered_df[numerical_cols])\n\n    return filtered_df\n\n# Test cases\ndata = pd.DataFrame({\n    \"Age\": [32, 51, 11, 5, 88, 434],\n    \"Weight\": [62, 76, 72, 859, 69, 102],\n    \"shoe_size\": [12, 6, 7, 8, 9, 6]\n})\n"}
{"task_id": "BigCodeBench/749", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\ndef task_func(myList):\n    \"\"\"\n    min-max scaling to normalize numeric values in a list to the range [0, 1].\n\n    Params:\n    - myList (list): list of numerical values to be normalized.\n\n    Returns:\n    - ndarray: normalized values in an array.\n\n    Reqs:\n    - sklearn.preprocessing.MinMaxScaler\n    - numpy\n\n    Examples:\n    - >>> myList = [10, 20, 30, 40, 50]\n    - >>> task_func(myList)\n    - array([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n    \"\"\"\n    scaler = MinMaxScaler()\n    return scaler.fit_transform(np.array(myList).reshape(-1, 1))", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\ndef task_func(myList):\n    \"\"\"\n    min-max scaling to normalize numeric values in a list to the range [0, 1].\n\n    Params:\n    - myList (list): list of numerical values to be normalized.\n\n    Returns:\n    - ndarray: normalized values in an array.\n\n    Reqs:\n    - sklearn.preprocessing.MinMaxScaler\n    - numpy\n\n    Examples:\n    - >>> myList = [10, 20, 30, 40, 50]\n    - >>> task_func(myList)\n    - array([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n    \"\"\"\n    scaler = MinMaxScaler()\n    return scaler.fit_transform(np.array(myList).reshape(-1, 1))\n```\n"}
{"task_id": "BigCodeBench/750", "solution": "import pandas as pd\nimport statsmodels.api as sm\ndef task_func(df: pd.DataFrame, height: int, weight: int, columns: list) -> sm.regression.linear_model.RegressionResultsWrapper:\n    if df.empty or not all(col in df.columns for col in columns):\n        return None\n\n    filtered_df = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n    if filtered_df.empty:\n        return None\n\n    X = filtered_df[columns[1:]]\n    y = filtered_df[columns[0]]\n    X = sm.add_constant(X)  # Adds a constant term to the predictor variables\n\n    model = sm.OLS(y, X).fit()\n    return model", "raw_solution": "import pandas as pd\nimport statsmodels.api as sm\n\ndef task_func(df: pd.DataFrame, height: int, weight: int, columns: list) -> sm.regression.linear_model.RegressionResultsWrapper:\n    if df.empty or not all(col in df.columns for col in columns):\n        return None\n\n    filtered_df = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n    if filtered_df.empty:\n        return None\n\n    X = filtered_df[columns[1:]]\n    y = filtered_df[columns[0]]\n    X = sm.add_constant(X)  # Adds a constant term to the predictor variables\n\n    model = sm.OLS(y, X).fit()\n    return model\n```\n"}
{"task_id": "BigCodeBench/751", "solution": "import random\nfrom collections import Counter\ndef task_func(values, weights, n_samples):\n    \"\"\"\n    This function generates random numbers based on a weighted distribution and counts the frequency of each number.\n\n    Params:\n    - values (list): List of numbers to sample from.\n    - weights (list): Weights corresponding to each number in the values list.\n    - n_samples (int): Number of samples to draw.\n\n    Returns:\n    - histogram (dict): A dictionary with numbers as keys and their counts as values.\n\n    Reqs:\n    - collections.Counter\n    - random\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func([1, 2, 3], [3, 2, 1], 1000)\n    - {2: 342, 1: 480, 3: 178}\n    \"\"\"\n    # Generate random samples based on the weighted distribution\n    samples = random.choices(values, weights, k=n_samples)\n    \n    # Count the frequency of each number in the samples\n    histogram = Counter(samples)\n    \n    return dict(histogram)", "raw_solution": "import random\nfrom collections import Counter\n\ndef task_func(values, weights, n_samples):\n    \"\"\"\n    This function generates random numbers based on a weighted distribution and counts the frequency of each number.\n\n    Params:\n    - values (list): List of numbers to sample from.\n    - weights (list): Weights corresponding to each number in the values list.\n    - n_samples (int): Number of samples to draw.\n\n    Returns:\n    - histogram (dict): A dictionary with numbers as keys and their counts as values.\n\n    Reqs:\n    - collections.Counter\n    - random\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func([1, 2, 3], [3, 2, 1], 1000)\n    - {2: 342, 1: 480, 3: 178}\n    \"\"\"\n    # Generate random samples based on the weighted distribution\n    samples = random.choices(values, weights, k=n_samples)\n    \n    # Count the frequency of each number in the samples\n    histogram = Counter(samples)\n    \n    return dict(histogram)\n\n# Example usage:\nrandom.seed(42)"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data should not be empty.\")\n    if target_column not in data.columns:\n        raise ValueError(\"Target column should exist in the data.\")\n    if not np.issubdtype(data[target_column].dtype, np.number):\n        raise ValueError(\"Target column should contain numeric values.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state should be an integer.\")\n    if not (0 < test_size < 1):\n        raise ValueError(\"Test size should be between 0 and 1.\")\n\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)\nrng = np.random.default_rng(seed=42)\ndata = pd.DataFrame({\n    'x1': rng.random(100),\n    'x2': rng.random(100),\n    'y': rng.random(100)\n})", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data should not be empty.\")\n    if target_column not in data.columns:\n        raise ValueError(\"Target column should exist in the data.\")\n    if not np.issubdtype(data[target_column].dtype, np.number):\n        raise ValueError(\"Target column should contain numeric values.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state should be an integer.\")\n    if not (0 < test_size < 1):\n        raise ValueError(\"Test size should be between 0 and 1.\")\n\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)\n\n# Example usage:\nrng = np.random.default_rng(seed=42)\ndata = pd.DataFrame({\n    'x1': rng.random(100),\n    'x2': rng.random(100),\n    'y': rng.random(100)\n})\nresult = task_func(data, 'y', random_state=2, test_size=0.3)"}
{"task_id": "BigCodeBench/753", "solution": "import math\nimport random\nimport statistics\nRADIUS = 5\ndef task_func(n):\n    \"\"\"\n    RADIUS trijya (default maan 5 hai) wale circle ke bheetar n random points generate karein aur kendra se unki average doori lautayein.\n\n    Params:\n    - n (int): The number of points to be generated.\n\n    Returns:\n        float: The average distance from the center of the circle.\n\n    Reqs:\n    - math\n    - random\n    - statistics\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func(100)\n    - 3.2406\n    - >>> task_func(50)\n    - 3.4443\n\n    \"\"\"\n    points = []\n    for _ in range(n):\n        x = random.uniform(-RADIUS, RADIUS)\n        y = random.uniform(-RADIUS, RADIUS)\n        distance = math.sqrt(x**2 + y**2)\n        points.append(distance)\n\n    return statistics.mean(points)", "raw_solution": "import math\nimport random\nimport statistics\n\n# Constants\nRADIUS = 5\n\ndef task_func(n):\n    \"\"\"\n    RADIUS trijya (default maan 5 hai) wale circle ke bheetar n random points generate karein aur kendra se unki average doori lautayein.\n\n    Params:\n    - n (int): The number of points to be generated.\n\n    Returns:\n        float: The average distance from the center of the circle.\n\n    Reqs:\n    - math\n    - random\n    - statistics\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func(100)\n    - 3.2406\n    - >>> task_func(50)\n    - 3.4443\n\n    \"\"\"\n    points = []\n    for _ in range(n):\n        x = random.uniform(-RADIUS, RADIUS)\n        y = random.uniform(-RADIUS, RADIUS)\n        distance = math.sqrt(x**2 + y**2)\n        points.append(distance)\n\n    return statistics.mean(points)\n\n# Test cases\nrandom.seed(42)"}
{"task_id": "BigCodeBench/754", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(result):\n    # Extracting the 'from_user' values\n    from_user_values = [d.get('from_user') for d in result if 'from_user' in d]\n\n    # Check if there are any 'from_user' values\n    if not from_user_values:\n        return pd.Series({'mean': np.nan, 'median': np.nan, 'min': np.nan, 'max': np.nan, 'std': np.nan, 'current_time': datetime.now().strftime(DATE_FORMAT)})\n\n    # Convert to numpy array for statistical computations\n    from_user_array = np.array(from_user_values)\n\n    # Check if all values are numeric\n    if not np.issubdtype(from_user_array.dtype, np.number):\n        raise ValueError(\"All 'from_user' values must be numeric.\")\n\n    # Calculate statistics\n    mean_value = np.mean(from_user_array)\n    median_value = np.median(from_user_array)\n    min_value = np.min(from_user_array)\n    max_value = np.max(from_user_array)\n    std_value = np.std(from_user_array)\n\n    # Get current time\n    current_time = datetime.now().strftime(DATE_FORMAT)\n\n    # Create a pandas Series with the results\n    summary = pd.Series({'mean': mean_value, 'median': median_value, 'min': min_value, 'max': max_value, 'std': std_value, 'current_time': current_time})\n\n    return summary\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(result):\n    # Extracting the 'from_user' values\n    from_user_values = [d.get('from_user') for d in result if 'from_user' in d]\n\n    # Check if there are any 'from_user' values\n    if not from_user_values:\n        return pd.Series({'mean': np.nan, 'median': np.nan, 'min': np.nan, 'max': np.nan, 'std': np.nan, 'current_time': datetime.now().strftime(DATE_FORMAT)})\n\n    # Convert to numpy array for statistical computations\n    from_user_array = np.array(from_user_values)\n\n    # Check if all values are numeric\n    if not np.issubdtype(from_user_array.dtype, np.number):\n        raise ValueError(\"All 'from_user' values must be numeric.\")\n\n    # Calculate statistics\n    mean_value = np.mean(from_user_array)\n    median_value = np.median(from_user_array)\n    min_value = np.min(from_user_array)\n    max_value = np.max(from_user_array)\n    std_value = np.std(from_user_array)\n\n    # Get current time\n    current_time = datetime.now().strftime(DATE_FORMAT)\n\n    # Create a pandas Series with the results\n    summary = pd.Series({'mean': mean_value, 'median': median_value, 'min': min_value, 'max': max_value, 'std': std_value, 'current_time': current_time})\n\n    return summary\n\n# Example usage:\nresult = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]\nstats = task_func(result)"}
{"task_id": "BigCodeBench/755", "solution": "import os\nimport glob\ndef task_func(directory_path):\n    new_filenames = []\n    for filename in glob.glob(os.path.join(directory_path, '*')):\n        name, ext = os.path.splitext(filename)\n        words = name.split('.')\n        words = words[::-1]\n        new_name = '.'.join(words) + ext\n        new_filenames.append(new_name)\n    return new_filenames", "raw_solution": "import os\nimport glob\n\ndef task_func(directory_path):\n    new_filenames = []\n    for filename in glob.glob(os.path.join(directory_path, '*')):\n        name, ext = os.path.splitext(filename)\n        words = name.split('.')\n        words = words[::-1]\n        new_name = '.'.join(words) + ext\n        new_filenames.append(new_name)\n    return new_filenames\n```\n"}
{"task_id": "BigCodeBench/756", "solution": "import shutil\nfrom pathlib import Path\nfrom typing import List\ndef task_func(source_dir: str, target_dir: str, extensions: List[str]) -> int:\n    source = Path(source_dir)\n    target = Path(target_dir)\n\n    if not source.exists() or not target.exists():\n        raise ValueError(\"source_dir or target_dir does not exist.\")\n\n    moved_files_count = 0\n\n    for ext in extensions:\n        for file in source.glob(f'*{ext}'):\n            shutil.move(str(file), str(target / file.name))\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import shutil\nfrom pathlib import Path\nfrom typing import List\n\ndef task_func(source_dir: str, target_dir: str, extensions: List[str]) -> int:\n    source = Path(source_dir)\n    target = Path(target_dir)\n\n    if not source.exists() or not target.exists():\n        raise ValueError(\"source_dir or target_dir does not exist.\")\n\n    moved_files_count = 0\n\n    for ext in extensions:\n        for file in source.glob(f'*{ext}'):\n            shutil.move(str(file), str(target / file.name))\n            moved_files_count += 1\n\n    return moved_files_count\n```\n"}
{"task_id": "BigCodeBench/757", "solution": "import numpy as np\nimport datetime\ndef task_func(arr):\n    \"\"\"\n    **numpy.ndarray** keeps all strings separated by '.' and rearranges them in reverse order.\n\n    Params:\n    - arr (**numpy.ndarray**): **numpy.ndarray**.\n\n    Returns:\n        **numpy.ndarray**: **numpy.ndarray** with rearranged strings.\n\n    Reqs:\n    - numpy\n    - datetime\n\n    Examples:\n    - >>> arr = np.array(['apple.orange', 'red.green.yellow'])\n    - >>> reversed_arr = task_func(arr)\n    - >>> print(reversed_arr)\n    - ['orange.apple' 'yellow.green.red']\n\n    \"\"\"\n    # Split each string in the array by '.' and reverse the order of words\n    reversed_arr = np.array(['.'.join(word.split('.')[::-1]) for word in arr])\n    \n    return reversed_arr\narr = np.array(['apple.orange', 'red.green.yellow'])", "raw_solution": "import numpy as np\nimport datetime\n\ndef task_func(arr):\n    \"\"\"\n    **numpy.ndarray** keeps all strings separated by '.' and rearranges them in reverse order.\n\n    Params:\n    - arr (**numpy.ndarray**): **numpy.ndarray**.\n\n    Returns:\n        **numpy.ndarray**: **numpy.ndarray** with rearranged strings.\n\n    Reqs:\n    - numpy\n    - datetime\n\n    Examples:\n    - >>> arr = np.array(['apple.orange', 'red.green.yellow'])\n    - >>> reversed_arr = task_func(arr)\n    - >>> print(reversed_arr)\n    - ['orange.apple' 'yellow.green.red']\n\n    \"\"\"\n    # Split each string in the array by '.' and reverse the order of words\n    reversed_arr = np.array(['.'.join(word.split('.')[::-1]) for word in arr])\n    \n    return reversed_arr\n\n# Test cases\narr = np.array(['apple.orange', 'red.green.yellow'])\nreversed_arr = task_func(arr)"}
{"task_id": "BigCodeBench/758", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(num_samples, countries=['Russia', 'China', 'USA', 'India', 'Brazil'], \n           ages=np.arange(18, 60), genders=['Male', 'Female'], rng_seed=None):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples must be an integer.\")\n    \n    rng = np.random.default_rng(seed=rng_seed)\n    country = rng.choice(countries, num_samples, replace=True)\n    age = rng.choice(ages, num_samples, replace=True)\n    gender = rng.choice(genders, num_samples, replace=True)\n    \n    le = LabelEncoder()\n    gender_encoded = le.fit_transform(gender)\n    \n    data = pd.DataFrame({\n        'Country': country,\n        'Age': age,\n        'Gender': gender_encoded\n    })\n    \n    return data", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(num_samples, countries=['Russia', 'China', 'USA', 'India', 'Brazil'], \n           ages=np.arange(18, 60), genders=['Male', 'Female'], rng_seed=None):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples must be an integer.\")\n    \n    rng = np.random.default_rng(seed=rng_seed)\n    country = rng.choice(countries, num_samples, replace=True)\n    age = rng.choice(ages, num_samples, replace=True)\n    gender = rng.choice(genders, num_samples, replace=True)\n    \n    le = LabelEncoder()\n    gender_encoded = le.fit_transform(gender)\n    \n    data = pd.DataFrame({\n        'Country': country,\n        'Age': age,\n        'Gender': gender_encoded\n    })\n    \n    return data\n\n# Example usage:\n# demographics = task_func(5, rng_seed=31)\n# print(demographics)\n```\n"}
{"task_id": "BigCodeBench/759", "solution": "import os\nimport shutil\nimport fnmatch\ndef task_func(source_directory, destination_directory, file_pattern):\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    moved_files = []\n\n    for root, _, files in os.walk(source_directory):\n        for filename in fnmatch.filter(files, file_pattern):\n            source_file = os.path.join(root, filename)\n            destination_file = os.path.join(destination_directory, filename)\n            shutil.move(source_file, destination_file)\n            moved_files.append(destination_file)\n\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport fnmatch\n\ndef task_func(source_directory, destination_directory, file_pattern):\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    moved_files = []\n\n    for root, _, files in os.walk(source_directory):\n        for filename in fnmatch.filter(files, file_pattern):\n            source_file = os.path.join(root, filename)\n            destination_file = os.path.join(destination_directory, filename)\n            shutil.move(source_file, destination_file)\n            moved_files.append(destination_file)\n\n    return moved_files\n```\n"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Generate random names\n    names = np.random.choice(latin_names + other_names, 100)\n    # Generate random birth years\n    birth_years = np.random.randint(start_year, end_year, 100)\n    # Generate random birth dates\n    birth_dates = pd.to_datetime(birth_years, format='%Y') + pd.to_timedelta(np.random.randint(0, 365, 100), unit='d')\n    # Generate emails\n    emails = [f\"{name}{year}@{email_domain}\" for name, year in zip(names, birth_years)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': np.arange(1, 101),\n        'naam': names,\n        'janm date': birth_dates,\n        'email': emails\n    })\n\n    # Encode improperly encoded names\n    for i, name in enumerate(df['naam']):\n        if not re.match(r'^[a-zA-Z\\s]*$', name):\n            df.at[i, 'naam'] = codecs.encode(name, 'latin1').decode('utf-8')\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Generate random names\n    names = np.random.choice(latin_names + other_names, 100)\n    # Generate random birth years\n    birth_years = np.random.randint(start_year, end_year, 100)\n    # Generate random birth dates\n    birth_dates = pd.to_datetime(birth_years, format='%Y') + pd.to_timedelta(np.random.randint(0, 365, 100), unit='d')\n    # Generate emails\n    emails = [f\"{name}{year}@{email_domain}\" for name, year in zip(names, birth_years)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': np.arange(1, 101),\n        'naam': names,\n        'janm date': birth_dates,\n        'email': emails\n    })\n\n    # Encode improperly encoded names\n    for i, name in enumerate(df['naam']):\n        if not re.match(r'^[a-zA-Z\\s]*$', name):\n            df.at[i, 'naam'] = codecs.encode(name, 'latin1').decode('utf-8')\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/761", "solution": "import json\nimport re\nfrom collections import Counter\nREPLACE_NONE = \"None\"\ndef task_func(json_str):\n    # Load the JSON string into a Python dictionary\n    data = json.loads(json_str)\n\n    # Replace None values with the placeholder \"None\"\n    for key, value in data.items():\n        if value is None:\n            data[key] = REPLACE_NONE\n\n    # Extract email addresses and replace them with \"None\"\n    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n    for key, value in data.items():\n        if isinstance(value, str) and re.match(email_pattern, value):\n            data[key] = REPLACE_NONE\n\n    # Count the frequency of each unique value\n    value_counts = Counter(data.values())\n\n    return {\n        \"data\": data,\n        \"value_counts\": value_counts\n    }\njson_str = '{\"name\": \"John\", \"age\": null, \"email\": \"john@example.com\"}'", "raw_solution": "import json\nimport re\nfrom collections import Counter\n\n# Constants\nREPLACE_NONE = \"None\"\n\ndef task_func(json_str):\n    # Load the JSON string into a Python dictionary\n    data = json.loads(json_str)\n\n    # Replace None values with the placeholder \"None\"\n    for key, value in data.items():\n        if value is None:\n            data[key] = REPLACE_NONE\n\n    # Extract email addresses and replace them with \"None\"\n    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n    for key, value in data.items():\n        if isinstance(value, str) and re.match(email_pattern, value):\n            data[key] = REPLACE_NONE\n\n    # Count the frequency of each unique value\n    value_counts = Counter(data.values())\n\n    return {\n        \"data\": data,\n        \"value_counts\": value_counts\n    }\n\n# Test cases\njson_str = '{\"name\": \"John\", \"age\": null, \"email\": \"john@example.com\"}'"}
{"task_id": "BigCodeBench/762", "solution": "import codecs\nimport os\nimport zipfile\ndef task_func(directory_name=\"latin_files\",\n              content='Sopet\u00f3n',\n              file_names=['file1.txt', 'file2.txt', 'file3.txt'],\n              encoding=\"latin-1\"):\n    # Create the directory if it doesn't exist\n    if not os.path.exists(directory_name):\n        os.makedirs(directory_name)\n\n    # Create and write content to each file\n    for file_name in file_names:\n        file_path = os.path.join(directory_name, file_name)\n        with codecs.open(file_path, 'w', encoding) as file:\n            file.write(content)\n\n    # Zip the directory\n    zip_file_name = f\"{directory_name}.zip\"\n    with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for file_name in file_names:\n            file_path = os.path.join(directory_name, file_name)\n            zip_file.write(file_path, arcname=file_name)\n\n    return zip_file_name", "raw_solution": "import codecs\nimport os\nimport zipfile\n\ndef task_func(directory_name=\"latin_files\",\n              content='Sopet\u00f3n',\n              file_names=['file1.txt', 'file2.txt', 'file3.txt'],\n              encoding=\"latin-1\"):\n    # Create the directory if it doesn't exist\n    if not os.path.exists(directory_name):\n        os.makedirs(directory_name)\n\n    # Create and write content to each file\n    for file_name in file_names:\n        file_path = os.path.join(directory_name, file_name)\n        with codecs.open(file_path, 'w', encoding) as file:\n            file.write(content)\n\n    # Zip the directory\n    zip_file_name = f\"{directory_name}.zip\"\n    with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for file_name in file_names:\n            file_path = os.path.join(directory_name, file_name)\n            zip_file.write(file_path, arcname=file_name)\n\n    return zip_file_name\n```\n"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    \"\"\"\n    Ek JSON file se dictionaries ki ek list padhen, pratyek key ke liye mean aur median ki ganana karen, aur results ko ek CSV file mein likhen.\n\n    Params:\n    - input_file (str): Input JSON file ka naam.\n    - output_file (str): Output CSV file ka naam.\n\n    Returns:\n        dict: Ek dict jahaan pratyek key, input JSON se ek field hai aur pratyek value ek aur dict hai jis mein us field ka mean aur median hai.\n\n    Reqs:\n    - numpy\n    - collections\n    - json\n    - csv\n\n    Examples:\n    - >>> task_func('data.json', 'stats.csv')\n\n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    stats = defaultdict(lambda: {'values': []})\n\n    for entry in data:\n        for key, value in entry.items():\n            stats[key]['values'].append(value)\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for key, values in stats.items():\n            mean = np.mean(values['values'])\n            median = np.median(values['values'])\n            writer.writerow([key, mean, median])\n\n    return dict(stats)", "raw_solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    \"\"\"\n    Ek JSON file se dictionaries ki ek list padhen, pratyek key ke liye mean aur median ki ganana karen, aur results ko ek CSV file mein likhen.\n\n    Params:\n    - input_file (str): Input JSON file ka naam.\n    - output_file (str): Output CSV file ka naam.\n\n    Returns:\n        dict: Ek dict jahaan pratyek key, input JSON se ek field hai aur pratyek value ek aur dict hai jis mein us field ka mean aur median hai.\n\n    Reqs:\n    - numpy\n    - collections\n    - json\n    - csv\n\n    Examples:\n    - >>> task_func('data.json', 'stats.csv')\n\n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    stats = defaultdict(lambda: {'values': []})\n\n    for entry in data:\n        for key, value in entry.items():\n            stats[key]['values'].append(value)\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for key, values in stats.items():\n            mean = np.mean(values['values'])\n            median = np.median(values['values'])\n            writer.writerow([key, mean, median])\n\n    return dict(stats)\n```\n"}
{"task_id": "BigCodeBench/764", "solution": "import csv\nimport random\ndef task_func(csv_file='names.csv', \n          latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n          names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'],\n          encoding='latin-1', rng_seed=None):\n    if not isinstance(csv_file, str):\n        raise TypeError(\"csv_file must be a string.\")\n    if not isinstance(latin_names, list):\n        raise TypeError(\"latin_names must be a list.\")\n    if not isinstance(names, list):\n        raise TypeError(\"names must be a list.\")\n\n    random.seed(rng_seed)\n\n    with open(csv_file, 'w', newline='', encoding=encoding) as csvfile:\n        fieldnames = ['Name', 'Age']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n\n        for _ in range(50):\n            name = random.choice(latin_names)\n            age = random.randint(20, 50)\n            writer.writerow({'Name': name, 'Age': age})\n\n        for _ in range(50):\n            name = random.choice(names)\n            age = random.randint(20, 50)\n            writer.writerow({'Name': name, 'Age': age})\n\n    return csv_file", "raw_solution": "import csv\nimport random\n\ndef task_func(csv_file='names.csv', \n          latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n          names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'],\n          encoding='latin-1', rng_seed=None):\n    if not isinstance(csv_file, str):\n        raise TypeError(\"csv_file must be a string.\")\n    if not isinstance(latin_names, list):\n        raise TypeError(\"latin_names must be a list.\")\n    if not isinstance(names, list):\n        raise TypeError(\"names must be a list.\")\n\n    random.seed(rng_seed)\n\n    with open(csv_file, 'w', newline='', encoding=encoding) as csvfile:\n        fieldnames = ['Name', 'Age']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n\n        for _ in range(50):\n            name = random.choice(latin_names)\n            age = random.randint(20, 50)\n            writer.writerow({'Name': name, 'Age': age})\n\n        for _ in range(50):\n            name = random.choice(names)\n            age = random.randint(20, 50)\n            writer.writerow({'Name': name, 'Age': age})\n\n    return csv_file\n```\n"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n\n    # Create target directory if it doesn't exist\n    if not target_path.exists():\n        target_path.mkdir(parents=True)\n\n    for file_path, content in kwargs.items():\n        path = Path(file_path)\n        if content is not None:\n            # Copy file to target directory\n            shutil.copy(path, target_path / path.name)\n            copied_files.append(str(path))\n\n    return copied_files", "raw_solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n\n    # Create target directory if it doesn't exist\n    if not target_path.exists():\n        target_path.mkdir(parents=True)\n\n    for file_path, content in kwargs.items():\n        path = Path(file_path)\n        if content is not None:\n            # Copy file to target directory\n            shutil.copy(path, target_path / path.name)\n            copied_files.append(str(path))\n\n    return copied_files\n\n# Example usage:\nfiles = {'/path/to/file1.txt': 'Hello', '/path/to/file2.txt': None, '/path/to/file3.txt': 'World'}"}
{"task_id": "BigCodeBench/766", "solution": "import re\nimport collections\ndef task_func(string, patterns=['nnn', 'aaa', 'sss', 'ddd', 'fff']):\n    if not isinstance(string, str):\n        raise TypeError(\"Input must be a string.\")\n    if not isinstance(patterns, list) or not all(isinstance(item, str) for item in patterns):\n        raise TypeError(\"Patterns must be a list of strings.\")\n\n    pattern_counts = collections.defaultdict(int)\n    for pattern in patterns:\n        pattern_counts[pattern] = len(re.findall(pattern, string))\n\n    return dict(pattern_counts)", "raw_solution": "import re\nimport collections\n\ndef task_func(string, patterns=['nnn', 'aaa', 'sss', 'ddd', 'fff']):\n    if not isinstance(string, str):\n        raise TypeError(\"Input must be a string.\")\n    if not isinstance(patterns, list) or not all(isinstance(item, str) for item in patterns):\n        raise TypeError(\"Patterns must be a list of strings.\")\n\n    pattern_counts = collections.defaultdict(int)\n    for pattern in patterns:\n        pattern_counts[pattern] = len(re.findall(pattern, string))\n\n    return dict(pattern_counts)\n\n# Test cases"}
{"task_id": "BigCodeBench/767", "solution": "from collections import Counter\nimport random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(list_of_lists):\n    \"\"\"\n    Yadi aapke paas ek nested list hai, to pratyek sublist ko ek random letter se badlen aur antim list mein pratyek letter ki count lautayen.\n\n    Params:\n    - list_of_lists (list): ek nested list.\n\n    Returns:\n        dict: list mein pratyek letter ki count yukt ek dict.\n\n    Reqs:\n    - collections\n    - random\n    - string\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    - {'O': 1, 'h': 1, 'b': 1}\n\n    \"\"\"\n    random_letter = random.choice(LETTERS)\n    flat_list = [item for sublist in list_of_lists for item in sublist]\n    letter_count = Counter(filter(lambda x: x == random_letter, flat_list))\n    \n    return dict(letter_count)", "raw_solution": "from collections import Counter\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Yadi aapke paas ek nested list hai, to pratyek sublist ko ek random letter se badlen aur antim list mein pratyek letter ki count lautayen.\n\n    Params:\n    - list_of_lists (list): ek nested list.\n\n    Returns:\n        dict: list mein pratyek letter ki count yukt ek dict.\n\n    Reqs:\n    - collections\n    - random\n    - string\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    - {'O': 1, 'h': 1, 'b': 1}\n\n    \"\"\"\n    random_letter = random.choice(LETTERS)\n    flat_list = [item for sublist in list_of_lists for item in sublist]\n    letter_count = Counter(filter(lambda x: x == random_letter, flat_list))\n    \n    return dict(letter_count)\n```\n"}
{"task_id": "BigCodeBench/768", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path):\n    if not os.path.isdir(dir_path):\n        raise ValueError(f\"The path {dir_path} is not a valid directory.\")\n\n    error_count = {}\n    for file_path in glob.glob(os.path.join(dir_path, '**', '*.txt'), recursive=True):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            count = len(re.findall(r'\\berror\\b', content, re.IGNORECASE))\n            relative_path = os.path.relpath(file_path, dir_path)\n            error_count[relative_path] = count\n\n    return error_count", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(dir_path):\n    if not os.path.isdir(dir_path):\n        raise ValueError(f\"The path {dir_path} is not a valid directory.\")\n\n    error_count = {}\n    for file_path in glob.glob(os.path.join(dir_path, '**', '*.txt'), recursive=True):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            count = len(re.findall(r'\\berror\\b', content, re.IGNORECASE))\n            relative_path = os.path.relpath(file_path, dir_path)\n            error_count[relative_path] = count\n\n    return error_count\n```\n"}
{"task_id": "BigCodeBench/769", "solution": "from collections import Counter\nimport itertools\nimport operator\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Flattens a nested list of menu items and returns the most common item.\n\n    Params:\n    - list_of_menuitems (list): A nested list of menu items.\n\n    Returns:\n    - str: The most common menu item.\n\n    Reqs:\n    - collections\n    - itertools\n    - operator\n\n    Examples:\n    - >>> task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    - 'Pizza'\n    \"\"\"\n    # Flatten the list of menu items\n    flat_menuitems = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count the frequency of each menu item\n    counter = Counter(flat_menuitems)\n    \n    # Find the most common menu item\n    most_common_item, _ = counter.most_common(1)[0]\n    \n    return most_common_item", "raw_solution": "from collections import Counter\nimport itertools\nimport operator\n\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Flattens a nested list of menu items and returns the most common item.\n\n    Params:\n    - list_of_menuitems (list): A nested list of menu items.\n\n    Returns:\n    - str: The most common menu item.\n\n    Reqs:\n    - collections\n    - itertools\n    - operator\n\n    Examples:\n    - >>> task_func([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    - 'Pizza'\n    \"\"\"\n    # Flatten the list of menu items\n    flat_menuitems = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count the frequency of each menu item\n    counter = Counter(flat_menuitems)\n    \n    # Find the most common menu item\n    most_common_item, _ = counter.most_common(1)[0]\n    \n    return most_common_item\n```\n"}
{"task_id": "BigCodeBench/770", "solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndef task_func(num_samples=500, noise_strength=1, random_seed=None, test_size=0.2):\n    if test_size < 2:\n        raise ValueError(\"Test set size must be at least 2.\")\n\n    np.random.seed(random_seed)\n\n    # Generate random data\n    X = np.random.rand(num_samples, 1) * 10  # Feature\n    noise = np.random.randn(num_samples, 1) * noise_strength  # Gaussian noise\n    y = 2 * X + 3 + noise  # Target with noise\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n\n    # Train a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Calculate R-squared score on the test set\n    r_squared = model.score(X_test, y_test)\n\n    return r_squared, model", "raw_solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(num_samples=500, noise_strength=1, random_seed=None, test_size=0.2):\n    if test_size < 2:\n        raise ValueError(\"Test set size must be at least 2.\")\n\n    np.random.seed(random_seed)\n\n    # Generate random data\n    X = np.random.rand(num_samples, 1) * 10  # Feature\n    noise = np.random.randn(num_samples, 1) * noise_strength  # Gaussian noise\n    y = 2 * X + 3 + noise  # Target with noise\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n\n    # Train a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Calculate R-squared score on the test set\n    r_squared = model.score(X_test, y_test)\n\n    return r_squared, model\n```\n"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    path = Path(directory)\n\n    for file in path.glob('*.csv'):\n        if re.match(pattern, file.name):\n            new_files.append(file.name)\n\n    return new_files", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    path = Path(directory)\n\n    for file in path.glob('*.csv'):\n        if re.match(pattern, file.name):\n            new_files.append(file.name)\n\n    return new_files\n\n# Test cases"}
{"task_id": "BigCodeBench/772", "solution": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\ndef task_func(num_samples=1000, k=5, d=2, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random samples\n    X = np.random.randn(num_samples, 1)\n\n    # Apply linear transformation\n    Y = k * X + d\n\n    # Standardize the data\n    scaler = StandardScaler()\n    X_standardized = scaler.fit_transform(X)\n    Y_standardized = scaler.transform(Y)\n\n    # Calculate mean squared error\n    mse = mean_squared_error(Y, Y_standardized)\n\n    return mse", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(num_samples=1000, k=5, d=2, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random samples\n    X = np.random.randn(num_samples, 1)\n\n    # Apply linear transformation\n    Y = k * X + d\n\n    # Standardize the data\n    scaler = StandardScaler()\n    X_standardized = scaler.fit_transform(X)\n    Y_standardized = scaler.transform(Y)\n\n    # Calculate mean squared error\n    mse = mean_squared_error(Y, Y_standardized)\n\n    return mse\n\n# Test cases"}
{"task_id": "BigCodeBench/773", "solution": "import os\nimport re\nimport shutil\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\ndef task_func():\n    \"\"\"\n    Ek source directory mein maujood sabhi json files ko ek target directory mein le jaen aur unke filename ko antim baar \"-\" aane par vibhajit karke aur filename ke prefix bhaag ko rakhkar unka naam badlen.\n\n    Params:\n    - None\n\n    Returns:\n        None\n\n    Reqs:\n    - os\n    - re\n    - shutil\n\n    Examples:\n    - >>> task_func()\n\n    \"\"\"\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    for filename in os.listdir(SOURCE_DIR):\n        match = FILE_PATTERN.match(filename)\n        if match:\n            new_filename = match.group(1) + '.json'\n            shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename))", "raw_solution": "import os\nimport re\nimport shutil\n\n# Constants\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\n\ndef task_func():\n    \"\"\"\n    Ek source directory mein maujood sabhi json files ko ek target directory mein le jaen aur unke filename ko antim baar \"-\" aane par vibhajit karke aur filename ke prefix bhaag ko rakhkar unka naam badlen.\n\n    Params:\n    - None\n\n    Returns:\n        None\n\n    Reqs:\n    - os\n    - re\n    - shutil\n\n    Examples:\n    - >>> task_func()\n\n    \"\"\"\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    for filename in os.listdir(SOURCE_DIR):\n        match = FILE_PATTERN.match(filename)\n        if match:\n            new_filename = match.group(1) + '.json'\n            shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename))\n\n# Example usage\ntask_func()\n```\nThis script will move all files in the source directory that match the pattern `^(.*?)-\\d+\\.json$` to the target directory, renaming them to `prefix.json` where `prefix` is the part of the filename before the first hyphen and the number. The target directory will be created if it does not exist.\n"}
{"task_id": "BigCodeBench/774", "solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ndef task_func(num_samples=100, n_estimators=100, random_seed=None, cv=5):\n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv must be at least 2\")\n\n    np.random.seed(random_seed)\n\n    # Generate random data\n    X = np.random.randn(num_samples, 5)\n    y = np.sum(X, axis=1) + np.random.randn(num_samples)\n\n    # Initialize and train the model\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_seed)\n    model.fit(X, y)\n\n    # Perform cross-validation\n    scores = cross_val_score(model, X, y, cv=cv)\n\n    return np.mean(scores), model", "raw_solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef task_func(num_samples=100, n_estimators=100, random_seed=None, cv=5):\n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv must be at least 2\")\n\n    np.random.seed(random_seed)\n\n    # Generate random data\n    X = np.random.randn(num_samples, 5)\n    y = np.sum(X, axis=1) + np.random.randn(num_samples)\n\n    # Initialize and train the model\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_seed)\n    model.fit(X, y)\n\n    # Perform cross-validation\n    scores = cross_val_score(model, X, y, cv=cv)\n\n    return np.mean(scores), model\n\n# Example usage:\n# res = task_func(random_seed=21, cv=3, n_estimators=90, num_samples=28)\n# print(res)\n```\n"}
{"task_id": "BigCodeBench/775", "solution": "from string import ascii_lowercase\nimport re\nfrom collections import Counter\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\nLETTERS = ascii_lowercase\ndef task_func(string):\n    \"\"\"\n    Yadi ek **string** aata hai, to use antim baar \"-\" par vibhajit karen aur **string** ke **prefix** mein pratyek **lowercase** **letter** ki **frequency** ki ganana karen.\n\n    Params:\n    - string (str): **input** **string**.\n\n    Returns:\n        dict: Pratyek **lowercase** **letter** ki **frequency** vaala ek **dictionary**.\n\n    Reqs:\n    - string\n    - re\n    - collections\n\n    Examples:\n    - >>> task_func('abc-def-ghij')\n    - {'a': 1, 'b': 1, 'c': 1, 'd': 1, 'e': 1, 'f': 1, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 0, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'u': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0}\n\n    \"\"\"\n    # Extract the prefix before the hyphen\n    match = LETTERS_PATTERN.match(string)\n    prefix = match.group(1) if match else string\n\n    # Count the frequency of each letter in the prefix\n    letter_count = Counter(prefix)\n\n    # Create a dictionary with all lowercase letters initialized to 0\n    result = {letter: letter_count.get(letter, 0) for letter in LETTERS}\n\n    return result", "raw_solution": "from string import ascii_lowercase\nimport re\nfrom collections import Counter\n\n# Constants\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\nLETTERS = ascii_lowercase\n\ndef task_func(string):\n    \"\"\"\n    Yadi ek **string** aata hai, to use antim baar \"-\" par vibhajit karen aur **string** ke **prefix** mein pratyek **lowercase** **letter** ki **frequency** ki ganana karen.\n\n    Params:\n    - string (str): **input** **string**.\n\n    Returns:\n        dict: Pratyek **lowercase** **letter** ki **frequency** vaala ek **dictionary**.\n\n    Reqs:\n    - string\n    - re\n    - collections\n\n    Examples:\n    - >>> task_func('abc-def-ghij')\n    - {'a': 1, 'b': 1, 'c': 1, 'd': 1, 'e': 1, 'f': 1, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 0, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'u': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0}\n\n    \"\"\"\n    # Extract the prefix before the hyphen\n    match = LETTERS_PATTERN.match(string)\n    prefix = match.group(1) if match else string\n\n    # Count the frequency of each letter in the prefix\n    letter_count = Counter(prefix)\n\n    # Create a dictionary with all lowercase letters initialized to 0\n    result = {letter: letter_count.get(letter, 0) for letter in LETTERS}\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/776", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(file_path, output_path=None, sort_key='title', linear_regression=False, x_column=None, y_column=None):\n    try:\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise Exception(f\"Error reading the file: {e}\")\n\n    try:\n        df = df.sort_values(by=sort_key)\n    except Exception as e:\n        raise Exception(f\"Error sorting the dataframe: {e}\")\n\n    if output_path:\n        try:\n            df.to_csv(output_path, index=False)\n        except Exception as e:\n            raise Exception(f\"Error writing the file: {e}\")\n\n    if linear_regression:\n        if x_column not in df.columns or y_column not in df.columns:\n            raise ValueError(\"Specified columns for linear regression are not in the dataframe.\")\n        try:\n            model = LinearRegression()\n            model.fit(df[[x_column]], df[y_column])\n        except Exception as e:\n            raise Exception(f\"Error fitting the linear regression model: {e}\")\n        return model\n    else:\n        return df", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(file_path, output_path=None, sort_key='title', linear_regression=False, x_column=None, y_column=None):\n    try:\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise Exception(f\"Error reading the file: {e}\")\n\n    try:\n        df = df.sort_values(by=sort_key)\n    except Exception as e:\n        raise Exception(f\"Error sorting the dataframe: {e}\")\n\n    if output_path:\n        try:\n            df.to_csv(output_path, index=False)\n        except Exception as e:\n            raise Exception(f\"Error writing the file: {e}\")\n\n    if linear_regression:\n        if x_column not in df.columns or y_column not in df.columns:\n            raise ValueError(\"Specified columns for linear regression are not in the dataframe.\")\n        try:\n            model = LinearRegression()\n            model.fit(df[[x_column]], df[y_column])\n        except Exception as e:\n            raise Exception(f\"Error fitting the linear regression model: {e}\")\n        return model\n    else:\n        return df\n```\n"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    backup_dir = os.path.join(directory, 'backup')\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    extracted_files = []\n    \n    for file in os.listdir(directory):\n        if zipfile.is_zipfile(os.path.join(directory, file)):\n            match = re.match(pattern, file)\n            if match:\n                zip_file = os.path.join(directory, file)\n                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n                    zip_ref.extractall(backup_dir)\n                    extracted_files.extend(zip_ref.namelist())\n                os.remove(zip_file)\n    \n    return backup_dir, extracted_files", "raw_solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    backup_dir = os.path.join(directory, 'backup')\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    extracted_files = []\n    \n    for file in os.listdir(directory):\n        if zipfile.is_zipfile(os.path.join(directory, file)):\n            match = re.match(pattern, file)\n            if match:\n                zip_file = os.path.join(directory, file)\n                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n                    zip_ref.extractall(backup_dir)\n                    extracted_files.extend(zip_ref.namelist())\n                os.remove(zip_file)\n    \n    return backup_dir, extracted_files\n```\nThis script will create a backup directory within the given directory, extract all zip files that match the specified pattern, and then delete the zip files after extraction. The function returns the path to the backup directory and a list of the extracted files.\n"}
{"task_id": "BigCodeBench/778", "solution": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\ndef task_func(news_articles):\n    if not all(isinstance(article, dict) for article in news_articles):\n        raise ValueError(\"All items in news_articles must be dictionaries.\")\n\n    if not all('title' in article and 'title_url' in article and 'id' in article and 'category' in article for article in news_articles):\n        raise ValueError(\"Each article must have 'title', 'title_url', 'id', and 'category' keys.\")\n\n    sorted_articles = defaultdict(list)\n    for category, articles in groupby(sorted(news_articles, key=itemgetter('category')), key=itemgetter('category')):\n        sorted_articles[category] = sorted(articles, key=itemgetter('title'))\n\n    return sorted_articles\narticles = [\n    {'title': 'Der Standard', 'title_url': 'standard', 'id': 2, 'category': 'climate'},\n    {'title': 'tecky', 'title_url': 'tecky', 'id': 4, 'category': 'climate'},\n    {'title': 'earth magazine', 'title_url': 'earth', 'id': 4, 'category': 'environment'}\n]", "raw_solution": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\n\ndef task_func(news_articles):\n    if not all(isinstance(article, dict) for article in news_articles):\n        raise ValueError(\"All items in news_articles must be dictionaries.\")\n\n    if not all('title' in article and 'title_url' in article and 'id' in article and 'category' in article for article in news_articles):\n        raise ValueError(\"Each article must have 'title', 'title_url', 'id', and 'category' keys.\")\n\n    sorted_articles = defaultdict(list)\n    for category, articles in groupby(sorted(news_articles, key=itemgetter('category')), key=itemgetter('category')):\n        sorted_articles[category] = sorted(articles, key=itemgetter('title'))\n\n    return sorted_articles\n\n# Test cases\narticles = [\n    {'title': 'Der Standard', 'title_url': 'standard', 'id': 2, 'category': 'climate'},\n    {'title': 'tecky', 'title_url': 'tecky', 'id': 4, 'category': 'climate'},\n    {'title': 'earth magazine', 'title_url': 'earth', 'id': 4, 'category': 'environment'}\n]\n\nsorted_articles = task_func(articles)"}
{"task_id": "BigCodeBench/779", "solution": "import os\nimport shutil\nimport time\nBACKUP_DIR = '/tmp/backup'\ndef task_func(directory):\n    \"\"\"\n    Given a directory, this function creates a backup of the directory and then safely removes the original directory.\n\n    Notes:\n        This function will return a tuple containing the backup directory path and a list of any errors encountered during the operation.\n\n    Params:\n    - directory (str): The path of the directory to be backed up and removed.\n\n    Returns:\n        Tuple: A tuple containing:\n        String: The path of the backup directory.\n        List: A list of any errors encountered during the operation (empty if no errors).\n\n    Reqs:\n    - os\n    - shutil\n    - time\n\n    Examples:\n    - >>> task_func('/tmp/my_data')\n    - ('/tmp/backup/backup_20230827010101', [])\n    \"\"\"\n    errors = []\n    backup_dir = os.path.join(BACKUP_DIR, f'backup_{time.strftime(\"%Y%m%d%H%M%S\")}')\n    \n    try:\n        shutil.copytree(directory, backup_dir)\n    except Exception as e:\n        errors.append(str(e))\n    \n    try:\n        shutil.rmtree(directory)\n    except Exception as e:\n        errors.append(str(e))\n    \n    return backup_dir, errors", "raw_solution": "import os\nimport shutil\nimport time\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef get_unique_backup_dir():\n    return \"/fake/backup/path\"\n\ndef task_func(directory):\n    \"\"\"\n    Given a directory, this function creates a backup of the directory and then safely removes the original directory.\n\n    Notes:\n        This function will return a tuple containing the backup directory path and a list of any errors encountered during the operation.\n\n    Params:\n    - directory (str): The path of the directory to be backed up and removed.\n\n    Returns:\n        Tuple: A tuple containing:\n        String: The path of the backup directory.\n        List: A list of any errors encountered during the operation (empty if no errors).\n\n    Reqs:\n    - os\n    - shutil\n    - time\n\n    Examples:\n    - >>> task_func('/tmp/my_data')\n    - ('/tmp/backup/backup_20230827010101', [])\n    \"\"\"\n    errors = []\n    backup_dir = os.path.join(BACKUP_DIR, f'backup_{time.strftime(\"%Y%m%d%H%M%S\")}')\n    \n    try:\n        shutil.copytree(directory, backup_dir)\n    except Exception as e:\n        errors.append(str(e))\n    \n    try:\n        shutil.rmtree(directory)\n    except Exception as e:\n        errors.append(str(e))\n    \n    return backup_dir, errors\n```\n"}
{"task_id": "BigCodeBench/780", "solution": "import pandas as pd\nimport pytz\ndef task_func(articles, timezone):\n    if not isinstance(articles, list) or not all(isinstance(article, dict) for article in articles):\n        raise TypeError(\"Articles should be a list of dictionaries.\")\n    \n    required_keys = {'title', 'title_url', 'id', 'category', 'published_time'}\n    for article in articles:\n        if not required_keys.issubset(article.keys()):\n            raise ValueError(\"Each article dictionary must contain the required keys.\")\n    \n    if not isinstance(timezone, str):\n        raise ValueError(\"Timezone should be a string.\")\n    \n    try:\n        tz = pytz.timezone(timezone)\n    except pytz.UnknownTimeZoneError:\n        raise ValueError(\"Unknown timezone provided.\")\n    \n    for article in articles:\n        try:\n            article['published_time'] = article['published_time'].astimezone(tz)\n        except (AttributeError, TypeError):\n            raise ValueError(\"Published time should be a datetime object.\")\n    \n    df = pd.DataFrame(articles)\n    df_grouped = df.groupby('category')['published_time'].agg(['count', 'mean', 'min', 'max'])\n    \n    return df_grouped", "raw_solution": "import pandas as pd\nimport pytz\n\ndef task_func(articles, timezone):\n    if not isinstance(articles, list) or not all(isinstance(article, dict) for article in articles):\n        raise TypeError(\"Articles should be a list of dictionaries.\")\n    \n    required_keys = {'title', 'title_url', 'id', 'category', 'published_time'}\n    for article in articles:\n        if not required_keys.issubset(article.keys()):\n            raise ValueError(\"Each article dictionary must contain the required keys.\")\n    \n    if not isinstance(timezone, str):\n        raise ValueError(\"Timezone should be a string.\")\n    \n    try:\n        tz = pytz.timezone(timezone)\n    except pytz.UnknownTimeZoneError:\n        raise ValueError(\"Unknown timezone provided.\")\n    \n    for article in articles:\n        try:\n            article['published_time'] = article['published_time'].astimezone(tz)\n        except (AttributeError, TypeError):\n            raise ValueError(\"Published time should be a datetime object.\")\n    \n    df = pd.DataFrame(articles)\n    df_grouped = df.groupby('category')['published_time'].agg(['count', 'mean', 'min', 'max'])\n    \n    return df_grouped\n```\n"}
{"task_id": "BigCodeBench/781", "solution": "import os\nfrom datetime import datetime\ndef task_func(filepath: str) -> dict:\n    \"\"\"\n    Ek file ke size aur last modification ki date nirdharit karein.\n\n    Params:\n    - filepath (str): file ka path.\n\n    Returns:\n        dict: Ek dictionary jismein file ka size (bytes mein) aur last modification\n        file ka date '%Y-%m-%d %H:%M:%S' format mein hai.\n\n    Reqs:\n    - os\n    - datetime\n\n    Examples:\n    - >>> task_func('/path/to/file.txt')\n    - {'size': '1024 bytes', 'last_modified': '2022-01-01 12:30:45'}\n\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n    \n    file_size = os.path.getsize(filepath)\n    last_modified_time = os.path.getmtime(filepath)\n    last_modified_date = datetime.fromtimestamp(last_modified_time).strftime('%Y-%m-%d %H:%M:%S')\n\n    return {'size': f\"{file_size} bytes\", 'last_modified': last_modified_date}", "raw_solution": "import os\nfrom datetime import datetime\n\ndef task_func(filepath: str) -> dict:\n    \"\"\"\n    Ek file ke size aur last modification ki date nirdharit karein.\n\n    Params:\n    - filepath (str): file ka path.\n\n    Returns:\n        dict: Ek dictionary jismein file ka size (bytes mein) aur last modification\n        file ka date '%Y-%m-%d %H:%M:%S' format mein hai.\n\n    Reqs:\n    - os\n    - datetime\n\n    Examples:\n    - >>> task_func('/path/to/file.txt')\n    - {'size': '1024 bytes', 'last_modified': '2022-01-01 12:30:45'}\n\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n    \n    file_size = os.path.getsize(filepath)\n    last_modified_time = os.path.getmtime(filepath)\n    last_modified_date = datetime.fromtimestamp(last_modified_time).strftime('%Y-%m-%d %H:%M:%S')\n\n    return {'size': f\"{file_size} bytes\", 'last_modified': last_modified_date}\n```\n"}
{"task_id": "BigCodeBench/782", "solution": "import random\nimport pandas as pd\nimport numpy as np\ndef task_func(n,\n          domain=\"samplewebsite.com\",\n          categories=['Sports', 'Technology', 'Health', 'Science', 'Business'],\n          random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n        np.random.seed(random_seed)\n\n    data = {\n        'title': [f'Article {i}' for i in range(n)],\n        'title_url': [f'{domain}/Article_{i}' for i in range(n)],\n        'id': list(range(n)),\n        'category': random.choices(categories, k=n),\n        'views': np.random.poisson(lam=1000, size=n).tolist()\n    }\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import random\nimport pandas as pd\nimport numpy as np\n\ndef task_func(n,\n          domain=\"samplewebsite.com\",\n          categories=['Sports', 'Technology', 'Health', 'Science', 'Business'],\n          random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n        np.random.seed(random_seed)\n\n    data = {\n        'title': [f'Article {i}' for i in range(n)],\n        'title_url': [f'{domain}/Article_{i}' for i in range(n)],\n        'id': list(range(n)),\n        'category': random.choices(categories, k=n),\n        'views': np.random.poisson(lam=1000, size=n).tolist()\n    }\n\n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\n# df = task_func(5, random_seed=1)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/783", "solution": "import os\nimport shutil\ndef task_func(src_dir, dest_dir, extension):\n    \"\"\"\n    Ek khaas extension wali sabhi files ko ek directory se doosre directory mein le jaayen.\n\n    Params:\n    - src_dir (str): source directory.\n    - dest_dir (str): destination directory.\n    - extension (str): file extension.\n\n    Returns:\n        files_moved (int): Le jaaye gaye files ki sankhya.\n\n    Reqs:\n    - os\n    - shutil\n\n    Examples:\n    - >>> task_func('/path/to/src', '/path/to/dest', '.txt')\n\n    \"\"\"\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    files_moved = 0\n    for filename in os.listdir(src_dir):\n        if filename.endswith(extension):\n            shutil.move(os.path.join(src_dir, filename), dest_dir)\n            files_moved += 1\n\n    return files_moved", "raw_solution": "import os\nimport shutil\n\ndef task_func(src_dir, dest_dir, extension):\n    \"\"\"\n    Ek khaas extension wali sabhi files ko ek directory se doosre directory mein le jaayen.\n\n    Params:\n    - src_dir (str): source directory.\n    - dest_dir (str): destination directory.\n    - extension (str): file extension.\n\n    Returns:\n        files_moved (int): Le jaaye gaye files ki sankhya.\n\n    Reqs:\n    - os\n    - shutil\n\n    Examples:\n    - >>> task_func('/path/to/src', '/path/to/dest', '.txt')\n\n    \"\"\"\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    files_moved = 0\n    for filename in os.listdir(src_dir):\n        if filename.endswith(extension):\n            shutil.move(os.path.join(src_dir, filename), dest_dir)\n            files_moved += 1\n\n    return files_moved\n```\n"}
{"task_id": "BigCodeBench/784", "solution": "import pandas as pd\nimport random\nimport csv\ndef task_func(n, \n           categories=['Sports', 'Technology', 'Business', 'Politics', 'Entertainment'],\n           news_sites=['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC'],\n           likert_scale=['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree'],\n           file_path='news_survey_data.csv',\n           random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1\n        data.append([site, category, response, value])\n\n    # Write to CSV\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Site', 'Category', 'Response', 'Value'])\n        writer.writerows(data)\n\n    # Read from CSV\n    df = pd.read_csv(file_path)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport csv\n\ndef task_func(n, \n           categories=['Sports', 'Technology', 'Business', 'Politics', 'Entertainment'],\n           news_sites=['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC'],\n           likert_scale=['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree'],\n           file_path='news_survey_data.csv',\n           random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1\n        data.append([site, category, response, value])\n\n    # Write to CSV\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Site', 'Category', 'Response', 'Value'])\n        writer.writerows(data)\n\n    # Read from CSV\n    df = pd.read_csv(file_path)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Nirdisht *pattern* se mel khane wali sabhi *files* ko *archive* karein aur phir mool *files* ko hat dein.\n\n    Notes:\n        Yeh *function* *archive* *file* ka *path* lautayega.\n\n    Params:\n    - pattern (str): *Files* se mel khane ke liye *pattern*.\n\n    Returns:\n        archive_file (str): *Archive* *file* ka *path*.\n\n    Reqs:\n    - subprocess\n    - os\n    - glob\n\n    Examples:\n    - >>> task_func('*.txt')\n\n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find all files matching the pattern\n    files = glob.glob(pattern)\n\n    # Create an archive file name\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar.gz')\n\n    # Create the archive\n    subprocess.run(['tar', '-czf', archive_file] + files, check=True)\n\n    # Remove the files\n    for file in files:\n        os.remove(file)\n\n    return archive_file", "raw_solution": "import subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    Nirdisht *pattern* se mel khane wali sabhi *files* ko *archive* karein aur phir mool *files* ko hat dein.\n\n    Notes:\n        Yeh *function* *archive* *file* ka *path* lautayega.\n\n    Params:\n    - pattern (str): *Files* se mel khane ke liye *pattern*.\n\n    Returns:\n        archive_file (str): *Archive* *file* ka *path*.\n\n    Reqs:\n    - subprocess\n    - os\n    - glob\n\n    Examples:\n    - >>> task_func('*.txt')\n\n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find all files matching the pattern\n    files = glob.glob(pattern)\n\n    # Create an archive file name\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar.gz')\n\n    # Create the archive\n    subprocess.run(['tar', '-czf', archive_file] + files, check=True)\n\n    # Remove the files\n    for file in files:\n        os.remove(file)\n\n    return archive_file\n```\n"}
{"task_id": "BigCodeBench/786", "solution": "import pandas as pd\nimport csv\nimport random\ndef task_func(\n    n, \n    countries=['USA', 'UK', 'China', 'India', 'Germany'], \n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'], \n    output_path=None,\n    random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        country = random.choice(countries)\n        product = random.choice(products)\n        sales = random.randint(1, 100)\n        data.append([country, product, sales])\n\n    df = pd.DataFrame(data, columns=['country', 'product', 'Sales'])\n\n    if output_path:\n        df.to_csv(output_path, index=False)\n\n    return df", "raw_solution": "import pandas as pd\nimport csv\nimport random\n\ndef task_func(\n    n, \n    countries=['USA', 'UK', 'China', 'India', 'Germany'], \n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'], \n    output_path=None,\n    random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        country = random.choice(countries)\n        product = random.choice(products)\n        sales = random.randint(1, 100)\n        data.append([country, product, sales])\n\n    df = pd.DataFrame(data, columns=['country', 'product', 'Sales'])\n\n    if output_path:\n        df.to_csv(output_path, index=False)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/787", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(array1, array2):\n    if len(array1) != len(array2):\n        raise ValueError(\"Input arrays must have the same length.\")\n\n    max_distance = 0.0\n\n    for point1, point2 in combinations(zip(array1, array2), 2):\n        distance = np.sqrt(np.sum((np.array(point1) - np.array(point2))**2))\n        max_distance = max(max_distance, distance)\n\n    return max_distance\narray1 = np.array([2, 3, 4])\narray2 = np.array([1, 5, 2])", "raw_solution": "import numpy as np\nfrom itertools import combinations\n\ndef task_func(array1, array2):\n    if len(array1) != len(array2):\n        raise ValueError(\"Input arrays must have the same length.\")\n\n    max_distance = 0.0\n\n    for point1, point2 in combinations(zip(array1, array2), 2):\n        distance = np.sqrt(np.sum((np.array(point1) - np.array(point2))**2))\n        max_distance = max(max_distance, distance)\n\n    return max_distance\n\n# Test cases\narray1 = np.array([2, 3, 4])\narray2 = np.array([1, 5, 2])"}
{"task_id": "BigCodeBench/788", "solution": "import heapq\nfrom scipy import stats\nimport pandas as pd\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Columns {col1} and/or {col2} not found in DataFrame.\")\n    if N <= 1:\n        raise ValueError(\"N must be greater than 1.\")\n    \n    # Calculate absolute differences\n    differences = [(abs(df[col1][i] - df[col2][i]), i) for i in range(len(df))]\n    \n    # Get the N largest differences\n    largest_diffs = heapq.nlargest(N, differences, key=lambda x: x[0])\n    \n    # Extract the elements corresponding to the largest differences\n    elements1 = [df[col1][diff[1]] for diff in largest_diffs]\n    elements2 = [df[col2][diff[1]] for diff in largest_diffs]\n    \n    # Perform T-Test\n    t_stat, p_value = stats.ttest_ind(elements1, elements2)\n    \n    return p_value", "raw_solution": "import heapq\nfrom scipy import stats\nimport pandas as pd\n\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Columns {col1} and/or {col2} not found in DataFrame.\")\n    if N <= 1:\n        raise ValueError(\"N must be greater than 1.\")\n    \n    # Calculate absolute differences\n    differences = [(abs(df[col1][i] - df[col2][i]), i) for i in range(len(df))]\n    \n    # Get the N largest differences\n    largest_diffs = heapq.nlargest(N, differences, key=lambda x: x[0])\n    \n    # Extract the elements corresponding to the largest differences\n    elements1 = [df[col1][diff[1]] for diff in largest_diffs]\n    elements2 = [df[col2][diff[1]] for diff in largest_diffs]\n    \n    # Perform T-Test\n    t_stat, p_value = stats.ttest_ind(elements1, elements2)\n    \n    return p_value\n\n# Example usage:\n# df = pd.DataFrame({\n#     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81],\n#     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n# })\n# p_value = task_func(df, 'col1', 'col2', N=5)\n# print(p_value)\n```\n"}
{"task_id": "BigCodeBench/789", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nARRAY_LENGTH = 10\ndef task_func():\n    # Create a random array of shape (ARRAY_LENGTH, 1)\n    array = np.random.rand(ARRAY_LENGTH, 1)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the array\n    scaled_array = scaler.fit_transform(array)\n\n    return scaled_array", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nARRAY_LENGTH = 10\n\ndef task_func():\n    # Create a random array of shape (ARRAY_LENGTH, 1)\n    array = np.random.rand(ARRAY_LENGTH, 1)\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the array\n    scaled_array = scaler.fit_transform(array)\n\n    return scaled_array\n\n# Test the function"}
{"task_id": "BigCodeBench/790", "solution": "import heapq\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the DataFrame\")\n\n    # Standardizing the columns\n    scaler = StandardScaler()\n    df[[col1, col2]] = scaler.fit_transform(df[[col1, col2]])\n\n    # Calculating differences and storing them with their indices\n    differences = [(abs(df.loc[i, col1] - df.loc[i, col2]), i) for i in df.index]\n    \n    # Using heapq to get the N largest differences\n    largest_diff_indices = [i[1] for i in heapq.nlargest(N, differences)]\n\n    return largest_diff_indices", "raw_solution": "import heapq\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the DataFrame\")\n\n    # Standardizing the columns\n    scaler = StandardScaler()\n    df[[col1, col2]] = scaler.fit_transform(df[[col1, col2]])\n\n    # Calculating differences and storing them with their indices\n    differences = [(abs(df.loc[i, col1] - df.loc[i, col2]), i) for i in df.index]\n    \n    # Using heapq to get the N largest differences\n    largest_diff_indices = [i[1] for i in heapq.nlargest(N, differences)]\n\n    return largest_diff_indices\n\n# Example usage:\n# df = pd.DataFrame({\n#     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81, 1, 2],\n#     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37, 3, 4]\n# })\n# indices = task_func(df, 'col1', 'col2', N=6)\n# print(indices)\n```\nThis script first standardizes the specified columns using `StandardScaler` from `sklearn.preprocessing`. It then calculates the absolute differences between the two columns for each row and stores these differences along with their indices. Finally, it uses `heapq.nlargest` to get the indices of the N largest differences.\n"}
{"task_id": "BigCodeBench/791", "solution": "from collections import Counter\nimport random\nfrom itertools import cycle\ndef task_func(l):\n    \"\"\"\n    l se a counter b aur first 3 elements ko l ke end mein le jaen.\n\n    Params:\n    - l (list): A list of elements that the function will process.\n\n    Returns:\n        counter (collections.Counter): ek frequency counter jo input l ke elements ko random, pherbadal ki gai l ke first 30 elements mein unki frequencyyon se map karta hai.\n\n    Reqs:\n    - collections\n    - random\n    - itertools\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func(ELEMENTS)\n    - Counter({'I': 3, 'F': 3, 'G': 3, 'J': 3, 'E': 3, 'A': 3, 'B': 3, 'H': 3, 'D': 3, 'C': 3})\n\n    \"\"\"\n    # Create a counter from the list\n    counter = Counter(l)\n    \n    # Get the first 3 elements of the list\n    first_three = l[:3]\n    \n    # Create a cycle iterator from the first 3 elements\n    cycle_iterator = cycle(first_three)\n    \n    # Update the counter with random elements from the cycle iterator\n    for _ in range(30):\n        element = next(cycle_iterator)\n        counter[element] += 1\n    \n    return counter", "raw_solution": "from collections import Counter\nimport random\nfrom itertools import cycle\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef task_func(l):\n    \"\"\"\n    l se a counter b aur first 3 elements ko l ke end mein le jaen.\n\n    Params:\n    - l (list): A list of elements that the function will process.\n\n    Returns:\n        counter (collections.Counter): ek frequency counter jo input l ke elements ko random, pherbadal ki gai l ke first 30 elements mein unki frequencyyon se map karta hai.\n\n    Reqs:\n    - collections\n    - random\n    - itertools\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func(ELEMENTS)\n    - Counter({'I': 3, 'F': 3, 'G': 3, 'J': 3, 'E': 3, 'A': 3, 'B': 3, 'H': 3, 'D': 3, 'C': 3})\n\n    \"\"\"\n    # Create a counter from the list\n    counter = Counter(l)\n    \n    # Get the first 3 elements of the list\n    first_three = l[:3]\n    \n    # Create a cycle iterator from the first 3 elements\n    cycle_iterator = cycle(first_three)\n    \n    # Update the counter with random elements from the cycle iterator\n    for _ in range(30):\n        element = next(cycle_iterator)\n        counter[element] += 1\n    \n    return counter\n\n# Test the function\nrandom.seed(42)"}
{"task_id": "BigCodeBench/792", "solution": "import heapq\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\ndef task_func(df, feature, target, n=10):\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(\"Specified columns must exist in the DataFrame\")\n\n    X = df[[feature]].values\n    y = df[target].values\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    residuals = model.predict(X) - y\n    largest_residual_indices = heapq.nlargest(n, range(len(residuals)), key=residuals.__getitem__)\n\n    return largest_residual_indices, model", "raw_solution": "import heapq\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\ndef task_func(df, feature, target, n=10):\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(\"Specified columns must exist in the DataFrame\")\n\n    X = df[[feature]].values\n    y = df[target].values\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    residuals = model.predict(X) - y\n    largest_residual_indices = heapq.nlargest(n, range(len(residuals)), key=residuals.__getitem__)\n\n    return largest_residual_indices, model\n\n# Example usage:\n# df = pd.DataFrame({\n#     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81],\n#     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n# })\n# indices, model = task_func(df, 'col1', 'col2', n=5)\n# print(indices)\n# print(model)\n```\n"}
{"task_id": "BigCodeBench/793", "solution": "import numpy as np\nimport random\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\ndef task_func(l=None):\n    if l is None:\n        l = random.sample(ELEMENTS, 3)\n    else:\n        l = random.sample(l, 3)\n    \n    arr = np.array(l)\n    arr = np.append(arr, arr[:3])\n    arr = np.delete(arr, np.s_[3:])\n    \n    return arr", "raw_solution": "import numpy as np\nimport random\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef task_func(l=None):\n    if l is None:\n        l = random.sample(ELEMENTS, 3)\n    else:\n        l = random.sample(l, 3)\n    \n    arr = np.array(l)\n    arr = np.append(arr, arr[:3])\n    arr = np.delete(arr, np.s_[3:])\n    \n    return arr\n\n# Test cases\nrandom.seed(42)"}
{"task_id": "BigCodeBench/794", "solution": "import string\nimport random\ndef task_func(length, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    characters = string.ascii_lowercase + string.punctuation + \"(){}[]\"\n    result = ''.join(random.choice(characters) for _ in range(length))\n    return result", "raw_solution": "import string\nimport random\n\ndef task_func(length, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    characters = string.ascii_lowercase + string.punctuation + \"(){}[]\"\n    result = ''.join(random.choice(characters) for _ in range(length))\n    return result\n\n# Test cases"}
{"task_id": "BigCodeBench/795", "solution": "from collections import deque\nimport math\ndef task_func(l):\n    # Filter numeric elements and calculate their squares\n    numeric_squares = [math.sqrt(x) for x in l if isinstance(x, (int, float))]\n\n    # Calculate the sum of squares\n    sum_of_squares = sum(numeric_squares)\n\n    # Create a deque and rotate it to the right by 3 positions\n    dq = deque(l)\n    dq.rotate(3)\n\n    # Print the sum of squares\n    print(f\"numeric elements ke yog ka square: {sum_of_squares}\")\n\n    return dq", "raw_solution": "from collections import deque\nimport math\n\ndef task_func(l):\n    # Filter numeric elements and calculate their squares\n    numeric_squares = [math.sqrt(x) for x in l if isinstance(x, (int, float))]\n\n    # Calculate the sum of squares\n    sum_of_squares = sum(numeric_squares)\n\n    # Create a deque and rotate it to the right by 3 positions\n    dq = deque(l)\n    dq.rotate(3)\n\n    # Print the sum of squares\n    print(f\"numeric elements ke yog ka square: {sum_of_squares}\")\n\n    return dq\n\n# Test cases"}
{"task_id": "BigCodeBench/796", "solution": "import os\nimport re\ndef task_func(directory):\n    BRACKET_PATTERN = '[(){}\\\\[\\\\]]'\n    bracket_files = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(BRACKET_PATTERN, file):\n                bracket_files.append(os.path.join(root, file))\n\n    return bracket_files", "raw_solution": "import os\nimport re\n\ndef task_func(directory):\n    BRACKET_PATTERN = '[(){}\\\\[\\\\]]'\n    bracket_files = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(BRACKET_PATTERN, file):\n                bracket_files.append(os.path.join(root, file))\n\n    return bracket_files\n\n# Example usage:\n# print(task_func('./some_directory/'))\n```\nThis script uses the `os` and `re` modules to walk through a directory and its subdirectories, checking each file name for the presence of any of the specified brackets. If a file name contains any of these brackets, the file path is added to the `bracket_files` list, which is then returned.\n"}
{"task_id": "BigCodeBench/797", "solution": "import re\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> int:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    pattern = r'[(){}[\\]]'\n    total_brackets = 0\n\n    for col in df.columns:\n        for item in df[col]:\n            total_brackets += len(re.findall(pattern, str(item)))\n\n    return total_brackets\ndf = pd.DataFrame({'Test': ['(a)', 'b', '[[[[))c']})", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> int:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    pattern = r'[(){}[\\]]'\n    total_brackets = 0\n\n    for col in df.columns:\n        for item in df[col]:\n            total_brackets += len(re.findall(pattern, str(item)))\n\n    return total_brackets\n\n# Test cases\ndf = pd.DataFrame({'Test': ['(a)', 'b', '[[[[))c']})"}
{"task_id": "BigCodeBench/798", "solution": "import os\nimport shutil\nBACKUP_DIR = '/tmp/backup'\ndef task_func(directory):\n    \"\"\"\n    This function performs a backup and then updates a directory. If the backup directory does not exist, it will not update the directory.\n\n    Constants:\n    - BACKUP_DIR: This is the directory where the backup will be stored. It defaults to '/tmp/backup'.\n\n    >>> task_func('/tmp/nonexistent')\n    'Backup directory /tmp/backup/nonexistent does not exist. Cannot update the directory.'\n\n    Notes:\n        This function will attempt to rollback to a previous directory path, or provide a fallback message.\n\n    Params:\n    - directory (str): The directory path to rollback to.\n\n    Returns:\n        directory (str): The safe rollback directory path, or a fallback message.\n\n    Reqs:\n    - os\n    - shutil\n\n    Examples:\n    - Examples:\n    - >>> task_func('/tmp/my_data')\n    - '/tmp/my_data'\n\n    \"\"\"\n    if not os.path.exists(BACKUP_DIR):\n        return f\"Backup directory {BACKUP_DIR} does not exist. Cannot update the directory.\"\n\n    try:\n        # Perform backup\n        shutil.copytree(directory, os.path.join(BACKUP_DIR, os.path.basename(directory)))\n\n        # Update the directory\n        shutil.rmtree(directory)\n        shutil.copytree(os.path.join(BACKUP_DIR, os.path.basename(directory)), directory)\n\n        return directory\n    except Exception as e:\n        return f\"An error occurred: {e}\"", "raw_solution": "import os\nimport shutil\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef task_func(directory):\n    \"\"\"\n    This function performs a backup and then updates a directory. If the backup directory does not exist, it will not update the directory.\n\n    Constants:\n    - BACKUP_DIR: This is the directory where the backup will be stored. It defaults to '/tmp/backup'.\n\n    >>> task_func('/tmp/nonexistent')\n    'Backup directory /tmp/backup/nonexistent does not exist. Cannot update the directory.'\n\n    Notes:\n        This function will attempt to rollback to a previous directory path, or provide a fallback message.\n\n    Params:\n    - directory (str): The directory path to rollback to.\n\n    Returns:\n        directory (str): The safe rollback directory path, or a fallback message.\n\n    Reqs:\n    - os\n    - shutil\n\n    Examples:\n    - Examples:\n    - >>> task_func('/tmp/my_data')\n    - '/tmp/my_data'\n\n    \"\"\"\n    if not os.path.exists(BACKUP_DIR):\n        return f\"Backup directory {BACKUP_DIR} does not exist. Cannot update the directory.\"\n\n    try:\n        # Perform backup\n        shutil.copytree(directory, os.path.join(BACKUP_DIR, os.path.basename(directory)))\n\n        # Update the directory\n        shutil.rmtree(directory)\n        shutil.copytree(os.path.join(BACKUP_DIR, os.path.basename(directory)), directory)\n\n        return directory\n    except Exception as e:\n        return f\"An error occurred: {e}\"\n```\n"}
{"task_id": "BigCodeBench/799", "solution": "import pandas as pd\nfrom random import seed, choices\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if not L:\n        return pd.DataFrame(), []\n\n    # Create a list of lowercase English letters\n    letters = [chr(i) for i in range(97, 123)]\n\n    # Create DataFrames\n    df_list = []\n    for _ in range(num_dataframes):\n        df = pd.DataFrame(choices(L, k=3), columns=letters[:len(L[0])])\n        df_list.append(df)\n\n    # Find common rows\n    common_rows = pd.concat(df_list).drop_duplicates(keep=False)\n    common_rows = common_rows[common_rows.duplicated(keep=False)]\n\n    return common_rows, df_list", "raw_solution": "import pandas as pd\nfrom random import seed, choices\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if not L:\n        return pd.DataFrame(), []\n\n    # Create a list of lowercase English letters\n    letters = [chr(i) for i in range(97, 123)]\n\n    # Create DataFrames\n    df_list = []\n    for _ in range(num_dataframes):\n        df = pd.DataFrame(choices(L, k=3), columns=letters[:len(L[0])])\n        df_list.append(df)\n\n    # Find common rows\n    common_rows = pd.concat(df_list).drop_duplicates(keep=False)\n    common_rows = common_rows[common_rows.duplicated(keep=False)]\n\n    return common_rows, df_list\n```\n"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Reads a CSV file and counts the goals and penalties for each team.\n\n    Params:\n    - goals (dict): A dictionary where keys are team names and values are the goals scored by the team.\n    - penalties (dict): A dictionary where keys are team names and values are the penalties scored by the team.\n\n    Returns:\n        count (Counter): A Counter object containing the total goals and penalties.\n\n    Reqs:\n    - csv\n    - os\n    - collections.Counter\n\n    Examples:\n    - >>> goals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\n    - >>> penalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\n    - >>> counts = task_func(goals, penalties)\n    - >>> print(counts)\n    - Counter({'goals': 8, 'penalties': 7})\n    \"\"\"\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    total_goals = 0\n    total_penalties = 0\n\n    with open(csv_file_path, newline='') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            team = row['team']\n            total_goals += int(row['goals'])\n            total_penalties += int(row['penalties'])\n\n    count = Counter({'goals': total_goals, 'penalties': total_penalties})\n\n    return count", "raw_solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Reads a CSV file and counts the goals and penalties for each team.\n\n    Params:\n    - goals (dict): A dictionary where keys are team names and values are the goals scored by the team.\n    - penalties (dict): A dictionary where keys are team names and values are the penalties scored by the team.\n\n    Returns:\n        count (Counter): A Counter object containing the total goals and penalties.\n\n    Reqs:\n    - csv\n    - os\n    - collections.Counter\n\n    Examples:\n    - >>> goals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\n    - >>> penalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\n    - >>> counts = task_func(goals, penalties)\n    - >>> print(counts)\n    - Counter({'goals': 8, 'penalties': 7})\n    \"\"\"\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    total_goals = 0\n    total_penalties = 0\n\n    with open(csv_file_path, newline='') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            team = row['team']\n            total_goals += int(row['goals'])\n            total_penalties += int(row['penalties'])\n\n    count = Counter({'goals': total_goals, 'penalties': total_penalties})\n\n    return count\n```\n"}
{"task_id": "BigCodeBench/801", "solution": "import collections\nimport numpy as np\ndef task_func(file_name):\n    \"\"\"\n    str file ke har column mein most common_values gyat karein, jismein column ke naam hain.\n    Yadi kuch maan same number mein aate hain, to maanon ko alphabetically mein karambaddh kiya jaata hai\n    and first ko most common_values jaata hai.\n    Agar ek khaali str pass kiya jaata hai, to ek khaali dictionary lautaya jaata hai.\n\n    Params:\n    - file_name (str): str file_name.\n\n    Returns:\n        dict: Ek dictionary jismein column ke naam keys ke roop mein and most common_values maanon ke roop mein hote hain.\n\n    Reqs:\n    - collections\n    - numpy\n\n    Examples:\n    - >>> common_values = task_func('sample.csv')\n    - >>> print(common_values)\n    - {'Name': 'Simon Velasquez',\n    - 'Age': 21,\n    - 'Fruit': 'Apple',\n    - 'Genre': 'HipHop',\n    - 'Height': 172}\n\n    \"\"\"\n    # Read the file\n    data = np.genfromtxt(file_name, delimiter=',', names=True, dtype=None)\n\n    # Initialize an empty dictionary to store the results\n    common_values = {}\n\n    # Iterate over each column\n    for column in data.dtype.names:\n        # Count the frequency of each value in the column\n        counter = collections.Counter(data[column])\n\n        # Sort the counter by frequency and then by value\n        sorted_counter = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n        # Get the most common value\n        most_common_value = sorted_counter[0][0]\n\n        # Add the most common value to the dictionary\n        common_values[column] = most_common_value\n\n    return common_values", "raw_solution": "import collections\nimport numpy as np\n\ndef task_func(file_name):\n    \"\"\"\n    str file ke har column mein most common_values gyat karein, jismein column ke naam hain.\n    Yadi kuch maan same number mein aate hain, to maanon ko alphabetically mein karambaddh kiya jaata hai\n    and first ko most common_values jaata hai.\n    Agar ek khaali str pass kiya jaata hai, to ek khaali dictionary lautaya jaata hai.\n\n    Params:\n    - file_name (str): str file_name.\n\n    Returns:\n        dict: Ek dictionary jismein column ke naam keys ke roop mein and most common_values maanon ke roop mein hote hain.\n\n    Reqs:\n    - collections\n    - numpy\n\n    Examples:\n    - >>> common_values = task_func('sample.csv')\n    - >>> print(common_values)\n    - {'Name': 'Simon Velasquez',\n    - 'Age': 21,\n    - 'Fruit': 'Apple',\n    - 'Genre': 'HipHop',\n    - 'Height': 172}\n\n    \"\"\"\n    # Read the file\n    data = np.genfromtxt(file_name, delimiter=',', names=True, dtype=None)\n\n    # Initialize an empty dictionary to store the results\n    common_values = {}\n\n    # Iterate over each column\n    for column in data.dtype.names:\n        # Count the frequency of each value in the column\n        counter = collections.Counter(data[column])\n\n        # Sort the counter by frequency and then by value\n        sorted_counter = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n        # Get the most common value\n        most_common_value = sorted_counter[0][0]\n\n        # Add the most common value to the dictionary\n        common_values[column] = most_common_value\n\n    return common_values\n```\n"}
{"task_id": "BigCodeBench/802", "solution": "import numpy as np\nimport itertools\ndef task_func(dimension, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.randint(1, 101, size=(dimension, dimension))\n    flat_list = list(itertools.chain.from_iterable(matrix))\n    return matrix, flat_list", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(dimension, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.randint(1, 101, size=(dimension, dimension))\n    flat_list = list(itertools.chain.from_iterable(matrix))\n    return matrix, flat_list\n\n# Example usage:\nmatrix, flat_list = task_func(3)"}
{"task_id": "BigCodeBench/803", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(file_name: str) -> pd.DataFrame:\n    df = pd.read_csv(file_name)\n    numeric_cols = df.select_dtypes(include=['number']).columns\n\n    if len(numeric_cols) == 0:\n        raise ValueError(\"Input must contain numeric columns.\")\n\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(file_name: str) -> pd.DataFrame:\n    df = pd.read_csv(file_name)\n    numeric_cols = df.select_dtypes(include=['number']).columns\n\n    if len(numeric_cols) == 0:\n        raise ValueError(\"Input must contain numeric columns.\")\n\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df\n\n# Example usage:\n# normalized_data = task_func(\"sample.csv\")\n# print(normalized_data.head())\n```\n"}
{"task_id": "BigCodeBench/804", "solution": "import os\nfrom datetime import datetime\nLOG_DIR = './logs'\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    \"\"\"\n    Yah function metrics kee ek dictionary ko nirdisht log file mein likhta hai, pratyek entry mein ek timestamp jodata hai.\n    >>> metrics = {'precision': 0.75, 'recall': 0.80}\n    >>> task_func(metrics, 'evaluation.log')\n    Ek error hui: [Errno 2] such koi file ya directory nahin: './logs/evaluation.log'\n    False\n\n    Params:\n    - metrics (dict): Ek dictionary jismein metric naam keys ke roop mein aur unke sambandhit maan shaamil hain.\n    - filename (str): Us file ka naam jismein metrics log in kiye jaenge.\n    - log_dir (str, optional): Vah directory jahan log file sangrahit hai. Default './logs' hai.\n\n    Returns:\n        bool: True yadi metrics successfully file mein likhe gaye the, to False otherwise.\n\n    Reqs:\n    - os\n    - datetime\n\n    Examples:\n    - Examples:\n    - >>> metrics = {'accuracy': 0.98, 'loss': 0.05}\n    - >>> task_func(metrics, 'metrics.log')\n    - Ek error hui: [Errno 2] such koi file ya directory nahin: './logs/metrics.log'\n    - False\n\n    \"\"\"\n    try:\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        with open(os.path.join(log_dir, filename), 'a') as file:\n            for metric, value in metrics.items():\n                file.write(f\"{timestamp} - {metric}: {value}\\n\")\n\n        return True\n    except Exception as e:\n        print(f\"Ek error hui: {e}\")\n        return False", "raw_solution": "import os\nfrom datetime import datetime\n\n# Constants\nLOG_DIR = './logs'\n\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    \"\"\"\n    Yah function metrics kee ek dictionary ko nirdisht log file mein likhta hai, pratyek entry mein ek timestamp jodata hai.\n    >>> metrics = {'precision': 0.75, 'recall': 0.80}\n    >>> task_func(metrics, 'evaluation.log')\n    Ek error hui: [Errno 2] such koi file ya directory nahin: './logs/evaluation.log'\n    False\n\n    Params:\n    - metrics (dict): Ek dictionary jismein metric naam keys ke roop mein aur unke sambandhit maan shaamil hain.\n    - filename (str): Us file ka naam jismein metrics log in kiye jaenge.\n    - log_dir (str, optional): Vah directory jahan log file sangrahit hai. Default './logs' hai.\n\n    Returns:\n        bool: True yadi metrics successfully file mein likhe gaye the, to False otherwise.\n\n    Reqs:\n    - os\n    - datetime\n\n    Examples:\n    - Examples:\n    - >>> metrics = {'accuracy': 0.98, 'loss': 0.05}\n    - >>> task_func(metrics, 'metrics.log')\n    - Ek error hui: [Errno 2] such koi file ya directory nahin: './logs/metrics.log'\n    - False\n\n    \"\"\"\n    try:\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        with open(os.path.join(log_dir, filename), 'a') as file:\n            for metric, value in metrics.items():\n                file.write(f\"{timestamp} - {metric}: {value}\\n\")\n\n        return True\n    except Exception as e:\n        print(f\"Ek error hui: {e}\")\n        return False\n```\n"}
{"task_id": "BigCodeBench/805", "solution": "import pandas as pd\nimport random\ndef task_func(dictionary, item, seed):\n    df = pd.DataFrame(dictionary)\n    locations = []\n\n    for col in df.columns:\n        for index, value in df[col].iteritems():\n            if value == item:\n                locations.append((index, col))\n\n    random.seed(seed)\n    random_number = random.randint(0, 9)\n\n    occurences = len(locations) + random_number\n\n    return locations, occurences, df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(dictionary, item, seed):\n    df = pd.DataFrame(dictionary)\n    locations = []\n\n    for col in df.columns:\n        for index, value in df[col].iteritems():\n            if value == item:\n                locations.append((index, col))\n\n    random.seed(seed)\n    random_number = random.randint(0, 9)\n\n    occurences = len(locations) + random_number\n\n    return locations, occurences, df\n\n# Test cases\ndict = {'A': ['a', 'b', 'e'], 'B': ['c', 'd', 'd'], '2': ['asdf', 'ddd', 'aaaa'], '12': ['e', 'e', 'd']}"}
{"task_id": "BigCodeBench/806", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text, n=2):\n    \"\"\"\n    Ek string \"text\" se duplicate aur stopwords hataen.\n    Phir, text mein n-gram ki count karen (default roop se bigrams).\n\n    Params:\n    - text (str): vishleshan karne ke liye text string.\n    - n (int): n-gram ka size.\n\n    Returns:\n        dict: text mein n-gram ki count.\n\n    Reqs:\n    - re\n    - nltk.corpus.stopwords\n    - collections.Counter\n\n    Examples:\n    - >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    - >>> ngrams = task_func(text)\n    - >>> print(ngrams)\n    - Counter({('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumps'): 1, ('jumps', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', 'dog'): 1, ('dog', 'quick'): 1, ('quick', 'respond'): 1})\n\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Remove stopwords\n    filtered_words = [word for word in words if word not in STOPWORDS]\n\n    # Create n-grams\n    ngrams = zip(*[filtered_words[i:] for i in range(n)])\n\n    # Count n-grams\n    ngram_counts = Counter(ngrams)\n\n    return ngram_counts", "raw_solution": "import re\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\n\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(text, n=2):\n    \"\"\"\n    Ek string \"text\" se duplicate aur stopwords hataen.\n    Phir, text mein n-gram ki count karen (default roop se bigrams).\n\n    Params:\n    - text (str): vishleshan karne ke liye text string.\n    - n (int): n-gram ka size.\n\n    Returns:\n        dict: text mein n-gram ki count.\n\n    Reqs:\n    - re\n    - nltk.corpus.stopwords\n    - collections.Counter\n\n    Examples:\n    - >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    - >>> ngrams = task_func(text)\n    - >>> print(ngrams)\n    - Counter({('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumps'): 1, ('jumps', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', 'dog'): 1, ('dog', 'quick'): 1, ('quick', 'respond'): 1})\n\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Remove stopwords\n    filtered_words = [word for word in words if word not in STOPWORDS]\n\n    # Create n-grams\n    ngrams = zip(*[filtered_words[i:] for i in range(n)])\n\n    # Count n-grams\n    ngram_counts = Counter(ngrams)\n\n    return ngram_counts\n```\n"}
{"task_id": "BigCodeBench/807", "solution": "import numpy as np\nfrom scipy.stats import norm\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    if data.size == 0:\n        return [], 0.0, 0.0\n\n    mean, var = norm.fit(data)\n    z_scores = np.abs((data - mean) / np.sqrt(var))\n\n    outliers = [i for i, score in enumerate(z_scores) if score > threshold]\n\n    return outliers, mean, var\ndata = np.array([-10, 3, 5, 5, 5, 5, 5, 7, 20])", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\n\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    if data.size == 0:\n        return [], 0.0, 0.0\n\n    mean, var = norm.fit(data)\n    z_scores = np.abs((data - mean) / np.sqrt(var))\n\n    outliers = [i for i, score in enumerate(z_scores) if score > threshold]\n\n    return outliers, mean, var\n\n# Test cases\ndata = np.array([-10, 3, 5, 5, 5, 5, 5, 7, 20])\noutliers, mean, var = task_func(data, threshold=4)"}
{"task_id": "BigCodeBench/808", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text):\n    \"\"\"\n    Ek \"text\" string se duplicates aur stopwords hataen.\n    Phir, TextBlob ka upyog karke text ki sentiment ka analysis karen.\n\n    Params:\n    - text (str): analysis karne ke liye text string.\n\n    Returns:\n        sentiment: text ki sentiment.\n\n    Reqs:\n    - re\n    - nltk.corpus.stopwords\n    - textblob.TextBlob\n\n    Examples:\n    - >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    - >>> sentiment = task_func(text)\n    - >>> print(sentiment)\n    - Sentiment(polarity=0.13888888888888887, subjectivity=0.6666666666666666)\n\n    \"\"\"\n    # Remove duplicates\n    text = re.sub(r'\\b(\\w+)(\\s+\\1\\b)+', r'\\1', text)\n\n    # Remove stopwords\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in STOPWORDS]\n\n    # Join the words back into a string\n    filtered_text = ' '.join(filtered_words)\n\n    # Analyze sentiment\n    blob = TextBlob(filtered_text)\n    sentiment = blob.sentiment\n\n    return sentiment", "raw_solution": "import re\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(text):\n    \"\"\"\n    Ek \"text\" string se duplicates aur stopwords hataen.\n    Phir, TextBlob ka upyog karke text ki sentiment ka analysis karen.\n\n    Params:\n    - text (str): analysis karne ke liye text string.\n\n    Returns:\n        sentiment: text ki sentiment.\n\n    Reqs:\n    - re\n    - nltk.corpus.stopwords\n    - textblob.TextBlob\n\n    Examples:\n    - >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    - >>> sentiment = task_func(text)\n    - >>> print(sentiment)\n    - Sentiment(polarity=0.13888888888888887, subjectivity=0.6666666666666666)\n\n    \"\"\"\n    # Remove duplicates\n    text = re.sub(r'\\b(\\w+)(\\s+\\1\\b)+', r'\\1', text)\n\n    # Remove stopwords\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in STOPWORDS]\n\n    # Join the words back into a string\n    filtered_text = ' '.join(filtered_words)\n\n    # Analyze sentiment\n    blob = TextBlob(filtered_text)\n    sentiment = blob.sentiment\n\n    return sentiment\n```\n"}
{"task_id": "BigCodeBench/809", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    cluster_dict = {}\n    \n    for i, label in enumerate(labels):\n        if label not in cluster_dict:\n            cluster_dict[label] = []\n        cluster_dict[label].append(i)\n    \n    return cluster_dict\ndata = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    cluster_dict = {}\n    \n    for i, label in enumerate(labels):\n        if label not in cluster_dict:\n            cluster_dict[label] = []\n        cluster_dict[label].append(i)\n    \n    return cluster_dict\n\n# Test cases\ndata = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ncluster = task_func(data, 2)\ncluster_list = list(cluster.values())\ncluster_list.sort(key=lambda x: x[0])"}
{"task_id": "BigCodeBench/810", "solution": "import re\nimport os\nimport subprocess\ndef task_func(dir_path, exe_pattern, execute_files=True):\n    results = []\n    for file_name in os.listdir(dir_path):\n        if re.search(exe_pattern, file_name):\n            file_path = os.path.join(dir_path, file_name)\n            if execute_files:\n                try:\n                    result = subprocess.run([file_path], capture_output=True, text=True)\n                    results.append(result.stdout)\n                except Exception as e:\n                    results.append(f\"Error executing {file_path}: {str(e)}\")\n            else:\n                results.append(file_path)\n    return results", "raw_solution": "import re\nimport os\nimport subprocess\n\ndef task_func(dir_path, exe_pattern, execute_files=True):\n    results = []\n    for file_name in os.listdir(dir_path):\n        if re.search(exe_pattern, file_name):\n            file_path = os.path.join(dir_path, file_name)\n            if execute_files:\n                try:\n                    result = subprocess.run([file_path], capture_output=True, text=True)\n                    results.append(result.stdout)\n                except Exception as e:\n                    results.append(f\"Error executing {file_path}: {str(e)}\")\n            else:\n                results.append(file_path)\n    return results\n\n# Test cases"}
{"task_id": "BigCodeBench/811", "solution": "import pandas as pd\nfrom random import randint, seed\ndef task_func(dictionary, item, sample_size=None, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    df = pd.DataFrame(dictionary)\n    positions = []\n\n    for col in df.columns:\n        for index, value in df[col].iteritems():\n            if value == item:\n                positions.append((index, col))\n\n    if sample_size is not None:\n        sampled_positions = []\n        for _ in range(sample_size):\n            index = randint(0, len(positions) - 1)\n            sampled_positions.append(positions[index])\n        return sampled_positions, df\n    else:\n        return positions, df", "raw_solution": "import pandas as pd\nfrom random import randint, seed\n\ndef task_func(dictionary, item, sample_size=None, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    df = pd.DataFrame(dictionary)\n    positions = []\n\n    for col in df.columns:\n        for index, value in df[col].iteritems():\n            if value == item:\n                positions.append((index, col))\n\n    if sample_size is not None:\n        sampled_positions = []\n        for _ in range(sample_size):\n            index = randint(0, len(positions) - 1)\n            sampled_positions.append(positions[index])\n        return sampled_positions, df\n    else:\n        return positions, df\n```\n"}
{"task_id": "BigCodeBench/812", "solution": "import re\nfrom pathlib import Path\nimport tarfile\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\ndef task_func(directory=DIRECTORY, file_pattern=PATTERN):\n    \"\"\"\n    In 'C:\\ SomeDir\\' directory mein '(? <! Distillr)\\\\ AcroTray\\.exe' regular expression ke pattern se mel khaane vaali files ko khojo. Agar mile, to in files ko tarfile mein archive karo.\n\n    Params:\n    - directory: Files ko search karne ke liye directory ek specified pattern se matching karke. Function is directory ke andar saare files ke upar iterate karega, including subdirectories.\n    - file_pattern: Ek regular expression pattern jo filenames se match karne ke liye use hota hai. Jin files ke naam is pattern se match karte hai unhe archive (tarfile) mein add kiya jayega.\n\n    Returns:\n        str: Banaye gaye tarfile ka path.\n\n    Reqs:\n    - re\n    - pathlib\n    - tarfile\n\n    Examples:\n    - >>> f_680('/path/to/source', '/path/to/target')\n\n    \"\"\"\n    # Convert directory to a Path object\n    dir_path = Path(directory)\n\n    # Create a tarfile in write mode\n    tar = tarfile.open(\"archive.tar\", \"w\")\n\n    # Iterate over all files in the directory and subdirectories\n    for file in dir_path.rglob('*'):\n        # Check if the file matches the pattern\n        if re.search(file_pattern, str(file)):\n            # Add the file to the tarfile\n            tar.add(file)\n\n    # Close the tarfile\n    tar.close()\n\n    # Return the path of the created tarfile\n    return str(Path.cwd() / \"archive.tar\")", "raw_solution": "import re\nfrom pathlib import Path\nimport tarfile\n\n# Constants\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\n\ndef task_func(directory=DIRECTORY, file_pattern=PATTERN):\n    \"\"\"\n    In 'C:\\ SomeDir\\' directory mein '(? <! Distillr)\\\\ AcroTray\\.exe' regular expression ke pattern se mel khaane vaali files ko khojo. Agar mile, to in files ko tarfile mein archive karo.\n\n    Params:\n    - directory: Files ko search karne ke liye directory ek specified pattern se matching karke. Function is directory ke andar saare files ke upar iterate karega, including subdirectories.\n    - file_pattern: Ek regular expression pattern jo filenames se match karne ke liye use hota hai. Jin files ke naam is pattern se match karte hai unhe archive (tarfile) mein add kiya jayega.\n\n    Returns:\n        str: Banaye gaye tarfile ka path.\n\n    Reqs:\n    - re\n    - pathlib\n    - tarfile\n\n    Examples:\n    - >>> f_680('/path/to/source', '/path/to/target')\n\n    \"\"\"\n    # Convert directory to a Path object\n    dir_path = Path(directory)\n\n    # Create a tarfile in write mode\n    tar = tarfile.open(\"archive.tar\", \"w\")\n\n    # Iterate over all files in the directory and subdirectories\n    for file in dir_path.rglob('*'):\n        # Check if the file matches the pattern\n        if re.search(file_pattern, str(file)):\n            # Add the file to the tarfile\n            tar.add(file)\n\n    # Close the tarfile\n    tar.close()\n\n    # Return the path of the created tarfile\n    return str(Path.cwd() / \"archive.tar\")\n```\n"}
{"task_id": "BigCodeBench/813", "solution": "from itertools import combinations\nimport pandas as pd\ndef task_func(number_list, element):\n    # Generate all unique combinations of 3 numbers from the list\n    comb_list = list(combinations(number_list, 3))\n    \n    # Filter combinations that sum up to the given element\n    valid_combinations = [comb for comb in comb_list if sum(comb) == element]\n    \n    # Create a DataFrame from the valid combinations\n    df = pd.DataFrame(valid_combinations, columns=['Combination'])\n    \n    return df", "raw_solution": "from itertools import combinations\nimport pandas as pd\n\ndef task_func(number_list, element):\n    # Generate all unique combinations of 3 numbers from the list\n    comb_list = list(combinations(number_list, 3))\n    \n    # Filter combinations that sum up to the given element\n    valid_combinations = [comb for comb in comb_list if sum(comb) == element]\n    \n    # Create a DataFrame from the valid combinations\n    df = pd.DataFrame(valid_combinations, columns=['Combination'])\n    \n    return df\n\n# Test cases"}
{"task_id": "BigCodeBench/814", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    matched_files = []\n    for root, _, files in os.walk(source_dir):\n        for file in files:\n            if re.search(file_pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    config_file_path = os.path.join(target_dir, 'config.txt')\n    with open(config_file_path, 'w') as config_file:\n        for file_path in matched_files:\n            config_file.write(file_path + '\\n')\n\n    return config_file_path", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    matched_files = []\n    for root, _, files in os.walk(source_dir):\n        for file in files:\n            if re.search(file_pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    config_file_path = os.path.join(target_dir, 'config.txt')\n    with open(config_file_path, 'w') as config_file:\n        for file_path in matched_files:\n            config_file.write(file_path + '\\n')\n\n    return config_file_path\n```\n"}
{"task_id": "BigCodeBench/815", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(test_scores, student):\n    df = pd.DataFrame(test_scores)\n    \n    if student not in df['Student'].values:\n        raise ValueError(f\"Student {student} is not present in the test_scores dataframe\")\n    \n    student_scores = df[df['Student'] == student]['Score']\n    average_score = student_scores.mean()\n    std_deviation = student_scores.std()\n    \n    return np.array([average_score, std_deviation]), df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(test_scores, student):\n    df = pd.DataFrame(test_scores)\n    \n    if student not in df['Student'].values:\n        raise ValueError(f\"Student {student} is not present in the test_scores dataframe\")\n    \n    student_scores = df[df['Student'] == student]['Score']\n    average_score = student_scores.mean()\n    std_deviation = student_scores.std()\n    \n    return np.array([average_score, std_deviation]), df\n\n# Example usage:\nSTUDENTS = range(1, 101)\nnp.random.seed(10)\nscores = {'Student': list(np.random.choice(STUDENTS, 50, replace=True)),\n         'Score': np.random.randint(50, 101, size=50)}"}
{"task_id": "BigCodeBench/816", "solution": "from collections import Counter\nimport random\nHAND_RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\nSUITS = ['H', 'D', 'C', 'S']\ndef task_func():\n    hand = [random.choice(HAND_RANKS) + random.choice(SUITS) for _ in range(5)]\n    rank_counts = Counter(card[0] for card in hand)\n    return hand, rank_counts", "raw_solution": "from collections import Counter\nimport random\n\n# Constants\nHAND_RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\nSUITS = ['H', 'D', 'C', 'S']\n\ndef task_func():\n    hand = [random.choice(HAND_RANKS) + random.choice(SUITS) for _ in range(5)]\n    rank_counts = Counter(card[0] for card in hand)\n    return hand, rank_counts\n```\n"}
{"task_id": "BigCodeBench/817", "solution": "from collections import Counter\nimport logging\ndef task_func(letter_list, element, log_path):\n    # Set up logging\n    logging.basicConfig(filename=f'{log_path}/task_func.log', \n                        filemode='w', \n                        level=logging.DEBUG, \n                        encoding='utf-8')\n    \n    # Log the function call\n    logging.info(f\"function ko list ke saath bula or ga or: {letter_list} aur element: {element}\")\n\n    # Check if the element is in the list\n    if element not in letter_list:\n        logging.error(f\"'{element}' letter list mein nahin hai.\")\n        raise ValueError(f\"'{element}' is not in the list.\")\n    \n    # Calculate the frequency of the element\n    element_frequency = Counter(letter_list)[element]\n    \n    # Log the frequency\n    logging.info(f\"'{element}' ki frequency {element_frequency}\")\n\n    return element_frequency", "raw_solution": "from collections import Counter\nimport logging\n\ndef task_func(letter_list, element, log_path):\n    # Set up logging\n    logging.basicConfig(filename=f'{log_path}/task_func.log', \n                        filemode='w', \n                        level=logging.DEBUG, \n                        encoding='utf-8')\n    \n    # Log the function call\n    logging.info(f\"function ko list ke saath bula or ga or: {letter_list} aur element: {element}\")\n\n    # Check if the element is in the list\n    if element not in letter_list:\n        logging.error(f\"'{element}' letter list mein nahin hai.\")\n        raise ValueError(f\"'{element}' is not in the list.\")\n    \n    # Calculate the frequency of the element\n    element_frequency = Counter(letter_list)[element]\n    \n    # Log the frequency\n    logging.info(f\"'{element}' ki frequency {element_frequency}\")\n\n    return element_frequency\n\n# Example usage\n# task_func(['a', 'b', 'a', 'c', 'a'], 'a', log_path='./')\n```\nThis script sets up logging, logs the function call, checks if the element is in the list, calculates the frequency, logs the frequency, and returns the frequency. It also raises a ValueError if the element is not in the list.\n"}
{"task_id": "BigCodeBench/818", "solution": "import re\nimport string\nPUNCTUATION = string.punctuation\ndef task_func(text):\n    \"\"\"\n    Ek string ko shabdon mein vibhajit karen, punctuation ko hata den aur unhen chhote letters mein badal den.\n\n    Params:\n    - text (str): text.\n\n    Returns:\n        cleaned_words (list): saaf kiye gae shabdon ki ek list.\n\n    Reqs:\n    - re\n    - string\n\n    Examples:\n    - >>> task_func(\"Hello, world! This is a test.\")\n    - ['hello', 'world', 'this', 'is', 'a', 'test']\n\n    \"\"\"\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', PUNCTUATION))\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Split into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    return words", "raw_solution": "import re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef task_func(text):\n    \"\"\"\n    Ek string ko shabdon mein vibhajit karen, punctuation ko hata den aur unhen chhote letters mein badal den.\n\n    Params:\n    - text (str): text.\n\n    Returns:\n        cleaned_words (list): saaf kiye gae shabdon ki ek list.\n\n    Reqs:\n    - re\n    - string\n\n    Examples:\n    - >>> task_func(\"Hello, world! This is a test.\")\n    - ['hello', 'world', 'this', 'is', 'a', 'test']\n\n    \"\"\"\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', PUNCTUATION))\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Split into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    return words\n\n# Test cases"}
{"task_id": "BigCodeBench/819", "solution": "import time\nimport random\ndef task_func(iterations=5, min_delay=1.0, max_delay=2.0, seed=None):\n    if not isinstance(iterations, int) or iterations <= 0:\n        raise ValueError(\"iterations must be a positive integer.\")\n    if not (isinstance(min_delay, float) and isinstance(max_delay, float)) or min_delay <= 0 or max_delay <= 0:\n        raise ValueError(\"min_delay and max_delay must be positive floating point values.\")\n\n    random.seed(seed)\n    messages = []\n    total_delay = 0.0\n\n    for _ in range(iterations):\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n        total_delay += delay\n        messages.append(f\"{delay:.02f} seconds have passed\")\n\n    return messages, total_delay", "raw_solution": "import time\nimport random\n\n\ndef task_func(iterations=5, min_delay=1.0, max_delay=2.0, seed=None):\n    if not isinstance(iterations, int) or iterations <= 0:\n        raise ValueError(\"iterations must be a positive integer.\")\n    if not (isinstance(min_delay, float) and isinstance(max_delay, float)) or min_delay <= 0 or max_delay <= 0:\n        raise ValueError(\"min_delay and max_delay must be positive floating point values.\")\n\n    random.seed(seed)\n    messages = []\n    total_delay = 0.0\n\n    for _ in range(iterations):\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n        total_delay += delay\n        messages.append(f\"{delay:.02f} seconds have passed\")\n\n    return messages, total_delay\n\n\n# Example usage:\n# messages, delay = task_func(2, 0.4, seed=1)\n# print(messages)\n# print(delay)\n```\n"}
{"task_id": "BigCodeBench/820", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(num_words, word_length):\n    if num_words < 0 or word_length < 0:\n        raise ValueError(\"num_words and word_length must be non-negative\")\n\n    words = []\n    for _ in range(num_words):\n        word = ''.join(random.choice(LETTERS) for _ in range(word_length))\n        words.append(word)\n\n    return words", "raw_solution": "import random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef task_func(num_words, word_length):\n    if num_words < 0 or word_length < 0:\n        raise ValueError(\"num_words and word_length must be non-negative\")\n\n    words = []\n    for _ in range(num_words):\n        word = ''.join(random.choice(LETTERS) for _ in range(word_length))\n        words.append(word)\n\n    return words\n\n# Example usage:\n# print(task_func(5, 3))\n```\n"}
{"task_id": "BigCodeBench/821", "solution": "import time\nimport threading\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    def delay_thread(thread_id):\n        time.sleep(delay_time)\n        completion_messages.append(f'Delay in thread {thread_id} completed')\n\n    completion_messages = []\n    threads = []\n\n    for i in range(num_threads):\n        thread = threading.Thread(target=delay_thread, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return completion_messages", "raw_solution": "import time\nimport threading\n\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    def delay_thread(thread_id):\n        time.sleep(delay_time)\n        completion_messages.append(f'Delay in thread {thread_id} completed')\n\n    completion_messages = []\n    threads = []\n\n    for i in range(num_threads):\n        thread = threading.Thread(target=delay_thread, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return completion_messages\n\n# Example usage:\n# print(task_func(0.1, 3))\n# print(task_func(1, 10))\n```\n"}
{"task_id": "BigCodeBench/822", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\nDIGITS = string.digits\ndef task_func(length, num_digits):\n    if length <= 0 or num_digits <= 0 or num_digits > length:\n        raise ValueError(\"Length and num_digits must be positive integers, and num_digits must be less than or equal to length.\")\n\n    # Generate random password\n    password = []\n    password.extend(random.choice(LETTERS) for _ in range(length - num_digits))\n    password.extend(random.choice(DIGITS) for _ in range(num_digits))\n    random.shuffle(password)\n\n    return ''.join(password)", "raw_solution": "import random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\nDIGITS = string.digits\n\ndef task_func(length, num_digits):\n    if length <= 0 or num_digits <= 0 or num_digits > length:\n        raise ValueError(\"Length and num_digits must be positive integers, and num_digits must be less than or equal to length.\")\n\n    # Generate random password\n    password = []\n    password.extend(random.choice(LETTERS) for _ in range(length - num_digits))\n    password.extend(random.choice(DIGITS) for _ in range(num_digits))\n    random.shuffle(password)\n\n    return ''.join(password)\n\n# Test cases"}
{"task_id": "BigCodeBench/823", "solution": "import time\nimport numpy as np\ndef task_func(samples=10, delay=0.1):\n    \"\"\"\n    This function measures the delay times and calculates the mean and standard deviation of the delays.\n\n    Parameters:\n    - samples (int): The number of samples to take. Default is 10.\n    - delay (float): The delay amount in seconds. Default is 0.1 seconds.\n\n    Returns:\n    - tuple: The mean and standard deviation of the delay times.\n\n    Requirements:\n    - time\n    - numpy\n\n    Examples:\n    - >>> mean, std = task_func(samples=5, delay=0.05)\n    - >>> print(f'Mean: %.3f, Std: %.1f' % (mean, std))\n    - Mean: 0.050, Std: 0.0\n    - >>> mean, std = task_func(100, 0.001)\n    - >>> print(f'Mean: %.3f, Std: %.4f' % (mean, std))\n    - Mean: 0.001, Std: 0.0000\n    \"\"\"\n\n    delays = []\n    for _ in range(samples):\n        start_time = time.time()\n        time.sleep(delay)\n        end_time = time.time()\n        delays.append(end_time - start_time)\n\n    mean_delay = np.mean(delays)\n    std_delay = np.std(delays)\n\n    return mean_delay, std_delay", "raw_solution": "import time\nimport numpy as np\n\ndef task_func(samples=10, delay=0.1):\n    \"\"\"\n    This function measures the delay times and calculates the mean and standard deviation of the delays.\n\n    Parameters:\n    - samples (int): The number of samples to take. Default is 10.\n    - delay (float): The delay amount in seconds. Default is 0.1 seconds.\n\n    Returns:\n    - tuple: The mean and standard deviation of the delay times.\n\n    Requirements:\n    - time\n    - numpy\n\n    Examples:\n    - >>> mean, std = task_func(samples=5, delay=0.05)\n    - >>> print(f'Mean: %.3f, Std: %.1f' % (mean, std))\n    - Mean: 0.050, Std: 0.0\n    - >>> mean, std = task_func(100, 0.001)\n    - >>> print(f'Mean: %.3f, Std: %.4f' % (mean, std))\n    - Mean: 0.001, Std: 0.0000\n    \"\"\"\n\n    delays = []\n    for _ in range(samples):\n        start_time = time.time()\n        time.sleep(delay)\n        end_time = time.time()\n        delays.append(end_time - start_time)\n\n    mean_delay = np.mean(delays)\n    std_delay = np.std(delays)\n\n    return mean_delay, std_delay\n```\n"}
{"task_id": "BigCodeBench/824", "solution": "import re\nimport string\nPUNCTUATION = string.punctuation\ndef task_func(text):\n    \"\"\"\n    Counts the number of words and punctuation in a given string.\n\n    Params:\n    - text (str): Input string.\n\n    Returns:\n        tuple: A tuple containing the number of words and punctuation marks.\n\n    Reqs:\n    - re\n    - string\n\n    Examples:\n    - >>> task_func(\"Hello, world! This is a test.\")\n    - (6, 3)\n\n    \"\"\"\n    # Remove punctuation from the text\n    text_no_punct = text.translate(str.maketrans('', '', PUNCTUATION))\n    \n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text_no_punct)\n    \n    # Count the number of words and punctuation\n    num_words = len(words)\n    num_punct = len(text) - len(text_no_punct)\n    \n    return num_words, num_punct", "raw_solution": "import re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef task_func(text):\n    \"\"\"\n    Counts the number of words and punctuation in a given string.\n\n    Params:\n    - text (str): Input string.\n\n    Returns:\n        tuple: A tuple containing the number of words and punctuation marks.\n\n    Reqs:\n    - re\n    - string\n\n    Examples:\n    - >>> task_func(\"Hello, world! This is a test.\")\n    - (6, 3)\n\n    \"\"\"\n    # Remove punctuation from the text\n    text_no_punct = text.translate(str.maketrans('', '', PUNCTUATION))\n    \n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text_no_punct)\n    \n    # Count the number of words and punctuation\n    num_words = len(words)\n    num_punct = len(text) - len(text_no_punct)\n    \n    return num_words, num_punct\n\n# Test cases"}
{"task_id": "BigCodeBench/825", "solution": "import numpy as np\nfrom itertools import product\nimport string\ndef task_func(length, seed=None, alphabets=list(string.ascii_lowercase)):\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate all possible combinations of the given length\n    combinations = product(alphabets, repeat=length)\n    \n    # Convert the combinations to strings\n    strings = [''.join(combination) for combination in combinations]\n    \n    # Shuffle the strings to ensure randomness\n    np.random.shuffle(strings)\n    \n    # Return the first 10 strings\n    return strings[:10]", "raw_solution": "import numpy as np\nfrom itertools import product\nimport string\n\ndef task_func(length, seed=None, alphabets=list(string.ascii_lowercase)):\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate all possible combinations of the given length\n    combinations = product(alphabets, repeat=length)\n    \n    # Convert the combinations to strings\n    strings = [''.join(combination) for combination in combinations]\n    \n    # Shuffle the strings to ensure randomness\n    np.random.shuffle(strings)\n    \n    # Return the first 10 strings\n    return strings[:10]\n\n# Example usage:\n# print(task_func(2, 123))\n```\n"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count\n```\n"}
{"task_id": "BigCodeBench/827", "solution": "import math\nfrom sympy import isprime\ndef task_func(input_list):\n    \"\"\"\n    Dee gayee list mein se prime numbers ko filter karein, prime numbers ko\n    unke radian maan ko degree mein badalne ke aadhaar par aarohi kram mein sort karein, aur sorted list lautaen.\n    function prime sankhyaein nirdharit karne ke liye sympy library se isprime function ka upyog karta hai\n    aur numbers ko unke degree maan ke aadhaar par sort karne ke liye math library se degree function ka upyog karta hai.\n    >>> task_func([101, 102, 103, 104])\n    [101, 103]\n\n    Params:\n    - input_list (list[int]): Filter aur sort kiye jaane waale list[int ki ek list.\n\n    Returns:\n        list[int]: degree maan ke aadhaar par prime numbers ki ek sorted list.\n\n    Reqs:\n    - math\n    - sympy\n\n    Examples:\n    - udaaharan:\n    - >>> task_func([4, 5, 2, 7, 89, 90])\n    - [2, 5, 7, 89]\n\n    \"\"\"\n    # Filter the list to only include prime numbers\n    primes = [num for num in input_list if isprime(num)]\n    \n    # Sort the list in degree (radian) representation\n    primes.sort(key=lambda x: math.degrees(math.atan(1) * 2 * x))\n    \n    return primes", "raw_solution": "import math\nfrom sympy import isprime\n\ndef task_func(input_list):\n    \"\"\"\n    Dee gayee list mein se prime numbers ko filter karein, prime numbers ko\n    unke radian maan ko degree mein badalne ke aadhaar par aarohi kram mein sort karein, aur sorted list lautaen.\n    function prime sankhyaein nirdharit karne ke liye sympy library se isprime function ka upyog karta hai\n    aur numbers ko unke degree maan ke aadhaar par sort karne ke liye math library se degree function ka upyog karta hai.\n    >>> task_func([101, 102, 103, 104])\n    [101, 103]\n\n    Params:\n    - input_list (list[int]): Filter aur sort kiye jaane waale list[int ki ek list.\n\n    Returns:\n        list[int]: degree maan ke aadhaar par prime numbers ki ek sorted list.\n\n    Reqs:\n    - math\n    - sympy\n\n    Examples:\n    - udaaharan:\n    - >>> task_func([4, 5, 2, 7, 89, 90])\n    - [2, 5, 7, 89]\n\n    \"\"\"\n    # Filter the list to only include prime numbers\n    primes = [num for num in input_list if isprime(num)]\n    \n    # Sort the list in degree (radian) representation\n    primes.sort(key=lambda x: math.degrees(math.atan(1) * 2 * x))\n    \n    return primes\n\n# Test cases"}
{"task_id": "BigCodeBench/828", "solution": "import os\nimport errno\nimport shutil\ndef task_func(filename, dest_dir):\n    try:\n        # Ensure the destination directory exists\n        os.makedirs(dest_dir, exist_ok=True)\n\n        # Copy the file to the destination directory\n        dest_file = os.path.join(dest_dir, os.path.basename(filename))\n        shutil.copy2(filename, dest_file)\n\n        # Truncate the original file to make it empty\n        with open(filename, 'w') as f:\n            pass\n\n        return dest_file\n\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            raise FileNotFoundError(f\"The file {filename} does not exist.\")\n        elif e.errno == errno.EACCES:\n            raise PermissionError(f\"Permission denied: {dest_dir}\")\n        else:\n            raise", "raw_solution": "import os\nimport errno\nimport shutil\n\ndef task_func(filename, dest_dir):\n    try:\n        # Ensure the destination directory exists\n        os.makedirs(dest_dir, exist_ok=True)\n\n        # Copy the file to the destination directory\n        dest_file = os.path.join(dest_dir, os.path.basename(filename))\n        shutil.copy2(filename, dest_file)\n\n        # Truncate the original file to make it empty\n        with open(filename, 'w') as f:\n            pass\n\n        return dest_file\n\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            raise FileNotFoundError(f\"The file {filename} does not exist.\")\n        elif e.errno == errno.EACCES:\n            raise PermissionError(f\"Permission denied: {dest_dir}\")\n        else:\n            raise\n\n# Example usage:\n# print(task_func('/path/to/original/test.txt', '/path/to/destination'))\n```\n"}
{"task_id": "BigCodeBench/829", "solution": "import pandas as pd\nfrom statistics import mean\ndef task_func(df: pd.DataFrame) -> dict:\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Name' and 'Score' columns.\")\n    \n    grouped = df.groupby('Name')['Score'].apply(lambda x: mean(x)).reset_index()\n    grouped_dict = {}\n    \n    for _, row in grouped.iterrows():\n        name = row['Name']\n        score = row['Score']\n        grouped_dict[name] = (name, score)\n    \n    return grouped_dict", "raw_solution": "import pandas as pd\nfrom statistics import mean\n\ndef task_func(df: pd.DataFrame) -> dict:\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Name' and 'Score' columns.\")\n    \n    grouped = df.groupby('Name')['Score'].apply(lambda x: mean(x)).reset_index()\n    grouped_dict = {}\n    \n    for _, row in grouped.iterrows():\n        name = row['Name']\n        score = row['Score']\n        grouped_dict[name] = (name, score)\n    \n    return grouped_dict\n\n# Example usage:\ndf_sample = pd.DataFrame({\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John'],\n    'Score': [85, 79, 90, 88, 82]\n})\n\ngen_dict = task_func(df_sample)"}
{"task_id": "BigCodeBench/830", "solution": "import json\nimport os\ndef task_func(filename, data):\n    # Check if file exists\n    file_exists = os.path.exists(filename)\n    \n    # Write data to file\n    try:\n        with open(filename, 'w') as file:\n            json.dump(data, file)\n        success = True\n    except Exception as e:\n        success = False\n        print(f\"An error occurred: {e}\")\n    \n    # Read data back from file to verify\n    try:\n        with open(filename, 'r') as file:\n            written_data = json.load(file)\n    except Exception as e:\n        print(f\"An error occurred while reading the file: {e}\")\n        written_data = None\n\n    return success, written_data", "raw_solution": "import json\nimport os\n\ndef task_func(filename, data):\n    # Check if file exists\n    file_exists = os.path.exists(filename)\n    \n    # Write data to file\n    try:\n        with open(filename, 'w') as file:\n            json.dump(data, file)\n        success = True\n    except Exception as e:\n        success = False\n        print(f\"An error occurred: {e}\")\n    \n    # Read data back from file to verify\n    try:\n        with open(filename, 'r') as file:\n            written_data = json.load(file)\n    except Exception as e:\n        print(f\"An error occurred while reading the file: {e}\")\n        written_data = None\n\n    return success, written_data\n\n# Example usage:\n# result, written_data = task_func('data.json', {'key': 'value'})\n# print(result)  # This should print: True\n# print(written_data)  # This should print: {'key': 'value'}\n```\n"}
{"task_id": "BigCodeBench/831", "solution": "import random\nimport math\ndef task_func(range_start=1, range_end=100, pairs_count=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    def generate_pairs():\n        for _ in range(pairs_count):\n            num1 = random.randint(range_start, range_end)\n            num2 = random.randint(range_start, range_end)\n            yield (num1, num2, math.sqrt(abs(num1 - num2)))\n\n    return generate_pairs()", "raw_solution": "import random\nimport math\n\ndef task_func(range_start=1, range_end=100, pairs_count=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    def generate_pairs():\n        for _ in range(pairs_count):\n            num1 = random.randint(range_start, range_end)\n            num2 = random.randint(range_start, range_end)\n            yield (num1, num2, math.sqrt(abs(num1 - num2)))\n\n    return generate_pairs()\n\n# Example usage:\n# generator = task_func(random_seed=1)\n# for pair in generator:\n#     print(pair)\n```\n"}
{"task_id": "BigCodeBench/832", "solution": "import pickle\nimport os\ndef task_func(filename, data):\n    # Check if directory exists, if not, create it\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Serialize the data and write it to the file\n    try:\n        with open(filename, 'wb') as file:\n            pickle.dump(data, file)\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False", "raw_solution": "import pickle\nimport os\n\ndef task_func(filename, data):\n    # Check if directory exists, if not, create it\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Serialize the data and write it to the file\n    try:\n        with open(filename, 'wb') as file:\n            pickle.dump(data, file)\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n\n# Example usage:\n# result = task_func('data.pkl', {'key': 'value'})\n# print(result)\n```\nThis script first checks if the directory of the given filename exists. If not, it creates the directory. Then, it tries to serialize the data and write it to the file. If the operation is successful, it returns True; otherwise, it returns False.\n"}
{"task_id": "BigCodeBench/833", "solution": "import random\nfrom collections import Counter\nfrom statistics import mode\ndef task_func(list_length=1000, range_start=1, range_end=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    random_list = [random.randint(range_start, range_end) for _ in range(list_length)]\n    counter = Counter(random_list)\n    mode_value = mode(random_list)\n\n    def number_frequency_generator():\n        for number, frequency in counter.items():\n            yield (number, frequency)\n\n    return mode_value, number_frequency_generator()", "raw_solution": "import random\nfrom collections import Counter\nfrom statistics import mode\n\ndef task_func(list_length=1000, range_start=1, range_end=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    random_list = [random.randint(range_start, range_end) for _ in range(list_length)]\n    counter = Counter(random_list)\n    mode_value = mode(random_list)\n\n    def number_frequency_generator():\n        for number, frequency in counter.items():\n            yield (number, frequency)\n\n    return mode_value, number_frequency_generator()\n\n# Example usage:\nmode, numbers = task_func(100, 1, 5, random_seed=1)"}
{"task_id": "BigCodeBench/834", "solution": "import binascii\nimport io\nimport gzip\ndef task_func(compressed_hex):\n    \"\"\"\n    Ek gzip-sankuchit hexadecimal string ko uncompress aur result ko utf-8 mein decrypt karein.\n\n    Params:\n    - compressed_hex (str): The gzip-compressed hexadecimal string.\n\n    Returns:\n        decoded_string (str): utf-8 format mein decode aur decompression ki gayi string, ya ek error sandesh.\n\n    Reqs:\n    - binascii\n    - io\n    - gzip\n\n    Examples:\n    - >>> task_func('1f8b08000000000002ff0b49494e55560304000000ffff8b202d0b000000')\n    - 'Error during decompression: CRC check failed 0xff000000 != 0x41449975'\n\n    \"\"\"\n    try:\n        # Convert the hexadecimal string to bytes\n        compressed_bytes = binascii.unhexlify(compressed_hex)\n        \n        # Use io.BytesIO to read the bytes as a stream\n        with io.BytesIO(compressed_bytes) as byte_stream:\n            # Decompress the bytes using gzip\n            with gzip.GzipFile(fileobj=byte_stream) as decompressed_file:\n                # Read the decompressed bytes\n                decompressed_bytes = decompressed_file.read()\n                # Decode the bytes to a utf-8 string\n                decoded_string = decompressed_bytes.decode('utf-8')\n                return decoded_string\n    except gzip.BadCrcError as e:\n        return f\"Error during decompression: {e}\"\n    except Exception as e:\n        return f\"An error occurred: {e}\"", "raw_solution": "import binascii\nimport io\nimport gzip\n\ndef task_func(compressed_hex):\n    \"\"\"\n    Ek gzip-sankuchit hexadecimal string ko uncompress aur result ko utf-8 mein decrypt karein.\n\n    Params:\n    - compressed_hex (str): The gzip-compressed hexadecimal string.\n\n    Returns:\n        decoded_string (str): utf-8 format mein decode aur decompression ki gayi string, ya ek error sandesh.\n\n    Reqs:\n    - binascii\n    - io\n    - gzip\n\n    Examples:\n    - >>> task_func('1f8b08000000000002ff0b49494e55560304000000ffff8b202d0b000000')\n    - 'Error during decompression: CRC check failed 0xff000000 != 0x41449975'\n\n    \"\"\"\n    try:\n        # Convert the hexadecimal string to bytes\n        compressed_bytes = binascii.unhexlify(compressed_hex)\n        \n        # Use io.BytesIO to read the bytes as a stream\n        with io.BytesIO(compressed_bytes) as byte_stream:\n            # Decompress the bytes using gzip\n            with gzip.GzipFile(fileobj=byte_stream) as decompressed_file:\n                # Read the decompressed bytes\n                decompressed_bytes = decompressed_file.read()\n                # Decode the bytes to a utf-8 string\n                decoded_string = decompressed_bytes.decode('utf-8')\n                return decoded_string\n    except gzip.BadCrcError as e:\n        return f\"Error during decompression: {e}\"\n    except Exception as e:\n        return f\"An error occurred: {e}\"\n\n# Test cases"}
{"task_id": "BigCodeBench/835", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    data = np.random.randint(0, 101, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n    \n    df = df.drop(df.columns[remove_cols], axis=1)\n    \n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    data = np.random.randint(0, 101, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n    \n    df = df.drop(df.columns[remove_cols], axis=1)\n    \n    return df\n\n# Example usage:\n# df = task_func(10, [1, 3], columns=['A', 'B', 'C', 'D', 'E'], random_seed=1)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/836", "solution": "import os\nimport shutil\nimport csv\ndef task_func(target_value='332', csv_dir='./csv_files/', processed_dir='./processed_files/', simulate=False):\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    result = {}\n\n    for file in os.listdir(csv_dir):\n        if not file.endswith('.csv'):\n            continue\n\n        file_path = os.path.join(csv_dir, file)\n        with open(file_path, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            for row_index, row in enumerate(reader):\n                if row and row[0] == target_value:\n                    result[file] = row_index\n                    if not simulate:\n                        shutil.move(file_path, os.path.join(processed_dir, file))\n                    break\n\n    return result", "raw_solution": "import os\nimport shutil\nimport csv\n\ndef task_func(target_value='332', csv_dir='./csv_files/', processed_dir='./processed_files/', simulate=False):\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    result = {}\n\n    for file in os.listdir(csv_dir):\n        if not file.endswith('.csv'):\n            continue\n\n        file_path = os.path.join(csv_dir, file)\n        with open(file_path, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            for row_index, row in enumerate(reader):\n                if row and row[0] == target_value:\n                    result[file] = row_index\n                    if not simulate:\n                        shutil.move(file_path, os.path.join(processed_dir, file))\n                    break\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/837", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(n_rows, scale_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    data = np.random.randint(0, 100, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    if scale_cols:\n        scaler = StandardScaler()\n        df[df.columns[scale_cols]] = scaler.fit_transform(df[df.columns[scale_cols]])\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(n_rows, scale_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    data = np.random.randint(0, 100, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    if scale_cols:\n        scaler = StandardScaler()\n        df[df.columns[scale_cols]] = scaler.fit_transform(df[df.columns[scale_cols]])\n\n    return df\n\n# Example usage:\n# df = task_func(5, [1, 2, 3], random_seed=12)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/838", "solution": "import re\nfrom nltk.stem import PorterStemmer\nimport pandas as pd\ndef task_func(text_series):\n    stemmer = PorterStemmer()\n    processed_series = text_series.apply(lambda text: ' '.join(\n        stemmer.stem(word) for word in re.sub(r'[^\\w\\s]', '', text.lower()).split()\n    ))\n    return processed_series", "raw_solution": "import re\nfrom nltk.stem import PorterStemmer\nimport pandas as pd\n\ndef task_func(text_series):\n    stemmer = PorterStemmer()\n    processed_series = text_series.apply(lambda text: ' '.join(\n        stemmer.stem(word) for word in re.sub(r'[^\\w\\s]', '', text.lower()).split()\n    ))\n    return processed_series\n\n# Example usage:\n# input_series = pd.Series([\"This is a sample text.\", \"Another example!\"])\n# output_series = task_func(input_series)\n# print(output_series.iloc[0])  # Output: thi is a sampl text\n# print(output_series.iloc[1])  # Output: anoth exampl\n```\n"}
{"task_id": "BigCodeBench/839", "solution": "import csv\nimport random\nimport string\ndef task_func(file_path,\n              num_rows,\n              gender=['Male', 'Female', 'Non-Binary'],\n              countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Gender', 'Country'])\n\n        for _ in range(num_rows):\n            name = ''.join(random.choices(string.ascii_letters, k=5))\n            age = random.randint(20, 60)\n            random_gender = random.choice(gender)\n            random_country = random.choice(countries)\n            writer.writerow([name, age, random_gender, random_country])\n\n    return file_path", "raw_solution": "import csv\nimport random\nimport string\n\ndef task_func(file_path,\n              num_rows,\n              gender=['Male', 'Female', 'Non-Binary'],\n              countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Gender', 'Country'])\n\n        for _ in range(num_rows):\n            name = ''.join(random.choices(string.ascii_letters, k=5))\n            age = random.randint(20, 60)\n            random_gender = random.choice(gender)\n            random_country = random.choice(countries)\n            writer.writerow([name, age, random_gender, random_country])\n\n    return file_path\n```\nThis script creates a CSV file with the specified number of rows, each containing a random name, age, gender, and country. The name is randomly generated using 5 random letters, and the age is a random integer between 20 and 60. The gender and country are randomly chosen from the provided lists. If a seed is provided, the random number generator is initialized with that seed for reproducibility.\n"}
{"task_id": "BigCodeBench/840", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(file_path, num_rows, data_dimensions=5, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random data\n    data = np.random.rand(num_rows, data_dimensions)\n\n    # Create column names\n    columns = [f'Feature_{i+1}' for i in range(data_dimensions)]\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=columns)\n\n    # Save DataFrame to CSV\n    df.to_csv(file_path, index=False)\n\n    return file_path", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(file_path, num_rows, data_dimensions=5, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random data\n    data = np.random.rand(num_rows, data_dimensions)\n\n    # Create column names\n    columns = [f'Feature_{i+1}' for i in range(data_dimensions)]\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=columns)\n\n    # Save DataFrame to CSV\n    df.to_csv(file_path, index=False)\n\n    return file_path\n\n# Example usage:\n# print(task_func('/tmp/data.csv', 100))\n```\n"}
{"task_id": "BigCodeBench/841", "solution": "import re\nimport json\nfrom collections import defaultdict\nimport string\ndef task_func(json_string):\n    try:\n        data = json.loads(json_string)\n        text = data.get(\"text\", \"\")\n    except (json.JSONDecodeError, KeyError):\n        return {}\n\n    # Remove punctuation and convert to lowercase\n    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text).lower()\n\n    # Split text into words\n    words = text.split()\n\n    # Count word frequencies\n    word_counts = defaultdict(int)\n    for word in words:\n        word_counts[word] += 1\n\n    return dict(word_counts)", "raw_solution": "import re\nimport json\nfrom collections import defaultdict\nimport string\n\ndef task_func(json_string):\n    try:\n        data = json.loads(json_string)\n        text = data.get(\"text\", \"\")\n    except (json.JSONDecodeError, KeyError):\n        return {}\n\n    # Remove punctuation and convert to lowercase\n    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text).lower()\n\n    # Split text into words\n    words = text.split()\n\n    # Count word frequencies\n    word_counts = defaultdict(int)\n    for word in words:\n        word_counts[word] += 1\n\n    return dict(word_counts)\n```\n"}
{"task_id": "BigCodeBench/842", "solution": "import sqlite3\nimport random\ndef task_func(db_path,\n          num_entries,\n          users=['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'],\n          countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n          random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute('''\n        CREATE TABLE users (\n            id INTEGER PRIMARY KEY,\n            name TEXT,\n            age INTEGER,\n            country TEXT\n        )\n    ''')\n\n    for i in range(num_entries):\n        name = random.choice(users)\n        age = random.randint(20, 60)\n        country = random.choice(countries)\n        c.execute('INSERT INTO users (name, age, country) VALUES (?, ?, ?)', (name, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path", "raw_solution": "import sqlite3\nimport random\n\ndef task_func(db_path,\n          num_entries,\n          users=['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'],\n          countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n          random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute('''\n        CREATE TABLE users (\n            id INTEGER PRIMARY KEY,\n            name TEXT,\n            age INTEGER,\n            country TEXT\n        )\n    ''')\n\n    for i in range(num_entries):\n        name = random.choice(users)\n        age = random.randint(20, 60)\n        country = random.choice(countries)\n        c.execute('INSERT INTO users (name, age, country) VALUES (?, ?, ?)', (name, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path\n```\nThis script creates a SQLite database at the specified `db_path` and populates it with random user data. The number of entries is determined by the `num_entries` parameter. The `users` and `countries` parameters allow you to specify the possible choices for the name and country fields, respectively. The `random_seed` parameter can be used to ensure reproducibility of the random data.\n"}
{"task_id": "BigCodeBench/843", "solution": "import random\nimport re\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\ndef task_func(n_sentences):\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choices(WORD_LIST, k=random.randint(1, 5)))\n        sentence += '.'\n        sentences.append(sentence)\n\n    # Join sentences into a single string\n    result = ' '.join(sentences)\n\n    # Remove all non-alphanumeric characters except spaces\n    result = re.sub(r'[^a-zA-Z0-9\\s]', '', result)\n\n    # Convert to lowercase\n    result = result.lower()\n\n    return result", "raw_solution": "import random\nimport re\n\n# Constants\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\n\ndef task_func(n_sentences):\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choices(WORD_LIST, k=random.randint(1, 5)))\n        sentence += '.'\n        sentences.append(sentence)\n\n    # Join sentences into a single string\n    result = ' '.join(sentences)\n\n    # Remove all non-alphanumeric characters except spaces\n    result = re.sub(r'[^a-zA-Z0-9\\s]', '', result)\n\n    # Convert to lowercase\n    result = result.lower()\n\n    return result\n\n# Example usage"}
{"task_id": "BigCodeBench/844", "solution": "import csv\nimport random\nfrom faker import Faker\ndef task_func(file_path, num_rows, random_seed=None):\n    if not isinstance(num_rows, int) or num_rows < 0:\n        raise ValueError(\"num_rows must be an integer >= 0\")\n\n    fake = Faker()\n    random.seed(random_seed)\n    Faker.seed(random_seed)\n\n    with open(file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Name', 'Age', 'Address', 'Email']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for _ in range(num_rows):\n            writer.writerow({\n                'Name': fake.first_name() + ' ' + fake.last_name(),\n                'Age': random.randint(20, 60),\n                'Address': fake.address().replace('\\n', ', '),\n                'Email': fake.email()\n            })\n\n    return file_path", "raw_solution": "import csv\nimport random\nfrom faker import Faker\n\ndef task_func(file_path, num_rows, random_seed=None):\n    if not isinstance(num_rows, int) or num_rows < 0:\n        raise ValueError(\"num_rows must be an integer >= 0\")\n\n    fake = Faker()\n    random.seed(random_seed)\n    Faker.seed(random_seed)\n\n    with open(file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Name', 'Age', 'Address', 'Email']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for _ in range(num_rows):\n            writer.writerow({\n                'Name': fake.first_name() + ' ' + fake.last_name(),\n                'Age': random.randint(20, 60),\n                'Address': fake.address().replace('\\n', ', '),\n                'Email': fake.email()\n            })\n\n    return file_path\n```\n"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Preprocess the texts\n    text1 = ALPHANUMERIC.sub('', text1).lower()\n    text2 = ALPHANUMERIC.sub('', text2).lower()\n\n    # Calculate term frequency\n    counter1 = Counter(text1.split())\n    counter2 = Counter(text2.split())\n\n    # Calculate cosine similarity\n    common_words = set(counter1.keys()) & set(counter2.keys())\n    dot_product = sum(counter1[word] * counter2[word] for word in common_words)\n    magnitude1 = np.sqrt(sum(counter1[word] ** 2 for word in counter1.keys()))\n    magnitude2 = np.sqrt(sum(counter2[word] ** 2 for word in counter2.keys()))\n    cosine_similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0\n\n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return cosine_similarity, levenshtein_ratio", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    # Preprocess the texts\n    text1 = ALPHANUMERIC.sub('', text1).lower()\n    text2 = ALPHANUMERIC.sub('', text2).lower()\n\n    # Calculate term frequency\n    counter1 = Counter(text1.split())\n    counter2 = Counter(text2.split())\n\n    # Calculate cosine similarity\n    common_words = set(counter1.keys()) & set(counter2.keys())\n    dot_product = sum(counter1[word] * counter2[word] for word in common_words)\n    magnitude1 = np.sqrt(sum(counter1[word] ** 2 for word in counter1.keys()))\n    magnitude2 = np.sqrt(sum(counter2[word] ** 2 for word in counter2.keys()))\n    cosine_similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0\n\n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return cosine_similarity, levenshtein_ratio\n\n# Test cases"}
{"task_id": "BigCodeBench/846", "solution": "import collections\nimport pandas as pd\nclass ExampleObject:\n    def __init__(self, animal, shape):\n        self.animal = animal\n        self.shape = shape\ndef task_func(obj_list, attr):\n    # Count the frequency of each attribute value\n    counter = collections.Counter(getattr(obj, attr) for obj in obj_list)\n    \n    # Create a pandas DataFrame from the counter\n    df = pd.DataFrame(list(counter.items()), columns=[attr, 'count'])\n    \n    return df\nobj_list = [ExampleObject('tiger', 'Square'), ExampleObject('leopard', 'Circle'), ExampleObject('cat', 'Rectangle'), ExampleObject('elephant', 'Rectangle')]", "raw_solution": "import collections\nimport pandas as pd\n\nclass ExampleObject:\n    def __init__(self, animal, shape):\n        self.animal = animal\n        self.shape = shape\n\ndef task_func(obj_list, attr):\n    # Count the frequency of each attribute value\n    counter = collections.Counter(getattr(obj, attr) for obj in obj_list)\n    \n    # Create a pandas DataFrame from the counter\n    df = pd.DataFrame(list(counter.items()), columns=[attr, 'count'])\n    \n    return df\n\n# Example usage:\nobj_list = [ExampleObject('tiger', 'Square'), ExampleObject('leopard', 'Circle'), ExampleObject('cat', 'Rectangle'), ExampleObject('elephant', 'Rectangle')]\ncount = task_func(obj_list, 'shape')"}
{"task_id": "BigCodeBench/847", "solution": "import re\nimport os\nimport string\nimport random\ndef task_func(input_string, directory='./text_files'):\n    # Create directory if it doesn't exist\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Remove special characters and split the string into lines\n    cleaned_lines = re.sub(r'[^\\w\\s]', '', input_string).split('\\n')\n\n    # Generate random file names\n    file_paths = []\n    for line in cleaned_lines:\n        if line.strip():  # Check if line is not empty\n            random_id = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n            file_name = f\"{random_id}.txt\"\n            file_path = os.path.join(directory, file_name)\n            with open(file_path, 'w') as file:\n                file.write(line)\n            file_paths.append(file_path)\n\n    return file_paths", "raw_solution": "import re\nimport os\nimport string\nimport random\n\ndef task_func(input_string, directory='./text_files'):\n    # Create directory if it doesn't exist\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Remove special characters and split the string into lines\n    cleaned_lines = re.sub(r'[^\\w\\s]', '', input_string).split('\\n')\n\n    # Generate random file names\n    file_paths = []\n    for line in cleaned_lines:\n        if line.strip():  # Check if line is not empty\n            random_id = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n            file_name = f\"{random_id}.txt\"\n            file_path = os.path.join(directory, file_name)\n            with open(file_path, 'w') as file:\n                file.write(line)\n            file_paths.append(file_path)\n\n    return file_paths\n\n# Example usage:\n# print(task_func('line a\\nfollows by line b\\n...bye\\n'))\n```\n"}
{"task_id": "BigCodeBench/848", "solution": "import heapq\nimport random\nclass Object:\n    def __init__(self, value):\n        self.value = value\ndef task_func(obj_list, attr, top_n=5, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Extract the specified attribute from each object in the list\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n\n    # Find the top N values using a heap\n    top_values = heapq.nlargest(top_n, attr_values)\n\n    # Randomly select a value from all attribute values\n    random_value = random.choice(attr_values) if attr_values else None\n\n    return top_values, random_value\nobj_list = [Object(random.randint(1, 100)) for _ in range(33)]", "raw_solution": "import heapq\nimport random\n\nclass Object:\n    def __init__(self, value):\n        self.value = value\n\ndef task_func(obj_list, attr, top_n=5, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Extract the specified attribute from each object in the list\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n\n    # Find the top N values using a heap\n    top_values = heapq.nlargest(top_n, attr_values)\n\n    # Randomly select a value from all attribute values\n    random_value = random.choice(attr_values) if attr_values else None\n\n    return top_values, random_value\n\n# Example usage:\nrandom.seed(1)\nobj_list = [Object(random.randint(1, 100)) for _ in range(33)]\ntop_values, random_value = task_func(obj_list, 'value', 5, seed=1)"}
{"task_id": "BigCodeBench/849", "solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(input_string):\n    # Split the input string into individual lines\n    lines = input_string.strip().split('\\n')\n    \n    # Initialize a counter for word frequencies\n    word_counter = Counter()\n\n    # Process each line\n    for line in lines:\n        # Remove punctuation and convert to lowercase\n        cleaned_line = re.sub(r'[^\\w\\s]', '', line).lower()\n        \n        # Split the line into words\n        words = cleaned_line.split()\n        \n        # Filter out stopwords and update the counter\n        filtered_words = [word for word in words if word not in STOPWORDS]\n        word_counter.update(filtered_words)\n    \n    return dict(word_counter)", "raw_solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(input_string):\n    # Split the input string into individual lines\n    lines = input_string.strip().split('\\n')\n    \n    # Initialize a counter for word frequencies\n    word_counter = Counter()\n\n    # Process each line\n    for line in lines:\n        # Remove punctuation and convert to lowercase\n        cleaned_line = re.sub(r'[^\\w\\s]', '', line).lower()\n        \n        # Split the line into words\n        words = cleaned_line.split()\n        \n        # Filter out stopwords and update the counter\n        filtered_words = [word for word in words if word not in STOPWORDS]\n        word_counter.update(filtered_words)\n    \n    return dict(word_counter)\n```\n"}
{"task_id": "BigCodeBench/850", "solution": "import pandas as pd\nimport statistics\nimport random\ndef task_func(students, subjects, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    report_data = []\n\n    for student in students:\n        grades = {subject: random.randint(0, 100) for subject in subjects}\n        average_grade = statistics.mean(grades.values())\n        report_data.append([student] + list(grades.values()) + [average_grade])\n\n    report_columns = ['Student'] + subjects + ['Average Grade']\n    report_df = pd.DataFrame(report_data, columns=report_columns)\n\n    return report_df\nstudents = ['Alice', 'Bob', 'Charlie']\nsubjects = ['Math', 'Physics', 'English']", "raw_solution": "import pandas as pd\nimport statistics\nimport random\n\ndef task_func(students, subjects, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    report_data = []\n\n    for student in students:\n        grades = {subject: random.randint(0, 100) for subject in subjects}\n        average_grade = statistics.mean(grades.values())\n        report_data.append([student] + list(grades.values()) + [average_grade])\n\n    report_columns = ['Student'] + subjects + ['Average Grade']\n    report_df = pd.DataFrame(report_data, columns=report_columns)\n\n    return report_df\n\n# Example usage:\nstudents = ['Alice', 'Bob', 'Charlie']\nsubjects = ['Math', 'Physics', 'English']\nreport = task_func(students, subjects, seed=123)"}
{"task_id": "BigCodeBench/851", "solution": "import textwrap\nimport re\ndef task_func(input_string, width):\n    \"\"\"\n    Ek multi string ko separate strings mein vibhajit karein aur pratyek line ko ek certain width tak lapetein.\n\n    Params:\n    - input_string (str): Vah multi string jise lapetne ki aavashyakta hai.\n    - width (int): Pratyek line ko lapetne ki width.\n\n    Returns:\n        str: Lapeta gaya string jahan pratyek line specified width tak lapeta gaya hai.\n\n    Reqs:\n    - textwrap\n    - re\n\n    Examples:\n    - >>> task_func('Another line\\nWith wrapping', 8)\n    - 'Another\\nline\\nWith\\nwrapping'\n\n    \"\"\"\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n    \n    # Wrap each line to the specified width\n    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n    \n    # Join the wrapped lines back into a single string\n    result = '\\n'.join(wrapped_lines)\n    \n    return result", "raw_solution": "import textwrap\nimport re\n\ndef task_func(input_string, width):\n    \"\"\"\n    Ek multi string ko separate strings mein vibhajit karein aur pratyek line ko ek certain width tak lapetein.\n\n    Params:\n    - input_string (str): Vah multi string jise lapetne ki aavashyakta hai.\n    - width (int): Pratyek line ko lapetne ki width.\n\n    Returns:\n        str: Lapeta gaya string jahan pratyek line specified width tak lapeta gaya hai.\n\n    Reqs:\n    - textwrap\n    - re\n\n    Examples:\n    - >>> task_func('Another line\\nWith wrapping', 8)\n    - 'Another\\nline\\nWith\\nwrapping'\n\n    \"\"\"\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n    \n    # Wrap each line to the specified width\n    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n    \n    # Join the wrapped lines back into a single string\n    result = '\\n'.join(wrapped_lines)\n    \n    return result\n\n# Test cases"}
{"task_id": "BigCodeBench/852", "solution": "import random\nimport string\ndef task_func(max_length, n_samples, seed=None):\n    if max_length < 1:\n        raise ValueError(\"max_length must be at least 1\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    return [''.join(random.choices(string.ascii_lowercase, k=length)) for length in random.choices(range(1, max_length + 1), k=n_samples)]", "raw_solution": "import random\nimport string\n\ndef task_func(max_length, n_samples, seed=None):\n    if max_length < 1:\n        raise ValueError(\"max_length must be at least 1\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    return [''.join(random.choices(string.ascii_lowercase, k=length)) for length in random.choices(range(1, max_length + 1), k=n_samples)]\n\n# Test cases"}
{"task_id": "BigCodeBench/853", "solution": "import os\nimport shutil\nimport string\nINVALID_CHARACTERS = string.punctuation + string.whitespace\ndef task_func(directory_path):\n    summary = {}\n    invalid_dir = os.path.join(directory_path, 'Invalid')\n\n    # Create a new directory for invalid files\n    if not os.path.exists(invalid_dir):\n        os.makedirs(invalid_dir)\n\n    for filename in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, filename)\n        if os.path.isfile(file_path):\n            # Extract file extension\n            _, extension = os.path.splitext(filename)\n            extension = extension[1:]  # Remove the leading dot\n\n            # Check for invalid characters in filename\n            if any(char in filename for char in INVALID_CHARACTERS):\n                # Move file to invalid directory\n                invalid_file_path = os.path.join(invalid_dir, filename)\n                shutil.move(file_path, invalid_file_path)\n                extension = 'Invalid'\n\n            # Update summary\n            if extension in summary:\n                summary[extension] += 1\n            else:\n                summary[extension] = 1\n\n    return summary", "raw_solution": "import os\nimport shutil\nimport string\n\n# Constants\nINVALID_CHARACTERS = string.punctuation + string.whitespace\n\ndef task_func(directory_path):\n    summary = {}\n    invalid_dir = os.path.join(directory_path, 'Invalid')\n\n    # Create a new directory for invalid files\n    if not os.path.exists(invalid_dir):\n        os.makedirs(invalid_dir)\n\n    for filename in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, filename)\n        if os.path.isfile(file_path):\n            # Extract file extension\n            _, extension = os.path.splitext(filename)\n            extension = extension[1:]  # Remove the leading dot\n\n            # Check for invalid characters in filename\n            if any(char in filename for char in INVALID_CHARACTERS):\n                # Move file to invalid directory\n                invalid_file_path = os.path.join(invalid_dir, filename)\n                shutil.move(file_path, invalid_file_path)\n                extension = 'Invalid'\n\n            # Update summary\n            if extension in summary:\n                summary[extension] += 1\n            else:\n                summary[extension] = 1\n\n    return summary\n\n# Example usage:\n# print(task_func('path_to_directory'))\n```\n"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    if not isinstance(numbers, list) or not all(isinstance(n, int) for n in numbers):\n        raise TypeError(\"Input must be a list of integers.\")\n    if any(n < 0 for n in numbers):\n        raise ValueError(\"Input numbers must be non-negative.\")\n\n    factorials = [math.factorial(n) for n in numbers]\n    perms = list(permutations(numbers))\n    factorial_sums = [reduce(lambda x, y: x + y, [factorials[i] for i in perm]) for perm in perms]\n\n    return factorial_sums, perms", "raw_solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    if not isinstance(numbers, list) or not all(isinstance(n, int) for n in numbers):\n        raise TypeError(\"Input must be a list of integers.\")\n    if any(n < 0 for n in numbers):\n        raise ValueError(\"Input numbers must be non-negative.\")\n\n    factorials = [math.factorial(n) for n in numbers]\n    perms = list(permutations(numbers))\n    factorial_sums = [reduce(lambda x, y: x + y, [factorials[i] for i in perm]) for perm in perms]\n\n    return factorial_sums, perms\n\n# Example usage:\n# fac, perm = task_func([1, 2, 3])\n# print(fac)\n# print(perm)\n```\n"}
{"task_id": "BigCodeBench/855", "solution": "import random\nimport string\nimport collections\nVALID_CHARACTERS = string.ascii_letters + string.digits\ndef task_func(n_strings, string_length):\n    \"\"\"\n    Nirdisht lambai ke n random (random) strings banaen, sabhi strings mein pratyek character (character) ki frequency (frequency) ginen, aur result (result) ko ek dictionary ke roop mein lautaen.\n    Constants:\n    - VALID_CHARACTERS: ek string jismein sabhi valid (valid) character (characters) shaamil hain (ASCII letters aur digits) jinka upyog random strings mein kiya ja sakta hai.\n\n    Params:\n    - n_strings (int): utpann karne ke liye random (random) strings ki number.\n    - string_length (int): pratyek random (random) string ki lambai.\n\n    Returns:\n        dict: ek dictionary jismein character (character) counts shaamil hain, jismein character (character) keys ke roop mein aur unki frequencies values ke roop mein hain.\n\n    Reqs:\n    - random\n    - string\n    - collections\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func(2, 3)\n    - {'O': 1, 'h': 1, 'b': 1, 'V': 1, 'r': 1, 'p': 1}\n\n    \"\"\"\n    # Generate random strings\n    random_strings = [''.join(random.choices(VALID_CHARACTERS, k=string_length)) for _ in range(n_strings)]\n    \n    # Count character frequencies\n    char_count = collections.Counter(''.join(random_strings))\n    \n    return dict(char_count)", "raw_solution": "import random\nimport string\nimport collections\n\n# Constants\nVALID_CHARACTERS = string.ascii_letters + string.digits\n\ndef task_func(n_strings, string_length):\n    \"\"\"\n    Nirdisht lambai ke n random (random) strings banaen, sabhi strings mein pratyek character (character) ki frequency (frequency) ginen, aur result (result) ko ek dictionary ke roop mein lautaen.\n    Constants:\n    - VALID_CHARACTERS: ek string jismein sabhi valid (valid) character (characters) shaamil hain (ASCII letters aur digits) jinka upyog random strings mein kiya ja sakta hai.\n\n    Params:\n    - n_strings (int): utpann karne ke liye random (random) strings ki number.\n    - string_length (int): pratyek random (random) string ki lambai.\n\n    Returns:\n        dict: ek dictionary jismein character (character) counts shaamil hain, jismein character (character) keys ke roop mein aur unki frequencies values ke roop mein hain.\n\n    Reqs:\n    - random\n    - string\n    - collections\n\n    Examples:\n    - >>> random.seed(42)\n    - >>> task_func(2, 3)\n    - {'O': 1, 'h': 1, 'b': 1, 'V': 1, 'r': 1, 'p': 1}\n\n    \"\"\"\n    # Generate random strings\n    random_strings = [''.join(random.choices(VALID_CHARACTERS, k=string_length)) for _ in range(n_strings)]\n    \n    # Count character frequencies\n    char_count = collections.Counter(''.join(random_strings))\n    \n    return dict(char_count)\n```\n"}
{"task_id": "BigCodeBench/856", "solution": "from functools import reduce\nfrom itertools import combinations\nimport numpy as np\ndef task_func(shape=(3, 3), low=1, high=10, seed=None):\n    if high <= low:\n        raise ValueError(\"High must be greater than low\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    matrix = np.random.randint(low, high, size=shape)\n    pairs = combinations(matrix.flatten(), 2)\n    sum_of_pairs = reduce(lambda x, y: x + y, (a + b for a, b in pairs))\n\n    return sum_of_pairs, matrix", "raw_solution": "from functools import reduce\nfrom itertools import combinations\nimport numpy as np\n\ndef task_func(shape=(3, 3), low=1, high=10, seed=None):\n    if high <= low:\n        raise ValueError(\"High must be greater than low\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    matrix = np.random.randint(low, high, size=shape)\n    pairs = combinations(matrix.flatten(), 2)\n    sum_of_pairs = reduce(lambda x, y: x + y, (a + b for a, b in pairs))\n\n    return sum_of_pairs, matrix\n\n# Example usage:\n# result, matrix = task_func((5, 4), 1, 10, seed=1)\n# print(result)\n# print(matrix)\n```\n"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Ek directory (SOURCE_DIR) se files ko, specified file extensions (EXTENSIONS) ke aadhaar par, doosri directory (DEST_DIR) mein transfer karein.\n    Yeh un files ke liye bhi warnings jaari karta hai jinhein kisi bhi samasya ke kaaran transfer nahin kiya ja saka.\n\n    Params:\n    - SOURCE_DIR (str): Vah source_dir directory path jahaan se files ko transfer kiya jaayega.\n    - DEST_DIR (str): Vah dest_dir directory path jahaan files ko transfer kiya jaayega.\n    - EXTENSIONS (list): Sthaanaantaran ke liye vichaar karne ke liye file extensions ki ek soochi. example: ['.txt', '.csv', '.xlsx']\n\n    Returns:\n        transferred_files (list): Un files ke naam ki ek soochi jo safaltapoorvak transfer ki gayeen.\n\n    Reqs:\n    - warnings\n    - os\n    - glob\n    - shutil\n    - time\n\n    Examples:\n    - >>> task_func('/path/to/source', '/path/to/destination', ['.txt', '.csv'])\n    - ['file1.txt', 'file2.csv']\n    - >>> task_func('/path/to/source', '/path/to/destination', ['.jpg'])\n    - []\n\n    \"\"\"\n    transferred_files = []\n\n    # Ensure destination directory exists\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    # Iterate over each file extension\n    for ext in EXTENSIONS:\n        # Find all files with the current extension in the source directory\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*{ext}'))\n        \n        for file in files:\n            try:\n                # Copy the file to the destination directory\n                shutil.copy(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                # Issue with file transfer, warn the user\n                warnings.warn(f\"Failed to transfer {file}: {e}\")\n\n    return transferred_files", "raw_solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Ek directory (SOURCE_DIR) se files ko, specified file extensions (EXTENSIONS) ke aadhaar par, doosri directory (DEST_DIR) mein transfer karein.\n    Yeh un files ke liye bhi warnings jaari karta hai jinhein kisi bhi samasya ke kaaran transfer nahin kiya ja saka.\n\n    Params:\n    - SOURCE_DIR (str): Vah source_dir directory path jahaan se files ko transfer kiya jaayega.\n    - DEST_DIR (str): Vah dest_dir directory path jahaan files ko transfer kiya jaayega.\n    - EXTENSIONS (list): Sthaanaantaran ke liye vichaar karne ke liye file extensions ki ek soochi. example: ['.txt', '.csv', '.xlsx']\n\n    Returns:\n        transferred_files (list): Un files ke naam ki ek soochi jo safaltapoorvak transfer ki gayeen.\n\n    Reqs:\n    - warnings\n    - os\n    - glob\n    - shutil\n    - time\n\n    Examples:\n    - >>> task_func('/path/to/source', '/path/to/destination', ['.txt', '.csv'])\n    - ['file1.txt', 'file2.csv']\n    - >>> task_func('/path/to/source', '/path/to/destination', ['.jpg'])\n    - []\n\n    \"\"\"\n    transferred_files = []\n\n    # Ensure destination directory exists\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    # Iterate over each file extension\n    for ext in EXTENSIONS:\n        # Find all files with the current extension in the source directory\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*{ext}'))\n        \n        for file in files:\n            try:\n                # Copy the file to the destination directory\n                shutil.copy(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                # Issue with file transfer, warn the user\n                warnings.warn(f\"Failed to transfer {file}: {e}\")\n\n    return transferred_files\n```\n"}
{"task_id": "BigCodeBench/858", "solution": "import string\nimport random\nfrom collections import Counter\ndef task_func(n, seed=None):\n    # Define the alphabet\n    alphabet = string.ascii_lowercase\n\n    # Set the seed for reproducibility\n    random.seed(seed)\n\n    # Generate random letters\n    random_letters = [random.choice(alphabet) for _ in range(n)]\n\n    # Count occurrences of each letter\n    letter_counts = Counter(random_letters)\n\n    return letter_counts", "raw_solution": "import string\nimport random\nfrom collections import Counter\n\ndef task_func(n, seed=None):\n    # Define the alphabet\n    alphabet = string.ascii_lowercase\n\n    # Set the seed for reproducibility\n    random.seed(seed)\n\n    # Generate random letters\n    random_letters = [random.choice(alphabet) for _ in range(n)]\n\n    # Count occurrences of each letter\n    letter_counts = Counter(random_letters)\n\n    return letter_counts\n\n# Example usage:\n# letter_counts = task_func(1000, seed=123)\n# print(letter_counts)\n```\n"}
{"task_id": "BigCodeBench/859", "solution": "import warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\ndef task_func():\n    # Load the iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n\n    # Create a SVM classifier\n    clf = svm.SVC()\n\n    # Train the classifier\n    clf.fit(X_train, y_train)\n\n    # Predict the test set\n    y_pred = clf.predict(X_test)\n\n    # Calculate the accuracy\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n\n    # Check if accuracy is less than 0.9 and issue a warning if so\n    warning_msg = None\n    if accuracy < 0.9:\n        warnings.warn(\"Accuracy is less than 0.9\", UserWarning)\n        warning_msg = \"Accuracy is less than 0.9\"\n\n    return accuracy, warning_msg", "raw_solution": "import warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\n\ndef task_func():\n    # Load the iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n\n    # Create a SVM classifier\n    clf = svm.SVC()\n\n    # Train the classifier\n    clf.fit(X_train, y_train)\n\n    # Predict the test set\n    y_pred = clf.predict(X_test)\n\n    # Calculate the accuracy\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n\n    # Check if accuracy is less than 0.9 and issue a warning if so\n    warning_msg = None\n    if accuracy < 0.9:\n        warnings.warn(\"Accuracy is less than 0.9\", UserWarning)\n        warning_msg = \"Accuracy is less than 0.9\"\n\n    return accuracy, warning_msg\n```\n"}
{"task_id": "BigCodeBench/860", "solution": "import re\nimport random\nimport string\ndef task_func(n, pattern, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Generate a random string of length 'n'\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=n))\n\n    # Find all matches of the pattern in the random string\n    matches = re.findall(pattern, random_string)\n\n    return matches", "raw_solution": "import re\nimport random\nimport string\n\ndef task_func(n, pattern, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Generate a random string of length 'n'\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=n))\n\n    # Find all matches of the pattern in the random string\n    matches = re.findall(pattern, random_string)\n\n    return matches\n\n# Test cases"}
{"task_id": "BigCodeBench/861", "solution": "from collections import Counter\nfrom random import choice, seed\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\ndef task_func(list_of_lists):\n    seed(42)  # For reproducibility\n    baskets = []\n\n    for basket in list_of_lists:\n        counter = Counter()\n        for item in basket:\n            counter[choice(POSSIBLE_ITEMS)] += 1\n        baskets.append(counter)\n\n    return baskets", "raw_solution": "from collections import Counter\nfrom random import choice, seed\n\n# Constants\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef task_func(list_of_lists):\n    seed(42)  # For reproducibility\n    baskets = []\n\n    for basket in list_of_lists:\n        counter = Counter()\n        for item in basket:\n            counter[choice(POSSIBLE_ITEMS)] += 1\n        baskets.append(counter)\n\n    return baskets\n\n# Test cases\nbaskets = task_func([[1, 2, 3], [4, 5]])\nassert all(isinstance(basket, Counter) for basket in baskets)\nassert sum(len(basket) for basket in baskets) == 3\n```\n"}
{"task_id": "BigCodeBench/862", "solution": "import random\nimport string\nfrom collections import defaultdict\ndef task_func(n, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    letters = string.ascii_lowercase\n    result = defaultdict(list)\n\n    for _ in range(n):\n        letter = random.choice(letters)\n        result[letter].append(letter)\n\n    return result", "raw_solution": "import random\nimport string\nfrom collections import defaultdict\n\ndef task_func(n, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    letters = string.ascii_lowercase\n    result = defaultdict(list)\n\n    for _ in range(n):\n        letter = random.choice(letters)\n        result[letter].append(letter)\n\n    return result\n\n# Example usage:\n# print(task_func(30, seed=1))\n```\n"}
{"task_id": "BigCodeBench/863", "solution": "import numpy as np\nimport math\nPOSSIBLE_NUMBERS = np.arange(1, 11)\ndef task_func(list_of_lists):\n    sums = []\n    for lst in list_of_lists:\n        sum = 0\n        for num in lst:\n            if num in POSSIBLE_NUMBERS:\n                sum += num\n        sums.append(sum)\n    return sums", "raw_solution": "import numpy as np\nimport math\n\n# Constants\nPOSSIBLE_NUMBERS = np.arange(1, 11)\n\ndef task_func(list_of_lists):\n    sums = []\n    for lst in list_of_lists:\n        sum = 0\n        for num in lst:\n            if num in POSSIBLE_NUMBERS:\n                sum += num\n        sums.append(sum)\n    return sums\n\n# Test cases"}
{"task_id": "BigCodeBench/864", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(fruit_data):\n    if not fruit_data:\n        return pd.DataFrame(columns=['Total Count', 'Average Count'])\n\n    df = pd.DataFrame(fruit_data, columns=['Fruit', 'Count'])\n    summary = df.groupby('Fruit').agg({'Count': ['sum', 'mean']})\n    summary.columns = ['Total Count', 'Average Count']\n    summary.sort_index(inplace=True)\n\n    return summary", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(fruit_data):\n    if not fruit_data:\n        return pd.DataFrame(columns=['Total Count', 'Average Count'])\n\n    df = pd.DataFrame(fruit_data, columns=['Fruit', 'Count'])\n    summary = df.groupby('Fruit').agg({'Count': ['sum', 'mean']})\n    summary.columns = ['Total Count', 'Average Count']\n    summary.sort_index(inplace=True)\n\n    return summary\n\n# Example usage:\nfruit_list = [('apple', 5), ('banana', 3), ('apple', 6), ('banana', 4), ('cherry', 5), ('banana', 2), ('apple', 4), ('cherry', 5)]\nreport = task_func(fruit_list)\nreport.sort_index(inplace=True)"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=['Item', 'Counts', 'Weight'])\n\n    # Apply z-score normalization to the 'Counts' column\n    df['Normalized Count'] = zscore(df['Counts'])\n\n    # Apply Min-Max scaling to the 'Weight' column\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    return df\ndata = [('A', 100, 0.5), ('B', 200, 0.6), ('C', 150, 0.7)]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=['Item', 'Counts', 'Weight'])\n\n    # Apply z-score normalization to the 'Counts' column\n    df['Normalized Count'] = zscore(df['Counts'])\n\n    # Apply Min-Max scaling to the 'Weight' column\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    return df\n\n# Example usage:\ndata = [('A', 100, 0.5), ('B', 200, 0.6), ('C', 150, 0.7)]\nreport = task_func(data)"}
{"task_id": "BigCodeBench/866", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=2, random_state=0):\n    # Extracting features from the data\n    features = np.array([point[1:] for point in data])\n    \n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(features)\n    \n    # Extracting cluster labels\n    labels = kmeans.labels_\n    \n    return labels\ndata = [('T1', 1, 1), ('T2', 1, 1.1), ('T2', 1.1, 1), ('C1', 400, 400), ('C2', 401, 401), ('B1', 35, 35)]", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters=2, random_state=0):\n    # Extracting features from the data\n    features = np.array([point[1:] for point in data])\n    \n    # Applying KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(features)\n    \n    # Extracting cluster labels\n    labels = kmeans.labels_\n    \n    return labels\n\n# Example usage:\ndata = [('T1', 1, 1), ('T2', 1, 1.1), ('T2', 1.1, 1), ('C1', 400, 400), ('C2', 401, 401), ('B1', 35, 35)]\nlabels = task_func(data, n_clusters=3, random_state=42)"}
{"task_id": "BigCodeBench/867", "solution": "import re\nimport string\ndef task_func(text1, text2):\n    # Define the pattern to match any punctuation\n    pattern = f\"[{re.escape(string.punctuation)}]\"\n\n    # Remove punctuation from both texts\n    cleaned_text1 = re.sub(pattern, \"\", text1)\n    cleaned_text2 = re.sub(pattern, \"\", text2)\n\n    return cleaned_text1, cleaned_text2", "raw_solution": "import re\nimport string\n\ndef task_func(text1, text2):\n    # Define the pattern to match any punctuation\n    pattern = f\"[{re.escape(string.punctuation)}]\"\n\n    # Remove punctuation from both texts\n    cleaned_text1 = re.sub(pattern, \"\", text1)\n    cleaned_text2 = re.sub(pattern, \"\", text2)\n\n    return cleaned_text1, cleaned_text2\n\n# Test cases\ncleaned_text1, cleaned_text2 = task_func(\"test (with parenthesis []!!)\", \"And, other; stuff ^_`\")"}
{"task_id": "BigCodeBench/868", "solution": "from itertools import cycle\nfrom random import choice, seed\ndef task_func(n_colors, colors=['Red', 'Green', 'Blue', 'Yellow', 'Purple'], rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    color_cycle = cycle(colors)\n    color_pattern = []\n\n    for _ in range(n_colors):\n        color_pattern.append(next(color_cycle))\n    \n    return color_pattern\ncolors = ['Brown', 'Hara', 'Kaala']", "raw_solution": "from itertools import cycle\nfrom random import choice, seed\n\ndef task_func(n_colors, colors=['Red', 'Green', 'Blue', 'Yellow', 'Purple'], rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    color_cycle = cycle(colors)\n    color_pattern = []\n\n    for _ in range(n_colors):\n        color_pattern.append(next(color_cycle))\n    \n    return color_pattern\n\n# Example usage:\ncolors = ['Brown', 'Hara', 'Kaala']\ncolor_pattern = task_func(12, colors=colors, rng_seed=42)"}
{"task_id": "BigCodeBench/869", "solution": "import pandas as pd\nfrom itertools import cycle\nfrom random import randint, seed\ndef task_func(\n    n_grades,\n    students=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    grade_range=range(1, 11),\n    rng_seed=None\n):\n    if not students:\n        raise ValueError(\"The students list cannot be empty.\")\n    \n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    student_cycle = cycle(students)\n    grades = [randint(grade_range.start, grade_range.stop) for _ in range(n_grades)]\n    \n    report_data = {\n        'Student': [next(student_cycle) for _ in range(n_grades)],\n        'Grade': grades\n    }\n    \n    report_df = pd.DataFrame(report_data)\n    \n    return report_df", "raw_solution": "import pandas as pd\nfrom itertools import cycle\nfrom random import randint, seed\n\ndef task_func(\n    n_grades,\n    students=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    grade_range=range(1, 11),\n    rng_seed=None\n):\n    if not students:\n        raise ValueError(\"The students list cannot be empty.\")\n    \n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    student_cycle = cycle(students)\n    grades = [randint(grade_range.start, grade_range.stop) for _ in range(n_grades)]\n    \n    report_data = {\n        'Student': [next(student_cycle) for _ in range(n_grades)],\n        'Grade': grades\n    }\n    \n    report_df = pd.DataFrame(report_data)\n    \n    return report_df\n```\n"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store mean values\n    mean_values = []\n\n    # Iterate over the data list\n    for i, data in enumerate(data_list):\n        # Filter out non-numerical data\n        numerical_data = [x for x in data if isinstance(x, (int, float))]\n\n        # Calculate the mean of numerical data\n        mean_value = np.mean(numerical_data) if numerical_data else np.nan\n\n        # Append the mean value to the list\n        mean_values.append(mean_value)\n\n    # Create a DataFrame with the mean values\n    df = pd.DataFrame(mean_values, columns=['Mean Value'], index=[f'Position {i}' for i in range(len(data_list))])\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store mean values\n    mean_values = []\n\n    # Iterate over the data list\n    for i, data in enumerate(data_list):\n        # Filter out non-numerical data\n        numerical_data = [x for x in data if isinstance(x, (int, float))]\n\n        # Calculate the mean of numerical data\n        mean_value = np.mean(numerical_data) if numerical_data else np.nan\n\n        # Append the mean value to the list\n        mean_values.append(mean_value)\n\n    # Create a DataFrame with the mean values\n    df = pd.DataFrame(mean_values, columns=['Mean Value'], index=[f'Position {i}' for i in range(len(data_list))])\n\n    return df\n\n# Test cases\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)"}
{"task_id": "BigCodeBench/871", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list, file_name):\n    mean_values = []\n    with open(file_name, 'w') as file:\n        for i, data in enumerate(data_list, start=1):\n            numeric_data = [x for x in data if isinstance(x, (int, float))]\n            if numeric_data:\n                mean = np.mean(numeric_data)\n                mean_values.append(mean)\n                file.write(f'Position {i}: {mean}\\n')\n            else:\n                mean_values.append(np.nan)\n    \n    return mean_values", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(data_list, file_name):\n    mean_values = []\n    with open(file_name, 'w') as file:\n        for i, data in enumerate(data_list, start=1):\n            numeric_data = [x for x in data if isinstance(x, (int, float))]\n            if numeric_data:\n                mean = np.mean(numeric_data)\n                mean_values.append(mean)\n                file.write(f'Position {i}: {mean}\\n')\n            else:\n                mean_values.append(np.nan)\n    \n    return mean_values\n```\n"}
{"task_id": "BigCodeBench/872", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        return []\n\n    # Unzipping the list of tuples\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=np.nan))\n\n    # Calculating mean for each position\n    mean_values = [np.nanmean(column) for column in unzipped_data]\n\n    return mean_values", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(data_list):\n    if not data_list:\n        return []\n\n    # Unzipping the list of tuples\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=np.nan))\n\n    # Calculating mean for each position\n    mean_values = [np.nanmean(column) for column in unzipped_data]\n\n    return mean_values\n\n# Test cases"}
{"task_id": "BigCodeBench/873", "solution": "import csv\nimport os\ndef task_func(data, file_path, headers):\n    if file_path is None:\n        raise ValueError(\"file_path cannot be None\")\n\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n\n        for row in data:\n            # Fill missing values with None\n            row = row + (None,) * (len(headers) - len(row))\n            writer.writerow(row)\n\n    return os.path.abspath(file_path)", "raw_solution": "import csv\nimport os\n\ndef task_func(data, file_path, headers):\n    if file_path is None:\n        raise ValueError(\"file_path cannot be None\")\n\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n\n        for row in data:\n            # Fill missing values with None\n            row = row + (None,) * (len(headers) - len(row))\n            writer.writerow(row)\n\n    return os.path.abspath(file_path)\n\n# Example usage:\n# full_path = task_func([(1, 'a', 2), ('a', 3, 5), ('c', 1, -2)], 'test.csv', ['a', 'b', 'c'])\n# print(full_path)\n# with open('test.csv', 'r', newline='') as csvfile:\n#     reader = csv.reader(csvfile)\n#     for row in reader:\n#         print(row)\n```\n"}
{"task_id": "BigCodeBench/874", "solution": "from itertools import zip_longest\nfrom scipy.spatial import distance\ndef task_func(points):\n    distances = []\n    for point1, point2 in zip_longest(points, points[1:], fillvalue=None):\n        if point1 is not None and point2 is not None:\n            dist = distance.euclidean(point1, point2)\n            distances.append(dist)\n    return distances", "raw_solution": "from itertools import zip_longest\nfrom scipy.spatial import distance\n\ndef task_func(points):\n    distances = []\n    for point1, point2 in zip_longest(points, points[1:], fillvalue=None):\n        if point1 is not None and point2 is not None:\n            dist = distance.euclidean(point1, point2)\n            distances.append(dist)\n    return distances\n\n# Test cases"}
{"task_id": "BigCodeBench/875", "solution": "import pandas as pd\nimport random\ndef task_func(data, columns=['Name', 'Age', 'Occupation'], fill_missing=False, num_range=(0, 100), seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=columns)\n\n    # Fill missing values\n    for col in df.select_dtypes(include=[float, int]).columns:\n        if df[col].isnull().any():\n            if fill_missing:\n                df[col] = df[col].apply(lambda x: random.uniform(*num_range) if pd.isnull(x) else x)\n            else:\n                df[col] = df[col].apply(lambda x: None if pd.isnull(x) else x)\n\n    return df\ndata = [('John', 25, 'Engineer'), ('Alice', ), ('Bob', )]", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(data, columns=['Name', 'Age', 'Occupation'], fill_missing=False, num_range=(0, 100), seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=columns)\n\n    # Fill missing values\n    for col in df.select_dtypes(include=[float, int]).columns:\n        if df[col].isnull().any():\n            if fill_missing:\n                df[col] = df[col].apply(lambda x: random.uniform(*num_range) if pd.isnull(x) else x)\n            else:\n                df[col] = df[col].apply(lambda x: None if pd.isnull(x) else x)\n\n    return df\n\n# Example usage:\ndata = [('John', 25, 'Engineer'), ('Alice', ), ('Bob', )]\ndf = task_func(data, fill_missing=True, num_range=(0, 10), seed=42)"}
{"task_id": "BigCodeBench/876", "solution": "import collections\nimport operator\nimport os\nimport shutil\ndef task_func(data_dict, source_directory, backup_directory):\n    # Add key 'a' with value 1 to the dictionary\n    data_dict['a'] = 1\n\n    # Sort the dictionary by value frequency\n    value_frequencies = sorted(collections.Counter(data_dict.values()).items(), key=operator.itemgetter(1), reverse=True)\n\n    # Backup all files from the source directory to the backup directory\n    backup_status = True\n    try:\n        if not os.path.exists(backup_directory):\n            os.makedirs(backup_directory)\n        for filename in os.listdir(source_directory):\n            shutil.copy(os.path.join(source_directory, filename), backup_directory)\n    except Exception as e:\n        backup_status = False\n\n    return data_dict, value_frequencies, backup_status", "raw_solution": "import collections\nimport operator\nimport os\nimport shutil\n\ndef task_func(data_dict, source_directory, backup_directory):\n    # Add key 'a' with value 1 to the dictionary\n    data_dict['a'] = 1\n\n    # Sort the dictionary by value frequency\n    value_frequencies = sorted(collections.Counter(data_dict.values()).items(), key=operator.itemgetter(1), reverse=True)\n\n    # Backup all files from the source directory to the backup directory\n    backup_status = True\n    try:\n        if not os.path.exists(backup_directory):\n            os.makedirs(backup_directory)\n        for filename in os.listdir(source_directory):\n            shutil.copy(os.path.join(source_directory, filename), backup_directory)\n    except Exception as e:\n        backup_status = False\n\n    return data_dict, value_frequencies, backup_status\n```\n"}
{"task_id": "BigCodeBench/877", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    if not all(data.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        raise ValueError(\"All columns in the DataFrame must be numeric.\")\n    \n    if n_components > len(data.columns):\n        raise ValueError(\"Number of components cannot be greater than the number of columns in the DataFrame.\")\n    \n    if data.empty:\n        raise ValueError(\"Input data cannot be empty.\")\n    \n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n    \n    pca = PCA(n_components=n_components)\n    principal_components = pca.fit_transform(scaled_data)\n    \n    result_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n    \n    return result_df\ndata = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [6, 7, 8, 9, 10],\n    'C': [11, 12, 13, 14, 15],\n    'D': [16, 17, 18, 19, 20]\n})", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    if not all(data.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        raise ValueError(\"All columns in the DataFrame must be numeric.\")\n    \n    if n_components > len(data.columns):\n        raise ValueError(\"Number of components cannot be greater than the number of columns in the DataFrame.\")\n    \n    if data.empty:\n        raise ValueError(\"Input data cannot be empty.\")\n    \n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n    \n    pca = PCA(n_components=n_components)\n    principal_components = pca.fit_transform(scaled_data)\n    \n    result_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n    \n    return result_df\n\n# Example usage:\ndata = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [6, 7, 8, 9, 10],\n    'C': [11, 12, 13, 14, 15],\n    'D': [16, 17, 18, 19, 20]\n})\n\nresult = task_func(data, n_components=2)"}
{"task_id": "BigCodeBench/878", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\ndef task_func(data, target, test_size=0.2, random_state=None):\n    df = pd.DataFrame(data)\n    if target not in df.columns:\n        raise ValueError(\"Target column not found in DataFrame\")\n\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse, model, df", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(data, target, test_size=0.2, random_state=None):\n    df = pd.DataFrame(data)\n    if target not in df.columns:\n        raise ValueError(\"Target column not found in DataFrame\")\n\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse, model, df\n```\n"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    if data.empty:\n        raise ValueError(\"DataFrame should not be empty.\")\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Columns {col1} or {col2} not found in DataFrame.\")\n    if not (data[col1].dtype == 'object' and data[col2].dtype == 'object'):\n        raise TypeError(\"Both columns should contain categorical data.\")\n\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    if data.empty:\n        raise ValueError(\"DataFrame should not be empty.\")\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Columns {col1} or {col2} not found in DataFrame.\")\n    if not (data[col1].dtype == 'object' and data[col2].dtype == 'object'):\n        raise TypeError(\"Both columns should contain categorical data.\")\n\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p\n\n# Example usage:\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})"}
{"task_id": "BigCodeBench/880", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=3, seed=None):\n    if not data.select_dtypes(include=[float, int]).shape[1] == data.shape[1]:\n        raise ValueError(\"DataFrame should only contain numeric values.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n    labels = kmeans.fit_predict(data)\n\n    return labels, kmeans\ndata = pd.DataFrame({\n    'a': [1, 20, 2, 22, 100],\n    'b': [1, 20, 2, 22, 100]\n})", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters=3, seed=None):\n    if not data.select_dtypes(include=[float, int]).shape[1] == data.shape[1]:\n        raise ValueError(\"DataFrame should only contain numeric values.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n    labels = kmeans.fit_predict(data)\n\n    return labels, kmeans\n\n# Example usage:\ndata = pd.DataFrame({\n    'a': [1, 20, 2, 22, 100],\n    'b': [1, 20, 2, 22, 100]\n})\n\nlabels, model = task_func(data, seed=213)"}
{"task_id": "BigCodeBench/881", "solution": "import pandas as pd\nimport random\ndef task_func(csv_file, column_name='data', pattern='\\d+[xX]', sample_size=None, seed=42):\n    df = pd.read_csv(csv_file)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n    \n    # Find all matches in the specified column\n    matches = df[column_name].apply(lambda x: [m for m in pattern.findall(x)])\n    \n    # Flatten the list of matches and filter out empty lists\n    all_matches = [m for sublist in matches for m in sublist]\n    \n    if not all_matches:\n        return pd.DataFrame()\n    \n    # Set the random seed for reproducibility\n    random.seed(seed)\n    \n    # Generate a random sample if sample_size is specified\n    if sample_size is not None:\n        sample = random.sample(all_matches, min(sample_size, len(all_matches)))\n    else:\n        sample = all_matches\n    \n    # Create a DataFrame from the sample\n    sample_df = pd.DataFrame(sample, columns=[column_name])\n    \n    return sample_df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(csv_file, column_name='data', pattern='\\d+[xX]', sample_size=None, seed=42):\n    df = pd.read_csv(csv_file)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n    \n    # Find all matches in the specified column\n    matches = df[column_name].apply(lambda x: [m for m in pattern.findall(x)])\n    \n    # Flatten the list of matches and filter out empty lists\n    all_matches = [m for sublist in matches for m in sublist]\n    \n    if not all_matches:\n        return pd.DataFrame()\n    \n    # Set the random seed for reproducibility\n    random.seed(seed)\n    \n    # Generate a random sample if sample_size is specified\n    if sample_size is not None:\n        sample = random.sample(all_matches, min(sample_size, len(all_matches)))\n    else:\n        sample = all_matches\n    \n    # Create a DataFrame from the sample\n    sample_df = pd.DataFrame(sample, columns=[column_name])\n    \n    return sample_df\n\n# Example usage:\n# result = task_func('sample.csv', column_name='data', pattern='\\d+[xX]', sample_size=10, seed=42)\n# print(result)\n```\n"}
{"task_id": "BigCodeBench/882", "solution": "import sqlite3\nimport pandas as pd\nimport os\nimport re\ndef task_func(db_file, table_name, column_name, pattern='\\d+[xX]'):\n    if not os.path.exists(db_file):\n        raise ValueError(f\"The database file {db_file} does not exist.\")\n\n    conn = sqlite3.connect(db_file)\n    query = f\"SELECT * FROM {table_name} WHERE {column_name} REGEXP ?\"\n    df = pd.read_sql_query(query, conn, params=(pattern,))\n    conn.close()\n\n    return df", "raw_solution": "import sqlite3\nimport pandas as pd\nimport os\nimport re\n\ndef task_func(db_file, table_name, column_name, pattern='\\d+[xX]'):\n    if not os.path.exists(db_file):\n        raise ValueError(f\"The database file {db_file} does not exist.\")\n\n    conn = sqlite3.connect(db_file)\n    query = f\"SELECT * FROM {table_name} WHERE {column_name} REGEXP ?\"\n    df = pd.read_sql_query(query, conn, params=(pattern,))\n    conn.close()\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/883", "solution": "import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\ndef task_func(df: pd.DataFrame, column_a: str, column_b: str, column_c: str) -> bool:\n    # Filter the DataFrame based on conditions\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] == 900)]\n\n    # Check if the filtered DataFrame is empty\n    if filtered_df.empty:\n        return False\n\n    # Perform Augmented Dickey-Fuller test on column_a\n    adf_test = adfuller(filtered_df[column_a])\n\n    # Check if p-value is less than 0.05\n    if adf_test[1] < 0.05:\n        return True\n\n    return False\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5, 6],\n    'B': [60, 70, 80, 90, 100, 110],\n    'C': [900, 900, 900, 900, 900, 900]\n})", "raw_solution": "import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\n\ndef task_func(df: pd.DataFrame, column_a: str, column_b: str, column_c: str) -> bool:\n    # Filter the DataFrame based on conditions\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] == 900)]\n\n    # Check if the filtered DataFrame is empty\n    if filtered_df.empty:\n        return False\n\n    # Perform Augmented Dickey-Fuller test on column_a\n    adf_test = adfuller(filtered_df[column_a])\n\n    # Check if p-value is less than 0.05\n    if adf_test[1] < 0.05:\n        return True\n\n    return False\n\n# Example usage:\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5, 6],\n    'B': [60, 70, 80, 90, 100, 110],\n    'C': [900, 900, 900, 900, 900, 900]\n})\n"}
{"task_id": "BigCodeBench/884", "solution": "import pandas as pd\nfrom scipy.stats import chi2_contingency\ndef task_func(df, columns=['A', 'B', 'C'], larger=50, equal=900):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if not all(col in df.columns for col in columns):\n        raise ValueError(\"All specified columns must exist in the DataFrame.\")\n    if len(columns) != 3:\n        raise ValueError(\"Exactly three columns are required for this operation.\")\n    \n    # Filter rows based on conditions\n    filtered_df = df[(df[columns[1]] > larger) & (df[columns[2]] == equal)]\n    \n    if filtered_df.empty:\n        raise ValueError(\"Insufficient data to perform the test.\")\n    \n    # Create contingency table\n    contingency_table = pd.crosstab(filtered_df[columns[0]], filtered_df[columns[1]])\n    \n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    return p", "raw_solution": "import pandas as pd\nfrom scipy.stats import chi2_contingency\n\ndef task_func(df, columns=['A', 'B', 'C'], larger=50, equal=900):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if not all(col in df.columns for col in columns):\n        raise ValueError(\"All specified columns must exist in the DataFrame.\")\n    if len(columns) != 3:\n        raise ValueError(\"Exactly three columns are required for this operation.\")\n    \n    # Filter rows based on conditions\n    filtered_df = df[(df[columns[1]] > larger) & (df[columns[2]] == equal)]\n    \n    if filtered_df.empty:\n        raise ValueError(\"Insufficient data to perform the test.\")\n    \n    # Create contingency table\n    contingency_table = pd.crosstab(filtered_df[columns[0]], filtered_df[columns[1]])\n    \n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    return p\n```\n"}
{"task_id": "BigCodeBench/885", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, col_a='A', col_b='B', col_c='C', seed=None):\n    if not isinstance(df, pd.DataFrame) or df.empty or not all(df.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        return None, None\n\n    if col_a not in df.columns or col_b not in df.columns or col_c not in df.columns:\n        return None, None\n\n    df_filtered = df[(df[col_b] > 50) & (df[col_c] == 900)]\n\n    if df_filtered.empty:\n        return None, None\n\n    X = df_filtered[[col_a]]\n    y = df_filtered[col_b]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    return predictions, model", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, col_a='A', col_b='B', col_c='C', seed=None):\n    if not isinstance(df, pd.DataFrame) or df.empty or not all(df.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        return None, None\n\n    if col_a not in df.columns or col_b not in df.columns or col_c not in df.columns:\n        return None, None\n\n    df_filtered = df[(df[col_b] > 50) & (df[col_c] == 900)]\n\n    if df_filtered.empty:\n        return None, None\n\n    X = df_filtered[[col_a]]\n    y = df_filtered[col_b]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    return predictions, model\n```\n"}
{"task_id": "BigCodeBench/886", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(data):\n    required_keys = {'Name', 'Age', 'Score'}\n    if not required_keys.issubset(data.keys()):\n        raise ValueError(\"Input dictionary must contain 'Name', 'Age', and 'Score' keys.\")\n\n    df = pd.DataFrame(data)\n    df = df.sort_values(by=['Name', 'Age'])\n\n    avg_scores = df.groupby('Name')['Score'].mean()\n\n    age_counter = Counter(df['Age'])\n    common_age = age_counter.most_common(1)[0][0] if age_counter else None\n\n    return df, avg_scores, common_age\ndata = {\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John', 'John', 'Nick', 'Tom', 'John', 'Tom'],\n    'Age': [20, 21, 19, 20, 19, 19, 21, 20, 19, 20],\n    'Score': [85, 79, 92, 88, 90, 92, 81, 86, 90, 85]\n}", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(data):\n    required_keys = {'Name', 'Age', 'Score'}\n    if not required_keys.issubset(data.keys()):\n        raise ValueError(\"Input dictionary must contain 'Name', 'Age', and 'Score' keys.\")\n\n    df = pd.DataFrame(data)\n    df = df.sort_values(by=['Name', 'Age'])\n\n    avg_scores = df.groupby('Name')['Score'].mean()\n\n    age_counter = Counter(df['Age'])\n    common_age = age_counter.most_common(1)[0][0] if age_counter else None\n\n    return df, avg_scores, common_age\n\n# Example usage:\ndata = {\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John', 'John', 'Nick', 'Tom', 'John', 'Tom'],\n    'Age': [20, 21, 19, 20, 19, 19, 21, 20, 19, 20],\n    'Score': [85, 79, 92, 88, 90, 92, 81, 86, 90, 85]\n}\n\ndf, avg_scores, common_age = task_func(data)"}
{"task_id": "BigCodeBench/887", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(T1, row_num=50, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Convert T1 to integers\n    T1 = [tuple(map(int, t)) for t in T1]\n    \n    # Calculate the total number of columns\n    total_cols = sum(len(sublist) for sublist in T1)\n    \n    # Generate random numbers\n    random_numbers = np.random.randint(0, 100, size=(row_num, total_cols))\n    \n    # Create DataFrame\n    df = pd.DataFrame(random_numbers, columns=[f'Col_{i+1}' for i in range(total_cols)])\n    \n    # Assign values from T1 to DataFrame\n    col_index = 0\n    for sublist in T1:\n        for value in sublist:\n            df.iloc[:, col_index] += value\n            col_index += 1\n\n    return df\nT1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\ndef task_func(T1, row_num=50, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Convert T1 to integers\n    T1 = [tuple(map(int, t)) for t in T1]\n    \n    # Calculate the total number of columns\n    total_cols = sum(len(sublist) for sublist in T1)\n    \n    # Generate random numbers\n    random_numbers = np.random.randint(0, 100, size=(row_num, total_cols))\n    \n    # Create DataFrame\n    df = pd.DataFrame(random_numbers, columns=[f'Col_{i+1}' for i in range(total_cols)])\n    \n    # Assign values from T1 to DataFrame\n    col_index = 0\n    for sublist in T1:\n        for value in sublist:\n            df.iloc[:, col_index] += value\n            col_index += 1\n\n    return df\n\n# Example usage:\nT1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\ndf = task_func(T1, row_num=5, seed=2022)"}
{"task_id": "BigCodeBench/888", "solution": "import pandas as pd\nimport os\ndef task_func(data_dir: str, csv_files: list) -> pd.DataFrame:\n    if not csv_files:\n        return pd.DataFrame()\n\n    dataframes = []\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        if os.path.exists(file_path):\n            df = pd.read_csv(file_path)\n            dataframes.append(df)\n        else:\n            print(f\"File {file} does not exist in directory {data_dir}\")\n\n    if dataframes:\n        merged_df = pd.concat(dataframes, ignore_index=True)\n        return merged_df\n    else:\n        return pd.DataFrame()", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(data_dir: str, csv_files: list) -> pd.DataFrame:\n    if not csv_files:\n        return pd.DataFrame()\n\n    dataframes = []\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        if os.path.exists(file_path):\n            df = pd.read_csv(file_path)\n            dataframes.append(df)\n        else:\n            print(f\"File {file} does not exist in directory {data_dir}\")\n\n    if dataframes:\n        merged_df = pd.concat(dataframes, ignore_index=True)\n        return merged_df\n    else:\n        return pd.DataFrame()\n\n# Example usage:\n# df = task_func('/path/to/data/directory', ['file1.csv', 'file2.csv', 'file3.csv'])\n# print(df.head())\n```\n"}
{"task_id": "BigCodeBench/889", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    file_path = os.path.join(data_dir, csv_file)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {csv_file} does not exist in the directory {data_dir}.\")\n\n    df = pd.read_csv(file_path)\n\n    for column in df.select_dtypes(include=[np.number]).columns:\n        mean_value = df[column].mean()\n        df[column].fillna(mean_value, inplace=True)\n\n    return df", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    file_path = os.path.join(data_dir, csv_file)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {csv_file} does not exist in the directory {data_dir}.\")\n\n    df = pd.read_csv(file_path)\n\n    for column in df.select_dtypes(include=[np.number]).columns:\n        mean_value = df[column].mean()\n        df[column].fillna(mean_value, inplace=True)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/890", "solution": "import os\nimport random\nimport pandas as pd\ndef task_func(data_dir,\n              csv_files=['file1.csv', 'file2.csv', 'file3.csv'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Randomly select a CSV file from the list\n    selected_file = random.choice(csv_files)\n    file_path = os.path.join(data_dir, selected_file)\n\n    # Read the selected CSV file\n    try:\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        return selected_file, pd.DataFrame()\n\n    # Return the name of the file and the DataFrame\n    return selected_file, df", "raw_solution": "import os\nimport random\nimport pandas as pd\n\ndef task_func(data_dir,\n              csv_files=['file1.csv', 'file2.csv', 'file3.csv'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Randomly select a CSV file from the list\n    selected_file = random.choice(csv_files)\n    file_path = os.path.join(data_dir, selected_file)\n\n    # Read the selected CSV file\n    try:\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        return selected_file, pd.DataFrame()\n\n    # Return the name of the file and the DataFrame\n    return selected_file, df\n```\n"}
{"task_id": "BigCodeBench/891", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\ndef task_func(csv_file_path, attribute, test_size=0.2, random_state=42):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the attribute exists in the dataset\n    if attribute not in df.columns:\n        raise ValueError(f\"Attribute '{attribute}' does not exist in the dataset.\")\n\n    # Split the dataset into features and target\n    X = df.drop(columns=[attribute])\n    y = df[attribute]\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    predictions = model.predict(X_test)\n\n    return model, predictions", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(csv_file_path, attribute, test_size=0.2, random_state=42):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the attribute exists in the dataset\n    if attribute not in df.columns:\n        raise ValueError(f\"Attribute '{attribute}' does not exist in the dataset.\")\n\n    # Split the dataset into features and target\n    X = df.drop(columns=[attribute])\n    y = df[attribute]\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    predictions = model.predict(X_test)\n\n    return model, predictions\n```\n"}
{"task_id": "BigCodeBench/892", "solution": "import random\nfrom collections import Counter\ndef task_func(strings: list) -> dict:\n    \"\"\"\n    Dee gaee strings kee ek list mein ek khaas pattern ke aane ka vishleshan karta hai aur uskee occurrences ginta hai.\n\n    Params:\n    - strings (list): vishleshan kie jaane wale strings kee ek list.\n\n    Returns:\n        dict: String analysis ke results wala ek dictionary, jismein pattern kee counts dikhtee hain.\n\n    Reqs:\n    - random\n    - collections\n\n    Examples:\n    - >>> task_func(['abcd}def}', 'pqrs}tuv}', 'wxyz}123}', '456}789}', '0ab}cde}'])\n    - counter({2({2: 10})\n\n    \"\"\"\n    # Generate a random pattern\n    pattern = random.choice(strings)\n    \n    # Count occurrences of the pattern in the list of strings\n    pattern_count = Counter(s.count(pattern) for s in strings)\n    \n    return dict(pattern_count)", "raw_solution": "import random\nfrom collections import Counter\n\ndef task_func(strings: list) -> dict:\n    \"\"\"\n    Dee gaee strings kee ek list mein ek khaas pattern ke aane ka vishleshan karta hai aur uskee occurrences ginta hai.\n\n    Params:\n    - strings (list): vishleshan kie jaane wale strings kee ek list.\n\n    Returns:\n        dict: String analysis ke results wala ek dictionary, jismein pattern kee counts dikhtee hain.\n\n    Reqs:\n    - random\n    - collections\n\n    Examples:\n    - >>> task_func(['abcd}def}', 'pqrs}tuv}', 'wxyz}123}', '456}789}', '0ab}cde}'])\n    - counter({2({2: 10})\n\n    \"\"\"\n    # Generate a random pattern\n    pattern = random.choice(strings)\n    \n    # Count occurrences of the pattern in the list of strings\n    pattern_count = Counter(s.count(pattern) for s in strings)\n    \n    return dict(pattern_count)\n```\n"}
{"task_id": "BigCodeBench/893", "solution": "import re\nfrom datetime import datetime, time\ndef task_func(logs: list):\n    error_times = []\n    total_seconds = 0\n    error_count = 0\n\n    for log in logs:\n        match = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) ERROR:', log)\n        if match:\n            error_count += 1\n            log_time = datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S')\n            error_times.append(log_time.time())\n            total_seconds += log_time.hour * 3600 + log_time.minute * 60 + log_time.second\n\n    average_time = time(hour=total_seconds // error_count // 3600, \n                          minute=(total_seconds // error_count // 60) % 60, \n                          second=total_seconds // error_count % 60) if error_count > 0 else None\n\n    return error_times, average_time", "raw_solution": "import re\nfrom datetime import datetime, time\n\ndef task_func(logs: list):\n    error_times = []\n    total_seconds = 0\n    error_count = 0\n\n    for log in logs:\n        match = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) ERROR:', log)\n        if match:\n            error_count += 1\n            log_time = datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S')\n            error_times.append(log_time.time())\n            total_seconds += log_time.hour * 3600 + log_time.minute * 60 + log_time.second\n\n    average_time = time(hour=total_seconds // error_count // 3600, \n                          minute=(total_seconds // error_count // 60) % 60, \n                          second=total_seconds // error_count % 60) if error_count > 0 else None\n\n    return error_times, average_time\n```\n"}
{"task_id": "BigCodeBench/894", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Generate random integers between 1 and 100\n    array = np.random.randint(1, 101, size=ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(array)\n    std = np.std(array)\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=20, color='blue', alpha=0.7, edgecolor='black')\n    \n    # Plot mean and standard deviation lines\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.4f}')\n    ax.axvline(mean + std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean + 1 std: {mean + std:.4f}')\n    ax.axvline(mean - std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean - 1 std: {mean - std:.4f}')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Random Integers Histogram')\n    ax.legend()\n    \n    return array, mean, std, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef task_func():\n    # Generate random integers between 1 and 100\n    array = np.random.randint(1, 101, size=ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(array)\n    std = np.std(array)\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=20, color='blue', alpha=0.7, edgecolor='black')\n    \n    # Plot mean and standard deviation lines\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.4f}')\n    ax.axvline(mean + std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean + 1 std: {mean + std:.4f}')\n    ax.axvline(mean - std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean - 1 std: {mean - std:.4f}')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Random Integers Histogram')\n    ax.legend()\n    \n    return array, mean, std, ax\n```\n"}
{"task_id": "BigCodeBench/895", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Generate random integers between 1 and 100\n    array = np.random.randint(1, 101, size=ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(array)\n    std = np.std(array)\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=20, color='blue', alpha=0.7)\n    \n    # Add mean and standard deviation lines\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.4f}')\n    ax.axvline(mean + std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean + 1 Std: {mean + std:.4f}')\n    ax.axvline(mean - std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean - 1 Std: {mean - std:.4f}')\n    \n    # Set labels and title\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n    ax.set_title('Random Values Distribution')\n    ax.legend()\n    \n    return array, mean, std, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef task_func():\n    # Generate random integers between 1 and 100\n    array = np.random.randint(1, 101, size=ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(array)\n    std = np.std(array)\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=20, color='blue', alpha=0.7)\n    \n    # Add mean and standard deviation lines\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.4f}')\n    ax.axvline(mean + std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean + 1 Std: {mean + std:.4f}')\n    ax.axvline(mean - std, color='purple', linestyle='dashed', linewidth=1, label=f'Mean - 1 Std: {mean - std:.4f}')\n    \n    # Set labels and title\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n    ax.set_title('Random Values Distribution')\n    ax.legend()\n    \n    return array, mean, std, ax\n```\n"}
{"task_id": "BigCodeBench/896", "solution": "from collections import Counter\nimport random\nimport itertools\ndef task_func(length, count, seed=0):\n    if length < 0 or count < 0:\n        raise ValueError(\"Length and count must be non-negative integers.\")\n    \n    random.seed(seed)\n    characters = 'abcde'\n    \n    # Generate random strings\n    random_strings = [''.join(random.choices(characters, k=length)) for _ in range(count)]\n    \n    # Flatten the list of strings into a single iterable\n    all_chars = itertools.chain.from_iterable(random_strings)\n    \n    # Count frequency of each character\n    char_frequency = Counter(all_chars)\n    \n    return char_frequency", "raw_solution": "from collections import Counter\nimport random\nimport itertools\n\ndef task_func(length, count, seed=0):\n    if length < 0 or count < 0:\n        raise ValueError(\"Length and count must be non-negative integers.\")\n    \n    random.seed(seed)\n    characters = 'abcde'\n    \n    # Generate random strings\n    random_strings = [''.join(random.choices(characters, k=length)) for _ in range(count)]\n    \n    # Flatten the list of strings into a single iterable\n    all_chars = itertools.chain.from_iterable(random_strings)\n    \n    # Count frequency of each character\n    char_frequency = Counter(all_chars)\n    \n    return char_frequency\n\n# Test cases"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequency = np.array([results.count(i) for i in NUMBERS])\n\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequency, tick_label=NUMBERS)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequency, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequency = np.array([results.count(i) for i in NUMBERS])\n\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequency, tick_label=NUMBERS)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequency, ax\n```\n"}
{"task_id": "BigCodeBench/898", "solution": "from collections import Counter\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(count, seed=0):\n    random.seed(seed)\n    pairs = []\n\n    for _ in range(count):\n        first_letter = random.choice(LETTERS)\n        second_letter = random.choice(LETTERS)\n        pairs.append((first_letter, second_letter))\n\n    return Counter(pairs)", "raw_solution": "from collections import Counter\nimport random\n\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef task_func(count, seed=0):\n    random.seed(seed)\n    pairs = []\n\n    for _ in range(count):\n        first_letter = random.choice(LETTERS)\n        second_letter = random.choice(LETTERS)\n        pairs.append((first_letter, second_letter))\n\n    return Counter(pairs)\n\n# Test cases"}
{"task_id": "BigCodeBench/899", "solution": "import numpy as np\nimport random\ndef task_func(length=10000, seed=0):\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative integer.\")\n    \n    random.seed(seed)\n    walk = np.zeros(length, dtype=int)\n    \n    for i in range(1, length):\n        step = 1 if random.random() > 0.5 else -1\n        walk[i] = walk[i-1] + step\n\n    return walk", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(length=10000, seed=0):\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative integer.\")\n    \n    random.seed(seed)\n    walk = np.zeros(length, dtype=int)\n    \n    for i in range(1, length):\n        step = 1 if random.random() > 0.5 else -1\n        walk[i] = walk[i-1] + step\n\n    return walk\n\n# Example usage:\n# random.seed(0)     # For reproducibility in doctest\n# walk = task_func(5)\n# print(walk.tolist())\n# Output: [0, 1, 2, 1, 0, 1]\n```\n"}
{"task_id": "BigCodeBench/900", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(d):\n    if not isinstance(d, list) or not all(isinstance(i, dict) for i in d):\n        raise ValueError(\"Input must be a list of dictionaries.\")\n\n    result = {'x': None, 'y': None, 'z': None}\n\n    if d:\n        df = pd.DataFrame(d)\n\n        for col in ['x', 'y', 'z']:\n            if col in df.columns:\n                result[col] = {\n                    'mean': np.mean(df[col]),\n                    'sum': np.sum(df[col]),\n                    'max': np.max(df[col]),\n                    'min': np.min(df[col]),\n                    'std': np.std(df[col])\n                }\n\n    return result", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(d):\n    if not isinstance(d, list) or not all(isinstance(i, dict) for i in d):\n        raise ValueError(\"Input must be a list of dictionaries.\")\n\n    result = {'x': None, 'y': None, 'z': None}\n\n    if d:\n        df = pd.DataFrame(d)\n\n        for col in ['x', 'y', 'z']:\n            if col in df.columns:\n                result[col] = {\n                    'mean': np.mean(df[col]),\n                    'sum': np.sum(df[col]),\n                    'max': np.max(df[col]),\n                    'min': np.min(df[col]),\n                    'std': np.std(df[col])\n                }\n\n    return result\n\n# Test cases\ndata = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]"}
{"task_id": "BigCodeBench/901", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n\n    scaled_values = scaler.fit_transform(df[['x', 'y', 'z']])\n    df_scaled = pd.DataFrame(scaled_values, columns=['x', 'y', 'z'])\n\n    return df_scaled", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n\n    scaled_values = scaler.fit_transform(df[['x', 'y', 'z']])\n    df_scaled = pd.DataFrame(scaled_values, columns=['x', 'y', 'z'])\n\n    return df_scaled\n\n# Test cases\ndata = [{'x': -1, 'y': 0, 'z': 5}, {'x': 3, 'y': -15, 'z': 0}, {'x': 0, 'y': 1, 'z': -7}]"}
{"task_id": "BigCodeBench/902", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(d):\n    result = {'x': Counter(), 'y': Counter(), 'z': Counter()}\n    \n    for item in d:\n        for key in result.keys():\n            if key in item:\n                result[key][item[key]] += 1\n    \n    return result", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(d):\n    result = {'x': Counter(), 'y': Counter(), 'z': Counter()}\n    \n    for item in d:\n        for key in result.keys():\n            if key in item:\n                result[key][item[key]] += 1\n    \n    return result\n\n# Test cases\ndata1 = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 5}, {'x': 2, 'y': 1, 'z': 7}]\ndata2 = [{'x': 2, 'y': 10}, {'y': 15, 'z': 5}, {'x': 2, 'z': 7}]\n"}
{"task_id": "BigCodeBench/903", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(d, target='z'):\n    df = pd.DataFrame(d)\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\ndata = [{'x': 4, 'y': 20, 'z': 10}, {'x': 5, 'y': 25, 'z': 15}, {'x': 6, 'y': 5, 'z': 20}]", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(d, target='z'):\n    df = pd.DataFrame(d)\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\n\n# Test cases\ndata = [{'x': 4, 'y': 20, 'z': 10}, {'x': 5, 'y': 25, 'z': 15}, {'x': 6, 'y': 5, 'z': 20}]\nmodel = task_func(data, target='y')"}
{"task_id": "BigCodeBench/904", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(d, keys=['x', 'y', 'z']):\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(d)\n\n    # Plot the data\n    ax = df.plot(x=keys[0], y=keys[1], kind='scatter', label=keys[1])\n\n    # Add labels and title\n    ax.set_xlabel(keys[0])\n    ax.set_ylabel(keys[1])\n    ax.set_title('Scatter Plot of {} vs {}'.format(keys[0], keys[1]))\n\n    # Return the Axes object\n    return ax\ndata = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(d, keys=['x', 'y', 'z']):\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(d)\n\n    # Plot the data\n    ax = df.plot(x=keys[0], y=keys[1], kind='scatter', label=keys[1])\n\n    # Add labels and title\n    ax.set_xlabel(keys[0])\n    ax.set_ylabel(keys[1])\n    ax.set_title('Scatter Plot of {} vs {}'.format(keys[0], keys[1]))\n\n    # Return the Axes object\n    return ax\n\n# Example usage:\ndata = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]\nax = task_func(data)\nplt.show()\n```\nThis script will create a scatter plot of the values in the 'x' and 'y' columns of the input data, and return the Matplotlib Axes object for further manipulation if needed.\n"}
{"task_id": "BigCodeBench/905", "solution": "import os\nimport glob\nimport csv\ndef task_func(directory_path, file_extension='.csv'):\n    data = {}\n    # Find all files with the given extension in the directory\n    for file_path in glob.glob(os.path.join(directory_path, f'*{file_extension}')):\n        # Extract the file name without extension\n        file_name = os.path.splitext(os.path.basename(file_path))[0]\n        # Read the CSV file\n        with open(file_path, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            # Convert the reader to a list and add to the dictionary\n            data[file_name] = list(reader)\n    return data", "raw_solution": "import os\nimport glob\nimport csv\n\ndef task_func(directory_path, file_extension='.csv'):\n    data = {}\n    # Find all files with the given extension in the directory\n    for file_path in glob.glob(os.path.join(directory_path, f'*{file_extension}')):\n        # Extract the file name without extension\n        file_name = os.path.splitext(os.path.basename(file_path))[0]\n        # Read the CSV file\n        with open(file_path, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            # Convert the reader to a list and add to the dictionary\n            data[file_name] = list(reader)\n    return data\n\n# Example usage:\n# data = task_func('/home/user/data')\n# print(data)\n```\n"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n\n    # List all files in the source directory\n    files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n\n    # Create a list of files to be archived\n    files_to_archive = []\n    for file in files:\n        # Check if the file name contains '_processed'\n        if re.search(r'_processed', file):\n            files_to_archive.append(os.path.join(source_dir, file))\n\n    # Create the archive file\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        for file in files_to_archive:\n            archive.write(file, os.path.basename(file))\n\n    return archive_path", "raw_solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n\n    # List all files in the source directory\n    files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n\n    # Create a list of files to be archived\n    files_to_archive = []\n    for file in files:\n        # Check if the file name contains '_processed'\n        if re.search(r'_processed', file):\n            files_to_archive.append(os.path.join(source_dir, file))\n\n    # Create the archive file\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        for file in files_to_archive:\n            archive.write(file, os.path.basename(file))\n\n    return archive_path\n```\n"}
{"task_id": "BigCodeBench/907", "solution": "import os\nimport re\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    \"\"\"\n    This function renames all files in a given directory whose names match a specified pattern, replacing the pattern with a new string.\n\n    Parameters:\n    - pattern (str): The pattern to search for in filenames.\n    - replacement (str): The string to replace the pattern with.\n    - directory (str): The directory to search for files.\n\n    Returns:\n    - A boolean value indicating whether the operation was successful. If the operation is successful, it returns True, otherwise it returns False.\n\n    Requirements:\n    - re\n    - os\n\n    Examples:\n    - >>> task_func('draft', 'final', '/home/user/documents')\n    - True\n    - >>> task_func('tmp', 'temp', '/home/user/downloads')\n    - False\n    \"\"\"\n    try:\n        for filename in os.listdir(directory):\n            if re.search(pattern, filename):\n                new_filename = re.sub(pattern, replacement, filename)\n                os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False", "raw_solution": "import os\nimport re\n\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    \"\"\"\n    This function renames all files in a given directory whose names match a specified pattern, replacing the pattern with a new string.\n\n    Parameters:\n    - pattern (str): The pattern to search for in filenames.\n    - replacement (str): The string to replace the pattern with.\n    - directory (str): The directory to search for files.\n\n    Returns:\n    - A boolean value indicating whether the operation was successful. If the operation is successful, it returns True, otherwise it returns False.\n\n    Requirements:\n    - re\n    - os\n\n    Examples:\n    - >>> task_func('draft', 'final', '/home/user/documents')\n    - True\n    - >>> task_func('tmp', 'temp', '/home/user/downloads')\n    - False\n    \"\"\"\n    try:\n        for filename in os.listdir(directory):\n            if re.search(pattern, filename):\n                new_filename = re.sub(pattern, replacement, filename)\n                os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```\n"}
{"task_id": "BigCodeBench/908", "solution": "import os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\ndef task_func(directory: str, pattern: str) -> list:\n    axes_list = []\n    for filename in os.listdir(directory):\n        if re.match(pattern, filename):\n            file_path = os.path.join(directory, filename)\n            df = pd.read_csv(file_path)\n            ax = df.plot(x='Month', y='sales', title=filename)\n            axes_list.append(ax)\n    return axes_list", "raw_solution": "import os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\ndef task_func(directory: str, pattern: str) -> list:\n    axes_list = []\n    for filename in os.listdir(directory):\n        if re.match(pattern, filename):\n            file_path = os.path.join(directory, filename)\n            df = pd.read_csv(file_path)\n            ax = df.plot(x='Month', y='sales', title=filename)\n            axes_list.append(ax)\n    return axes_list\n```\n"}
{"task_id": "BigCodeBench/909", "solution": "import pandas as pd\nimport itertools\nfrom random import shuffle\ndef task_func(letters=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], categories=['Category 1', 'Category 2', 'Category 3']):\n    # Create all possible combinations of letters and categories\n    combinations = list(itertools.product(letters, categories))\n    \n    # Shuffle the combinations to randomize the assignment\n    shuffle(combinations)\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Letter', 'Category'])\n    \n    return df", "raw_solution": "import pandas as pd\nimport itertools\nfrom random import shuffle\n\ndef task_func(letters=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], categories=['Category 1', 'Category 2', 'Category 3']):\n    # Create all possible combinations of letters and categories\n    combinations = list(itertools.product(letters, categories))\n    \n    # Shuffle the combinations to randomize the assignment\n    shuffle(combinations)\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Letter', 'Category'])\n    \n    return df\n\n# Example usage:\n# df = task_func(['A', 'B'], ['Cat 1', 'Cat 2'])\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/910", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(letters, repetitions, colors):\n    if len(letters) != len(repetitions) or len(letters) != len(colors):\n        raise ValueError(\"All input lists must have the same length.\")\n    if not letters or not repetitions or not colors:\n        raise ValueError(\"Input lists cannot be empty.\")\n\n    # Calculate the frequency of each letter\n    frequency = np.array(repetitions)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(letters, frequency, color=colors)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Letters')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(letters, repetitions, colors):\n    if len(letters) != len(repetitions) or len(letters) != len(colors):\n        raise ValueError(\"All input lists must have the same length.\")\n    if not letters or not repetitions or not colors:\n        raise ValueError(\"Input lists cannot be empty.\")\n\n    # Calculate the frequency of each letter\n    frequency = np.array(repetitions)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(letters, frequency, color=colors)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Letters')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/911", "solution": "from functools import reduce\nimport operator\nimport string\ndef task_func(letters):\n    \"\"\"\n    Uppercase letters ki list ke liye sambaddhit numbers ka *product* gyaat karen,\n    jahan \\\"A\\\" ka matlab 1 hai, \\\"B\\\" ka matlab 2 hai, aadi.\n    >>> task_func([\\\"A\\\", \\\"E\\\", \\\"I\\\"])\n    45\n\n    Notes:\n        Function pratyek uppercase letter ko uske sambaddhit *number* se map karne ke liye ek purvanirdharit *dictionary* ka upyog karta hai.\n\n    Params:\n    - letters (list of str): Ek uppercase letters ki list.\n\n    Returns:\n        int: Input *letters* se sambaddhit *numbers* ka *product*.\n\n    Reqs:\n    - functools.reduce\n    - operator\n    - string\n\n    Examples:\n    - Examples:\n    - >>> task_func([\\\"A\\\", \\\"B\\\", \\\"C\\\"])\n    - 6\n\n    \"\"\"\n    # Create a dictionary mapping letters to their corresponding numbers\n    letter_to_number = {letter: index + 1 for index, letter in enumerate(string.ascii_uppercase)}\n    \n    # Map each letter in the input list to its corresponding number\n    numbers = [letter_to_number[letter] for letter in letters]\n    \n    # Calculate the product of all numbers using reduce and operator.mul\n    product = reduce(operator.mul, numbers, 1)\n    \n    return product", "raw_solution": "from functools import reduce\nimport operator\nimport string\n\ndef task_func(letters):\n    \"\"\"\n    Uppercase letters ki list ke liye sambaddhit numbers ka *product* gyaat karen,\n    jahan \\\"A\\\" ka matlab 1 hai, \\\"B\\\" ka matlab 2 hai, aadi.\n    >>> task_func([\\\"A\\\", \\\"E\\\", \\\"I\\\"])\n    45\n\n    Notes:\n        Function pratyek uppercase letter ko uske sambaddhit *number* se map karne ke liye ek purvanirdharit *dictionary* ka upyog karta hai.\n\n    Params:\n    - letters (list of str): Ek uppercase letters ki list.\n\n    Returns:\n        int: Input *letters* se sambaddhit *numbers* ka *product*.\n\n    Reqs:\n    - functools.reduce\n    - operator\n    - string\n\n    Examples:\n    - Examples:\n    - >>> task_func([\\\"A\\\", \\\"B\\\", \\\"C\\\"])\n    - 6\n\n    \"\"\"\n    # Create a dictionary mapping letters to their corresponding numbers\n    letter_to_number = {letter: index + 1 for index, letter in enumerate(string.ascii_uppercase)}\n    \n    # Map each letter in the input list to its corresponding number\n    numbers = [letter_to_number[letter] for letter in letters]\n    \n    # Calculate the product of all numbers using reduce and operator.mul\n    product = reduce(operator.mul, numbers, 1)\n    \n    return product\n\n# Test cases"}
{"task_id": "BigCodeBench/912", "solution": "from collections import Counter\nimport itertools\ndef task_func(letters: list, repetitions: int) -> dict:\n    \"\"\"\n    Dee gaee number mein doharae jaane ke baad, ek list mein pratyek akshar kee frequency ginen.\n\n    Params:\n    - letters (list): Aksharon ka pratinidhitv karane vaalee single-character strings kee ek list.\n    - repetitions (int): List ko repetitions kee number.\n\n    Returns:\n        Return mein ek dictionary hota hai jahaan keys akshar hote hain aur maan unakee frequencies hotee hain.\n\n    Reqs:\n    - collections.Counter\n    - itertools\n\n    Examples:\n    - >>> task_func(['A', 'B', 'C'], 2)\n    - {'A': 2, 'B': 2, 'C': 2}\n    - >>> task_func(['A', 'B'], 3)\n    - {'A': 3, 'B': 3}\n\n    \"\"\"\n    # Repeat the list for the given number of repetitions\n    repeated_letters = list(itertools.chain.from_iterable([letters] * repetitions))\n    \n    # Count the frequency of each letter\n    letter_counts = Counter(repeated_letters)\n    \n    return dict(letter_counts)", "raw_solution": "from collections import Counter\nimport itertools\n\ndef task_func(letters: list, repetitions: int) -> dict:\n    \"\"\"\n    Dee gaee number mein doharae jaane ke baad, ek list mein pratyek akshar kee frequency ginen.\n\n    Params:\n    - letters (list): Aksharon ka pratinidhitv karane vaalee single-character strings kee ek list.\n    - repetitions (int): List ko repetitions kee number.\n\n    Returns:\n        Return mein ek dictionary hota hai jahaan keys akshar hote hain aur maan unakee frequencies hotee hain.\n\n    Reqs:\n    - collections.Counter\n    - itertools\n\n    Examples:\n    - >>> task_func(['A', 'B', 'C'], 2)\n    - {'A': 2, 'B': 2, 'C': 2}\n    - >>> task_func(['A', 'B'], 3)\n    - {'A': 3, 'B': 3}\n\n    \"\"\"\n    # Repeat the list for the given number of repetitions\n    repeated_letters = list(itertools.chain.from_iterable([letters] * repetitions))\n    \n    # Count the frequency of each letter\n    letter_counts = Counter(repeated_letters)\n    \n    return dict(letter_counts)\n```\n"}
{"task_id": "BigCodeBench/913", "solution": "from typing import List, Union\nimport numpy as np\nimport scipy.fft\ndef task_func(data: List[Union[int, str]], repetitions: int = 1):\n    if not data or repetitions <= 0:\n        return {'mode': np.array([]), 'count': np.array([]), 'fft': np.array([])}\n\n    # Convert all elements to string for uniformity\n    data = [str(item) for item in data]\n\n    # Repeat the data for the specified number of times\n    repeated_data = data * repetitions\n\n    # Calculate the mode and count of each element\n    unique, counts = np.unique(repeated_data, return_counts=True)\n    mode = unique[np.argmax(counts)]\n    count = np.max(counts)\n\n    # Perform Fast Fourier Transform\n    fft_result = scipy.fft.fft(repeated_data)\n\n    return {'mode': np.array([mode]), 'count': np.array([count]), 'fft': fft_result}", "raw_solution": "from typing import List, Union\nimport numpy as np\nimport scipy.fft\n\ndef task_func(data: List[Union[int, str]], repetitions: int = 1):\n    if not data or repetitions <= 0:\n        return {'mode': np.array([]), 'count': np.array([]), 'fft': np.array([])}\n\n    # Convert all elements to string for uniformity\n    data = [str(item) for item in data]\n\n    # Repeat the data for the specified number of times\n    repeated_data = data * repetitions\n\n    # Calculate the mode and count of each element\n    unique, counts = np.unique(repeated_data, return_counts=True)\n    mode = unique[np.argmax(counts)]\n    count = np.max(counts)\n\n    # Perform Fast Fourier Transform\n    fft_result = scipy.fft.fft(repeated_data)\n\n    return {'mode': np.array([mode]), 'count': np.array([count]), 'fft': fft_result}\n```\n"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Convert date to timestamp\n    df['timestamp'] = (df['date'] - pd.to_datetime('1970-01-01')).dt.total_seconds()\n\n    # Prepare data for regression\n    X = df[['timestamp']].values\n    y = df['closing_price'].values\n\n    # Create and train the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predict prices for the next 7 days\n    future_timestamps = np.array([df['timestamp'].max() + 86400 * i for i in range(7)]).reshape(-1, 1)\n    pred_prices = model.predict(future_timestamps)\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['date'], df['closing_price'], color='blue', label='Historical Data')\n    plt.plot(pd.to_datetime(future_timestamps.flatten(), unit='s'), pred_prices, color='red', label='Predicted Data')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Price Prediction')\n    plt.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return pred_prices, plt.gca()", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Convert date to timestamp\n    df['timestamp'] = (df['date'] - pd.to_datetime('1970-01-01')).dt.total_seconds()\n\n    # Prepare data for regression\n    X = df[['timestamp']].values\n    y = df['closing_price'].values\n\n    # Create and train the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predict prices for the next 7 days\n    future_timestamps = np.array([df['timestamp'].max() + 86400 * i for i in range(7)]).reshape(-1, 1)\n    pred_prices = model.predict(future_timestamps)\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['date'], df['closing_price'], color='blue', label='Historical Data')\n    plt.plot(pd.to_datetime(future_timestamps.flatten(), unit='s'), pred_prices, color='red', label='Predicted Data')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Price Prediction')\n    plt.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return pred_prices, plt.gca()\n```\n"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    # Ensure the dataframe has a 'closing_price' column\n    if 'closing_price' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'closing_price' column.\")\n    \n    # Calculate Z-Scores\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    # Plot the outliers\n    fig, ax = plt.subplots()\n    ax.scatter(outliers.index, outliers['closing_price'], color='red')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    return outliers, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    # Ensure the dataframe has a 'closing_price' column\n    if 'closing_price' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'closing_price' column.\")\n    \n    # Calculate Z-Scores\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    # Plot the outliers\n    fig, ax = plt.subplots()\n    ax.scatter(outliers.index, outliers['closing_price'], color='red')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    return outliers, ax\n```\n"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    if 'closing_price' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'closing_price' column.\")\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Box plot\n    sns.boxplot(x=df['closing_price'], ax=ax1)\n    ax1.set_title('Box plot of closing prices')\n\n    # Histogram\n    sns.histplot(df['closing_price'], bins=10, ax=ax2, kde=True)\n    ax2.set_title('Histogram of closing prices')\n\n    plt.tight_layout()\n    return ax1, ax2", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    if 'closing_price' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'closing_price' column.\")\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Box plot\n    sns.boxplot(x=df['closing_price'], ax=ax1)\n    ax1.set_title('Box plot of closing prices')\n\n    # Histogram\n    sns.histplot(df['closing_price'], bins=10, ax=ax2, kde=True)\n    ax2.set_title('Histogram of closing prices')\n\n    plt.tight_layout()\n    return ax1, ax2\n\n# Example usage:\n# df = pd.DataFrame({'closing_price': [100, 101, 102, 103, 104, 150]})\n# boxplot_ax, histplot_ax = task_func(df)\n# print(boxplot_ax.get_title())  # Box Plot of Closing Prices\n# print(histplot_ax.get_title())  # Histogram of Closing Prices\n```\n"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame has the correct columns and types\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df['closing_price'] = df['closing_price'].astype(float)\n\n    # Sort the DataFrame by date\n    df = df.sort_values('date')\n\n    # Fit the ARIMA model\n    model = ARIMA(df['closing_price'], order=(1, 1, 1))\n    model_fit = model.fit()\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot the original data and the forecast\n    fig, ax = plt.subplots()\n    ax.plot(df['date'], df['closing_price'], label='Original Data')\n    ax.plot(pd.date_range(start=df['date'].iloc[-1], periods=7, freq='D'), forecast, label='Forecast', linestyle='--')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('ARIMA Forecast')\n    ax.legend()\n\n    return forecast.tolist(), ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame has the correct columns and types\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df['closing_price'] = df['closing_price'].astype(float)\n\n    # Sort the DataFrame by date\n    df = df.sort_values('date')\n\n    # Fit the ARIMA model\n    model = ARIMA(df['closing_price'], order=(1, 1, 1))\n    model_fit = model.fit()\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot the original data and the forecast\n    fig, ax = plt.subplots()\n    ax.plot(df['date'], df['closing_price'], label='Original Data')\n    ax.plot(pd.date_range(start=df['date'].iloc[-1], periods=7, freq='D'), forecast, label='Forecast', linestyle='--')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('ARIMA Forecast')\n    ax.legend()\n\n    return forecast.tolist(), ax\n```\n"}
{"task_id": "BigCodeBench/918", "solution": "import pandas as pd\nimport re\ndef task_func(data, mapping):\n    df = pd.DataFrame(data)\n    for col in df.columns:\n        df[col] = df[col].apply(lambda x: re.sub(r'\\b(' + '|'.join(mapping.keys()) + r')\\b', lambda m: mapping[m.group(0)], x))\n    return df\ndata = {'text': ['NASA is great', 'I live in the USA']}\nmapping = {'NASA': 'National Aeronautics and Space Administration', 'USA': 'United States of America'}", "raw_solution": "import pandas as pd\nimport re\n\n# Function to replace acronyms in DataFrame\ndef task_func(data, mapping):\n    df = pd.DataFrame(data)\n    for col in df.columns:\n        df[col] = df[col].apply(lambda x: re.sub(r'\\b(' + '|'.join(mapping.keys()) + r')\\b', lambda m: mapping[m.group(0)], x))\n    return df\n\n# Example usage:\ndata = {'text': ['NASA is great', 'I live in the USA']}\nmapping = {'NASA': 'National Aeronautics and Space Administration', 'USA': 'United States of America'}"}
{"task_id": "BigCodeBench/919", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, column):\n    df = pd.DataFrame(data)\n    categories = ['A', 'B', 'C', 'D', 'E']\n    \n    # Count occurrences of each category\n    counts = df[column].value_counts()\n    \n    # Fill missing categories with 0\n    counts = counts.reindex(categories, fill_value=0)\n    \n    # Plotting\n    ax = counts.plot(kind='bar', color='skyblue')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of {column}')\n    \n    return ax\ndata = {'Category': ['A', 'B', 'B', 'C', 'A', 'D', 'E', 'E', 'D']}", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column):\n    df = pd.DataFrame(data)\n    categories = ['A', 'B', 'C', 'D', 'E']\n    \n    # Count occurrences of each category\n    counts = df[column].value_counts()\n    \n    # Fill missing categories with 0\n    counts = counts.reindex(categories, fill_value=0)\n    \n    # Plotting\n    ax = counts.plot(kind='bar', color='skyblue')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of {column}')\n    \n    return ax\n\n# Example usage:\ndata = {'Category': ['A', 'B', 'B', 'C', 'A', 'D', 'E', 'E', 'D']}\nax = task_func(data, 'Category')\nplt.show()\n```\n"}
{"task_id": "BigCodeBench/920", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    df = pd.DataFrame(data)\n    corr = df.corr()\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n    ax.set_title('Correlation Matrix')\n\n    return ax\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    df = pd.DataFrame(data)\n    corr = df.corr()\n\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n    ax.set_title('Correlation Matrix')\n\n    return ax\n\n# Example usage:\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\nax = task_func(data)\nplt.show()\n```\nThis script imports the necessary libraries, defines a function that takes a dictionary of data, converts it into a pandas DataFrame, calculates the correlation matrix, and then plots a heatmap of the correlation matrix using seaborn. The heatmap is annotated with the correlation values and has a 'coolwarm' color map. The function returns the matplotlib axes object for further manipulation if needed.\n"}
{"task_id": "BigCodeBench/921", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data, columns):\n    df = pd.DataFrame(data)\n    scaler = MinMaxScaler()\n    df[columns] = scaler.fit_transform(df[columns])\n    return df\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data, columns):\n    df = pd.DataFrame(data)\n    scaler = MinMaxScaler()\n    df[columns] = scaler.fit_transform(df[columns])\n    return df\n\n# Example usage:\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}\nnormalized_df = task_func(data, ['a', 'b'])"}
{"task_id": "BigCodeBench/922", "solution": "import pandas as pd\nimport re\nSTOPWORDS = set([\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n    \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n    \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n    \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n    \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\",\n    \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\",\n    \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n    \"don\", \"should\", \"now\"\n])\ndef task_func(data, column):\n    df = pd.DataFrame(data)\n    df[column] = df[column].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in STOPWORDS]))\n    return df", "raw_solution": "import pandas as pd\nimport re\n\n# Constants\nSTOPWORDS = set([\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n    \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n    \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n    \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n    \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\",\n    \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\",\n    \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n    \"don\", \"should\", \"now\"\n])\n\ndef task_func(data, column):\n    df = pd.DataFrame(data)\n    df[column] = df[column].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in STOPWORDS]))\n    return df\n```\n"}
{"task_id": "BigCodeBench/923", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(person_names, email_domains, num_records=5):\n    if len(person_names) < num_records or not email_domains:\n        raise ValueError(\"Insufficient names or no email domains provided.\")\n\n    data = []\n    for _ in range(num_records):\n        name = random.choice(person_names)\n        domain = random.choice(email_domains)", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(person_names, email_domains, num_records=5):\n    if len(person_names) < num_records or not email_domains:\n        raise ValueError(\"Insufficient names or no email domains provided.\")\n\n    data = []\n    for _ in range(num_records):\n        name = random.choice(person_names)\n        domain = random.choice(email_domains)\n        email = f\"{re.sub(r'[^\\w\\s]', '', name.lower())}[at]{domain}\"\n        data.append({'Name': name, 'Email': email})\n\n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\nrandom.seed(0)  # For reproducibility"}
{"task_id": "BigCodeBench/924", "solution": "import pandas as pd\nimport os\nimport sys\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Ek csv file ko Pandas DataFrame mein load karein, string '\n' ki sabhi occurrences ko string '<br>' se badlein\n    specified column mein, and saaf kiya gaya DataFrame lautaayein.\n\n    Params:\n    - file_path (str): csv file ka path jise padha jaana hai.\n    - column_name (str): Us column ka naam jismein '\n' ki occurrences ko '<br>' se badalna hai.\n\n    Returns:\n        pd.DataFrame: Saaf kiya gaya Pandas DataFrame.\n\n    Reqs:\n    - pandas\n    - os\n    - sys\n\n    Examples:\n    - Examples:\n    - >>> df = task_func('data.csv', 'value')\n    - >>> print(df['value'].iloc[0])\n    - \"some<br>text\"\n    - >>> df = task_func('another_data.csv', 'comments')\n    - >>> print(df['comments'].iloc[1])\n    - \"hello<br>world\"\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    df = pd.read_csv(file_path)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n    \n    df[column_name] = df[column_name].apply(lambda x: str(x).replace(\"'\", \"<br>\"))\n    \n    return df", "raw_solution": "import pandas as pd\nimport os\nimport sys\n\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Ek csv file ko Pandas DataFrame mein load karein, string '\n' ki sabhi occurrences ko string '<br>' se badlein\n    specified column mein, and saaf kiya gaya DataFrame lautaayein.\n\n    Params:\n    - file_path (str): csv file ka path jise padha jaana hai.\n    - column_name (str): Us column ka naam jismein '\n' ki occurrences ko '<br>' se badalna hai.\n\n    Returns:\n        pd.DataFrame: Saaf kiya gaya Pandas DataFrame.\n\n    Reqs:\n    - pandas\n    - os\n    - sys\n\n    Examples:\n    - Examples:\n    - >>> df = task_func('data.csv', 'value')\n    - >>> print(df['value'].iloc[0])\n    - \"some<br>text\"\n    - >>> df = task_func('another_data.csv', 'comments')\n    - >>> print(df['comments'].iloc[1])\n    - \"hello<br>world\"\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    df = pd.read_csv(file_path)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n    \n    df[column_name] = df[column_name].apply(lambda x: str(x).replace(\"'\", \"<br>\"))\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/925", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_size=1000, column_names=['A', 'B', 'C', 'D', 'E'], seed=0):\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(data_size, len(column_names)))\n    df = pd.DataFrame(data, columns=column_names)\n\n    # Replace all occurrences of values less than 10 with -1\n    df[df < 10] = -1\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data_size=1000, column_names=['A', 'B', 'C', 'D', 'E'], seed=0):\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(data_size, len(column_names)))\n    df = pd.DataFrame(data, columns=column_names)\n\n    # Replace all occurrences of values less than 10 with -1\n    df[df < 10] = -1\n\n    return df\n\n# Example usage:\n# df = task_func(data_size=100, column_names=['X', 'Y', 'Z'], seed=42)\n# print(df.shape)  # Output: (100, 3)\n```\n"}
{"task_id": "BigCodeBench/926", "solution": "import pandas as pd\nimport sqlite3\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_path)\n    \n    # Load data from the specified table into a Pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Close the database connection\n    conn.close()\n    \n    # Replace newline characters with HTML line breaks\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    \n    return df", "raw_solution": "import pandas as pd\nimport sqlite3\n\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_path)\n    \n    # Load data from the specified table into a Pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Close the database connection\n    conn.close()\n    \n    # Replace newline characters with HTML line breaks\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    \n    return df\n\n# Example usage:\n# df = task_func('./data.db', 'messages', 'content')\n# print(df.loc[0, 'content'])  # Output: 'Hello<br>World'\n```\n"}
{"task_id": "BigCodeBench/927", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace all occurrences of ' ' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace(' ', '<br>')\n\n    # Initialize the LabelEncoder\n    le = LabelEncoder()\n\n    # Encode the specified column\n    df[column_name] = le.fit_transform(df[column_name])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace all occurrences of ' ' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace(' ', '<br>')\n\n    # Initialize the LabelEncoder\n    le = LabelEncoder()\n\n    # Encode the specified column\n    df[column_name] = le.fit_transform(df[column_name])\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations from the input word\n    two_letter_combinations = [''.join(combination) for combination in itertools.permutations(word, 2)]\n\n    # Count the occurrences of each two-letter combination in the input word\n    combination_counter = Counter(two_letter_combinations)\n\n    # Create a dictionary with the combinations as keys and their counts as values\n    combination_dict = {combination: combination_counter.get(combination, 0) for combination in two_letter_combinations}\n\n    return combination_dict", "raw_solution": "from collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations from the input word\n    two_letter_combinations = [''.join(combination) for combination in itertools.permutations(word, 2)]\n\n    # Count the occurrences of each two-letter combination in the input word\n    combination_counter = Counter(two_letter_combinations)\n\n    # Create a dictionary with the combinations as keys and their counts as values\n    combination_dict = {combination: combination_counter.get(combination, 0) for combination in two_letter_combinations}\n\n    return combination_dict\n\n# Example usage:\n# print(task_func('abcdef'))\n```\n"}
{"task_id": "BigCodeBench/929", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(word: str) -> np.ndarray:\n    \"\"\"\n    Input word mein adjacent letters ke pratyek pair ke ASCII maanon ke beech ka difference calculate karein.\n    Difference ki ganana karne ke baad, differences ki entropy calculate karein.\n\n    Params:\n    - word (str): Input word ek string ke roop mein.\n\n    Returns:\n        np.ndarray: Ek numpy array jismein word mein adjacent letters ke pratyek pair ke ASCII maanon ke beech ka difference shaamil hai.\n        float: Differences ki entropy.\n\n    Reqs:\n    - numpy\n    - scipy.stats\n\n    Examples:\n    - Examples:\n    - >>> task_func('abcdef')\n    - (array([1, 1, 1, 1, 1]), 1.6094379124341005)\n    - >>> task_func('hello')\n    - (array([-3,  7,  0,  3]), -inf)\n\n    \"\"\"\n    # Calculate ASCII differences\n    ascii_diffs = np.diff(np.array([ord(c) for c in word]))\n    \n    # Calculate entropy\n    entropy = stats.entropy(ascii_diffs, base=2)\n    \n    return ascii_diffs, entropy", "raw_solution": "import numpy as np\nfrom scipy import stats\n\ndef task_func(word: str) -> np.ndarray:\n    \"\"\"\n    Input word mein adjacent letters ke pratyek pair ke ASCII maanon ke beech ka difference calculate karein.\n    Difference ki ganana karne ke baad, differences ki entropy calculate karein.\n\n    Params:\n    - word (str): Input word ek string ke roop mein.\n\n    Returns:\n        np.ndarray: Ek numpy array jismein word mein adjacent letters ke pratyek pair ke ASCII maanon ke beech ka difference shaamil hai.\n        float: Differences ki entropy.\n\n    Reqs:\n    - numpy\n    - scipy.stats\n\n    Examples:\n    - Examples:\n    - >>> task_func('abcdef')\n    - (array([1, 1, 1, 1, 1]), 1.6094379124341005)\n    - >>> task_func('hello')\n    - (array([-3,  7,  0,  3]), -inf)\n\n    \"\"\"\n    # Calculate ASCII differences\n    ascii_diffs = np.diff(np.array([ord(c) for c in word]))\n    \n    # Calculate entropy\n    entropy = stats.entropy(ascii_diffs, base=2)\n    \n    return ascii_diffs, entropy\n\n# Test cases"}
{"task_id": "BigCodeBench/930", "solution": "import random\nimport string\ndef task_func(word):\n    if not word.isalpha():\n        raise ValueError(\"Input should only contain letters.\")\n\n    pairs = []\n    for i in range(len(word) - 1):\n        pairs.append(word[i:i+2])\n\n    random_pairs = random.sample(pairs, len(pairs))\n\n    return random_pairs", "raw_solution": "import random\nimport string\n\nPOSSIBLE_LETTERS = ['a', 'b', 'c']\ndef task_func(word):\n    if not word.isalpha():\n        raise ValueError(\"Input should only contain letters.\")\n\n    pairs = []\n    for i in range(len(word) - 1):\n        pairs.append(word[i:i+2])\n\n    random_pairs = random.sample(pairs, len(pairs))\n\n    return random_pairs\n\n# Test cases"}
{"task_id": "BigCodeBench/931", "solution": "from collections import defaultdict\nimport re\ndef task_func(word: str) -> dict:\n    \"\"\"\n    saaf word mein pratyek do-letter combinations kee occurrences khojen\n    jahaan only varnamala ke characters par vichaar kiya jaata hai.\n\n    Params:\n    - word (str): input string.\n\n    Returns:\n        collections.defaultdict: ek dictionary jiski keys do-letter combinations hain aur values unki saaf word mein counts hain.\n\n    Reqs:\n    - collections.defaultdict\n    - re\n\n    Examples:\n    - >>> task_func('abcdef')\n    - defaultdict(<class 'int'>, {'ab': 1, 'bc': 1, 'cd': 1, 'de': 1, 'ef': 1})\n    - >>> task_func('aabbcc')\n    - defaultdict(<class 'int'>, {'aa': 1, 'ab': 1, 'bb': 1, 'bc': 1, 'cc': 1})\n    - >>> task_func('a1!b@c#d$')\n    - defaultdict(<class 'int'>, {'ab': 1, 'bc': 1, 'cd': 1})\n\n    \"\"\"\n    # Filter only alphabetic characters\n    filtered_word = re.sub(r'[^a-zA-Z]', '', word)\n    \n    # Create a defaultdict to store combinations and their counts\n    combinations = defaultdict(int)\n    \n    # Iterate through the word to find all 2-letter combinations\n    for i in range(len(filtered_word) - 1):\n        combination = filtered_word[i:i+2].lower()\n        combinations[combination] += 1\n    \n    return combinations", "raw_solution": "from collections import defaultdict\nimport re\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    saaf word mein pratyek do-letter combinations kee occurrences khojen\n    jahaan only varnamala ke characters par vichaar kiya jaata hai.\n\n    Params:\n    - word (str): input string.\n\n    Returns:\n        collections.defaultdict: ek dictionary jiski keys do-letter combinations hain aur values unki saaf word mein counts hain.\n\n    Reqs:\n    - collections.defaultdict\n    - re\n\n    Examples:\n    - >>> task_func('abcdef')\n    - defaultdict(<class 'int'>, {'ab': 1, 'bc': 1, 'cd': 1, 'de': 1, 'ef': 1})\n    - >>> task_func('aabbcc')\n    - defaultdict(<class 'int'>, {'aa': 1, 'ab': 1, 'bb': 1, 'bc': 1, 'cc': 1})\n    - >>> task_func('a1!b@c#d$')\n    - defaultdict(<class 'int'>, {'ab': 1, 'bc': 1, 'cd': 1})\n\n    \"\"\"\n    # Filter only alphabetic characters\n    filtered_word = re.sub(r'[^a-zA-Z]', '', word)\n    \n    # Create a defaultdict to store combinations and their counts\n    combinations = defaultdict(int)\n    \n    # Iterate through the word to find all 2-letter combinations\n    for i in range(len(filtered_word) - 1):\n        combination = filtered_word[i:i+2].lower()\n        combinations[combination] += 1\n    \n    return combinations\n```\n"}
{"task_id": "BigCodeBench/932", "solution": "from collections import Counter\nimport re\ndef task_func(word: str) -> list:\n    # Remove non-alphabetic characters\n    cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n    \n    # Check if the word is too short\n    if len(cleaned_word) < 2:\n        return []\n\n    # Find all 2-letter combinations\n    combinations = [cleaned_word[i:i+2].lower() for i in range(len(cleaned_word) - 1)]\n\n    # Count the frequency of each combination\n    combination_counts = Counter(combinations)\n\n    # Find the most common combination\n    most_common_combination = combination_counts.most_common(1)\n\n    # Return the most common combination and its frequency\n    return [most_common_combination[0]] if most_common_combination else []", "raw_solution": "from collections import Counter\nimport re\n\ndef task_func(word: str) -> list:\n    # Remove non-alphabetic characters\n    cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n    \n    # Check if the word is too short\n    if len(cleaned_word) < 2:\n        return []\n\n    # Find all 2-letter combinations\n    combinations = [cleaned_word[i:i+2].lower() for i in range(len(cleaned_word) - 1)]\n\n    # Count the frequency of each combination\n    combination_counts = Counter(combinations)\n\n    # Find the most common combination\n    most_common_combination = combination_counts.most_common(1)\n\n    # Return the most common combination and its frequency\n    return [most_common_combination[0]] if most_common_combination else []\n\n# Test cases"}
{"task_id": "BigCodeBench/933", "solution": "import string\nimport wordninja\ndef task_func(word):\n    \"\"\"\n    Ek word ko tuples ki ek list mein badalta hai, jismein pratyek tuple mein word ka ek chhota letter aur alphabet mein uski position shaamil hoti hai.\n    Phir, diye gaye word ko words ki ek list mein vibhaajit karein.\n\n    Params:\n    - word (str): Ek chhota letter se bana string.\n\n    Returns:\n        tuples ki list: Pratyek tuple mein input string ka ek letter aur alphabet mein uski sangat position hoti hai.\n\n    Reqs:\n    - string\n    - wordninja\n\n    Examples:\n    - udaaharan:\n    - >>> task_func('abc')\n    - ([('a', 1), ('b', 2), ('c', 3)], ['abc'])\n    - >>> task_func('howistheweathertoday')\n    - ([('h', 8), ('o', 15), ('w', 23), ('i', 9), ('s', 19), ('t', 20), ('h', 8), ('e', 5), ('w', 23), ('e', 5), ('a', 1), ('t', 20), ('h', 8), ('e', 5), ('r', 18), ('t', 20), ('o', 15), ('d', 4), ('a', 1), ('y', 25)], ['how', 'is', 'the', 'weather', 'today'])\n\n    \"\"\"\n    # Create a list of tuples with each letter and its position\n    letter_positions = [(letter, word.index(letter) + 1) for letter in word]\n\n    # Split the word into words using wordninja\n    words = wordninja.split(word)\n\n    return letter_positions, words", "raw_solution": "import string\nimport wordninja\n\ndef task_func(word):\n    \"\"\"\n    Ek word ko tuples ki ek list mein badalta hai, jismein pratyek tuple mein word ka ek chhota letter aur alphabet mein uski position shaamil hoti hai.\n    Phir, diye gaye word ko words ki ek list mein vibhaajit karein.\n\n    Params:\n    - word (str): Ek chhota letter se bana string.\n\n    Returns:\n        tuples ki list: Pratyek tuple mein input string ka ek letter aur alphabet mein uski sangat position hoti hai.\n\n    Reqs:\n    - string\n    - wordninja\n\n    Examples:\n    - udaaharan:\n    - >>> task_func('abc')\n    - ([('a', 1), ('b', 2), ('c', 3)], ['abc'])\n    - >>> task_func('howistheweathertoday')\n    - ([('h', 8), ('o', 15), ('w', 23), ('i', 9), ('s', 19), ('t', 20), ('h', 8), ('e', 5), ('w', 23), ('e', 5), ('a', 1), ('t', 20), ('h', 8), ('e', 5), ('r', 18), ('t', 20), ('o', 15), ('d', 4), ('a', 1), ('y', 25)], ['how', 'is', 'the', 'weather', 'today'])\n\n    \"\"\"\n    # Create a list of tuples with each letter and its position\n    letter_positions = [(letter, word.index(letter) + 1) for letter in word]\n\n    # Split the word into words using wordninja\n    words = wordninja.split(word)\n\n    return letter_positions, words\n\n# Test cases"}
{"task_id": "BigCodeBench/934", "solution": "from collections import Counter\nimport hashlib\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Ek shabd mein baaeein se daaeein aur letters ke pratyek adjacent pair kee occurrence karen aur result ko MD5 hash ke roop mein encode karen.\n\n    Params:\n    - word (str): Vah shabd jisamen adjacent akshar jode kee occurrence karnee hai.\n\n    Returns:\n        dict: Ek dictionary jahaan keys adjacent akshar jode hain aur maan unakee occurrence hain.\n\n    Reqs:\n    - collections.Counter\n\n    Examples:\n    - Examples:\n    - >>> task_func('abracadabra')\n    - 'bc9af285d87b312e61ab3661e66b741b'\n    - >>> task_func('hello')\n    - 'dd5dec1a853625e2dc48f3d42665c337'\n\n    \"\"\"\n    # Create a Counter object to count occurrences of adjacent pairs\n    counter = Counter(word[i:i+2] for i in range(len(word) - 1))\n    \n    # Convert the Counter object to a dictionary\n    result = dict(counter)\n    \n    # Convert the dictionary to a string and encode it to bytes\n    result_str = str(result)\n    result_bytes = result_str.encode('utf-8')\n    \n    # Create an MD5 hash of the encoded string\n    md5_hash = hashlib.md5(result_bytes).hexdigest()\n    \n    return md5_hash", "raw_solution": "from collections import Counter\nimport hashlib\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Ek shabd mein baaeein se daaeein aur letters ke pratyek adjacent pair kee occurrence karen aur result ko MD5 hash ke roop mein encode karen.\n\n    Params:\n    - word (str): Vah shabd jisamen adjacent akshar jode kee occurrence karnee hai.\n\n    Returns:\n        dict: Ek dictionary jahaan keys adjacent akshar jode hain aur maan unakee occurrence hain.\n\n    Reqs:\n    - collections.Counter\n\n    Examples:\n    - Examples:\n    - >>> task_func('abracadabra')\n    - 'bc9af285d87b312e61ab3661e66b741b'\n    - >>> task_func('hello')\n    - 'dd5dec1a853625e2dc48f3d42665c337'\n\n    \"\"\"\n    # Create a Counter object to count occurrences of adjacent pairs\n    counter = Counter(word[i:i+2] for i in range(len(word) - 1))\n    \n    # Convert the Counter object to a dictionary\n    result = dict(counter)\n    \n    # Convert the dictionary to a string and encode it to bytes\n    result_str = str(result)\n    result_bytes = result_str.encode('utf-8')\n    \n    # Create an MD5 hash of the encoded string\n    md5_hash = hashlib.md5(result_bytes).hexdigest()\n    \n    return md5_hash\n\n# Test cases"}
{"task_id": "BigCodeBench/935", "solution": "import pandas as pd\nimport string\ndef task_func(word):\n    if not word.islower() or not word.isalpha():\n        raise ValueError(\"Word should be lowercase and contain only alphabetic characters.\")\n    \n    # Create a dictionary to map each letter to its position in the alphabet\n    letter_positions = {letter: index + 1 for index, letter in enumerate(string.ascii_lowercase)}\n    \n    # Create a DataFrame from the word\n    data = {'letter': list(word), 'position': [letter_positions[letter] for letter in word]}\n    df = pd.DataFrame(data)\n    \n    return df", "raw_solution": "import pandas as pd\nimport string\n\ndef task_func(word):\n    if not word.islower() or not word.isalpha():\n        raise ValueError(\"Word should be lowercase and contain only alphabetic characters.\")\n    \n    # Create a dictionary to map each letter to its position in the alphabet\n    letter_positions = {letter: index + 1 for index, letter in enumerate(string.ascii_lowercase)}\n    \n    # Create a DataFrame from the word\n    data = {'letter': list(word), 'position': [letter_positions[letter] for letter in word]}\n    df = pd.DataFrame(data)\n    \n    return df\n\n# Test cases"}
{"task_id": "BigCodeBench/936", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport string\nALPHABET = list(string.ascii_lowercase)\ndef task_func(word):\n    \"\"\"\n    Diye gaye word mein har letter ki position ko darshaane waala ek bar chart banaata hai.\n    Numpy aur matplotlib.pyplot ka istemaal karte hue.\n    Constants:\n    - ALPHABET: english alphabet ke sabhi lowercase letters ki ek list.\n\n    Notes:\n        function har letter ki english alphabet mein index ka istemaal uski position ko darshaane ke liye karta hai.\n        example ke liye, 'a' ko 1 se, 'b' ko 2 se, aur so represent kiya jaata hai.\n\n    Params:\n    - word (str): Wo word jiski letters ki position ko plot kiya jaaega.\n    - Sirf lowercase alphabetic characters hona chahiye.\n\n    Returns:\n        axes: Banaye gaye plot ko darshaane waala ek matplotlib.axes._axes.axes object.\n\n    Reqs:\n    - numpy\n    - matplotlib.pyplot\n\n    Examples:\n    - Examples:\n    - >>> ax = task_func('abc')\n    - >>> ax = task_func('hello')\n\n    \"\"\"\n    # Initialize an array of zeros with length equal to the length of the alphabet\n    positions = np.zeros(len(ALPHABET))\n\n    # Iterate over each character in the word\n    for char in word:\n        # Find the index of the character in the alphabet\n        index = ALPHABET.index(char)\n        # Increment the count for that index\n        positions[index] += 1\n\n    # Create a bar chart of the positions\n    fig, ax = plt.subplots()\n    ax.bar(ALPHABET, positions, color='blue')\n    ax.set_xlabel('Alphabet')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of each letter in the word')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport string\n\n# Constants\nALPHABET = list(string.ascii_lowercase)\n\ndef task_func(word):\n    \"\"\"\n    Diye gaye word mein har letter ki position ko darshaane waala ek bar chart banaata hai.\n    Numpy aur matplotlib.pyplot ka istemaal karte hue.\n    Constants:\n    - ALPHABET: english alphabet ke sabhi lowercase letters ki ek list.\n\n    Notes:\n        function har letter ki english alphabet mein index ka istemaal uski position ko darshaane ke liye karta hai.\n        example ke liye, 'a' ko 1 se, 'b' ko 2 se, aur so represent kiya jaata hai.\n\n    Params:\n    - word (str): Wo word jiski letters ki position ko plot kiya jaaega.\n    - Sirf lowercase alphabetic characters hona chahiye.\n\n    Returns:\n        axes: Banaye gaye plot ko darshaane waala ek matplotlib.axes._axes.axes object.\n\n    Reqs:\n    - numpy\n    - matplotlib.pyplot\n\n    Examples:\n    - Examples:\n    - >>> ax = task_func('abc')\n    - >>> ax = task_func('hello')\n\n    \"\"\"\n    # Initialize an array of zeros with length equal to the length of the alphabet\n    positions = np.zeros(len(ALPHABET))\n\n    # Iterate over each character in the word\n    for char in word:\n        # Find the index of the character in the alphabet\n        index = ALPHABET.index(char)\n        # Increment the count for that index\n        positions[index] += 1\n\n    # Create a bar chart of the positions\n    fig, ax = plt.subplots()\n    ax.bar(ALPHABET, positions, color='blue')\n    ax.set_xlabel('Alphabet')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of each letter in the word')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/937", "solution": "import re\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove non-alphanumeric characters\n    cleaned_str = re.sub(r'\\W+', '', input_str)\n    \n    # Convert to lowercase\n    cleaned_str = cleaned_str.lower()\n    \n    # Count frequency of each character\n    char_count = Counter(cleaned_str)\n    \n    return char_count", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(input_str):\n    # Remove non-alphanumeric characters\n    cleaned_str = re.sub(r'\\W+', '', input_str)\n    \n    # Convert to lowercase\n    cleaned_str = cleaned_str.lower()\n    \n    # Count frequency of each character\n    char_count = Counter(cleaned_str)\n    \n    return char_count\n\n# Test cases"}
{"task_id": "BigCodeBench/938", "solution": "import re\nimport pandas as pd\ndef task_func(input_df):\n    \"\"\"\n    Ek pandas dataframe column 'text' mein maujood text ko saaf karta hai, sabhi vishesh characters, punctuation marks, aur spaces ko hatakar, phir saaf kiye gaye text ki length ki ganana karta hai.\n\n    Params:\n    - input_df (pandas.dataframe): Ek dataframe jismein 'text' naam ka ek column hai jismein alphanumeric and/or special characters ke strings shaamil hain.\n\n    Returns:\n        pandas.dataframe: Ek dataframe jismein do naye columns 'clean_text' aur 'text_length' honge, jahan 'clean_text' saaf kiya gaya text hai aur 'text_length' uski length hai.\n\n    Reqs:\n    - re\n    - pandas\n\n    Examples:\n    - Examples:\n    - >>> df = pd.dataframe({'text': ['Special $#! characters   spaces 888323']})\n    - >>> print(task_func(df))\n    - clean_text  text_length\n    - 0  Specialcharactersspaces888323           29\n    - >>> df = pd.dataframe({'text': ['Hello, World!']})\n    - >>> print(task_func(df))\n    - clean_text  text_length\n    - 0  HelloWorld           10\n\n    \"\"\"\n    # Remove all non-alphanumeric characters and spaces\n    input_df['clean_text'] = input_df['text'].apply(lambda x: re.sub(r'\\W+', '', x))\n    # Calculate the length of the cleaned text\n    input_df['text_length'] = input_df['clean_text'].apply(len)\n    \n    return input_df[['clean_text', 'text_length']]", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(input_df):\n    \"\"\"\n    Ek pandas dataframe column 'text' mein maujood text ko saaf karta hai, sabhi vishesh characters, punctuation marks, aur spaces ko hatakar, phir saaf kiye gaye text ki length ki ganana karta hai.\n\n    Params:\n    - input_df (pandas.dataframe): Ek dataframe jismein 'text' naam ka ek column hai jismein alphanumeric and/or special characters ke strings shaamil hain.\n\n    Returns:\n        pandas.dataframe: Ek dataframe jismein do naye columns 'clean_text' aur 'text_length' honge, jahan 'clean_text' saaf kiya gaya text hai aur 'text_length' uski length hai.\n\n    Reqs:\n    - re\n    - pandas\n\n    Examples:\n    - Examples:\n    - >>> df = pd.dataframe({'text': ['Special $#! characters   spaces 888323']})\n    - >>> print(task_func(df))\n    - clean_text  text_length\n    - 0  Specialcharactersspaces888323           29\n    - >>> df = pd.dataframe({'text': ['Hello, World!']})\n    - >>> print(task_func(df))\n    - clean_text  text_length\n    - 0  HelloWorld           10\n\n    \"\"\"\n    # Remove all non-alphanumeric characters and spaces\n    input_df['clean_text'] = input_df['text'].apply(lambda x: re.sub(r'\\W+', '', x))\n    # Calculate the length of the cleaned text\n    input_df['text_length'] = input_df['clean_text'].apply(len)\n    \n    return input_df[['clean_text', 'text_length']]\n```\n"}
{"task_id": "BigCodeBench/939", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path: str) -> list:\n    # Get all files in the directory\n    files = glob.glob(os.path.join(dir_path, '*'))\n\n    # List to store cleaned file names\n    cleaned_files = []\n\n    for file in files:\n        # Extract the file name\n        file_name = os.path.basename(file)\n\n        # Remove special characters, whitespace, and non-alphanumeric characters\n        cleaned_name = re.sub(r'[^a-zA-Z0-9]', '', file_name)\n\n        # Add cleaned file name to the list\n        cleaned_files.append(cleaned_name)\n\n    return cleaned_files", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(dir_path: str) -> list:\n    # Get all files in the directory\n    files = glob.glob(os.path.join(dir_path, '*'))\n\n    # List to store cleaned file names\n    cleaned_files = []\n\n    for file in files:\n        # Extract the file name\n        file_name = os.path.basename(file)\n\n        # Remove special characters, whitespace, and non-alphanumeric characters\n        cleaned_name = re.sub(r'[^a-zA-Z0-9]', '', file_name)\n\n        # Add cleaned file name to the list\n        cleaned_files.append(cleaned_name)\n\n    return cleaned_files\n\n# Example usage:\n# print(task_func('path/to/directory'))\n```\n"}
{"task_id": "BigCodeBench/940", "solution": "import re\nfrom nltk import word_tokenize\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove special characters, punctuation marks, and spaces\n    cleaned_str = re.sub(r'[^\\w\\s]', '', input_str)\n    \n    # Tokenize the cleaned string into words\n    words = word_tokenize(cleaned_str)\n    \n    # Count the frequency of each word\n    word_count = Counter(words)\n    \n    return word_count", "raw_solution": "import re\nfrom nltk import word_tokenize\nfrom collections import Counter\n\ndef task_func(input_str):\n    # Remove special characters, punctuation marks, and spaces\n    cleaned_str = re.sub(r'[^\\w\\s]', '', input_str)\n    \n    # Tokenize the cleaned string into words\n    words = word_tokenize(cleaned_str)\n    \n    # Count the frequency of each word\n    word_count = Counter(words)\n    \n    return word_count\n\n# Example usage:\n# print(task_func('Special $#! characters   spaces 888323'))\n# Output: Counter({'Special': 1, 'characters': 1, 'spaces': 1, '888323': 1})\n```\n"}
{"task_id": "BigCodeBench/941", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_date: str, periods: int, freq: str, random_seed: int = 0) -> (pd.DataFrame, plt.Axes):\n    np.random.seed(random_seed)\n    \n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random sales data\n    sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date: str, periods: int, freq: str, random_seed: int = 0) -> (pd.DataFrame, plt.Axes):\n    np.random.seed(random_seed)\n    \n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random sales data\n    sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Ek samay avadhi mein vibhinn category ke liye bikri report banaen aur visualize Karen.\n\n    Params:\n    - start_date (str): Report ke liye 'YYYY-MM-DD' str mein start_date. Default '2016-01-01' hai.\n    - periods (int): Report ke liye periods ki number. Default 13 hai.\n    - freq (str): Utpann hone wali dateyon ki frequency. Default 'WOM-2FRI' (WeekOfMonth-2nd Friday) hai.\n    - categories (list): Report mein shaamil karne ke liye category ki list. Default ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports'] hai.\n\n    Returns:\n        Nimnlikhit columns ke saath bikri data wala ek DataFrame lautata hai: 'Date', 'Category', 'Sales'.\n        Plot ke liye Matplotlib Axes object lautata hai.\n\n    Reqs:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Examples:\n    - >>> df, ax = task_func(start_date='2020-01-01', periods=5, freq='W-MON', categories=['Electronics', 'Fashion'])\n    - >>> df\n    - Date     Category  Sales\n    - 0 2020-01-06  Electronics    272\n    - 1 2020-01-06      Fashion    147\n    - 2 2020-01-13  Electronics    217\n    - 3 2020-01-13      Fashion    292\n    - 4 2020-01-20  Electronics    423\n    - 5 2020-01-20      Fashion    351\n    - 6 2020-01-27  Electronics    295\n    - 7 2020-01-27      Fashion    459\n    - 8 2020-02-03  Electronics    109\n    - 9 2020-02-03      Fashion    311\n\n    \"\"\"\n    # Generate dates based on frequency\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with the given categories and random sales data\n    data = {\n        'Date': dates,\n        'Category': np.random.choice(categories, size=periods),\n        'Sales': np.random.randint(100, 1000, size=periods)\n    }\n    df = pd.DataFrame(data)\n    \n    # Plot the sales data\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Data by Category')\n    ax.legend()\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Ek samay avadhi mein vibhinn category ke liye bikri report banaen aur visualize Karen.\n\n    Params:\n    - start_date (str): Report ke liye 'YYYY-MM-DD' str mein start_date. Default '2016-01-01' hai.\n    - periods (int): Report ke liye periods ki number. Default 13 hai.\n    - freq (str): Utpann hone wali dateyon ki frequency. Default 'WOM-2FRI' (WeekOfMonth-2nd Friday) hai.\n    - categories (list): Report mein shaamil karne ke liye category ki list. Default ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports'] hai.\n\n    Returns:\n        Nimnlikhit columns ke saath bikri data wala ek DataFrame lautata hai: 'Date', 'Category', 'Sales'.\n        Plot ke liye Matplotlib Axes object lautata hai.\n\n    Reqs:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Examples:\n    - >>> df, ax = task_func(start_date='2020-01-01', periods=5, freq='W-MON', categories=['Electronics', 'Fashion'])\n    - >>> df\n    - Date     Category  Sales\n    - 0 2020-01-06  Electronics    272\n    - 1 2020-01-06      Fashion    147\n    - 2 2020-01-13  Electronics    217\n    - 3 2020-01-13      Fashion    292\n    - 4 2020-01-20  Electronics    423\n    - 5 2020-01-20      Fashion    351\n    - 6 2020-01-27  Electronics    295\n    - 7 2020-01-27      Fashion    459\n    - 8 2020-02-03  Electronics    109\n    - 9 2020-02-03      Fashion    311\n\n    \"\"\"\n    # Generate dates based on frequency\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with the given categories and random sales data\n    data = {\n        'Date': dates,\n        'Category': np.random.choice(categories, size=periods),\n        'Sales': np.random.randint(100, 1000, size=periods)\n    }\n    df = pd.DataFrame(data)\n    \n    # Plot the sales data\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Data by Category')\n    ax.legend()\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/943", "solution": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(start_date='2016-01-01', periods=24, freq='M', model='additive'):\n    # Generate a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random sales data\n    np.random.seed(0)\n    sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Sales': sales_data}, index=dates)\n    \n    # Perform seasonal decomposition\n    decomposition = seasonal_decompose(df['Sales'], model=model)\n    \n    # Extract trend, seasonal, and residual components\n    result = {\n        'trend': decomposition.trend,\n        'seasonal': decomposition.seasonal,\n        'residual': decomposition.resid\n    }\n    \n    return result", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(start_date='2016-01-01', periods=24, freq='M', model='additive'):\n    # Generate a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random sales data\n    np.random.seed(0)\n    sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Sales': sales_data}, index=dates)\n    \n    # Perform seasonal decomposition\n    decomposition = seasonal_decompose(df['Sales'], model=model)\n    \n    # Extract trend, seasonal, and residual components\n    result = {\n        'trend': decomposition.trend,\n        'seasonal': decomposition.seasonal,\n        'residual': decomposition.resid\n    }\n    \n    return result\n\n# Example usage\nresult = task_func('2016-01-01', 24, 'M', 'additive')\nassert len(result['seasonal']) == 24\nassert all(key in result for key in ['trend', 'seasonal', 'residual'])\n```\n"}
{"task_id": "BigCodeBench/944", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    np.random.seed(seed)\n    \n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random prices\n    prices = np.random.uniform(100, 500, size=periods)\n    \n    # Create DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Price': prices})\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Price'], marker='o')\n    ax.set_title('Stock Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    np.random.seed(seed)\n    \n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random prices\n    prices = np.random.uniform(100, 500, size=periods)\n    \n    # Create DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Price': prices})\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Price'], marker='o')\n    ax.set_title('Stock Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Generate sales data if not provided\n    if sales_data is None:\n        sales_data = np.random.randint(100, 500, size=periods)\n\n    # Create DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n\n    # Prepare data for regression\n    X = np.arange(len(df)).reshape(-1, 1)\n    y = df['Sales'].values\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predict future sales\n    future_dates = np.arange(len(df), len(df) + periods).reshape(-1, 1)\n    future_sales = model.predict(future_dates)\n\n    return future_sales", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Generate sales data if not provided\n    if sales_data is None:\n        sales_data = np.random.randint(100, 500, size=periods)\n\n    # Create DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n\n    # Prepare data for regression\n    X = np.arange(len(df)).reshape(-1, 1)\n    y = df['Sales'].values\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predict future sales\n    future_dates = np.arange(len(df), len(df) + periods).reshape(-1, 1)\n    future_sales = model.predict(future_dates)\n\n    return future_sales\n```\n"}
{"task_id": "BigCodeBench/946", "solution": "import numpy as np\nimport pandas as pd\nimport random\ndef task_func(rows=3, cols=2, min_val=0, max_val=100, seed=0):\n    \"\"\"\n    Ek *range* mein *random* *integers* ke saath specified *dimensions* ka ek *matrix* banata hai, aur phir ise ek *pandas* DataFrame mein badal deta hai.\n\n    Params:\n    - rows (int): *Matrix* mein rows ki *number*. *Default* 3 hai.\n    - cols (int): *Matrix* mein *columns* ki *number*. *Default* 2 hai.\n    - min_val (int): *Random* *integers* ke liye *minimum* *integer* *value*. *Default* 0 hai.\n    - max_val (int): *Random* *integers* ke liye *maximum* *integer* *value*. *Default* 100 hai.\n\n    Returns:\n        DataFrame: Ek *pandas* DataFrame jismein specified *range* ke bheetar *random* *integers* shaamil hain.\n\n    Reqs:\n    - numpy\n    - pandas\n    - random\n\n    Examples:\n    - >>> df = task_func(3, 2, 0, 100)\n    - >>> print(type(df))\n    - <class 'pandas.core.frame.DataFrame'>\n    - >>> print(df.shape)\n    - (3, 2)\n\n    \"\"\"\n    random.seed(seed)\n    data = np.random.randint(min_val, max_val, size=(rows, cols))\n    df = pd.DataFrame(data, columns=[f'col_{i}' for i in range(cols)])\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nimport random\n\ndef task_func(rows=3, cols=2, min_val=0, max_val=100, seed=0):\n    \"\"\"\n    Ek *range* mein *random* *integers* ke saath specified *dimensions* ka ek *matrix* banata hai, aur phir ise ek *pandas* DataFrame mein badal deta hai.\n\n    Params:\n    - rows (int): *Matrix* mein rows ki *number*. *Default* 3 hai.\n    - cols (int): *Matrix* mein *columns* ki *number*. *Default* 2 hai.\n    - min_val (int): *Random* *integers* ke liye *minimum* *integer* *value*. *Default* 0 hai.\n    - max_val (int): *Random* *integers* ke liye *maximum* *integer* *value*. *Default* 100 hai.\n\n    Returns:\n        DataFrame: Ek *pandas* DataFrame jismein specified *range* ke bheetar *random* *integers* shaamil hain.\n\n    Reqs:\n    - numpy\n    - pandas\n    - random\n\n    Examples:\n    - >>> df = task_func(3, 2, 0, 100)\n    - >>> print(type(df))\n    - <class 'pandas.core.frame.DataFrame'>\n    - >>> print(df.shape)\n    - (3, 2)\n\n    \"\"\"\n    random.seed(seed)\n    data = np.random.randint(min_val, max_val, size=(rows, cols))\n    df = pd.DataFrame(data, columns=[f'col_{i}' for i in range(cols)])\n    return df\n```\n"}
{"task_id": "BigCodeBench/947", "solution": "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Calculate the total number of unique dates needed\n    total_dates = rows * columns\n\n    # Generate a list of unique dates between start_date and end_date\n    date_range = (end_date - start_date).days\n    unique_dates = random.sample(range(date_range), total_dates)\n    unique_dates = [start_date + timedelta(days=date) for date in unique_dates]\n\n    # Create a matrix with the unique dates\n    matrix = np.array(unique_dates[:rows*columns]).reshape(rows, columns)\n\n    # Convert datetime objects to strings\n    matrix = np.array(matrix, dtype=object)\n    matrix = np.array([[d.strftime('%Y-%m-%dT%H:%M:%S.%f') for d in row] for row in matrix])\n\n    return matrix", "raw_solution": "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Calculate the total number of unique dates needed\n    total_dates = rows * columns\n\n    # Generate a list of unique dates between start_date and end_date\n    date_range = (end_date - start_date).days\n    unique_dates = random.sample(range(date_range), total_dates)\n    unique_dates = [start_date + timedelta(days=date) for date in unique_dates]\n\n    # Create a matrix with the unique dates\n    matrix = np.array(unique_dates[:rows*columns]).reshape(rows, columns)\n\n    # Convert datetime objects to strings\n    matrix = np.array(matrix, dtype=object)\n    matrix = np.array([[d.strftime('%Y-%m-%dT%H:%M:%S.%f') for d in row] for row in matrix])\n\n    return matrix\n\n# Example usage:\nmatrix = task_func(2, 2, datetime(2021, 1, 1), datetime(2021, 1, 10))"}
{"task_id": "BigCodeBench/948", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler()\n    scaled_matrix = scaler.fit_transform(matrix)\n    return scaled_matrix", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler()\n    scaled_matrix = scaler.fit_transform(matrix)\n    return scaled_matrix\n\n# Test cases"}
{"task_id": "BigCodeBench/949", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(rows, columns, seed=None):\n    \"\"\"\n    Ek DataFrame ko ek specified range ke bheetar random values ke saath utpann karen.\n    Yah function diye gaye dimensions ka ek matrix banaata hai jo 0 aur 1 ke beech random values se bhara hota hai aur ise Pandas DataFrame ke roop mein lautaata hai. users ke paas punah pesh karne yogya results ke liye ek random beej set karne ka option hota hai.\n\n    Params:\n    - rows (int): Matrix ke liye rows ki number.\n    - columns (int): Matrix ke liye column ki number.\n    - seed (int, optional): Random number generator ke liye beej. Default None hai.\n\n    Returns:\n        DataFrame: Ek Pandas DataFrame jismein utpann random maan shaamil hain.\n\n    Reqs:\n    - numpy\n    - pandas\n\n    Examples:\n    - Examples:\n    - >>> df = task_func(3, 2, seed=42)\n    - >>> print(df.shape)\n    - (3, 2)\n    - >>> df = task_func(1, 1, seed=24)\n    - >>> print(df.shape)\n    - (1, 1)\n\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate a random matrix of 0s and 1s\n    matrix = np.random.randint(0, 2, size=(rows, columns))\n    \n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(matrix, columns=[f'col_{i}' for i in range(columns)])\n    \n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(rows, columns, seed=None):\n    \"\"\"\n    Ek DataFrame ko ek specified range ke bheetar random values ke saath utpann karen.\n    Yah function diye gaye dimensions ka ek matrix banaata hai jo 0 aur 1 ke beech random values se bhara hota hai aur ise Pandas DataFrame ke roop mein lautaata hai. users ke paas punah pesh karne yogya results ke liye ek random beej set karne ka option hota hai.\n\n    Params:\n    - rows (int): Matrix ke liye rows ki number.\n    - columns (int): Matrix ke liye column ki number.\n    - seed (int, optional): Random number generator ke liye beej. Default None hai.\n\n    Returns:\n        DataFrame: Ek Pandas DataFrame jismein utpann random maan shaamil hain.\n\n    Reqs:\n    - numpy\n    - pandas\n\n    Examples:\n    - Examples:\n    - >>> df = task_func(3, 2, seed=42)\n    - >>> print(df.shape)\n    - (3, 2)\n    - >>> df = task_func(1, 1, seed=24)\n    - >>> print(df.shape)\n    - (1, 1)\n\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate a random matrix of 0s and 1s\n    matrix = np.random.randint(0, 2, size=(rows, columns))\n    \n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(matrix, columns=[f'col_{i}' for i in range(columns)])\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/950", "solution": "import numpy as np\nfrom scipy.linalg import svd\ndef task_func(rows=3, columns=2, seed=0):\n    \"\"\"\n    Generate a random matrix and perform singular value decomposition (SVD).\n\n    Params:\n    - rows (int): Number of rows in the random matrix. Default is 3.\n    - columns (int): Number of columns in the random matrix. Default is 2.\n    - seed (int, optional): Seed for the random number generator for reproducibility. Default is 0.\n\n    Returns:\n        tuple: A tuple containing three numpy arrays:\n        U (ndarray): Unitary matrix U.\n        s (ndarray): Singular values, sorted in descending order.\n        Vh (ndarray): Conjugate transpose of the unitary matrix V.\n\n    Reqs:\n    - numpy\n    - scipy.linalg.svd\n\n    Examples:\n    - >>> U, s, Vh = task_func(3, 2, seed=42)\n    - >>> print('U shape:', U.shape)\n    - U shape: (3, 3)\n    - >>> print('s shape:', s.shape)\n    - s shape: (2,)\n    - >>> print('Vh shape:', Vh.shape)\n    - Vh shape: (2, 2)\n\n    \"\"\"\n    np.random.seed(seed)\n    A = np.random.rand(rows, columns)\n    U, s, Vh = svd(A)\n    s = np.sort(s)[::-1]  # Sort singular values in descending order\n    return U, s, Vh", "raw_solution": "import numpy as np\nfrom scipy.linalg import svd\n\ndef task_func(rows=3, columns=2, seed=0):\n    \"\"\"\n    Generate a random matrix and perform singular value decomposition (SVD).\n\n    Params:\n    - rows (int): Number of rows in the random matrix. Default is 3.\n    - columns (int): Number of columns in the random matrix. Default is 2.\n    - seed (int, optional): Seed for the random number generator for reproducibility. Default is 0.\n\n    Returns:\n        tuple: A tuple containing three numpy arrays:\n        U (ndarray): Unitary matrix U.\n        s (ndarray): Singular values, sorted in descending order.\n        Vh (ndarray): Conjugate transpose of the unitary matrix V.\n\n    Reqs:\n    - numpy\n    - scipy.linalg.svd\n\n    Examples:\n    - >>> U, s, Vh = task_func(3, 2, seed=42)\n    - >>> print('U shape:', U.shape)\n    - U shape: (3, 3)\n    - >>> print('s shape:', s.shape)\n    - s shape: (2,)\n    - >>> print('Vh shape:', Vh.shape)\n    - Vh shape: (2, 2)\n\n    \"\"\"\n    np.random.seed(seed)\n    A = np.random.rand(rows, columns)\n    U, s, Vh = svd(A)\n    s = np.sort(s)[::-1]  # Sort singular values in descending order\n    return U, s, Vh\n```\n"}
{"task_id": "BigCodeBench/951", "solution": "import pandas as pd\nimport numpy as np\nimport random\nfrom random import randint, seed\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\ndef task_func(mystrings, n_products, seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    products = []\n    for _ in range(n_products):\n        product_name = random.choice(mystrings).replace(' ', '_')\n        category = random.choice(CATEGORIES)\n        price = round(np.random.normal(50, 10), 2)\n        products.append({'Product Name': product_name, 'Category': category, 'Price': price})\n\n    df = pd.DataFrame(products)\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\nfrom random import randint, seed\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\n\ndef task_func(mystrings, n_products, seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    products = []\n    for _ in range(n_products):\n        product_name = random.choice(mystrings).replace(' ', '_')\n        category = random.choice(CATEGORIES)\n        price = round(np.random.normal(50, 10), 2)\n        products.append({'Product Name': product_name, 'Category': category, 'Price': price})\n\n    df = pd.DataFrame(products)\n    return df\n\n# Example usage:\n# df = task_func(['Mobile Phone', 'T Shirt', 'Coffee Maker', 'python_book', 'Toy Car'], 2)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if seed is not None:\n        random.seed(seed)\n\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative.\")\n\n    if n_tasks > len(task_list):\n        n_tasks = len(task_list)\n\n    tasks = random.sample(task_list, n_tasks)\n    assignments = []\n\n    for task in tasks:\n        assigned_to = random.choice(employees)\n        due_date = datetime.now() + timedelta(days=random.randint(1, 30))\n        assignments.append({\n            \"Task Name\": task.replace(\" \", \"_\"),\n            \"Assigned To\": assigned_to,\n            \"Due Date\": due_date.strftime(\"%Y-%m-%d\")\n        })\n\n    df = pd.DataFrame(assignments)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if seed is not None:\n        random.seed(seed)\n\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative.\")\n\n    if n_tasks > len(task_list):\n        n_tasks = len(task_list)\n\n    tasks = random.sample(task_list, n_tasks)\n    assignments = []\n\n    for task in tasks:\n        assigned_to = random.choice(employees)\n        due_date = datetime.now() + timedelta(days=random.randint(1, 30))\n        assignments.append({\n            \"Task Name\": task.replace(\" \", \"_\"),\n            \"Assigned To\": assigned_to,\n            \"Due Date\": due_date.strftime(\"%Y-%m-%d\")\n        })\n\n    df = pd.DataFrame(assignments)\n\n    return df\n\n# Example usage:\n# df = task_func(['Clean Office', 'Prepare Report', 'Client Meeting'], 2, seed=42)\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/953", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(mystrings, folder_path, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    if not os.path.exists(folder_path):\n        try:\n            os.makedirs(folder_path)\n        except Exception as e:\n            raise FileNotFoundError(f\"Unable to create directory at {folder_path}: {e}\")\n\n    filenames = []\n    for title in mystrings:\n        # Generate random data points\n        data_points = np.random.rand(10)\n        \n        # Create a bar chart\n        plt.figure()\n        plt.bar(range(10), data_points, color='blue')\n        plt.title(title.replace(' ', '_'))\n        plt.xlabel('Index')\n        plt.ylabel('Value')\n        \n        # Save the plot as a PNG file\n        filename = f\"{title.replace(' ', '_')}.png\"\n        filepath = os.path.join(folder_path, filename)\n        plt.savefig(filepath)\n        plt.close()\n        \n        filenames.append(filename)\n\n    return filenames", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(mystrings, folder_path, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    if not os.path.exists(folder_path):\n        try:\n            os.makedirs(folder_path)\n        except Exception as e:\n            raise FileNotFoundError(f\"Unable to create directory at {folder_path}: {e}\")\n\n    filenames = []\n    for title in mystrings:\n        # Generate random data points\n        data_points = np.random.rand(10)\n        \n        # Create a bar chart\n        plt.figure()\n        plt.bar(range(10), data_points, color='blue')\n        plt.title(title.replace(' ', '_'))\n        plt.xlabel('Index')\n        plt.ylabel('Value')\n        \n        # Save the plot as a PNG file\n        filename = f\"{title.replace(' ', '_')}.png\"\n        filepath = os.path.join(folder_path, filename)\n        plt.savefig(filepath)\n        plt.close()\n        \n        filenames.append(filename)\n\n    return filenames\n```\n"}
{"task_id": "BigCodeBench/954", "solution": "import random\nimport re\ndef task_func(target_words, n_sentences, vocabulary):\n    if n_sentences < 0:\n        raise ValueError(\"n_sentences must not be negative.\")\n    if not vocabulary:\n        raise ValueError(\"Vocabulary must not be empty.\")\n\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choices(vocabulary, k=10))\n        for target in target_words:\n            sentence = re.sub(r'\\b' + re.escape(target.lower()) + r'\\b', target.lower().replace(' ', '_'), sentence, flags=re.IGNORECASE)\n        sentences.append(sentence.lower())\n\n    return sentences", "raw_solution": "import random\nimport re\n\ndef task_func(target_words, n_sentences, vocabulary):\n    if n_sentences < 0:\n        raise ValueError(\"n_sentences must not be negative.\")\n    if not vocabulary:\n        raise ValueError(\"Vocabulary must not be empty.\")\n\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choices(vocabulary, k=10))\n        for target in target_words:\n            sentence = re.sub(r'\\b' + re.escape(target.lower()) + r'\\b', target.lower().replace(' ', '_'), sentence, flags=re.IGNORECASE)\n        sentences.append(sentence.lower())\n\n    return sentences\n\n# Example usage:\nrandom.seed(42)"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input text must be a string.\")\n\n    # Replace spaces with underscores in each string in the list\n    modified_strings = [s.replace(\" \", \"_\") for s in mystrings]\n\n    # Combine all modified strings into a single string\n    combined_text = \" \".join(modified_strings)\n\n    # Find all words in the text (case-insensitive)\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Prepare data for plotting\n    words = list(word_counts.keys())\n    frequencies = np.array(list(word_counts.values()))\n\n    # Plot the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency Plot')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input text must be a string.\")\n\n    # Replace spaces with underscores in each string in the list\n    modified_strings = [s.replace(\" \", \"_\") for s in mystrings]\n\n    # Combine all modified strings into a single string\n    combined_text = \" \".join(modified_strings)\n\n    # Find all words in the text (case-insensitive)\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Prepare data for plotting\n    words = list(word_counts.keys())\n    frequencies = np.array(list(word_counts.values()))\n\n    # Plot the frequencies\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency Plot')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/956", "solution": "import re\nimport string\nimport random\ndef task_func(text: str, seed=None) -> str:\n    if seed is not None:\n        random.seed(seed)\n\n    # Remove punctuation and replace spaces with underscores\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\s+', '_', text)\n\n    # Randomly convert characters to uppercase based on a probability of 0.5\n    result = []\n    for char in text:\n        if random.random() < 0.5:\n            result.append(char.upper())\n        else:\n            result.append(char)\n\n    return ''.join(result)", "raw_solution": "import re\nimport string\nimport random\n\ndef task_func(text: str, seed=None) -> str:\n    if seed is not None:\n        random.seed(seed)\n\n    # Remove punctuation and replace spaces with underscores\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\s+', '_', text)\n\n    # Randomly convert characters to uppercase based on a probability of 0.5\n    result = []\n    for char in text:\n        if random.random() < 0.5:\n            result.append(char.upper())\n        else:\n            result.append(char)\n\n    return ''.join(result)\n\n# Test cases"}
{"task_id": "BigCodeBench/957", "solution": "import string\nimport re\ndef task_func(text: str) -> tuple:\n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split text into words\n    words = text.split()\n    \n    # Count characters, words, and unique characters\n    num_words = len(words)\n    num_characters = len(text)\n    num_unique_characters = len(set(text))\n    \n    return num_words, num_characters, num_unique_characters", "raw_solution": "import string\nimport re\n\ndef task_func(text: str) -> tuple:\n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split text into words\n    words = text.split()\n    \n    # Count characters, words, and unique characters\n    num_words = len(words)\n    num_characters = len(text)\n    num_unique_characters = len(set(text))\n    \n    return num_words, num_characters, num_unique_characters\n\n# Test cases"}
{"task_id": "BigCodeBench/958", "solution": "import random\nimport re\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def scramble_word(word):\n        if len(word) > 3:\n            middle = list(word[1:-1])\n            random.shuffle(middle)\n            return word[0] + ''.join(middle) + word[-1]\n        return word\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    scrambled_words = [scramble_word(word) for word in words]\n\n    return re.sub(r'\\b\\w+\\b', lambda m: scrambled_words.pop(0), text)", "raw_solution": "import random\nimport re\n\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def scramble_word(word):\n        if len(word) > 3:\n            middle = list(word[1:-1])\n            random.shuffle(middle)\n            return word[0] + ''.join(middle) + word[-1]\n        return word\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    scrambled_words = [scramble_word(word) for word in words]\n\n    return re.sub(r'\\b\\w+\\b', lambda m: scrambled_words.pop(0), text)\n\n# Test cases"}
{"task_id": "BigCodeBench/959", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a translation table that maps each character to a random character\n    translation_table = str.maketrans(\n        string.ascii_letters,  # Characters to replace\n        ''.join(random.choices(string.ascii_letters, k=len(string.ascii_letters)))  # Random characters\n    )\n\n    # Translate the text using the translation table\n    result = text.translate(translation_table)\n\n    return result\ntext = 'Hello, world!'", "raw_solution": "import string\nimport random\n\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a translation table that maps each character to a random character\n    translation_table = str.maketrans(\n        string.ascii_letters,  # Characters to replace\n        ''.join(random.choices(string.ascii_letters, k=len(string.ascii_letters)))  # Random characters\n    )\n\n    # Translate the text using the translation table\n    result = text.translate(translation_table)\n\n    return result\n\n# Example usage:\ntext = 'Hello, world!'"}
{"task_id": "BigCodeBench/960", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    if not text:\n        raise ValueError(\"Text cannot be empty\")\n\n    random.seed(seed)\n\n    result = []\n    for char in text:\n        if char in string.ascii_letters:\n            if random.random() < 0.5:\n                result.append(random.choice(string.ascii_lowercase))\n            else:\n                result.append(char)\n        elif char in string.digits:\n            result.append(random.choice(string.digits))\n        elif char == ' ':\n            result.append(random.choice(string.ascii_lowercase))\n        else:\n            result.append(char)\n\n    return ''.join(result)", "raw_solution": "import string\nimport random\n\ndef task_func(text, seed=None):\n    if not text:\n        raise ValueError(\"Text cannot be empty\")\n\n    random.seed(seed)\n\n    result = []\n    for char in text:\n        if char in string.ascii_letters:\n            if random.random() < 0.5:\n                result.append(random.choice(string.ascii_lowercase))\n            else:\n                result.append(char)\n        elif char in string.digits:\n            result.append(random.choice(string.digits))\n        elif char == ' ':\n            result.append(random.choice(string.ascii_lowercase))\n        else:\n            result.append(char)\n\n    return ''.join(result)\n\n# Test cases"}
{"task_id": "BigCodeBench/961", "solution": "import os\nimport glob\nfrom collections import Counter\ndef task_func(directory, extensions=[\".txt\", \".docx\", \".xlsx\", \".csv\"], keep_zero=True):\n    if not os.path.exists(directory):\n        raise OSError(f\"The directory {directory} does not exist.\")\n\n    file_counter = Counter()\n\n    for ext in extensions:\n        pattern = os.path.join(directory, '**', f'*{ext}')\n        files = glob.glob(pattern, recursive=True)\n        file_counter[ext] = len(files)\n\n    if not keep_zero:\n        file_counter = Counter({ext: count for ext, count in file_counter.items() if count > 0})\n\n    return file_counter", "raw_solution": "import os\nimport glob\nfrom collections import Counter\n\ndef task_func(directory, extensions=[\".txt\", \".docx\", \".xlsx\", \".csv\"], keep_zero=True):\n    if not os.path.exists(directory):\n        raise OSError(f\"The directory {directory} does not exist.\")\n\n    file_counter = Counter()\n\n    for ext in extensions:\n        pattern = os.path.join(directory, '**', f'*{ext}')\n        files = glob.glob(pattern, recursive=True)\n        file_counter[ext] = len(files)\n\n    if not keep_zero:\n        file_counter = Counter({ext: count for ext, count in file_counter.items() if count > 0})\n\n    return file_counter\n```\n"}
{"task_id": "BigCodeBench/962", "solution": "import os\nfrom pathlib import Path\nimport glob\nimport shutil\ndef task_func(source_directory: str, target_directory: str):\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist.\")\n    \n    source_path = Path(source_directory)\n    target_path = Path(target_directory)\n    \n    # Ensure target directory exists\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # Supported file extensions\n    extensions = ('*.txt', '*.docx', '*.xlsx', '*.csv')\n    \n    # Find all files matching the extensions in the source directory and subdirectories\n    files = []\n    for ext in extensions:\n        files.extend(source_path.rglob(ext))\n    \n    moved_files_count = 0\n    \n    for file in files:\n        # Create a new file name to avoid naming conflicts\n        new_file_name = file.stem + '_copy' + file.suffix\n        new_file_path = target_path / new_file_name\n        \n        # Resolve naming conflicts\n        counter = 1\n        while new_file_path.exists():\n            new_file_name = f\"{file.stem}_copy_{counter}{file.suffix}\"\n            new_file_path = target_path / new_file_name\n            counter += 1\n        \n        # Move the file\n        shutil.move(str(file), str(new_file_path))\n        moved_files_count += 1\n    \n    return moved_files_count", "raw_solution": "import os\nfrom pathlib import Path\nimport glob\nimport shutil\n\ndef task_func(source_directory: str, target_directory: str):\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist.\")\n    \n    source_path = Path(source_directory)\n    target_path = Path(target_directory)\n    \n    # Ensure target directory exists\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # Supported file extensions\n    extensions = ('*.txt', '*.docx', '*.xlsx', '*.csv')\n    \n    # Find all files matching the extensions in the source directory and subdirectories\n    files = []\n    for ext in extensions:\n        files.extend(source_path.rglob(ext))\n    \n    moved_files_count = 0\n    \n    for file in files:\n        # Create a new file name to avoid naming conflicts\n        new_file_name = file.stem + '_copy' + file.suffix\n        new_file_path = target_path / new_file_name\n        \n        # Resolve naming conflicts\n        counter = 1\n        while new_file_path.exists():\n            new_file_name = f\"{file.stem}_copy_{counter}{file.suffix}\"\n            new_file_path = target_path / new_file_name\n            counter += 1\n        \n        # Move the file\n        shutil.move(str(file), str(new_file_path))\n        moved_files_count += 1\n    \n    return moved_files_count\n```\n"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory {source_directory} does not exist.\")\n\n    # Define the extensions to look for\n    extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Create a Path object for the target directory\n    target_path = Path(target_directory)\n\n    # Ensure the target directory exists\n    if not target_path.exists():\n        os.makedirs(target_path)\n\n    # Create a zip file path\n    zip_path = target_path / f\"{zip_name}.zip\"\n\n    # Create a zip file\n    with zipfile.ZipFile(zip_path, 'w') as zip_file:\n        for ext in extensions:\n            # Find all files with the specified extensions in the source directory\n            for file_path in glob.glob(os.path.join(source_directory, f\"*{ext}\")):\n                # Add the file to the zip file\n                zip_file.write(file_path, arcname=Path(file_path).name)\n\n    return str(zip_path)", "raw_solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory {source_directory} does not exist.\")\n\n    # Define the extensions to look for\n    extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Create a Path object for the target directory\n    target_path = Path(target_directory)\n\n    # Ensure the target directory exists\n    if not target_path.exists():\n        os.makedirs(target_path)\n\n    # Create a zip file path\n    zip_path = target_path / f\"{zip_name}.zip\"\n\n    # Create a zip file\n    with zipfile.ZipFile(zip_path, 'w') as zip_file:\n        for ext in extensions:\n            # Find all files with the specified extensions in the source directory\n            for file_path in glob.glob(os.path.join(source_directory, f\"*{ext}\")):\n                # Add the file to the zip file\n                zip_file.write(file_path, arcname=Path(file_path).name)\n\n    return str(zip_path)\n```\n"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\ndef task_func(source_directory: str, target_directory: str) -> int:\n    source_path = Path(source_directory)\n    target_path = Path(target_directory)\n\n    if not source_path.exists() or not source_path.is_dir():\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist or is not a directory.\")\n\n    if not target_path.exists():\n        os.makedirs(target_path)\n\n    converted_files_count = 0\n\n    for file in source_path.glob('*.txt'):\n        with open(file, 'r') as f:\n            text = f.read()\n            df = pd.DataFrame({'text': [text]})\n            csv_file = target_path / f\"{file.stem}.csv\"\n            df.to_csv(csv_file, index=False)\n            converted_files_count += 1\n\n    for file in source_path.glob('*.docx'):\n        doc = docx.Document(file)\n        text = '\\n'.join([para.text for para in doc.paragraphs])\n        df = pd.DataFrame({'text': [text]})\n        csv_file = target_path / f\"{file.stem}.csv\"\n        df.to_csv(csv_file, index=False)\n        converted_files_count += 1\n\n    for file in source_path.glob('*.xlsx'):\n        df = pd.read_excel(file)\n        csv_file = target_path / f\"{file.stem}.csv\"\n        df.to_csv(csv_file, index=False)\n        converted_files_count += 1\n\n    for file in source_path.glob('*.csv'):\n        df = pd.read_csv(file)\n        csv_file = target_path / f\"{file.stem}.csv\"\n        df.to_csv(csv_file, index=False)\n        converted_files_count += 1\n\n    return converted_files_count", "raw_solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    source_path = Path(source_directory)\n    target_path = Path(target_directory)\n\n    if not source_path.exists() or not source_path.is_dir():\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist or is not a directory.\")\n\n    if not target_path.exists():\n        os.makedirs(target_path)\n\n    converted_files_count = 0\n\n    for file in source_path.glob('*.txt'):\n        with open(file, 'r') as f:\n            text = f.read()\n            df = pd.DataFrame({'text': [text]})\n            csv_file = target_path / f\"{file.stem}.csv\"\n            df.to_csv(csv_file, index=False)\n            converted_files_count += 1\n\n    for file in source_path.glob('*.docx'):\n        doc = docx.Document(file)\n        text = '\\n'.join([para.text for para in doc.paragraphs])\n        df = pd.DataFrame({'text': [text]})\n        csv_file = target_path / f\"{file.stem}.csv\"\n        df.to_csv(csv_file, index=False)\n        converted_files_count += 1\n\n    for file in source_path.glob('*.xlsx'):\n        df = pd.read_excel(file)\n        csv_file = target_path / f\"{file.stem}.csv\"\n        df.to_csv(csv_file, index=False)\n        converted_files_count += 1\n\n    for file in source_path.glob('*.csv'):\n        df = pd.read_csv(file)\n        csv_file = target_path / f\"{file.stem}.csv\"\n        df.to_csv(csv_file, index=False)\n        converted_files_count += 1\n\n    return converted_files_count\n```\n"}
{"task_id": "BigCodeBench/965", "solution": "import os\nimport re\nimport shutil\ndef task_func(source_directory: str, target_directory: str, pattern: str = r\"\\d{4}\") -> int:\n    if not os.path.exists(source_directory) or not os.path.isdir(source_directory):\n        return 0\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    moved_files_count = 0\n    for filename in os.listdir(source_directory):\n        if re.match(pattern, filename):\n            shutil.move(os.path.join(source_directory, filename), target_directory)\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import os\nimport re\nimport shutil\n\ndef task_func(source_directory: str, target_directory: str, pattern: str = r\"\\d{4}\") -> int:\n    if not os.path.exists(source_directory) or not os.path.isdir(source_directory):\n        return 0\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    moved_files_count = 0\n    for filename in os.listdir(source_directory):\n        if re.match(pattern, filename):\n            shutil.move(os.path.join(source_directory, filename), target_directory)\n            moved_files_count += 1\n\n    return moved_files_count\n```\n"}
{"task_id": "BigCodeBench/966", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    if not all(df.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        raise ValueError(\"All columns in the DataFrame must be numeric.\")\n    \n    # Calculate cumulative sum for each column\n    df_cumsum = df.cumsum()\n    \n    # Plotting the cumulative sums\n    fig, ax = plt.subplots()\n    for column in df_cumsum.columns:\n        ax.plot(df_cumsum.index, df_cumsum[column], label=column)\n    \n    ax.set_title('Cumulative Sum per Column')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.legend()\n    \n    return df_cumsum, fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    if not all(df.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        raise ValueError(\"All columns in the DataFrame must be numeric.\")\n    \n    # Calculate cumulative sum for each column\n    df_cumsum = df.cumsum()\n    \n    # Plotting the cumulative sums\n    fig, ax = plt.subplots()\n    for column in df_cumsum.columns:\n        ax.plot(df_cumsum.index, df_cumsum[column], label=column)\n    \n    ax.set_title('Cumulative Sum per Column')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.legend()\n    \n    return df_cumsum, fig\n```\n"}
{"task_id": "BigCodeBench/967", "solution": "import numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    x = np.linspace(x_range[0], x_range[1], num_points)\n    y = func(x)\n    integral, _ = integrate.quad(func, x_range[0], x_range[1])\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'{func.__name__}(x)')\n    ax.fill_between(x, y, alpha=0.2)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'Function and its integral: {func.__name__}(x) and \u222b{func.__name__}(x) dx')\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    x = np.linspace(x_range[0], x_range[1], num_points)\n    y = func(x)\n    integral, _ = integrate.quad(func, x_range[0], x_range[1])\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=f'{func.__name__}(x)')\n    ax.fill_between(x, y, alpha=0.2)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'Function and its integral: {func.__name__}(x) and \u222b{func.__name__}(x) dx')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/968", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    df = pd.DataFrame(data)\n    \n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    \n    numeric_df = df.select_dtypes(include='number')\n    \n    if numeric_df.empty:\n        raise ValueError(\"No numeric columns found.\")\n    \n    corr_matrix = numeric_df.corr()\n    \n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Cumulative Yog Heatmap')\n    plt.show()\n    \n    return ax\ndata = {'A': [1, 2, 3], 'B': [4, 5, 6]}", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    df = pd.DataFrame(data)\n    \n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    \n    numeric_df = df.select_dtypes(include='number')\n    \n    if numeric_df.empty:\n        raise ValueError(\"No numeric columns found.\")\n    \n    corr_matrix = numeric_df.corr()\n    \n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Cumulative Yog Heatmap')\n    plt.show()\n    \n    return ax\n\n# Example usage:\ndata = {'A': [1, 2, 3], 'B': [4, 5, 6]}\nax = task_func(data)\n```\n"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(df, pd.DataFrame) or df.empty or df.isnull().values.any():\n        raise ValueError(\"Input must be a non-empty pandas DataFrame without NaN values.\")\n    \n    if not all(df.dtypes.apply(np.issubdtype, args=(np.number,))):\n        raise TypeError(\"All columns in the DataFrame must be numeric.\")\n    \n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    df_cumsum = df_normalized.cumsum()\n    \n    return df_cumsum", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(df, pd.DataFrame) or df.empty or df.isnull().values.any():\n        raise ValueError(\"Input must be a non-empty pandas DataFrame without NaN values.\")\n    \n    if not all(df.dtypes.apply(np.issubdtype, args=(np.number,))):\n        raise TypeError(\"All columns in the DataFrame must be numeric.\")\n    \n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    df_cumsum = df_normalized.cumsum()\n    \n    return df_cumsum\n```\n"}
{"task_id": "BigCodeBench/970", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data: np.ndarray) -> plt.Axes:\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"Input must be a NumPy array.\")\n    if np.isnan(data).any():\n        raise ValueError(\"Input array contains NaN values.\")\n    \n    sorted_data = np.sort(data)\n    cumulative_prob = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n\n    fig, ax = plt.subplots()\n    ax.plot(sorted_data, cumulative_prob, marker='o', linestyle='-', color='b')\n    ax.set_title('Cumulative Probability Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Probability')\n    \n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data: np.ndarray) -> plt.Axes:\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"Input must be a NumPy array.\")\n    if np.isnan(data).any():\n        raise ValueError(\"Input array contains NaN values.\")\n    \n    sorted_data = np.sort(data)\n    cumulative_prob = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n\n    fig, ax = plt.subplots()\n    ax.plot(sorted_data, cumulative_prob, marker='o', linestyle='-', color='b')\n    ax.set_title('Cumulative Probability Plot')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Probability')\n    \n    return ax\n\n# Example usage:\n# ax = task_func(np.array([1, 2, 3, 4, 5]))\n# print(ax.get_title())  # Output: 'Cumulative Probability Plot'\n```\n"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    path = Path(directory_path)\n    if not path.exists() or not path.is_dir():\n        raise ValueError(f\"The directory {directory_path} does not exist or is not a directory.\")\n\n    result = []\n    for file in path.iterdir():\n        if file.is_file():\n            file_size = file.stat().st_size\n            creation_time = file.stat().st_ctime\n            modification_time = file.stat().st_mtime\n\n            # Convert times to ISO format\n            creation_time_iso = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat() + 'Z'\n            modification_time_iso = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat() + 'Z'\n\n            result.append((file.name, file_size, creation_time_iso, modification_time_iso))\n\n    return result", "raw_solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\n\ndef task_func(directory_path: str):\n    path = Path(directory_path)\n    if not path.exists() or not path.is_dir():\n        raise ValueError(f\"The directory {directory_path} does not exist or is not a directory.\")\n\n    result = []\n    for file in path.iterdir():\n        if file.is_file():\n            file_size = file.stat().st_size\n            creation_time = file.stat().st_ctime\n            modification_time = file.stat().st_mtime\n\n            # Convert times to ISO format\n            creation_time_iso = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat() + 'Z'\n            modification_time_iso = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat() + 'Z'\n\n            result.append((file.name, file_size, creation_time_iso, modification_time_iso))\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/972", "solution": "import pathlib\nimport os\ndef task_func(path: str, delimiter: str = os.path.sep) -> list:\n    \"\"\"\n    This function takes a file path and a delimiter, then splits the path into components using the specified delimiter.\n    It also replaces backslashes with slashes for proper processing.\n\n    Parameters:\n    - path (str): The file path to be split.\n    - delimiter (str): The delimiter to use for splitting the path. Defaults to the system's path separator.\n\n    Returns:\n    - list: A list of path components if the path is valid; otherwise, an empty list.\n\n    Requirements:\n    - pathlib\n    - os\n\n    Raises:\n    - ValueError: If the path contains invalid characters.\n\n    Examples:\n    - >>> task_func('Docs/src/Scripts/temp', '/')\n    - ['Docs', 'src', 'Scripts', 'temp']\n    - >>> task_func(r'Docs\\src\\Scripts\\temp', '\\\\')\n    - ['Docs', 'src', 'Scripts', 'temp']\n    \"\"\"\n    # Replace backslashes with slashes\n    path = path.replace('\\\\', '/')\n\n    # Check if the path contains invalid characters\n    invalid_chars = '<>:\"|?*'\n    if any(char in path for char in invalid_chars):\n        raise ValueError(\"The path contains invalid characters.\")\n\n    # Split the path into components\n    path_components = path.split(delimiter)\n\n    # Return the path components\n    return path_components", "raw_solution": "import pathlib\nimport os\n\ndef task_func(path: str, delimiter: str = os.path.sep) -> list:\n    \"\"\"\n    This function takes a file path and a delimiter, then splits the path into components using the specified delimiter.\n    It also replaces backslashes with slashes for proper processing.\n\n    Parameters:\n    - path (str): The file path to be split.\n    - delimiter (str): The delimiter to use for splitting the path. Defaults to the system's path separator.\n\n    Returns:\n    - list: A list of path components if the path is valid; otherwise, an empty list.\n\n    Requirements:\n    - pathlib\n    - os\n\n    Raises:\n    - ValueError: If the path contains invalid characters.\n\n    Examples:\n    - >>> task_func('Docs/src/Scripts/temp', '/')\n    - ['Docs', 'src', 'Scripts', 'temp']\n    - >>> task_func(r'Docs\\src\\Scripts\\temp', '\\\\')\n    - ['Docs', 'src', 'Scripts', 'temp']\n    \"\"\"\n    # Replace backslashes with slashes\n    path = path.replace('\\\\', '/')\n\n    # Check if the path contains invalid characters\n    invalid_chars = '<>:\"|?*'\n    if any(char in path for char in invalid_chars):\n        raise ValueError(\"The path contains invalid characters.\")\n\n    # Split the path into components\n    path_components = path.split(delimiter)\n\n    # Return the path components\n    return path_components\n```\n"}
{"task_id": "BigCodeBench/973", "solution": "import os\nimport shutil\ndef task_func(path, delimiter=\"/\"):\n    if not isinstance(path, str):\n        raise ValueError(\"Path must be a string.\")\n    if not isinstance(delimiter, str):\n        raise ValueError(\"Delimiter must be a string.\")\n    if not path:\n        raise ValueError(\"Path cannot be empty.\")\n    if not delimiter:\n        raise ValueError(\"Delimiter cannot be empty.\")\n\n    components = path.split(delimiter)\n    if not components:\n        raise ValueError(\"Path must have at least one component.\")\n\n    disk_usage_list = []\n    for component in components:\n        if not os.path.exists(component):\n            raise FileNotFoundError(f\"The path component '{component}' does not exist.\")\n        \n        total, used, free = shutil.disk_usage(component)\n        disk_usage_list.append((component, {'total': total, 'used': used, 'free': free}))\n\n    return disk_usage_list", "raw_solution": "import os\nimport shutil\n\n\ndef task_func(path, delimiter=\"/\"):\n    if not isinstance(path, str):\n        raise ValueError(\"Path must be a string.\")\n    if not isinstance(delimiter, str):\n        raise ValueError(\"Delimiter must be a string.\")\n    if not path:\n        raise ValueError(\"Path cannot be empty.\")\n    if not delimiter:\n        raise ValueError(\"Delimiter cannot be empty.\")\n\n    components = path.split(delimiter)\n    if not components:\n        raise ValueError(\"Path must have at least one component.\")\n\n    disk_usage_list = []\n    for component in components:\n        if not os.path.exists(component):\n            raise FileNotFoundError(f\"The path component '{component}' does not exist.\")\n        \n        total, used, free = shutil.disk_usage(component)\n        disk_usage_list.append((component, {'total': total, 'used': used, 'free': free}))\n\n    return disk_usage_list\n\n\n# Example usage:\n# print(task_func('a/b', '/'))\n```\n"}
{"task_id": "BigCodeBench/974", "solution": "import shutil\nimport pathlib\ndef task_func(source_path, destination_path):\n    source = pathlib.Path(source_path)\n    destination = pathlib.Path(destination_path)\n\n    if not source.exists() or not source.is_dir():\n        raise ValueError(f\"The source path {source_path} does not exist or is not a directory.\")\n\n    if not destination.exists():\n        destination.mkdir(parents=True)\n\n    copied_files = []\n    for file in source.iterdir():\n        if file.is_file():\n            shutil.copy(file, destination / file.name)\n            copied_files.append(file.name)\n\n    return str(source.name), copied_files", "raw_solution": "import shutil\nimport pathlib\n\ndef task_func(source_path, destination_path):\n    source = pathlib.Path(source_path)\n    destination = pathlib.Path(destination_path)\n\n    if not source.exists() or not source.is_dir():\n        raise ValueError(f\"The source path {source_path} does not exist or is not a directory.\")\n\n    if not destination.exists():\n        destination.mkdir(parents=True)\n\n    copied_files = []\n    for file in source.iterdir():\n        if file.is_file():\n            shutil.copy(file, destination / file.name)\n            copied_files.append(file.name)\n\n    return str(source.name), copied_files\n```\n"}
{"task_id": "BigCodeBench/975", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(rows, columns=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=0) -> pd.DataFrame:\n    if rows < 0:\n        raise ValueError(\"Number of rows cannot be negative.\")\n    \n    np.random.seed(seed)\n    data = np.random.rand(rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    \n    # Shuffle the columns\n    df = df.loc[:, np.random.permutation(df.columns)]\n    \n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(rows, columns=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=0) -> pd.DataFrame:\n    if rows < 0:\n        raise ValueError(\"Number of rows cannot be negative.\")\n    \n    np.random.seed(seed)\n    data = np.random.rand(rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    \n    # Shuffle the columns\n    df = df.loc[:, np.random.permutation(df.columns)]\n    \n    return df\n\n# Example usage:\n# df = task_func(10)\n# print(df.head(2))\n```\n"}
{"task_id": "BigCodeBench/976", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    if records.ndim != 2:\n        raise ValueError(\"Input records must be a 2D array.\")\n\n    scaler = StandardScaler()\n    scaled_records = scaler.fit_transform(records)\n\n    feature_names = [f'f{i+1}' for i in range(records.shape[1])]\n    df = pd.DataFrame(scaled_records, columns=feature_names)\n\n    return df\ndata = np.array([[1, 2, 3], [4, 5, 6]])", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    if records.ndim != 2:\n        raise ValueError(\"Input records must be a 2D array.\")\n\n    scaler = StandardScaler()\n    scaled_records = scaler.fit_transform(records)\n\n    feature_names = [f'f{i+1}' for i in range(records.shape[1])]\n    df = pd.DataFrame(scaled_records, columns=feature_names)\n\n    return df\n\n# Example usage:\ndata = np.array([[1, 2, 3], [4, 5, 6]])\ndf = task_func(data, random_seed=42)"}
{"task_id": "BigCodeBench/977", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(array, features=None, seed=None):\n    if not isinstance(array, np.ndarray) or array.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    if features is None:\n        features = [f\"Feature {i+1}\" for i in range(array.shape[1])]\n    \n    if len(features) != array.shape[1]:\n        raise ValueError(\"Number of features must match the number of columns in the array.\")\n    \n    # Shuffle the columns of the array\n    shuffled_indices = np.random.permutation(array.shape[1])\n    shuffled_array = array[:, shuffled_indices]\n    \n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(shuffled_array, annot=True, fmt=\".2f\", cmap=\"viridis\", xticklabels=features, yticklabels=features)\n    plt.title(\"Heatmap of Shuffled Array\")\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Features\")\n    \n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(array, features=None, seed=None):\n    if not isinstance(array, np.ndarray) or array.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    if features is None:\n        features = [f\"Feature {i+1}\" for i in range(array.shape[1])]\n    \n    if len(features) != array.shape[1]:\n        raise ValueError(\"Number of features must match the number of columns in the array.\")\n    \n    # Shuffle the columns of the array\n    shuffled_indices = np.random.permutation(array.shape[1])\n    shuffled_array = array[:, shuffled_indices]\n    \n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(shuffled_array, annot=True, fmt=\".2f\", cmap=\"viridis\", xticklabels=features, yticklabels=features)\n    plt.title(\"Heatmap of Shuffled Array\")\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Features\")\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/978", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(array, seed=None):\n    if array.ndim != 2:\n        raise ValueError(\"Input array must be 2D.\")\n\n    pca = PCA(n_components=2, random_state=seed)\n    components = pca.fit_transform(array)\n\n    df = pd.DataFrame(components, columns=['PC1', 'PC2'])\n\n    return df\narray = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\ndef task_func(array, seed=None):\n    if array.ndim != 2:\n        raise ValueError(\"Input array must be 2D.\")\n\n    pca = PCA(n_components=2, random_state=seed)\n    components = pca.fit_transform(array)\n\n    df = pd.DataFrame(components, columns=['PC1', 'PC2'])\n\n    return df\n\n# Example usage:\narray = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\ndf = task_func(array, seed=42)"}
{"task_id": "BigCodeBench/979", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\ndef task_func(\n    feature_array,\n    target_array,\n    feature_names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n    target_name=\"target\",\n    seed=None,\n):\n    # Convert numpy arrays to pandas DataFrame\n    df = pd.DataFrame(feature_array, columns=feature_names)\n    df[target_name] = target_array\n\n    # Shuffle the DataFrame\n    if seed is not None:\n        np.random.seed(seed)\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # Extract features and target\n    X = df[feature_names]\n    y = df[target_name]\n\n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X, y)\n\n    return clf", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef task_func(\n    feature_array,\n    target_array,\n    feature_names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n    target_name=\"target\",\n    seed=None,\n):\n    # Convert numpy arrays to pandas DataFrame\n    df = pd.DataFrame(feature_array, columns=feature_names)\n    df[target_name] = target_array\n\n    # Shuffle the DataFrame\n    if seed is not None:\n        np.random.seed(seed)\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # Extract features and target\n    X = df[feature_names]\n    y = df[target_name]\n\n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X, y)\n\n    return clf\n```\n"}
{"task_id": "BigCodeBench/980", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    numeric_df = df.select_dtypes(include=[np.number])\n    if numeric_df.empty:\n        raise ValueError(\"No numeric columns present in the DataFrame.\")\n    \n    # Calculate correlation matrix\n    corr_matrix = numeric_df.corr()\n    \n    # Plot heatmap\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n    plt.title(\"Correlation Matrix Heatmap\")\n    \n    # Standardize numeric columns\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(numeric_df)\n    standardized_df = pd.DataFrame(standardized_data, columns=numeric_df.columns)\n    \n    return standardized_df, fig", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    numeric_df = df.select_dtypes(include=[np.number])\n    if numeric_df.empty:\n        raise ValueError(\"No numeric columns present in the DataFrame.\")\n    \n    # Calculate correlation matrix\n    corr_matrix = numeric_df.corr()\n    \n    # Plot heatmap\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n    plt.title(\"Correlation Matrix Heatmap\")\n    \n    # Standardize numeric columns\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(numeric_df)\n    standardized_df = pd.DataFrame(standardized_data, columns=numeric_df.columns)\n    \n    return standardized_df, fig\n```\n"}
{"task_id": "BigCodeBench/981", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, num_series, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n    if start > end:\n        raise ValueError(\"start_date must be before end_date\")\n\n    if num_series < 1:\n        raise ValueError(\"num_series must be at least 1\")\n\n    dates = pd.date_range(start=start, end=end)\n    data = {f\"series_{i+1}\": [random.randint(0, 100) for _ in range(len(dates))] for i in range(num_series)}\n    df = pd.DataFrame(data, index=dates)\n\n    ax = df.plot(title=\"Random Time Series\", xlabel=\"Date\", ylabel=\"Value\")\n    ax.legend(title=\"Series\")\n\n    return df, ax", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date, end_date, num_series, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n    if start > end:\n        raise ValueError(\"start_date must be before end_date\")\n\n    if num_series < 1:\n        raise ValueError(\"num_series must be at least 1\")\n\n    dates = pd.date_range(start=start, end=end)\n    data = {f\"series_{i+1}\": [random.randint(0, 100) for _ in range(len(dates))] for i in range(num_series)}\n    df = pd.DataFrame(data, index=dates)\n\n    ax = df.plot(title=\"Random Time Series\", xlabel=\"Date\", ylabel=\"Value\")\n    ax.legend(title=\"Series\")\n\n    return df, ax\n\n# Example usage:\n# df, ax = task_func('2020-01-01', '2020-12-31', 3, 42)\n# df.head(2)\n```\n"}
{"task_id": "BigCodeBench/982", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport pandas as pd\ndef task_func(df, column, bins=30, density=True, alpha=0.6, color=\"g\", seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Extract the data from the specified column\n    data = df[column].dropna()\n\n    # Plot the histogram\n    count, bins, ignored = plt.hist(data, bins=bins, density=density, alpha=alpha, color=color, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n\n    # Plot the normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    # Set the title and labels\n    plt.title(f\"Normal Fit for '{column}'\")\n    plt.xlabel(column)\n    plt.ylabel('Density' if density else 'Frequency')\n\n    # Return the axes object\n    return plt.gca()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport pandas as pd\n\ndef task_func(df, column, bins=30, density=True, alpha=0.6, color=\"g\", seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Extract the data from the specified column\n    data = df[column].dropna()\n\n    # Plot the histogram\n    count, bins, ignored = plt.hist(data, bins=bins, density=density, alpha=alpha, color=color, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n\n    # Plot the normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    # Set the title and labels\n    plt.title(f\"Normal Fit for '{column}'\")\n    plt.xlabel(column)\n    plt.ylabel('Density' if density else 'Frequency')\n\n    # Return the axes object\n    return plt.gca()\n\n# Example usage:\n# np.random.seed(0)\n# df = pd.DataFrame({'A': np.random.normal(0, 1, 1000)})\n# ax = task_func(df, 'A')\n# print(ax.get_title())\n```\n"}
{"task_id": "BigCodeBench/983", "solution": "import seaborn as sns\nimport numpy as np\nimport pandas as pd\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"DataFrame should not be empty.\")\n    \n    if not all(df.dtypes.apply(np.issubdtype, args=(np.number,))):\n        raise TypeError(\"DataFrame should only contain numeric data.\")\n    \n    covariance_df = df.cov()\n    pair_plot = sns.pairplot(df)\n    \n    return covariance_df, pair_plot", "raw_solution": "import seaborn as sns\nimport numpy as np\nimport pandas as pd\n\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"DataFrame should not be empty.\")\n    \n    if not all(df.dtypes.apply(np.issubdtype, args=(np.number,))):\n        raise TypeError(\"DataFrame should only contain numeric data.\")\n    \n    covariance_df = df.cov()\n    pair_plot = sns.pairplot(df)\n    \n    return covariance_df, pair_plot\n```\n"}
{"task_id": "BigCodeBench/984", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\ndef task_func(df, x_column, y_column):\n    # Extracting the data\n    X = df[[x_column]].values\n    y = df[y_column].values\n\n    # Creating and training the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predicting values\n    y_pred = model.predict(X)\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    ax.scatter(X, y, color='blue', label='Data points')\n    ax.plot(X, y_pred, color='red', label='Regression line')\n    ax.set_xlabel(x_column)\n    ax.set_ylabel(y_column)\n    ax.set_title('Scatter Plot with Regression Line')\n    ax.legend()\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\ndef task_func(df, x_column, y_column):\n    # Extracting the data\n    X = df[[x_column]].values\n    y = df[y_column].values\n\n    # Creating and training the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predicting values\n    y_pred = model.predict(X)\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    ax.scatter(X, y, color='blue', label='Data points')\n    ax.plot(X, y_pred, color='red', label='Regression line')\n    ax.set_xlabel(x_column)\n    ax.set_ylabel(y_column)\n    ax.set_title('Scatter Plot with Regression Line')\n    ax.legend()\n\n    return ax\n\n# Example usage"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data provided.\")\n\n    if \"Countries\" not in data:\n        raise ValueError(\"JSON data must contain 'Countries' key.\")\n\n    countries_data = data[\"Countries\"]\n    if not isinstance(countries_data, dict):\n        raise ValueError(\"Countries data must be a dictionary.\")\n\n    report_data = []\n    for country, population in countries_data.items():\n        if not isinstance(country, str) or not isinstance(population, (int, float)):\n            raise ValueError(\"Country names must be strings and population must be a number.\")\n        if population < 0:\n            raise ValueError(\"Population must be a non-negative number.\")\n        report_data.append({\"Country\": country, \"Population\": math.floor(population)})\n\n    df = pd.DataFrame(report_data)\n\n    if not os.path.exists(output_dir):\n        raise IOError(f\"Output directory '{output_dir}' does not exist.\")\n\n    csv_file_path = os.path.join(output_dir, file_name)\n    df.to_csv(csv_file_path, index=False)\n\n    return csv_file_path, df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data provided.\")\n\n    if \"Countries\" not in data:\n        raise ValueError(\"JSON data must contain 'Countries' key.\")\n\n    countries_data = data[\"Countries\"]\n    if not isinstance(countries_data, dict):\n        raise ValueError(\"Countries data must be a dictionary.\")\n\n    report_data = []\n    for country, population in countries_data.items():\n        if not isinstance(country, str) or not isinstance(population, (int, float)):\n            raise ValueError(\"Country names must be strings and population must be a number.\")\n        if population < 0:\n            raise ValueError(\"Population must be a non-negative number.\")\n        report_data.append({\"Country\": country, \"Population\": math.floor(population)})\n\n    df = pd.DataFrame(report_data)\n\n    if not os.path.exists(output_dir):\n        raise IOError(f\"Output directory '{output_dir}' does not exist.\")\n\n    csv_file_path = os.path.join(output_dir, file_name)\n    df.to_csv(csv_file_path, index=False)\n\n    return csv_file_path, df\n```\n"}
{"task_id": "BigCodeBench/986", "solution": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ndef task_func(json_data: str, key_path: list):\n    try:\n        # Parse the JSON data\n        data = json.loads(json_data)\n        \n        # Traverse the key path to get the data\n        for key in key_path:\n            if key in data:\n                data = data[key]\n            else:\n                raise KeyError(f\"Key '{key}' not found in JSON data.\")\n        \n        # Ensure the data is a list of numbers\n        if isinstance(data, str):\n            data = list(map(float, data.split(',')))\n        elif not isinstance(data, list):\n            raise ValueError(\"Data is not a list of numbers.\")\n        \n        # Create a DataFrame\n        df = pd.DataFrame(data, columns=['Values'])\n        \n        # Plot the data\n        fig, ax = plt.subplots()\n        sns.boxplot(x='Values', data=df, ax=ax)\n        ax.set_title('Boxplot of Values')\n        \n        return fig\n    \n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data.\")\n    except ValueError as e:\n        raise ValueError(f\"Invalid data: {e}\")", "raw_solution": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef task_func(json_data: str, key_path: list):\n    try:\n        # Parse the JSON data\n        data = json.loads(json_data)\n        \n        # Traverse the key path to get the data\n        for key in key_path:\n            if key in data:\n                data = data[key]\n            else:\n                raise KeyError(f\"Key '{key}' not found in JSON data.\")\n        \n        # Ensure the data is a list of numbers\n        if isinstance(data, str):\n            data = list(map(float, data.split(',')))\n        elif not isinstance(data, list):\n            raise ValueError(\"Data is not a list of numbers.\")\n        \n        # Create a DataFrame\n        df = pd.DataFrame(data, columns=['Values'])\n        \n        # Plot the data\n        fig, ax = plt.subplots()\n        sns.boxplot(x='Values', data=df, ax=ax)\n        ax.set_title('Boxplot of Values')\n        \n        return fig\n    \n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data.\")\n    except ValueError as e:\n        raise ValueError(f\"Invalid data: {e}\")\n```\n"}
{"task_id": "BigCodeBench/987", "solution": "import json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(json_data: str, data_key: str):\n    try:\n        data = json.loads(json_data)\n        series = pd.Series(data[data_key])\n    except KeyError:\n        raise KeyError(f\"Key '{data_key}' not found in JSON data.\")\n\n    original_data = series.copy()\n\n    if series.empty:\n        return original_data, None, None\n\n    scaler = MinMaxScaler()\n    normalized_data = pd.Series(scaler.fit_transform(series.values.reshape(-1, 1)).flatten())\n\n    fig, ax = plt.subplots()\n    ax.plot(original_data, label='Original Data', color='blue')\n    ax.plot(normalized_data, label='Normalized Data', color='red')\n    ax.set_title('Original and Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return original_data, normalized_data, ax", "raw_solution": "import json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(json_data: str, data_key: str):\n    try:\n        data = json.loads(json_data)\n        series = pd.Series(data[data_key])\n    except KeyError:\n        raise KeyError(f\"Key '{data_key}' not found in JSON data.\")\n\n    original_data = series.copy()\n\n    if series.empty:\n        return original_data, None, None\n\n    scaler = MinMaxScaler()\n    normalized_data = pd.Series(scaler.fit_transform(series.values.reshape(-1, 1)).flatten())\n\n    fig, ax = plt.subplots()\n    ax.plot(original_data, label='Original Data', color='blue')\n    ax.plot(normalized_data, label='Normalized Data', color='red')\n    ax.set_title('Original and Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return original_data, normalized_data, ax\n```\n"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    if not os.path.exists(dir_path) or not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The directory {dir_path} does not exist or is not a directory.\")\n    \n    if not all(predicate in ['is_file', 'is_dir', 'has_special_chars', 'has_numbers'] for predicate in predicates):\n        raise ValueError(\"Invalid predicates provided.\")\n    \n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = Path(dir_path) / item\n        item_result = {}\n        \n        if 'is_file' in predicates:\n            item_result['is_file'] = item_path.is_file()\n        if 'is_dir' in predicates:\n            item_result['is_dir'] = item_path.is_dir()\n        if 'has_special_chars' in predicates:\n            item_result['has_special_chars'] = bool(re.search(r'[^a-zA-Z0-9_]', item))\n        if 'has_numbers' in predicates:\n            item_result['has_numbers'] = bool(re.search(r'\\d', item))\n        \n        result[item] = item_result\n    \n    return result", "raw_solution": "import os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    if not os.path.exists(dir_path) or not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The directory {dir_path} does not exist or is not a directory.\")\n    \n    if not all(predicate in ['is_file', 'is_dir', 'has_special_chars', 'has_numbers'] for predicate in predicates):\n        raise ValueError(\"Invalid predicates provided.\")\n    \n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = Path(dir_path) / item\n        item_result = {}\n        \n        if 'is_file' in predicates:\n            item_result['is_file'] = item_path.is_file()\n        if 'is_dir' in predicates:\n            item_result['is_dir'] = item_path.is_dir()\n        if 'has_special_chars' in predicates:\n            item_result['has_special_chars'] = bool(re.search(r'[^a-zA-Z0-9_]', item))\n        if 'has_numbers' in predicates:\n            item_result['has_numbers'] = bool(re.search(r'\\d', item))\n        \n        result[item] = item_result\n    \n    return result\n\n# Example usage:\n# print(task_func('/path/to/dir', ['is_file', 'has_numbers']))\n```\n"}
{"task_id": "BigCodeBench/989", "solution": "import random\nimport string\ndef task_func(length: int, predicates: list, seed: int = None):\n    if seed is not None:\n        random.seed(seed)\n\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative integer.\")\n\n    characters = string.ascii_letters + string.punctuation + string.digits\n\n    def generate_random_string(length, characters):\n        return ''.join(random.choice(characters) for _ in range(length))\n\n    def check_predicates(s, predicates):\n        result = {}\n        result['has_uppercase'] = any(c.isupper() for c in s)\n        result['has_lowercase'] = any(c.islower() for c in s)\n        result['has_special_chars'] = any(c in string.punctuation for c in s)\n        result['has_numbers'] = any(c.isdigit() for c in s)\n\n        for predicate in predicates:\n            if predicate not in result:\n                raise KeyError(f\"Predicate '{predicate}' is not valid.\")\n        \n        return {predicate: result[predicate] for predicate in predicates}\n\n    random_string = generate_random_string(length, characters)\n    predicates_result = check_predicates(random_string, predicates)\n\n    return random_string, predicates_result", "raw_solution": "import random\nimport string\n\ndef task_func(length: int, predicates: list, seed: int = None):\n    if seed is not None:\n        random.seed(seed)\n\n    if length < 0:\n        raise ValueError(\"Length must be a non-negative integer.\")\n\n    characters = string.ascii_letters + string.punctuation + string.digits\n\n    def generate_random_string(length, characters):\n        return ''.join(random.choice(characters) for _ in range(length))\n\n    def check_predicates(s, predicates):\n        result = {}\n        result['has_uppercase'] = any(c.isupper() for c in s)\n        result['has_lowercase'] = any(c.islower() for c in s)\n        result['has_special_chars'] = any(c in string.punctuation for c in s)\n        result['has_numbers'] = any(c.isdigit() for c in s)\n\n        for predicate in predicates:\n            if predicate not in result:\n                raise KeyError(f\"Predicate '{predicate}' is not valid.\")\n        \n        return {predicate: result[predicate] for predicate in predicates}\n\n    random_string = generate_random_string(length, characters)\n    predicates_result = check_predicates(random_string, predicates)\n\n    return random_string, predicates_result\n\n# Example usage:\n# print(task_func(10, ['has_uppercase', 'has_numbers'], seed=42))\n# print(task_func(5, ['has_lowercase'], seed=123))\n```\n"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    try:\n        # Convert hexadecimal string to bytes\n        byte_data = binascii.unhexlify(hex_string)\n        \n        # Convert bytes to different encodings\n        encodings = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('utf-8'),\n            'utf-8': byte_data.decode('utf-8'),\n            'utf-16': byte_data.decode('utf-16'),\n            'utf-32': byte_data.decode('utf-32'),\n            'ascii': byte_data.decode('ascii'),\n            'url': urllib.parse.quote(byte_data.decode('utf-8')),\n            'rot13': codecs.encode(byte_data.decode('utf-8'), 'rot_13')\n        }\n        \n        # Handle non-ascii characters\n        for key, value in encodings.items():\n            try:\n                value.encode('ascii')\n            except UnicodeEncodeError:\n                encodings[key] = 'ascii mein representable nahin kiya jaa sakta'\n        \n        return encodings\n\n    except binascii.Error:\n        return {'error': 'Invalid hexadecimal string'}", "raw_solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    try:\n        # Convert hexadecimal string to bytes\n        byte_data = binascii.unhexlify(hex_string)\n        \n        # Convert bytes to different encodings\n        encodings = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('utf-8'),\n            'utf-8': byte_data.decode('utf-8'),\n            'utf-16': byte_data.decode('utf-16'),\n            'utf-32': byte_data.decode('utf-32'),\n            'ascii': byte_data.decode('ascii'),\n            'url': urllib.parse.quote(byte_data.decode('utf-8')),\n            'rot13': codecs.encode(byte_data.decode('utf-8'), 'rot_13')\n        }\n        \n        # Handle non-ascii characters\n        for key, value in encodings.items():\n            try:\n                value.encode('ascii')\n            except UnicodeEncodeError:\n                encodings[key] = 'ascii mein representable nahin kiya jaa sakta'\n        \n        return encodings\n\n    except binascii.Error:\n        return {'error': 'Invalid hexadecimal string'}\n\n# Test cases"}
{"task_id": "BigCodeBench/991", "solution": "import binascii\nimport string\nimport random\ndef task_func(length):\n    if length <= 0:\n        raise ValueError(\"Length must be a positive integer.\")\n    \n    # Generate a random hexadecimal string of the given length\n    hex_string = ''.join(random.choice(string.hexdigits) for _ in range(length))\n    \n    # Convert the hexadecimal string to bytes\n    byte_data = binascii.unhexlify(hex_string)\n    \n    # Decode the bytes to an ASCII string\n    ascii_string = byte_data.decode('ascii', errors='ignore')\n    \n    return ascii_string", "raw_solution": "import binascii\nimport string\nimport random\n\ndef task_func(length):\n    if length <= 0:\n        raise ValueError(\"Length must be a positive integer.\")\n    \n    # Generate a random hexadecimal string of the given length\n    hex_string = ''.join(random.choice(string.hexdigits) for _ in range(length))\n    \n    # Convert the hexadecimal string to bytes\n    byte_data = binascii.unhexlify(hex_string)\n    \n    # Decode the bytes to an ASCII string\n    ascii_string = byte_data.decode('ascii', errors='ignore')\n    \n    return ascii_string\n\n# Test cases\nrandom.seed(0)"}
{"task_id": "BigCodeBench/992", "solution": "import sys\nimport sqlite3\nPATH_TO_APPEND = \"path/to/whatever\"\nDATABASE = \"path/to/database.db\"\ndef task_func(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    \"\"\"\n    This function appends a given path to sys.path and updates a SQLite database with the path.\n    It ensures that a table is created and duplicates are handled.\n\n    Params:\n    - `path_to_append` (str): The file system path to append to sys.path and the database.\n    - `database` (str): The file system path to the SQLite database file. The function interacts with this database.\n\n    Returns:\n        str: The path that was appended to sys.path and the database.\n\n    Reqs:\n    - sys\n    - sqlite3\n\n    Examples:\n    - >>> task_func('path/to/new_new_directory', 'path/to/new_database.db')\n    - 'path/to/new_new_directory'\n    - >>> task_func()\n    - 'path/to/whatever'\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Connect to the SQLite database\n    conn = sqlite3.connect(database)\n    cursor = conn.cursor()\n\n    # Create a table if it doesn't exist\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS paths (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            path TEXT NOT NULL\n        )\n    ''')\n\n    # Insert the path into the database, handling duplicates\n    cursor.execute('''\n        INSERT OR IGNORE INTO paths (path) VALUES (?)\n    ''', (path_to_append,))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    conn.close()\n\n    return path_to_append", "raw_solution": "import sys\nimport sqlite3\n\n# Constants\nPATH_TO_APPEND = \"path/to/whatever\"\nDATABASE = \"path/to/database.db\"\n\n\ndef task_func(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    \"\"\"\n    This function appends a given path to sys.path and updates a SQLite database with the path.\n    It ensures that a table is created and duplicates are handled.\n\n    Params:\n    - `path_to_append` (str): The file system path to append to sys.path and the database.\n    - `database` (str): The file system path to the SQLite database file. The function interacts with this database.\n\n    Returns:\n        str: The path that was appended to sys.path and the database.\n\n    Reqs:\n    - sys\n    - sqlite3\n\n    Examples:\n    - >>> task_func('path/to/new_new_directory', 'path/to/new_database.db')\n    - 'path/to/new_new_directory'\n    - >>> task_func()\n    - 'path/to/whatever'\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Connect to the SQLite database\n    conn = sqlite3.connect(database)\n    cursor = conn.cursor()\n\n    # Create a table if it doesn't exist\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS paths (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            path TEXT NOT NULL\n        )\n    ''')\n\n    # Insert the path into the database, handling duplicates\n    cursor.execute('''\n        INSERT OR IGNORE INTO paths (path) VALUES (?)\n    ''', (path_to_append,))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    conn.close()\n\n    return path_to_append\n```\n"}
{"task_id": "BigCodeBench/993", "solution": "import re\nfrom scipy.stats import gaussian_kde\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Calculate word frequencies\n    word_freq = {word: words.count(word) for word in set(words)}\n\n    # Prepare data for plotting\n    word_lengths = [len(word) for word in words]\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(word_lengths, bins=range(1, max(word_lengths) + 2), alpha=0.7, color='blue', edgecolor='black')\n\n    # Calculate KDE if there are enough data points\n    if len(word_lengths) > 2:\n        kde = gaussian_kde(word_lengths)\n        x_range = range(1, max(word_lengths) + 1)\n        ax.plot(x_range, kde(x_range), color='red', label='KDE')\n\n    ax.set_title('Word Length Distribution')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n\n    return ax", "raw_solution": "import re\nfrom scipy.stats import gaussian_kde\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Extract words from the text\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Calculate word frequencies\n    word_freq = {word: words.count(word) for word in set(words)}\n\n    # Prepare data for plotting\n    word_lengths = [len(word) for word in words]\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(word_lengths, bins=range(1, max(word_lengths) + 2), alpha=0.7, color='blue', edgecolor='black')\n\n    # Calculate KDE if there are enough data points\n    if len(word_lengths) > 2:\n        kde = gaussian_kde(word_lengths)\n        x_range = range(1, max(word_lengths) + 1)\n        ax.plot(x_range, kde(x_range), color='red', label='KDE')\n\n    ax.set_title('Word Length Distribution')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/994", "solution": "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\ndef task_func(url: str, csv_file_path: str) -> list:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise RuntimeError(f\"Error fetching the URL: {e}\") from e\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    articles = soup.find_all('div', class_='container')\n    data = []\n\n    for article in articles:\n        title = article.find('h1').text if article.find('h1') else 'No title'\n        date = article.find('span', class_='date').text if article.find('span', class_='date') else 'No date'\n        author = article.find('span', class_='author').text if article.find('span', class_='author') else 'No author'\n        data.append((title, date, author))\n\n    df = pd.DataFrame(data, columns=['Title', 'Date', 'Author'])\n    df.to_csv(csv_file_path, index=False)\n\n    return data", "raw_solution": "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\ndef task_func(url: str, csv_file_path: str) -> list:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise RuntimeError(f\"Error fetching the URL: {e}\") from e\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    articles = soup.find_all('div', class_='container')\n    data = []\n\n    for article in articles:\n        title = article.find('h1').text if article.find('h1') else 'No title'\n        date = article.find('span', class_='date').text if article.find('span', class_='date') else 'No date'\n        author = article.find('span', class_='author').text if article.find('span', class_='author') else 'No author'\n        data.append((title, date, author))\n\n    df = pd.DataFrame(data, columns=['Title', 'Date', 'Author'])\n    df.to_csv(csv_file_path, index=False)\n\n    return data\n```\n"}
{"task_id": "BigCodeBench/995", "solution": "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(file_path: str, plot_path: str) -> (float, float, str):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"No file found at {file_path}\")\n\n    df = pd.read_csv(file_path)\n\n    if 'maan' not in df.columns:\n        raise ValueError(\"CSV file must contain 'maan' column\")\n\n    data = df['maan'].dropna()\n\n    if data.empty:\n        return np.nan, np.nan, plot_path\n\n    mean_value = data.mean()\n    median_value = data.median()\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(data, label='Data')\n    plt.title('Data Visualization')\n    plt.xlabel('Index')\n    plt.ylabel('maan')\n    plt.legend()\n    plt.savefig(plot_path)\n    plt.close()\n\n    return mean_value, median_value, plot_path", "raw_solution": "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(file_path: str, plot_path: str) -> (float, float, str):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"No file found at {file_path}\")\n\n    df = pd.read_csv(file_path)\n\n    if 'maan' not in df.columns:\n        raise ValueError(\"CSV file must contain 'maan' column\")\n\n    data = df['maan'].dropna()\n\n    if data.empty:\n        return np.nan, np.nan, plot_path\n\n    mean_value = data.mean()\n    median_value = data.median()\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(data, label='Data')\n    plt.title('Data Visualization')\n    plt.xlabel('Index')\n    plt.ylabel('maan')\n    plt.legend()\n    plt.savefig(plot_path)\n    plt.close()\n\n    return mean_value, median_value, plot_path\n```\n"}
{"task_id": "BigCodeBench/996", "solution": "import requests\nimport json\nfrom bs4 import BeautifulSoup\ndef task_func(url: str, file_name: str = \"Output.txt\") -> str:\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n\n    data = {'title': title.string if title else None}\n    \n    with open(file_name, 'a') as file:\n        json.dump(data, file)\n        file.write('\\n')\n\n    return file_name", "raw_solution": "import requests\nimport json\nfrom bs4 import BeautifulSoup\n\ndef task_func(url: str, file_name: str = \"Output.txt\") -> str:\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n\n    data = {'title': title.string if title else None}\n    \n    with open(file_name, 'a') as file:\n        json.dump(data, file)\n        file.write('\\n')\n\n    return file_name\n```\n"}
{"task_id": "BigCodeBench/997", "solution": "import urllib.request\nimport os\nimport zipfile\nTARGET_DIR = \"downloaded_files\"\nTARGET_ZIP_FILE = \"downloaded_files.zip\"\ndef task_func(url):\n    # Create target directory if it doesn't exist\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    # Download the zip file\n    try:\n        urllib.request.urlretrieve(url, TARGET_ZIP_FILE)\n    except Exception as e:\n        print(f\"Error downloading the file: {e}\")\n        return None\n\n    # Extract the zip file\n    try:\n        with zipfile.ZipFile(TARGET_ZIP_FILE, 'r') as zip_ref:\n            zip_ref.extractall(TARGET_DIR)\n    except zipfile.BadZipFile as e:\n        print(f\"Error extracting the zip file: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Error extracting the zip file: {e}\")\n        return None\n\n    # Clean up by deleting the zip file\n    try:\n        os.remove(TARGET_ZIP_FILE)\n    except Exception as e:\n        print(f\"Error deleting the zip file: {e}\")\n\n    return TARGET_DIR", "raw_solution": "import urllib.request\nimport os\nimport zipfile\n\n# Constants\nTARGET_DIR = \"downloaded_files\"\nTARGET_ZIP_FILE = \"downloaded_files.zip\"\n\n\ndef task_func(url):\n    # Create target directory if it doesn't exist\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    # Download the zip file\n    try:\n        urllib.request.urlretrieve(url, TARGET_ZIP_FILE)\n    except Exception as e:\n        print(f\"Error downloading the file: {e}\")\n        return None\n\n    # Extract the zip file\n    try:\n        with zipfile.ZipFile(TARGET_ZIP_FILE, 'r') as zip_ref:\n            zip_ref.extractall(TARGET_DIR)\n    except zipfile.BadZipFile as e:\n        print(f\"Error extracting the zip file: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Error extracting the zip file: {e}\")\n        return None\n\n    # Clean up by deleting the zip file\n    try:\n        os.remove(TARGET_ZIP_FILE)\n    except Exception as e:\n        print(f\"Error deleting the zip file: {e}\")\n\n    return TARGET_DIR\n```\n"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the tar.gz file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n\n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(byte_block)\n        checksum = md5_hash.hexdigest()\n\n        # Validate the checksum\n        if checksum == EXPECTED_MD5_CHECKSUM:\n            # Extract the tar.gz file if the checksum is correct\n            with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n                tar.extractall()\n            return True\n        else:\n            # Remove the file if the checksum is incorrect\n            os.remove(TARGET_TAR_FILE)\n            return False\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False", "raw_solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\n\ndef task_func(url):\n    try:\n        # Download the tar.gz file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n\n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(byte_block)\n        checksum = md5_hash.hexdigest()\n\n        # Validate the checksum\n        if checksum == EXPECTED_MD5_CHECKSUM:\n            # Extract the tar.gz file if the checksum is correct\n            with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n                tar.extractall()\n            return True\n        else:\n            # Remove the file if the checksum is incorrect\n            os.remove(TARGET_TAR_FILE)\n            return False\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```\n"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Check if the file already exists\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n        \n        # Read the CSV file\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n            reader = csv.DictReader(file)\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"Column '{column_name}' not found in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            counter = collections.Counter(row[column_name] for row in reader)\n            \n            return dict(counter)\n    \n    except urllib.error.URLError as e:\n        print(f\"Failed to download the file: {e}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")", "raw_solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Check if the file already exists\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n        \n        # Read the CSV file\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n            reader = csv.DictReader(file)\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"Column '{column_name}' not found in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            counter = collections.Counter(row[column_name] for row in reader)\n            \n            return dict(counter)\n    \n    except urllib.error.URLError as e:\n        print(f\"Failed to download the file: {e}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n# Example usage:\n# print(task_func('http://example.com/data.csv', 'category', 'downloaded_data.csv'))\n```\nThis script downloads a CSV file from a given URL, reads it, and counts the occurrences of each value in a specified column. It handles errors such as missing columns and file download issues.\n"}
{"task_id": "BigCodeBench/1000", "solution": "import urllib.request\nimport os\nimport json\nimport pandas as pd\nTARGET_JSON_FILE = \"downloaded_file.json\"\ndef task_func(url):\n    # Download the JSON file\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n\n    # Read the JSON file\n    with open(TARGET_JSON_FILE, 'r') as file:\n        data = json.load(file)\n\n    # Convert JSON data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Remove the temporary JSON file\n    os.remove(TARGET_JSON_FILE)\n\n    return df", "raw_solution": "import urllib.request\nimport os\nimport json\nimport pandas as pd\n\n# Constants\nTARGET_JSON_FILE = \"downloaded_file.json\"\n\n\ndef task_func(url):\n    # Download the JSON file\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n\n    # Read the JSON file\n    with open(TARGET_JSON_FILE, 'r') as file:\n        data = json.load(file)\n\n    # Convert JSON data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Remove the temporary JSON file\n    os.remove(TARGET_JSON_FILE)\n\n    return df\n```\n"}
{"task_id": "BigCodeBench/1001", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path: str):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Check if 'column1' exists in the DataFrame\n    if 'column1' not in df.columns:\n        raise ValueError(\"CSV file must contain 'column1' column\")\n\n    # Normalize the 'column1' data\n    df['column1_normalized'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n\n    # Plot the normalized data\n    fig, ax = plt.subplots()\n    ax.plot(df['column1_normalized'], label='Normalized Column 1')\n\n    # Set the title and labels with formatting\n    title = \"{:<20} : {:<20}\".format('Plot Title', 'Normalized Column 1')\n    xlabel = \"{:<20} : {:<20}\".format('Index', 'Normalized Value')\n    ylabel = \"{:<20} : {:<20}\".format('Frequency', 'Normalized Value')\n\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file_path: str):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Check if 'column1' exists in the DataFrame\n    if 'column1' not in df.columns:\n        raise ValueError(\"CSV file must contain 'column1' column\")\n\n    # Normalize the 'column1' data\n    df['column1_normalized'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n\n    # Plot the normalized data\n    fig, ax = plt.subplots()\n    ax.plot(df['column1_normalized'], label='Normalized Column 1')\n\n    # Set the title and labels with formatting\n    title = \"{:<20} : {:<20}\".format('Plot Title', 'Normalized Column 1')\n    xlabel = \"{:<20} : {:<20}\".format('Index', 'Normalized Value')\n    ylabel = \"{:<20} : {:<20}\".format('Frequency', 'Normalized Value')\n\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1002", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, column_name=\"target_column\"):\n    df = pd.DataFrame(data)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n    \n    df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n    \n    ax = df[column_name].plot(kind='hist', title=f'Histogram of {column_name}', bins=10)\n    ax.set_xlabel(column_name)\n    \n    plt.show()\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column_name=\"target_column\"):\n    df = pd.DataFrame(data)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n    \n    df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n    \n    ax = df[column_name].plot(kind='hist', title=f'Histogram of {column_name}', bins=10)\n    ax.set_xlabel(column_name)\n    \n    plt.show()\n    \n    return df, ax\n\n# Example usage:\nsample_data = [{'userid': 1, 'value': 10}, {'userid': 2, 'value': 15}]\ndf, ax = task_func(sample_data, 'userid')"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n\n        # Parse the XML content\n        try:\n            root = etree.fromstring(xml_content)\n        except etree.XMLSyntaxError:\n            raise ValueError(\"XML structure expected format se mel nahi khaati.\")\n\n        # Extract data from XML and convert to DataFrame\n        data = []\n        for item in root.findall('item'):\n            row = {}\n            for child in item:\n                row[child.tag] = child.text\n            data.append(row)\n\n        df = pd.DataFrame(data)\n\n        return df\n\n    except urllib.error.URLError:\n        raise ValueError(\"URL invalid hai ya XML file URL se prapt nahi ki ja sakti.\")", "raw_solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n\n        # Parse the XML content\n        try:\n            root = etree.fromstring(xml_content)\n        except etree.XMLSyntaxError:\n            raise ValueError(\"XML structure expected format se mel nahi khaati.\")\n\n        # Extract data from XML and convert to DataFrame\n        data = []\n        for item in root.findall('item'):\n            row = {}\n            for child in item:\n                row[child.tag] = child.text\n            data.append(row)\n\n        df = pd.DataFrame(data)\n\n        return df\n\n    except urllib.error.URLError:\n        raise ValueError(\"URL invalid hai ya XML file URL se prapt nahi ki ja sakti.\")\n```\n"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the file from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Extract words using regular expressions\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # Plot the top 10 most common words\n    common_words = word_freq.most_common(10)\n    words, counts = zip(*common_words)\n\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 most common words')\n\n    return word_freq, ax", "raw_solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the file from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Extract words using regular expressions\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # Plot the top 10 most common words\n    common_words = word_freq.most_common(10)\n    words, counts = zip(*common_words)\n\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 most common words')\n\n    return word_freq, ax\n```\n"}
{"task_id": "BigCodeBench/1005", "solution": "import urllib.request\nimport zipfile\nimport os\nimport urllib.error\ndef task_func(\n    url: str,\n    save_path: str = \"downloaded_file.zip\",\n    extract_path: str = \"extracted_files\",\n) -> str:\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, save_path)\n\n        # Extract the file\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_path)\n\n        # Return the path where files were extracted\n        return extract_path\n\n    except urllib.error.URLError as e:\n        raise Exception(f\"URL Error: {e.reason}\")\n\n    except zipfile.BadZipFile:\n        raise Exception(\"Bad ZIP file\")\n\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")", "raw_solution": "import urllib.request\nimport zipfile\nimport os\nimport urllib.error\n\ndef task_func(\n    url: str,\n    save_path: str = \"downloaded_file.zip\",\n    extract_path: str = \"extracted_files\",\n) -> str:\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, save_path)\n\n        # Extract the file\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_path)\n\n        # Return the path where files were extracted\n        return extract_path\n\n    except urllib.error.URLError as e:\n        raise Exception(f\"URL Error: {e.reason}\")\n\n    except zipfile.BadZipFile:\n        raise Exception(\"Bad ZIP file\")\n\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")\n\n# Example usage:\n# extracted_path = task_func('http://www.example.com/data.zip')\n# print(extracted_path)\n```\n"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n\n        # Check if the content is a zip file\n        if 'zip' not in response.headers.get('content-type', '').lower():\n            return \"Error: url ZIP file ko point nahin downloads.\"\n\n        # Save the zip file\n        zip_path = os.path.join(download_path, 'downloaded.zip')\n        with open(zip_path, 'wb') as file:\n            file.write(response.content)\n\n        # Extract the zip file\n        with ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(download_path)\n\n        # Clean up the zip file\n        os.remove(zip_path)\n\n        # Return the path to the extracted directory\n        return download_path\n\n    except requests.RequestException as e:\n        return f\"Error: Provided url se file download karne mein asamarth hai. {str(e)}\"\n    except BadZipFile as e:\n        return \"Error: Download ki gayi file ek valid ZIP file nahi hai.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"", "raw_solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n\n        # Check if the content is a zip file\n        if 'zip' not in response.headers.get('content-type', '').lower():\n            return \"Error: url ZIP file ko point nahin downloads.\"\n\n        # Save the zip file\n        zip_path = os.path.join(download_path, 'downloaded.zip')\n        with open(zip_path, 'wb') as file:\n            file.write(response.content)\n\n        # Extract the zip file\n        with ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(download_path)\n\n        # Clean up the zip file\n        os.remove(zip_path)\n\n        # Return the path to the extracted directory\n        return download_path\n\n    except requests.RequestException as e:\n        return f\"Error: Provided url se file download karne mein asamarth hai. {str(e)}\"\n    except BadZipFile as e:\n        return \"Error: Download ki gayi file ek valid ZIP file nahi hai.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n```\n"}
{"task_id": "BigCodeBench/1007", "solution": "import requests\nimport pandas as pd\ndef task_func(url: str) -> pd.DataFrame:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise SystemError(f\"An error occurred while fetching the data: {e}\")\n\n    try:\n        data = response.json()\n    except ValueError as e:\n        raise ValueError(\"The response is not a valid JSON format.\")\n\n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise ValueError(\"The JSON data is not in a format suitable for conversion to a DataFrame.\")\n\n    df = pd.DataFrame(data)\n\n    return df", "raw_solution": "import requests\nimport pandas as pd\n\ndef task_func(url: str) -> pd.DataFrame:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise SystemError(f\"An error occurred while fetching the data: {e}\")\n\n    try:\n        data = response.json()\n    except ValueError as e:\n        raise ValueError(\"The response is not a valid JSON format.\")\n\n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise ValueError(\"The JSON data is not in a format suitable for conversion to a DataFrame.\")\n\n    df = pd.DataFrame(data)\n\n    return df\n\n# Example usage:\n# df = task_func('https://example.com/data.json')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        raise requests.exceptions.HTTPError(f\"HTTP error occurred: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', id=table_id)\n\n    if table is None:\n        raise ValueError(\"specified ID wali table nahi mili.\")\n\n    rows = table.find_all('tr')\n    if not rows:\n        return pd.DataFrame()\n\n    data = []\n    for row in rows:\n        cols = row.find_all(['td', 'th'])\n        data.append([col.get_text(strip=True) for col in cols])\n\n    df = pd.DataFrame(data[1:], columns=data[0])\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        raise requests.exceptions.HTTPError(f\"HTTP error occurred: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', id=table_id)\n\n    if table is None:\n        raise ValueError(\"specified ID wali table nahi mili.\")\n\n    rows = table.find_all('tr')\n    if not rows:\n        return pd.DataFrame()\n\n    data = []\n    for row in rows:\n        cols = row.find_all(['td', 'th'])\n        data.append([col.get_text(strip=True) for col in cols])\n\n    df = pd.DataFrame(data[1:], columns=data[0])\n    return df\n\n# Example usage:\n# df = task_func('https://example.com/data.html', 'table1')\n# print(df)\n```\n"}
{"task_id": "BigCodeBench/1009", "solution": "import xml.etree.ElementTree as ET\nimport csv\ndef task_func(xml_content, output_csv_path):\n    try:\n        root = ET.fromstring(xml_content)\n    except ET.ParseError as e:\n        raise ET.ParseError(f\"Error parsing XML content: {e}\")\n\n    try:\n        with open(output_csv_path, 'w', newline='') as csvfile:\n            fieldnames = [elem.tag for elem in root.findall('./*')]\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n            writer.writeheader()\n            for element in root.findall('./*'):\n                row_data = {child.tag: child.text for child in element}\n                writer.writerow(row_data)\n    except IOError as e:\n        raise IOError(f\"Error writing to CSV file: {e}\")", "raw_solution": "import xml.etree.ElementTree as ET\nimport csv\n\ndef task_func(xml_content, output_csv_path):\n    try:\n        root = ET.fromstring(xml_content)\n    except ET.ParseError as e:\n        raise ET.ParseError(f\"Error parsing XML content: {e}\")\n\n    try:\n        with open(output_csv_path, 'w', newline='') as csvfile:\n            fieldnames = [elem.tag for elem in root.findall('./*')]\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n            writer.writeheader()\n            for element in root.findall('./*'):\n                row_data = {child.tag: child.text for child in element}\n                writer.writerow(row_data)\n    except IOError as e:\n        raise IOError(f\"Error writing to CSV file: {e}\")\n\n# Example usage:\n# task_func('<root><element>data</element></root>', 'output.csv')\n```\nThis script reads XML content from a string, parses it into an ElementTree, and then writes the data to a CSV file at the specified path. It handles XML parsing errors and IO errors appropriately.\n"}
{"task_id": "BigCodeBench/1010", "solution": "import requests\nfrom PIL import Image\nimport io\ndef task_func(url):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise ValueError(f\"Error downloading image: {e}\")\n\n    try:\n        image = Image.open(io.BytesIO(response.content))\n    except IOError as e:\n        raise ValueError(f\"Error processing image: {e}\")\n\n    return image", "raw_solution": "import requests\nfrom PIL import Image\nimport io\n\ndef task_func(url):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise ValueError(f\"Error downloading image: {e}\")\n\n    try:\n        image = Image.open(io.BytesIO(response.content))\n    except IOError as e:\n        raise ValueError(f\"Error processing image: {e}\")\n\n    return image\n\n# Example usage:\n# img = task_func('https://example.com/image.jpg')\n# print(isinstance(img, Image.Image))  # Output: True\n```\n"}
{"task_id": "BigCodeBench/1011", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Group by the specified column and calculate the mean of the second column\n    grouped_df = df.groupby(col1_name)[col2_name].mean()\n\n    # Plot the results\n    fig, ax = plt.subplots()\n    grouped_df.plot(kind='bar', ax=ax)\n\n    # Set the title and labels\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(f\"{col1_name}\")\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n\n    # Group by the specified column and calculate the mean of the second column\n    grouped_df = df.groupby(col1_name)[col2_name].mean()\n\n    # Plot the results\n    fig, ax = plt.subplots()\n    grouped_df.plot(kind='bar', ax=ax)\n\n    # Set the title and labels\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(f\"{col1_name}\")\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    try:\n        # Ensure the download directory exists\n        DOWNLOAD_DIR.mkdir(exist_ok=True)\n        ZIP_DIR.mkdir(exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n\n        # Save the file\n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n\n        # Extract the zip file\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # List the extracted files\n        extracted_files = [f.name for f in ZIP_DIR.iterdir()]\n\n        return 'Download and extraction successful', extracted_files\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error downloading file: {e}\", []\n    except zipfile.BadZipFile as e:\n        return f\"Error extracting zip file: {e}\", []\n    except Exception as e:\n        return f\"An unexpected error occurred: {e}\", []", "raw_solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    try:\n        # Ensure the download directory exists\n        DOWNLOAD_DIR.mkdir(exist_ok=True)\n        ZIP_DIR.mkdir(exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n\n        # Save the file\n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n\n        # Extract the zip file\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # List the extracted files\n        extracted_files = [f.name for f in ZIP_DIR.iterdir()]\n\n        return 'Download and extraction successful', extracted_files\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error downloading file: {e}\", []\n    except zipfile.BadZipFile as e:\n        return f\"Error extracting zip file: {e}\", []\n    except Exception as e:\n        return f\"An unexpected error occurred: {e}\", []\n```\n"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    response = requests.get(urljoin(base_url, url))\n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = set()\n\n    for a_tag in soup.find_all('a', href=True):\n        full_url = urljoin(base_url, a_tag['href'])\n        links.add(full_url)\n\n    with open(csv_file, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for link in links:\n            writer.writerow([link])\n\n    return len(links)", "raw_solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    response = requests.get(urljoin(base_url, url))\n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = set()\n\n    for a_tag in soup.find_all('a', href=True):\n        full_url = urljoin(base_url, a_tag['href'])\n        links.add(full_url)\n\n    with open(csv_file, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for link in links:\n            writer.writerow([link])\n\n    return len(links)\n```\n"}
{"task_id": "BigCodeBench/1014", "solution": "import requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(api_url):\n    if not isinstance(api_url, str):\n        raise TypeError(\"api_url must be a string\")\n\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP error occurred: {e}\")\n\n    try:\n        data = response.json()\n    except ValueError as e:\n        raise ValueError(f\"Error parsing JSON response: {e}\")\n\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        return df, None\n\n    ax = df.plot(kind='line', title='Data from API')\n    plt.xlabel('Index')\n    plt.ylabel('Values')\n    plt.show()\n\n    return df, ax", "raw_solution": "import requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(api_url):\n    if not isinstance(api_url, str):\n        raise TypeError(\"api_url must be a string\")\n\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP error occurred: {e}\")\n\n    try:\n        data = response.json()\n    except ValueError as e:\n        raise ValueError(f\"Error parsing JSON response: {e}\")\n\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        return df, None\n\n    ax = df.plot(kind='line', title='Data from API')\n    plt.xlabel('Index')\n    plt.ylabel('Values')\n    plt.show()\n\n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the webpage content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n\n    # Parse the HTML content\n    tree = html.fromstring(response.content)\n    tables = tree.findall('.//table')\n\n    if not tables:\n        print(\"No table found in the webpage.\")\n        return 0\n\n    # Extract the first table\n    table = tables[0]\n    df = pd.read_html(html.tostring(table))[0]\n\n    if df.empty:\n        print(\"The table is empty.\")\n        return 0\n\n    # Connect to the SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n        df.to_sql(\"my_table\", conn, if_exists=\"replace\", index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error interacting with the database: {e}\")\n        return 0\n    finally:\n        conn.close()\n\n    return len(df)", "raw_solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the webpage content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n\n    # Parse the HTML content\n    tree = html.fromstring(response.content)\n    tables = tree.findall('.//table')\n\n    if not tables:\n        print(\"No table found in the webpage.\")\n        return 0\n\n    # Extract the first table\n    table = tables[0]\n    df = pd.read_html(html.tostring(table))[0]\n\n    if df.empty:\n        print(\"The table is empty.\")\n        return 0\n\n    # Connect to the SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n        df.to_sql(\"my_table\", conn, if_exists=\"replace\", index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error interacting with the database: {e}\")\n        return 0\n    finally:\n        conn.close()\n\n    return len(df)\n```\n"}
{"task_id": "BigCodeBench/1016", "solution": "import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error downloading image: {e}\")\n\n    try:\n        image = Image.open(BytesIO(response.content))\n        grayscale_image = image.convert(\"L\")\n        image_array = np.array(grayscale_image)\n    except IOError as e:\n        raise IOError(f\"Error processing image: {e}\")\n\n    plt.hist(image_array.ravel(), bins=256, range=[0, 256], color='gray')\n    ax = plt.gca()\n    plt.close()\n\n    return ax", "raw_solution": "import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error downloading image: {e}\")\n\n    try:\n        image = Image.open(BytesIO(response.content))\n        grayscale_image = image.convert(\"L\")\n        image_array = np.array(grayscale_image)\n    except IOError as e:\n        raise IOError(f\"Error processing image: {e}\")\n\n    plt.hist(image_array.ravel(), bins=256, range=[0, 256], color='gray')\n    ax = plt.gca()\n    plt.close()\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1017", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\ndef task_func(csv_file_path, target_column=\"target\", test_size=0.2, n_estimators=100):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n    \n    # Check if the target column exists\n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in the CSV file.\")\n    \n    # Split the dataset into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    \n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Predict the target for the test set\n    y_pred = clf.predict(X_test)\n    \n    # Generate and return the classification report\n    report = classification_report(y_test, y_pred)\n    return report", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\ndef task_func(csv_file_path, target_column=\"target\", test_size=0.2, n_estimators=100):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n    \n    # Check if the target column exists\n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in the CSV file.\")\n    \n    # Split the dataset into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    \n    # Initialize and train the RandomForestClassifier\n    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Predict the target for the test set\n    y_pred = clf.predict(X_test)\n    \n    # Generate and return the classification report\n    report = classification_report(y_test, y_pred)\n    return report\n```\n"}
{"task_id": "BigCodeBench/1018", "solution": "from bs4 import BeautifulSoup\nimport requests\nURL = \"http://example.com\"\ndef task_func(url=URL, from_encoding=\"cp1251\", use_lxml=False):\n    if not url:\n        return None\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return None\n\n    if response.status_code != 200:\n        return None\n\n    try:\n        content = response.content.decode(from_encoding)\n    except (UnicodeDecodeError, LookupError):\n        return None\n\n    if use_lxml:\n        parser = \"lxml\"\n    else:\n        parser = \"html.parser\"\n\n    try:\n        soup = BeautifulSoup(content, parser)\n    except Exception:\n        return None\n\n    return soup", "raw_solution": "from bs4 import BeautifulSoup\nimport requests\n\n# Constants\nURL = \"http://example.com\"\n\n\ndef task_func(url=URL, from_encoding=\"cp1251\", use_lxml=False):\n    if not url:\n        return None\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return None\n\n    if response.status_code != 200:\n        return None\n\n    try:\n        content = response.content.decode(from_encoding)\n    except (UnicodeDecodeError, LookupError):\n        return None\n\n    if use_lxml:\n        parser = \"lxml\"\n    else:\n        parser = \"html.parser\"\n\n    try:\n        soup = BeautifulSoup(content, parser)\n    except Exception:\n        return None\n\n    return soup\n```\n"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Load the image\n        img = Image.open(filename)\n        \n        # Use pytesseract to extract text\n        extracted_text = pytesseract.image_to_string(img)\n        \n        # Decode the extracted text from the original encoding to UTF-8\n        decoded_text = codecs.decode(extracted_text, from_encoding, 'ignore')\n        \n        # Encode the decoded text to the target encoding\n        comment = codecs.encode(decoded_text, to_encoding)\n        \n        return comment.decode(to_encoding)\n    except (UnicodeDecodeError, LookupError):\n        return \"\"\n    except ValueError as e:\n        raise ValueError(f\"An error occurred: {e}\")", "raw_solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Load the image\n        img = Image.open(filename)\n        \n        # Use pytesseract to extract text\n        extracted_text = pytesseract.image_to_string(img)\n        \n        # Decode the extracted text from the original encoding to UTF-8\n        decoded_text = codecs.decode(extracted_text, from_encoding, 'ignore')\n        \n        # Encode the decoded text to the target encoding\n        comment = codecs.encode(decoded_text, to_encoding)\n        \n        return comment.decode(to_encoding)\n    except (UnicodeDecodeError, LookupError):\n        return \"\"\n    except ValueError as e:\n        raise ValueError(f\"An error occurred: {e}\")\n```\n"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    response = requests.get(url, timeout=5)\n    content = response.content\n\n    if not content:\n        return {}\n\n    if from_encoding is None:\n        try:\n            detected = chardet.detect(content)\n            from_encoding = detected['encoding']\n        except Exception as e:\n            raise ValueError(\"Unable to detect encoding for non-empty content\") from e\n\n    try:\n        decoded_content = content.decode(from_encoding)\n        data = json.loads(decoded_content)\n        encoded_content = json.dumps(data, ensure_ascii=False).encode(to_encoding)\n        return json.loads(encoded_content)\n    except Exception as e:\n        raise ValueError(\"Error in encoding/decoding process\") from e", "raw_solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    response = requests.get(url, timeout=5)\n    content = response.content\n\n    if not content:\n        return {}\n\n    if from_encoding is None:\n        try:\n            detected = chardet.detect(content)\n            from_encoding = detected['encoding']\n        except Exception as e:\n            raise ValueError(\"Unable to detect encoding for non-empty content\") from e\n\n    try:\n        decoded_content = content.decode(from_encoding)\n        data = json.loads(decoded_content)\n        encoded_content = json.dumps(data, ensure_ascii=False).encode(to_encoding)\n        return json.loads(encoded_content)\n    except Exception as e:\n        raise ValueError(\"Error in encoding/decoding process\") from e\n```\n"}
{"task_id": "BigCodeBench/1021", "solution": "import binascii\nimport hashlib\ndef task_func(input_string, verify_hash=None):\n    if not isinstance(input_string, str):\n        raise TypeError(\"input_string must be a string.\")\n    if verify_hash is not None and not isinstance(verify_hash, str):\n        raise TypeError(\"verify_hash must be a string or None.\")\n    \n    # Compute SHA256 hash\n    hash_object = hashlib.sha256(input_string.encode())\n    hex_dig = hash_object.hexdigest()\n    \n    # Verify the hash if provided\n    if verify_hash is not None:\n        return hex_dig == verify_hash, hex_dig\n    else:\n        return hex_dig", "raw_solution": "import binascii\nimport hashlib\n\ndef task_func(input_string, verify_hash=None):\n    if not isinstance(input_string, str):\n        raise TypeError(\"input_string must be a string.\")\n    if verify_hash is not None and not isinstance(verify_hash, str):\n        raise TypeError(\"verify_hash must be a string or None.\")\n    \n    # Compute SHA256 hash\n    hash_object = hashlib.sha256(input_string.encode())\n    hex_dig = hash_object.hexdigest()\n    \n    # Verify the hash if provided\n    if verify_hash is not None:\n        return hex_dig == verify_hash, hex_dig\n    else:\n        return hex_dig\n\n# Test cases"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not in the CSV file.\")\n    \n    try:\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    except Exception as e:\n        raise ValueError(f\"Error converting {column_name} to datetime: {e}\")\n    \n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n    \n    return df", "raw_solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not in the CSV file.\")\n    \n    try:\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    except Exception as e:\n        raise ValueError(f\"Error converting {column_name} to datetime: {e}\")\n    \n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n    \n    return df\n```\n"}
{"task_id": "BigCodeBench/1023", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(dataframe):\n    if dataframe.empty:\n        raise ValueError(\"Input DataFrame should not be empty.\")\n    \n    if not all(dataframe.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise TypeError(\"All columns in the DataFrame should be numeric.\")\n    \n    if dataframe.shape[1] < 2:\n        raise ValueError(\"DataFrame should have at least two columns.\")\n    \n    correlation_matrix = dataframe.corr()\n    max_corr = correlation_matrix.abs().max().max()\n    max_corr_column = correlation_matrix.abs().idxmax()[1]\n    \n    fig, ax = plt.subplots()\n    ax.scatter(dataframe[max_corr_column], dataframe.drop(columns=[max_corr_column]).iloc[:, 0])\n    ax.set_title(f'Scatter plot of {max_corr_column} vs {correlation_matrix.columns[1]}')\n    ax.set_xlabel(max_corr_column)\n    ax.set_ylabel(correlation_matrix.columns[1])\n    \n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(dataframe):\n    if dataframe.empty:\n        raise ValueError(\"Input DataFrame should not be empty.\")\n    \n    if not all(dataframe.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise TypeError(\"All columns in the DataFrame should be numeric.\")\n    \n    if dataframe.shape[1] < 2:\n        raise ValueError(\"DataFrame should have at least two columns.\")\n    \n    correlation_matrix = dataframe.corr()\n    max_corr = correlation_matrix.abs().max().max()\n    max_corr_column = correlation_matrix.abs().idxmax()[1]\n    \n    fig, ax = plt.subplots()\n    ax.scatter(dataframe[max_corr_column], dataframe.drop(columns=[max_corr_column]).iloc[:, 0])\n    ax.set_title(f'Scatter plot of {max_corr_column} vs {correlation_matrix.columns[1]}')\n    ax.set_xlabel(max_corr_column)\n    ax.set_ylabel(correlation_matrix.columns[1])\n    \n    return ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     'A': np.random.rand(100),\n#     'B': np.random.rand(100),\n#     'C': np.random.rand(100)\n# })\n# ax = task_func(df)\n# print(ax)\n```\n"}
{"task_id": "BigCodeBench/1024", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nPLOT_TITLE = \"Value Distribution\"\ndef task_func(data_dict):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(data_dict)\n    \n    # Drop rows with None values\n    df = df.dropna()\n    \n    # Check if DataFrame is empty or all values are the same\n    if df.empty or df.nunique().sum() == 1:\n        return df, None\n    \n    # Calculate number of bins dynamically\n    num_bins = max(2, min(11, len(df) // 2))\n    \n    # Create histogram plot\n    fig, ax = plt.subplots()\n    sns.histplot(df.values.flatten(), bins=num_bins, kde=True, ax=ax)\n    ax.set_title(PLOT_TITLE)\n    \n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPLOT_TITLE = \"Value Distribution\"\n\n\ndef task_func(data_dict):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(data_dict)\n    \n    # Drop rows with None values\n    df = df.dropna()\n    \n    # Check if DataFrame is empty or all values are the same\n    if df.empty or df.nunique().sum() == 1:\n        return df, None\n    \n    # Calculate number of bins dynamically\n    num_bins = max(2, min(11, len(df) // 2))\n    \n    # Create histogram plot\n    fig, ax = plt.subplots()\n    sns.histplot(df.values.flatten(), bins=num_bins, kde=True, ax=ax)\n    ax.set_title(PLOT_TITLE)\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/1025", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nPLOT_TITLE = \"Scaled Values\"\ndef task_func(data_dict):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply scaling\n    scaled_values = scaler.fit_transform(df)\n\n    # Convert scaled values back to DataFrame\n    scaled_df = pd.DataFrame(scaled_values, columns=df.columns)\n\n    # Plot scaled values\n    ax = scaled_df.plot(title=PLOT_TITLE)\n\n    return scaled_df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nPLOT_TITLE = \"Scaled Values\"\n\n\ndef task_func(data_dict):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply scaling\n    scaled_values = scaler.fit_transform(df)\n\n    # Convert scaled values back to DataFrame\n    scaled_df = pd.DataFrame(scaled_values, columns=df.columns)\n\n    # Plot scaled values\n    ax = scaled_df.plot(title=PLOT_TITLE)\n\n    return scaled_df, ax\n```\n"}
{"task_id": "BigCodeBench/1026", "solution": "import numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\ndef task_func(kwargs):\n    # Check if the required groups are present in the input\n    if 'group1' not in kwargs or 'group2' not in kwargs:\n        raise ValueError(\"Both 'group1' and 'group2' must be present in the input dictionary.\")\n\n    # Remove NaN values from the groups\n    group1 = np.array(kwargs['group1'])\n    group2 = np.array(kwargs['group2'])\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n\n    # Check if groups are empty after removing NaN values\n    if len(group1) == 0 or len(group2) == 0:\n        raise ValueError(\"One or both groups are empty after removing NaN values.\")\n\n    # Perform t-test to check if the means are significantly different\n    t_stat, p_value = ttest_ind(group1, group2)\n    significant = p_value < 0.05\n\n    # Calculate statistics for each group\n    group1_stats = {'mean': np.mean(group1), 'std': np.std(group1)}\n    group2_stats = {'mean': np.mean(group2), 'std': np.std(group2)}\n\n    # Create boxplot and histograms\n    fig, ax_boxplot = plt.subplots()\n    fig, ax_histogram = plt.subplots(1, 2, figsize=(12, 5))\n\n    ax_boxplot.boxplot([group1, group2], labels=['Group 1', 'Group 2'])\n    ax_histogram[0].hist(group1, bins=10, alpha=0.5, label='Group 1')\n    ax_histogram[0].set_title('Group 1 Histogram')\n    ax_histogram[0].set_xlabel('Value')\n    ax_histogram[0].set_ylabel('Frequency')\n    ax_histogram[1].hist(group2, bins=10, alpha=0.5, label='Group 2')\n    ax_histogram[1].set_title('Group 2 Histogram')\n    ax_histogram[1].set_xlabel('Value')\n    ax_histogram[1].set_ylabel('Frequency')\n\n    return {\n        'significant': significant,\n        'group1_stats': group1_stats,\n        'group2_stats': group2_stats,\n        'ax_boxplot': ax_boxplot,\n        'ax_histogram': ax_histogram\n    }", "raw_solution": "import numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\n\ndef task_func(kwargs):\n    # Check if the required groups are present in the input\n    if 'group1' not in kwargs or 'group2' not in kwargs:\n        raise ValueError(\"Both 'group1' and 'group2' must be present in the input dictionary.\")\n\n    # Remove NaN values from the groups\n    group1 = np.array(kwargs['group1'])\n    group2 = np.array(kwargs['group2'])\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n\n    # Check if groups are empty after removing NaN values\n    if len(group1) == 0 or len(group2) == 0:\n        raise ValueError(\"One or both groups are empty after removing NaN values.\")\n\n    # Perform t-test to check if the means are significantly different\n    t_stat, p_value = ttest_ind(group1, group2)\n    significant = p_value < 0.05\n\n    # Calculate statistics for each group\n    group1_stats = {'mean': np.mean(group1), 'std': np.std(group1)}\n    group2_stats = {'mean': np.mean(group2), 'std': np.std(group2)}\n\n    # Create boxplot and histograms\n    fig, ax_boxplot = plt.subplots()\n    fig, ax_histogram = plt.subplots(1, 2, figsize=(12, 5))\n\n    ax_boxplot.boxplot([group1, group2], labels=['Group 1', 'Group 2'])\n    ax_histogram[0].hist(group1, bins=10, alpha=0.5, label='Group 1')\n    ax_histogram[0].set_title('Group 1 Histogram')\n    ax_histogram[0].set_xlabel('Value')\n    ax_histogram[0].set_ylabel('Frequency')\n    ax_histogram[1].hist(group2, bins=10, alpha=0.5, label='Group 2')\n    ax_histogram[1].set_title('Group 2 Histogram')\n    ax_histogram[1].set_xlabel('Value')\n    ax_histogram[1].set_ylabel('Frequency')\n\n    return {\n        'significant': significant,\n        'group1_stats': group1_stats,\n        'group2_stats': group2_stats,\n        'ax_boxplot': ax_boxplot,\n        'ax_histogram': ax_histogram\n    }\n```\n"}
{"task_id": "BigCodeBench/1027", "solution": "import binascii\nimport urllib.parse\ndef task_func(url):\n    \"\"\"\n    url ke 'q' query parameter se ek hexadecimal string ko decode karein.\n    Yeh function diye gaye url se 'q' query parameter nikaalta hai,\n    Yeh maan kar ki yeh ek hexadecimal string hai, aur ise utf-8 string mein decode karta hai.\n    Yadi hexadecimal string invalid hai ya ek valid utf-8 string mein decode nahi kiya jaa sakta hai, to None lautaya jaata hai.\n\n    Params:\n    - url (str): Vah url jisse query parameter nikaala jaana hai.\n\n    Returns:\n        str or None: Yadi 'q' parameter maujood hai aur ek valid hexadecimal hai, to decoded string, anyatha None.\n\n    Reqs:\n    - binascii\n    - urllib.parse\n\n    Examples:\n    - >>> task_func('https://www.example.com?q=4a4b4c')\n    - 'JKL'\n\n    \"\"\"\n    # Parse the URL to extract query parameters\n    parsed_url = urllib.parse.urlparse(url)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n\n    # Check if 'q' parameter exists\n    if 'q' in query_params:\n        # Extract the hexadecimal string\n        hex_string = query_params['q'][0]\n\n        try:\n            # Decode the hexadecimal string to utf-8\n            decoded_string = binascii.unhexlify(hex_string).decode('utf-8')\n            return decoded_string\n        except (binascii.Error, UnicodeDecodeError):\n            # Return None if decoding fails\n            return None\n    else:\n        # Return None if 'q' parameter does not exist\n        return None", "raw_solution": "import binascii\nimport urllib.parse\n\ndef task_func(url):\n    \"\"\"\n    url ke 'q' query parameter se ek hexadecimal string ko decode karein.\n    Yeh function diye gaye url se 'q' query parameter nikaalta hai,\n    Yeh maan kar ki yeh ek hexadecimal string hai, aur ise utf-8 string mein decode karta hai.\n    Yadi hexadecimal string invalid hai ya ek valid utf-8 string mein decode nahi kiya jaa sakta hai, to None lautaya jaata hai.\n\n    Params:\n    - url (str): Vah url jisse query parameter nikaala jaana hai.\n\n    Returns:\n        str or None: Yadi 'q' parameter maujood hai aur ek valid hexadecimal hai, to decoded string, anyatha None.\n\n    Reqs:\n    - binascii\n    - urllib.parse\n\n    Examples:\n    - >>> task_func('https://www.example.com?q=4a4b4c')\n    - 'JKL'\n\n    \"\"\"\n    # Parse the URL to extract query parameters\n    parsed_url = urllib.parse.urlparse(url)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n\n    # Check if 'q' parameter exists\n    if 'q' in query_params:\n        # Extract the hexadecimal string\n        hex_string = query_params['q'][0]\n\n        try:\n            # Decode the hexadecimal string to utf-8\n            decoded_string = binascii.unhexlify(hex_string).decode('utf-8')\n            return decoded_string\n        except (binascii.Error, UnicodeDecodeError):\n            # Return None if decoding fails\n            return None\n    else:\n        # Return None if 'q' parameter does not exist\n        return None\n```\n"}
{"task_id": "BigCodeBench/1028", "solution": "import subprocess\nimport time\nimport json\nimport platform\nLOGFILE_PATH = \"logfile.log\"\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0 or interval >= duration:\n        raise ValueError(\"Interval and duration must be positive and interval must be less than duration.\")\n\n    cpu_usage_data = []\n    start_time = time.time()\n\n    while (time.time() - start_time) < duration:\n        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n        if platform.system() == \"Windows\":\n            cpu_usage = subprocess.check_output(\"wmic cpu get loadpercentage\", shell=True).decode().strip()\n        else:\n            cpu_usage = subprocess.check_output(\"top -bn1 | grep 'Cpu(s)'\", shell=True).decode().strip()\n        cpu_usage_data.append({\"timestamp\": current_time, \"cpu_usage\": cpu_usage})\n\n        time.sleep(interval)\n\n    try:\n        with open(LOGFILE_PATH, 'w') as logfile:\n            json.dump(cpu_usage_data, logfile)\n    except IOError as e:\n        print(f\"Error writing to log file: {e}\")\n        return None\n\n    return LOGFILE_PATH", "raw_solution": "import subprocess\nimport time\nimport json\nimport platform\n\nLOGFILE_PATH = \"logfile.log\"\n\n\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0 or interval >= duration:\n        raise ValueError(\"Interval and duration must be positive and interval must be less than duration.\")\n\n    cpu_usage_data = []\n    start_time = time.time()\n\n    while (time.time() - start_time) < duration:\n        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n        if platform.system() == \"Windows\":\n            cpu_usage = subprocess.check_output(\"wmic cpu get loadpercentage\", shell=True).decode().strip()\n        else:\n            cpu_usage = subprocess.check_output(\"top -bn1 | grep 'Cpu(s)'\", shell=True).decode().strip()\n        cpu_usage_data.append({\"timestamp\": current_time, \"cpu_usage\": cpu_usage})\n\n        time.sleep(interval)\n\n    try:\n        with open(LOGFILE_PATH, 'w') as logfile:\n            json.dump(cpu_usage_data, logfile)\n    except IOError as e:\n        print(f\"Error writing to log file: {e}\")\n        return None\n\n    return LOGFILE_PATH\n```\nThis script uses the `subprocess` module to execute shell commands to get the CPU usage. It then writes the data to a log file in JSON format. The log file path is returned if the operation is successful, otherwise, it returns `None`.\n"}
{"task_id": "BigCodeBench/1029", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(rows=100, columns=3):\n    # Create a list of column names\n    column_names = [chr(97 + i) for i in range(columns)]\n\n    # Generate a DataFrame with random characters\n    data = np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=(rows, columns))\n    df = pd.DataFrame(data, columns=column_names)\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(rows=100, columns=3):\n    # Create a list of column names\n    column_names = [chr(97 + i) for i in range(columns)]\n\n    # Generate a DataFrame with random characters\n    data = np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=(rows, columns))\n    df = pd.DataFrame(data, columns=column_names)\n\n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/1030", "solution": "import itertools\nimport string\nimport pandas as pd\ndef task_func():\n    # Generate all possible combinations of 3 letters from the alphabet\n    combinations = list(itertools.product(string.ascii_lowercase, repeat=3))\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Letter 1', 'Letter 2', 'Letter 3'])\n    \n    return df", "raw_solution": "import itertools\nimport string\nimport pandas as pd\n\ndef task_func():\n    # Generate all possible combinations of 3 letters from the alphabet\n    combinations = list(itertools.product(string.ascii_lowercase, repeat=3))\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Letter 1', 'Letter 2', 'Letter 3'])\n    \n    return df\n\n# Example usage:"}
{"task_id": "BigCodeBench/1031", "solution": "import random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(n_rows=1000):\n    if n_rows <= 0:\n        raise ValueError(\"n_rows must be a positive integer\")\n\n    # Generate random 3-letter strings\n    random_strings = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(n_rows)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # Count frequency of each string\n    frequency = df['String'].value_counts().reset_index()\n    frequency.columns = ['String', 'Frequency']\n\n    # Get top 30 strings\n    top_30 = frequency.head(30)\n\n    # Plot the histogram\n    ax = top_30.plot(kind='bar', x='String', y='Frequency', legend=False)\n    ax.set_title('Top 30 Frequencies of Random 3-Letter Strings')\n    ax.set_xlabel('String')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(n_rows=1000):\n    if n_rows <= 0:\n        raise ValueError(\"n_rows must be a positive integer\")\n\n    # Generate random 3-letter strings\n    random_strings = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(n_rows)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # Count frequency of each string\n    frequency = df['String'].value_counts().reset_index()\n    frequency.columns = ['String', 'Frequency']\n\n    # Get top 30 strings\n    top_30 = frequency.head(30)\n\n    # Plot the histogram\n    ax = top_30.plot(kind='bar', x='String', y='Frequency', legend=False)\n    ax.set_title('Top 30 Frequencies of Random 3-Letter Strings')\n    ax.set_xlabel('String')\n    ax.set_ylabel('Frequency')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1032", "solution": "import matplotlib.pyplot as plt\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\nLETTERS = list(string.ascii_lowercase)\ndef task_func(rows=1000, string_length=3):\n    # Generate random strings\n    random_strings = [''.join(random.choices(LETTERS, k=string_length)) for _ in range(rows)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # Check if DataFrame is empty\n    if df.empty:\n        print(\"Heatmap generation requires data.\")\n        return None\n\n    # Calculate frequency of each letter in each string\n    frequency_df = df['String'].apply(lambda x: pd.Series(list(x)).value_counts()).fillna(0)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(frequency_df, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Frequency'})\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Strings')\n    ax.set_title('Frequency of Letters in Random Strings')\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\n\ndef task_func(rows=1000, string_length=3):\n    # Generate random strings\n    random_strings = [''.join(random.choices(LETTERS, k=string_length)) for _ in range(rows)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # Check if DataFrame is empty\n    if df.empty:\n        print(\"Heatmap generation requires data.\")\n        return None\n\n    # Calculate frequency of each letter in each string\n    frequency_df = df['String'].apply(lambda x: pd.Series(list(x)).value_counts()).fillna(0)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(frequency_df, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Frequency'})\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Strings')\n    ax.set_title('Frequency of Letters in Random Strings')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1033", "solution": "import itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate all possible 3-letter combinations\n    combinations = list(itertools.product(string.ascii_lowercase, repeat=3))\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['a', 'b', 'c'])\n    \n    # Calculate the frequency of the first letter in each combination\n    first_letter_counts = df['a'].value_counts()\n    \n    # Plot the frequency of the first letter\n    ax = first_letter_counts.plot(kind='bar', title='Frequency of the First Letter in 3-Letter Combinations')\n    ax.set_xlabel('First Letter')\n    ax.set_ylabel('Frequency')\n    \n    return df, ax", "raw_solution": "import itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    # Generate all possible 3-letter combinations\n    combinations = list(itertools.product(string.ascii_lowercase, repeat=3))\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['a', 'b', 'c'])\n    \n    # Calculate the frequency of the first letter in each combination\n    first_letter_counts = df['a'].value_counts()\n    \n    # Plot the frequency of the first letter\n    ax = first_letter_counts.plot(kind='bar', title='Frequency of the First Letter in 3-Letter Combinations')\n    ax.set_xlabel('First Letter')\n    ax.set_ylabel('Frequency')\n    \n    return df, ax\n```\n"}
{"task_id": "BigCodeBench/1034", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nCATEGORIES = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Automotive\", \"Books\"]\ndef task_func(s1, s2):\n    # Check if the series have the same categories\n    if not all(s1.index == s2.index):\n        return None, 0.0\n\n    # Calculate the Euclidean distance between the two series\n    edit_distance = np.linalg.norm(s1 - s2)\n\n    # Create a bar plot for the sales data above a certain threshold\n    above_threshold = s1[s1 > 200] & s2[s2 > 200]\n    if above_threshold.empty:\n        return None, edit_distance\n\n    ax = above_threshold.plot(kind='bar', title='Sales Comparison Above Threshold in Categories')\n    ax.set_ylabel('Sales')\n    ax.set_xlabel('Categories')\n\n    return ax, edit_distance\ns1 = pd.Series(np.random.randint(100, 500, size=5), index=CATEGORIES)\ns2 = pd.Series(np.random.randint(150, 600, size=5), index=CATEGORIES)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nCATEGORIES = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Automotive\", \"Books\"]\n\ndef task_func(s1, s2):\n    # Check if the series have the same categories\n    if not all(s1.index == s2.index):\n        return None, 0.0\n\n    # Calculate the Euclidean distance between the two series\n    edit_distance = np.linalg.norm(s1 - s2)\n\n    # Create a bar plot for the sales data above a certain threshold\n    above_threshold = s1[s1 > 200] & s2[s2 > 200]\n    if above_threshold.empty:\n        return None, edit_distance\n\n    ax = above_threshold.plot(kind='bar', title='Sales Comparison Above Threshold in Categories')\n    ax.set_ylabel('Sales')\n    ax.set_xlabel('Categories')\n\n    return ax, edit_distance\n\n# Example usage:\nnp.random.seed(seed=32)\ns1 = pd.Series(np.random.randint(100, 500, size=5), index=CATEGORIES)\ns2 = pd.Series(np.random.randint(150, 600, size=5), index=CATEGORIES)\nax, edit_distance = task_func(s1, s2)"}
{"task_id": "BigCodeBench/1035", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(feature: pd.Series, target: pd.Series) -> (np.ndarray, plt.Axes):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(feature.values.reshape(-1, 1), target, test_size=0.2, random_state=42)\n\n    # Initialize and train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target variable for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Plot the confusion matrix\n    fig, ax = plt.subplots()\n    cax = ax.matshow(cm, cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    fig.colorbar(cax)\n\n    return cm, ax", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(feature: pd.Series, target: pd.Series) -> (np.ndarray, plt.Axes):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(feature.values.reshape(-1, 1), target, test_size=0.2, random_state=42)\n\n    # Initialize and train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target variable for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Plot the confusion matrix\n    fig, ax = plt.subplots()\n    cax = ax.matshow(cm, cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    fig.colorbar(cax)\n\n    return cm, ax\n```\n"}
{"task_id": "BigCodeBench/1036", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2):\n    # Find the intersection of the two series\n    intersection = s1[s1.isin(s2)]\n    intersection_count = intersection.nunique()\n\n    # Create a swarm plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.swarmplot(x=s1.name, y=s1.values, color='blue', alpha=0.5)\n    sns.swarmplot(x=s2.name, y=s2.values, color='red', alpha=0.5, ax=ax)\n\n    # Highlight the intersection points\n    for value in intersection:\n        ax.axhline(y=value, color='green', linestyle='--', linewidth=0.5)\n\n    # Set the title and labels\n    ax.set_title(f'Overlap Between {s1.name} and {s2.name}')\n    ax.set_xlabel('Series')\n    ax.set_ylabel('Values')\n\n    return ax, intersection_count\ns1 = pd.Series([1, 2, 3, 4, 5], name='Series1')\ns2 = pd.Series([4, 5, 6, 7, 8], name='Series2')", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(s1, s2):\n    # Find the intersection of the two series\n    intersection = s1[s1.isin(s2)]\n    intersection_count = intersection.nunique()\n\n    # Create a swarm plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.swarmplot(x=s1.name, y=s1.values, color='blue', alpha=0.5)\n    sns.swarmplot(x=s2.name, y=s2.values, color='red', alpha=0.5, ax=ax)\n\n    # Highlight the intersection points\n    for value in intersection:\n        ax.axhline(y=value, color='green', linestyle='--', linewidth=0.5)\n\n    # Set the title and labels\n    ax.set_title(f'Overlap Between {s1.name} and {s2.name}')\n    ax.set_xlabel('Series')\n    ax.set_ylabel('Values')\n\n    return ax, intersection_count\n\n# Example usage\ns1 = pd.Series([1, 2, 3, 4, 5], name='Series1')\ns2 = pd.Series([4, 5, 6, 7, 8], name='Series2')\nax, count = task_func(s1, s2)"}
{"task_id": "BigCodeBench/1037", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2, n_clusters=3):\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"s1 and s2 must be pandas Series.\")\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must have the same length.\")\n\n    data = pd.DataFrame({'Feature1': s1, 'Feature2': s2})\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n    labels = kmeans.fit_predict(data)\n\n    plt.scatter(data['Feature1'], data['Feature2'], c=labels, cmap='viridis', marker='o', edgecolor='k')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='x')\n    plt.title('K-means clustering')\n    plt.xlabel('Feature1')\n    plt.ylabel('Feature2')\n    plt.legend(['Cluster 1', 'Cluster 2', 'Cluster 3'])\n    ax = plt.gca()\n\n    return labels, ax", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(s1, s2, n_clusters=3):\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"s1 and s2 must be pandas Series.\")\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must have the same length.\")\n\n    data = pd.DataFrame({'Feature1': s1, 'Feature2': s2})\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n    labels = kmeans.fit_predict(data)\n\n    plt.scatter(data['Feature1'], data['Feature2'], c=labels, cmap='viridis', marker='o', edgecolor='k')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='x')\n    plt.title('K-means clustering')\n    plt.xlabel('Feature1')\n    plt.ylabel('Feature2')\n    plt.legend(['Cluster 1', 'Cluster 2', 'Cluster 3'])\n    ax = plt.gca()\n\n    return labels, ax\n```\n"}
{"task_id": "BigCodeBench/1038", "solution": "from datetime import datetime\nimport json\ndef task_func(client_socket):\n    \"\"\"\n    client ke request ka jawab JSON-swaroopit message bhejakar deta hai\n    current server time aur greeting shaamil hai.\n\n    Params:\n    - client_socket (socket.socket): vah client socket jisse request prapt hota hai.\n\n    Returns:\n        None\n\n    Reqs:\n    - datetime.datetime\n    - json\n\n    Examples:\n    - >>> import socket\n    - >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    - >>> server_socket.bind((SERVER_ADDRESS, 0))  # ek free port se bandhe\n    - >>> server_socket.bind((SERVER_ADDRESS, 8080))\n    - >>> server_socket.listen(1)\n    - >>> try:\n    - ...     client_socket, _ = server_socket.accept()\n    - ...     task_func(client_socket(client_socket)\n    - ... finally:\n    - ...     server_socket.close()\n\n    \"\"\"\n    # Get current server time\n    server_time = datetime.now().isoformat()\n\n    # Create a greeting message\n    greeting = \"Hello, client!\"\n\n    # Create a dictionary to be converted to JSON\n    response_data = {\n        \"server_time\": server_time,\n        \"greeting\": greeting\n    }\n\n    # Convert the dictionary to a JSON string\n    response_json = json.dumps(response_data)\n\n    # Send the JSON string to the client\n    client_socket.sendall(response_json.encode('utf-8'))\n\n    # Close the client socket\n    client_socket.close()", "raw_solution": "from datetime import datetime\nimport json\n\nSERVER_ADDRESS = \"localhost\"\nBUFFER_SIZE = 1024\n\ndef task_func(client_socket):\n    \"\"\"\n    client ke request ka jawab JSON-swaroopit message bhejakar deta hai\n    current server time aur greeting shaamil hai.\n\n    Params:\n    - client_socket (socket.socket): vah client socket jisse request prapt hota hai.\n\n    Returns:\n        None\n\n    Reqs:\n    - datetime.datetime\n    - json\n\n    Examples:\n    - >>> import socket\n    - >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    - >>> server_socket.bind((SERVER_ADDRESS, 0))  # ek free port se bandhe\n    - >>> server_socket.bind((SERVER_ADDRESS, 8080))\n    - >>> server_socket.listen(1)\n    - >>> try:\n    - ...     client_socket, _ = server_socket.accept()\n    - ...     task_func(client_socket(client_socket)\n    - ... finally:\n    - ...     server_socket.close()\n\n    \"\"\"\n    # Get current server time\n    server_time = datetime.now().isoformat()\n\n    # Create a greeting message\n    greeting = \"Hello, client!\"\n\n    # Create a dictionary to be converted to JSON\n    response_data = {\n        \"server_time\": server_time,\n        \"greeting\": greeting\n    }\n\n    # Convert the dictionary to a JSON string\n    response_json = json.dumps(response_data)\n\n    # Send the JSON string to the client\n    client_socket.sendall(response_json.encode('utf-8'))\n\n    # Close the client socket\n    client_socket.close()\n```\n"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Create SSL context\n        context = ssl.create_default_context(ssl.FILETYPE_PEM)\n        context.load_cert_chain(cert_file, key_file)\n\n        # Wrap socket with SSL\n        ssl_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive file path from client\n        file_path = ssl_socket.recv(buffer_size).decode('utf-8')\n\n        # Check if file exists\n        if not os.path.exists(file_path):\n            ssl_socket.sendall('File not found'.encode('utf-8'))\n            return 'File not found'\n\n        # Calculate SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as f:\n            for byte_block in iter(lambda: f.read(buffer_size), b\"\"):\n                sha256_hash.update(byte_block)\n\n        # Send the hash back to the client\n        ssl_socket.sendall(sha256_hash.hexdigest().encode('utf-8'))\n\n        # Close SSL socket\n        ssl_socket.close()\n\n        return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"", "raw_solution": "import ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Create SSL context\n        context = ssl.create_default_context(ssl.FILETYPE_PEM)\n        context.load_cert_chain(cert_file, key_file)\n\n        # Wrap socket with SSL\n        ssl_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive file path from client\n        file_path = ssl_socket.recv(buffer_size).decode('utf-8')\n\n        # Check if file exists\n        if not os.path.exists(file_path):\n            ssl_socket.sendall('File not found'.encode('utf-8'))\n            return 'File not found'\n\n        # Calculate SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as f:\n            for byte_block in iter(lambda: f.read(buffer_size), b\"\"):\n                sha256_hash.update(byte_block)\n\n        # Send the hash back to the client\n        ssl_socket.sendall(sha256_hash.hexdigest().encode('utf-8'))\n\n        # Close SSL socket\n        ssl_socket.close()\n\n        return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n```\n"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(0)\n    server_address = (server_address, server_port)\n    server.bind(server_address)\n    server.listen(5)\n\n    # Sockets from which we expect to read\n    inputs = [server]\n\n    # Sockets to which we expect to write\n    outputs = []\n\n    # Message queues\n    message_queues = {}\n\n    # Start time\n    start_time = datetime.now()\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        # Wait for at least one of the sockets to be ready for processing\n        readable, writable, exceptional = select.select(inputs, outputs, inputs)\n\n        for s in readable:\n            if s is server:\n                # A \"readable\" server socket is ready to accept a connection\n                connection, client_address = s.accept()\n                connection.setblocking(0)\n                inputs.append(connection)\n\n                # Give the connection a queue for data we want to send\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    # A readable client socket has data\n                    message_queues[s].put(data)\n                    # Add output channel for response\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # Interpret empty result as closed connection\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n\n                    # Remove message queue\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                # Broadcast the message to all other connected clients\n                for output in outputs:\n                    if output != s:\n                        output.send(next_msg)\n\n        for s in exceptional:\n            # Handle \"exceptional conditions\"\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n\n            # Remove message queue\n            del message_queues[s]\n\n    return f\"Server started on {server_address[0]}:{server_address[1]}. Ran for {run_duration} seconds.\"", "raw_solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(0)\n    server_address = (server_address, server_port)\n    server.bind(server_address)\n    server.listen(5)\n\n    # Sockets from which we expect to read\n    inputs = [server]\n\n    # Sockets to which we expect to write\n    outputs = []\n\n    # Message queues\n    message_queues = {}\n\n    # Start time\n    start_time = datetime.now()\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        # Wait for at least one of the sockets to be ready for processing\n        readable, writable, exceptional = select.select(inputs, outputs, inputs)\n\n        for s in readable:\n            if s is server:\n                # A \"readable\" server socket is ready to accept a connection\n                connection, client_address = s.accept()\n                connection.setblocking(0)\n                inputs.append(connection)\n\n                # Give the connection a queue for data we want to send\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    # A readable client socket has data\n                    message_queues[s].put(data)\n                    # Add output channel for response\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # Interpret empty result as closed connection\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n\n                    # Remove message queue\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                # Broadcast the message to all other connected clients\n                for output in outputs:\n                    if output != s:\n                        output.send(next_msg)\n\n        for s in exceptional:\n            # Handle \"exceptional conditions\"\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n\n            # Remove message queue\n            del message_queues[s]\n\n    return f\"Server started on {server_address[0]}:{server_address[1]}. Ran for {run_duration} seconds.\"\n```\n"}
{"task_id": "BigCodeBench/1041", "solution": "import re\nimport os\ndef task_func(request):\n    # Regular expression to match the GET request format\n    get_request_pattern = re.compile(r\"GET /([^ ]+) HTTP/1.1\")\n    \n    # Match the request\n    match = get_request_pattern.match(request)\n    \n    if not match:\n        return \"HTTP/1.1 400 BAD REQUEST\\n\\nBad Request\"\n    \n    filename = match.group(1)\n    \n    try:\n        # Check if the file exists\n        if not os.path.exists(filename):\n            return \"HTTP/1.1 404 NOT FOUND\\n\\nFile Not Found\"\n        \n        # Read the file content\n        with open(filename, 'r') as file:\n            content = file.read()\n        \n        # Return the HTTP response\n        return f\"HTTP/1.1 200 OK\\nContent-Length: {len(content)}\\n\\n{content}\"\n    \n    except IOError:\n        return \"HTTP/1.1 500 INTERNAL SERVER ERROR\\n\\nInternal Server Error\"", "raw_solution": "import re\nimport os\n\ndef task_func(request):\n    # Regular expression to match the GET request format\n    get_request_pattern = re.compile(r\"GET /([^ ]+) HTTP/1.1\")\n    \n    # Match the request\n    match = get_request_pattern.match(request)\n    \n    if not match:\n        return \"HTTP/1.1 400 BAD REQUEST\\n\\nBad Request\"\n    \n    filename = match.group(1)\n    \n    try:\n        # Check if the file exists\n        if not os.path.exists(filename):\n            return \"HTTP/1.1 404 NOT FOUND\\n\\nFile Not Found\"\n        \n        # Read the file content\n        with open(filename, 'r') as file:\n            content = file.read()\n        \n        # Return the HTTP response\n        return f\"HTTP/1.1 200 OK\\nContent-Length: {len(content)}\\n\\n{content}\"\n    \n    except IOError:\n        return \"HTTP/1.1 500 INTERNAL SERVER ERROR\\n\\nInternal Server Error\"\n\n# Test cases"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    \"\"\"\n    Client socket se ek message prapt karein aur ise ek SMTP server ke madhyam se email ke roop mein bhejein.\n\n    Notes:\n        Ek kaam kar rahe internet connection aur SMTP server tak pahunch ki aavashyakta hai.\n        Function sender ka email, recipient ka email,\n        aur authentication ke liye sender ka email password poochta hai.\n\n    Params:\n    - client_socket (socket.socket): Vah client socket jisse sandesh prapt hota hai.\n\n    Returns:\n        None\n\n    Reqs:\n    - smtplib\n    - email.message.EmailMessage\n    - getpass\n\n    Examples:\n    - >>> import socket\n    - >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    - >>> server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n    - >>> server_socket.listen(5)\n    - >>> client_socket, addr = server_socket.accept()\n    - >>> task_func(client_socket)\n\n    \"\"\"\n    # Receive the email message from the client socket\n    message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n\n    # Create an EmailMessage object\n    email = EmailMessage()\n    email.set_content(message)\n\n    # Set the email headers\n    email['Subject'] = 'New Message'\n    email['From'] = input(\"Enter your email: \")\n    email['To'] = input(\"Enter recipient email: \")\n\n    # Prompt the user for their email password\n    password = getpass.getpass(\"Enter your email password: \")\n\n    # Connect to the SMTP server and send the email\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n        server.starttls()  # Secure the connection\n        server.login(email['From'], password)\n        server.send_message(email)\n\n    print(\"Email sent successfully!\")", "raw_solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    \"\"\"\n    Client socket se ek message prapt karein aur ise ek SMTP server ke madhyam se email ke roop mein bhejein.\n\n    Notes:\n        Ek kaam kar rahe internet connection aur SMTP server tak pahunch ki aavashyakta hai.\n        Function sender ka email, recipient ka email,\n        aur authentication ke liye sender ka email password poochta hai.\n\n    Params:\n    - client_socket (socket.socket): Vah client socket jisse sandesh prapt hota hai.\n\n    Returns:\n        None\n\n    Reqs:\n    - smtplib\n    - email.message.EmailMessage\n    - getpass\n\n    Examples:\n    - >>> import socket\n    - >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    - >>> server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n    - >>> server_socket.listen(5)\n    - >>> client_socket, addr = server_socket.accept()\n    - >>> task_func(client_socket)\n\n    \"\"\"\n    # Receive the email message from the client socket\n    message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n\n    # Create an EmailMessage object\n    email = EmailMessage()\n    email.set_content(message)\n\n    # Set the email headers\n    email['Subject'] = 'New Message'\n    email['From'] = input(\"Enter your email: \")\n    email['To'] = input(\"Enter recipient email: \")\n\n    # Prompt the user for their email password\n    password = getpass.getpass(\"Enter your email password: \")\n\n    # Connect to the SMTP server and send the email\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n        server.starttls()  # Secure the connection\n        server.login(email['From'], password)\n        server.send_message(email)\n\n    print(\"Email sent successfully!\")\n\n# Example usage:\n# import socket\n# server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n# server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n# server_socket.listen(5)\n# client_socket, addr = server_socket.accept()\n# task_func(client_socket)\n```\n"}
{"task_id": "BigCodeBench/1043", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"data list is empty.\")\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data_list, columns=['Category'])\n    \n    # Count occurrences of each category\n    category_counts = df['Category'].value_counts()\n    \n    # Check if all categories are present\n    missing_categories = set(CATEGORIES) - set(category_counts.index)\n    if missing_categories:\n        print(f\"Warning: The following categories are missing from the data: {missing_categories}\")\n    \n    # Create a bar plot\n    ax = category_counts.reindex(CATEGORIES, fill_value=0).plot(kind='bar', color='skyblue')\n    ax.set_title('Category Distribution')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    \n    return ax\ndata = ['A', 'B', 'C', 'D', 'E', 'F', 'G']", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"data list is empty.\")\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data_list, columns=['Category'])\n    \n    # Count occurrences of each category\n    category_counts = df['Category'].value_counts()\n    \n    # Check if all categories are present\n    missing_categories = set(CATEGORIES) - set(category_counts.index)\n    if missing_categories:\n        print(f\"Warning: The following categories are missing from the data: {missing_categories}\")\n    \n    # Create a bar plot\n    ax = category_counts.reindex(CATEGORIES, fill_value=0).plot(kind='bar', color='skyblue')\n    ax.set_title('Category Distribution')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    \n    return ax\n\n# Example usage:\ndata = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\nax = task_func(data)\nplt.show()\n```\nThis script uses pandas to count the occurrences of each category in the data list and matplotlib to create a bar plot of the category distribution. It also handles the case where some categories might be missing from the data by filling in missing counts with zero.\n"}
{"task_id": "BigCodeBench/1044", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nROOMS = [\"Room1\", \"Room2\", \"Room3\", \"Room4\", \"Room5\"]\ndef task_func(date_str, booking_data):\n    try:\n        # Convert date string to datetime object\n        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n        if date < datetime.now():\n            raise ValueError(\"The date provided is a past date.\")\n    except ValueError as e:\n        raise ValueError(f\"Invalid date: {e}\") from e\n\n    # Create a DataFrame with room names and booking status\n    data = {room: booking_data.get(room, \"Not Listed\") for room in ROOMS}\n    df = pd.DataFrame(list(data.items()), columns=[\"Room\", \"Booking Status\"])\n\n    # Plot the booking status\n    ax = df.plot(x=\"Room\", y=\"Booking Status\", kind=\"bar\", legend=False)\n    ax.set_title(f\"Booking Status for {date_str}\")\n    ax.set_ylabel(\"Status\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return df, ax", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nROOMS = [\"Room1\", \"Room2\", \"Room3\", \"Room4\", \"Room5\"]\n\ndef task_func(date_str, booking_data):\n    try:\n        # Convert date string to datetime object\n        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n        if date < datetime.now():\n            raise ValueError(\"The date provided is a past date.\")\n    except ValueError as e:\n        raise ValueError(f\"Invalid date: {e}\") from e\n\n    # Create a DataFrame with room names and booking status\n    data = {room: booking_data.get(room, \"Not Listed\") for room in ROOMS}\n    df = pd.DataFrame(list(data.items()), columns=[\"Room\", \"Booking Status\"])\n\n    # Plot the booking status\n    ax = df.plot(x=\"Room\", y=\"Booking Status\", kind=\"bar\", legend=False)\n    ax.set_title(f\"Booking Status for {date_str}\")\n    ax.set_ylabel(\"Status\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return df, ax\n\n# Example usage"}
{"task_id": "BigCodeBench/1045", "solution": "from datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\ndef task_func(date_str):\n    \"\"\"\n    Dee gaee date se current time tak beete hue total seconds kee gananaa karen,\n    is period mein huee kisee bhee leap seconds sahit.\n\n    Notes:\n        Yah function datetime, numpy, aur dateutil.parser modules ka upayog karta hai.\n        LEAP_SECONDS aire mein ve saal hone chaahie jab leap seconds jode gae the.\n\n    Params:\n    - date_str (str): Vah date aur time jisase gananaa karnee hai, \"yyyy-mm-dd hh:mm:ss\" format mein.\n\n    Returns:\n        int: Dee gaee date ke baad se, leap seconds sahit, beete hue total seconds kee number.\n\n    Reqs:\n    - datetime.datetime\n    - numpy\n    - dateutil.parser.parse\n\n    Examples:\n    - >>> total_seconds = task_func('1970-01-01 00:00:00')\n    - >>> print(total_seconds)\n    - 1702597276\n\n    \"\"\"\n    # Parse the input date string into a datetime object\n    start_time = parse(date_str)\n    \n    # Calculate the total seconds from the start time to the current time\n    total_seconds = int((datetime.now() - start_time).total_seconds())\n    \n    # Find the number of leap seconds that have occurred since the start time\n    leap_seconds_passed = np.searchsorted(LEAP_SECONDS, start_time.year)\n    \n    # Subtract the number of leap seconds from the total seconds\n    total_seconds -= leap_seconds_passed\n    \n    return total_seconds", "raw_solution": "from datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\n\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\n\n\ndef task_func(date_str):\n    \"\"\"\n    Dee gaee date se current time tak beete hue total seconds kee gananaa karen,\n    is period mein huee kisee bhee leap seconds sahit.\n\n    Notes:\n        Yah function datetime, numpy, aur dateutil.parser modules ka upayog karta hai.\n        LEAP_SECONDS aire mein ve saal hone chaahie jab leap seconds jode gae the.\n\n    Params:\n    - date_str (str): Vah date aur time jisase gananaa karnee hai, \"yyyy-mm-dd hh:mm:ss\" format mein.\n\n    Returns:\n        int: Dee gaee date ke baad se, leap seconds sahit, beete hue total seconds kee number.\n\n    Reqs:\n    - datetime.datetime\n    - numpy\n    - dateutil.parser.parse\n\n    Examples:\n    - >>> total_seconds = task_func('1970-01-01 00:00:00')\n    - >>> print(total_seconds)\n    - 1702597276\n\n    \"\"\"\n    # Parse the input date string into a datetime object\n    start_time = parse(date_str)\n    \n    # Calculate the total seconds from the start time to the current time\n    total_seconds = int((datetime.now() - start_time).total_seconds())\n    \n    # Find the number of leap seconds that have occurred since the start time\n    leap_seconds_passed = np.searchsorted(LEAP_SECONDS, start_time.year)\n    \n    # Subtract the number of leap seconds from the total seconds\n    total_seconds -= leap_seconds_passed\n    \n    return total_seconds\n```\n"}
{"task_id": "BigCodeBench/1046", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nfrom itertools import product\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\ndef task_func(date_str):\n    # Convert the input string to a datetime object\n    start_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Create a list of dates for the next 10 days\n    dates = [start_date + timedelta(days=i) for i in range(10)]\n    \n    # Generate all combinations of employees and dates\n    data = list(product(EMPLOYEES, dates))\n    \n    # Create a DataFrame from the generated data\n    df = pd.DataFrame(data, columns=[\"Employee\", \"Date\"])\n    \n    return df", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nfrom itertools import product\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef task_func(date_str):\n    # Convert the input string to a datetime object\n    start_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Create a list of dates for the next 10 days\n    dates = [start_date + timedelta(days=i) for i in range(10)]\n    \n    # Generate all combinations of employees and dates\n    data = list(product(EMPLOYEES, dates))\n    \n    # Create a DataFrame from the generated data\n    df = pd.DataFrame(data, columns=[\"Employee\", \"Date\"])\n    \n    return df\n\n# Example usage:\ndf = task_func('2023-06-15')"}
{"task_id": "BigCodeBench/1047", "solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    # Convert the date string to a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Generate random integers\n    random.seed(date.day)  # Seed the random number generator with the day of the month\n    random_integers = [random.randint(0, 100) for _ in range(date.day)]\n    \n    # Create a line plot of the random integers\n    fig, ax = plt.subplots()\n    ax.plot(random_integers, marker='o')\n    ax.set_title(f\"Random Integers for {date_str}\")\n    ax.set_xlabel(\"Index\")\n    ax.set_ylabel(\"Random Integer\")\n    \n    return ax", "raw_solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(date_str):\n    # Convert the date string to a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Generate random integers\n    random.seed(date.day)  # Seed the random number generator with the day of the month\n    random_integers = [random.randint(0, 100) for _ in range(date.day)]\n    \n    # Create a line plot of the random integers\n    fig, ax = plt.subplots()\n    ax.plot(random_integers, marker='o')\n    ax.set_title(f\"Random Integers for {date_str}\")\n    ax.set_xlabel(\"Index\")\n    ax.set_ylabel(\"Random Integer\")\n    \n    return ax\n\n# Example usage:\n# ax = task_func('2023-06-15')\n# print(type(ax))\n```\n"}
{"task_id": "BigCodeBench/1048", "solution": "from datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    # Convert the date string to a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Generate a time array from 0 to 2\u03c0\n    t = np.linspace(0, 2 * np.pi, 1000)\n    \n    # Calculate the frequency as the day of the month\n    frequency = date.day\n    \n    # Generate the sine wave\n    y = np.sin(frequency * t)\n    \n    # Plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(t, y)\n    ax.set_title(f'Sine Wave for {date_str} (Frequency: {frequency})')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    \n    return ax", "raw_solution": "from datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(date_str):\n    # Convert the date string to a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Generate a time array from 0 to 2\u03c0\n    t = np.linspace(0, 2 * np.pi, 1000)\n    \n    # Calculate the frequency as the day of the month\n    frequency = date.day\n    \n    # Generate the sine wave\n    y = np.sin(frequency * t)\n    \n    # Plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(t, y)\n    ax.set_title(f'Sine Wave for {date_str} (Frequency: {frequency})')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    \n    return ax\n\n# Example usage:\n# ax = task_func('2023-06-15')\n# print(ax.get_title())\n```\n"}
{"task_id": "BigCodeBench/1049", "solution": "import re\nimport pandas as pd\ndef task_func(input_string: str) -> pd.DataFrame:\n    # Remove empty lines and lines with only whitespace\n    lines = [line.strip() for line in input_string.split('\\n') if line.strip()]\n    \n    # Replace tabs with spaces\n    lines = [re.sub(r'\\t', ' ', line) for line in lines]\n    \n    # Create a DataFrame from the lines\n    df = pd.DataFrame(lines, columns=['Text'])\n    \n    return df\ninput_string = 'line a\\nfollowed by line b with a\\ttab\\n\\nbye\\n'", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(input_string: str) -> pd.DataFrame:\n    # Remove empty lines and lines with only whitespace\n    lines = [line.strip() for line in input_string.split('\\n') if line.strip()]\n    \n    # Replace tabs with spaces\n    lines = [re.sub(r'\\t', ' ', line) for line in lines]\n    \n    # Create a DataFrame from the lines\n    df = pd.DataFrame(lines, columns=['Text'])\n    \n    return df\n\n# Example usage:\ninput_string = 'line a\\nfollowed by line b with a\\ttab\\n\\nbye\\n'\ndf = task_func(input_string)"}
{"task_id": "BigCodeBench/1050", "solution": "import os\nimport hashlib\nDIRECTORY = \"./hashed_files\"\ndef task_func(input_string):\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    file_paths = []\n    lines = input_string.strip().split('\\n')\n    for i, line in enumerate(lines):\n        if line.strip() == '':\n            continue\n        hash_object = hashlib.sha256(line.encode())\n        file_name = f\"{hash_object.hexdigest()[:10]}.txt\"\n        file_path = os.path.join(DIRECTORY, file_name)\n        with open(file_path, 'w') as f:\n            f.write(line)\n        file_paths.append(file_path)\n\n    return file_paths", "raw_solution": "import os\nimport hashlib\n\n# Constants\nDIRECTORY = \"./hashed_files\"\n\n\ndef task_func(input_string):\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    file_paths = []\n    lines = input_string.strip().split('\\n')\n    for i, line in enumerate(lines):\n        if line.strip() == '':\n            continue\n        hash_object = hashlib.sha256(line.encode())\n        file_name = f\"{hash_object.hexdigest()[:10]}.txt\"\n        file_path = os.path.join(DIRECTORY, file_name)\n        with open(file_path, 'w') as f:\n            f.write(line)\n        file_paths.append(file_path)\n\n    return file_paths\n```\n"}
{"task_id": "BigCodeBench/1051", "solution": "import collections\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    if not data_dict:\n        return None, \"distribution ek samaan hai.\"\n\n    counts = list(data_dict.values())\n    average_counts = np.mean(counts)\n    if np.max(np.abs(np.array(counts) - average_counts)) > 1e-5:\n        return None, \"distribution samaan nahi hai.\"\n\n    categories = list(data_dict.keys())\n    num_bins = min(10, len(set(counts)))\n\n    fig, ax = plt.subplots()\n    ax.hist(counts, bins=num_bins, edgecolor='black')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Counts')\n    ax.set_title('Distribution of Categories')\n\n    return ax, \"distribution ek samaan hai.\"", "raw_solution": "import collections\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    if not data_dict:\n        return None, \"distribution ek samaan hai.\"\n\n    counts = list(data_dict.values())\n    average_counts = np.mean(counts)\n    if np.max(np.abs(np.array(counts) - average_counts)) > 1e-5:\n        return None, \"distribution samaan nahi hai.\"\n\n    categories = list(data_dict.keys())\n    num_bins = min(10, len(set(counts)))\n\n    fig, ax = plt.subplots()\n    ax.hist(counts, bins=num_bins, edgecolor='black')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Counts')\n    ax.set_title('Distribution of Categories')\n\n    return ax, \"distribution ek samaan hai.\"\n```\n"}
{"task_id": "BigCodeBench/1052", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    # Load the data\n    df = pd.read_csv(file_path)\n    \n    # Check if the 'Text' column exists\n    if 'Text' not in df.columns:\n        raise ValueError(\"CSV file must contain a 'Text' column.\")\n    \n    # Check if the data is empty\n    if df.empty:\n        print(\"The input file is empty.\")\n        return None\n    \n    # Initialize the vectorizer\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    \n    # Fit and transform the data\n    X = vectorizer.fit_transform(df['Text'])\n    \n    # Sum the counts of each word\n    word_counts = X.sum(axis=0)\n    \n    # Get the words\n    words = vectorizer.get_feature_names_out()\n    \n    # Create a DataFrame\n    df_word_counts = pd.DataFrame(word_counts, index=words, columns=['Counts'])\n    \n    # Sort the DataFrame\n    df_word_counts = df_word_counts.sort_values(by='Counts', ascending=False)\n    \n    # Plot the histogram\n    ax = df_word_counts.plot(kind='bar', figsize=(10, 6), legend=False)\n    ax.set_title('Word Counts')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    \n    # Save the plot if save_path is provided\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n        return None\n    \n    # Show the plot\n    plt.show()\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    # Load the data\n    df = pd.read_csv(file_path)\n    \n    # Check if the 'Text' column exists\n    if 'Text' not in df.columns:\n        raise ValueError(\"CSV file must contain a 'Text' column.\")\n    \n    # Check if the data is empty\n    if df.empty:\n        print(\"The input file is empty.\")\n        return None\n    \n    # Initialize the vectorizer\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    \n    # Fit and transform the data\n    X = vectorizer.fit_transform(df['Text'])\n    \n    # Sum the counts of each word\n    word_counts = X.sum(axis=0)\n    \n    # Get the words\n    words = vectorizer.get_feature_names_out()\n    \n    # Create a DataFrame\n    df_word_counts = pd.DataFrame(word_counts, index=words, columns=['Counts'])\n    \n    # Sort the DataFrame\n    df_word_counts = df_word_counts.sort_values(by='Counts', ascending=False)\n    \n    # Plot the histogram\n    ax = df_word_counts.plot(kind='bar', figsize=(10, 6), legend=False)\n    ax.set_title('Word Counts')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    \n    # Save the plot if save_path is provided\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n        return None\n    \n    # Show the plot\n    plt.show()\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Load the CSV file\n        df = pd.read_csv(file_path)\n        \n        # Check if the DataFrame is empty\n        if df.empty:\n            raise ValueError(\"The CSV file is empty.\")\n        \n        # Check if the 'Text' column exists\n        if 'Text' not in df.columns:\n            raise ValueError(\"The CSV file must contain a 'Text' column.\")\n        \n        # Initialize CountVectorizer\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the data\n        X = vectorizer.fit_transform(df['Text'])\n        \n        # Sum the word counts\n        word_counts = X.sum(axis=0)\n        \n        # Get the words\n        words = vectorizer.get_feature_names_out()\n        \n        # Create a DataFrame for word counts\n        word_count_df = pd.DataFrame(word_counts, index=words, columns=['Count'])\n        \n        # Sort the DataFrame by count\n        word_count_df = word_count_df.sort_values(by='Count', ascending=False)\n        \n        # Plot the top 10 words\n        top_words = word_count_df.head(10)\n        ax = top_words.plot(kind='bar', legend=False)\n        ax.set_title('Top 10 Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Count')\n        \n        # Save or show the plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            return ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Load the CSV file\n        df = pd.read_csv(file_path)\n        \n        # Check if the DataFrame is empty\n        if df.empty:\n            raise ValueError(\"The CSV file is empty.\")\n        \n        # Check if the 'Text' column exists\n        if 'Text' not in df.columns:\n            raise ValueError(\"The CSV file must contain a 'Text' column.\")\n        \n        # Initialize CountVectorizer\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the data\n        X = vectorizer.fit_transform(df['Text'])\n        \n        # Sum the word counts\n        word_counts = X.sum(axis=0)\n        \n        # Get the words\n        words = vectorizer.get_feature_names_out()\n        \n        # Create a DataFrame for word counts\n        word_count_df = pd.DataFrame(word_counts, index=words, columns=['Count'])\n        \n        # Sort the DataFrame by count\n        word_count_df = word_count_df.sort_values(by='Count', ascending=False)\n        \n        # Plot the top 10 words\n        top_words = word_count_df.head(10)\n        ax = top_words.plot(kind='bar', legend=False)\n        ax.set_title('Top 10 Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Count')\n        \n        # Save or show the plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            return ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n```\n"}
{"task_id": "BigCodeBench/1054", "solution": "import csv\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    try:\n        # Reading data from the CSV file\n        with open(file_path, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            data = [float(row[0]) for row in reader]\n\n        # Generating a random sample of 30 numbers\n        sample = np.random.choice(data, size=30, replace=False)\n\n        # Calculating mean and standard deviation\n        mean = np.mean(sample)\n        std_dev = np.std(sample, ddof=1)\n\n        # Plotting the histogram\n        fig, ax = plt.subplots()\n        ax.hist(sample, bins='auto', density=True, alpha=0.6, color='g')\n\n        # Overlaying the normal distribution curve\n        xmin, xmax = plt.xlim()\n        x = np.linspace(xmin, xmax, 100)\n        p = stats.norm.pdf(x, mean, std_dev)\n        ax.plot(x, p, 'k', linewidth=2)\n        ax.set_title(\"Histogram with Normal Distribution Overlay\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        return mean, std_dev, ax\n\n    except FileNotFoundError:\n        print(f\"Error: The file {file_path} was not found.\")\n        return None, None, None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None, None, None", "raw_solution": "import csv\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    try:\n        # Reading data from the CSV file\n        with open(file_path, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            data = [float(row[0]) for row in reader]\n\n        # Generating a random sample of 30 numbers\n        sample = np.random.choice(data, size=30, replace=False)\n\n        # Calculating mean and standard deviation\n        mean = np.mean(sample)\n        std_dev = np.std(sample, ddof=1)\n\n        # Plotting the histogram\n        fig, ax = plt.subplots()\n        ax.hist(sample, bins='auto', density=True, alpha=0.6, color='g')\n\n        # Overlaying the normal distribution curve\n        xmin, xmax = plt.xlim()\n        x = np.linspace(xmin, xmax, 100)\n        p = stats.norm.pdf(x, mean, std_dev)\n        ax.plot(x, p, 'k', linewidth=2)\n        ax.set_title(\"Histogram with Normal Distribution Overlay\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        return mean, std_dev, ax\n\n    except FileNotFoundError:\n        print(f\"Error: The file {file_path} was not found.\")\n        return None, None, None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None, None, None\n```\n"}
{"task_id": "BigCodeBench/1055", "solution": "import pandas as pd\nimport itertools\nimport random\ndef task_func(colors, states):\n    # Generate all combinations of colors and states\n    combinations = list(itertools.product(colors, states))\n    \n    # Shuffle the combinations\n    random.shuffle(combinations)\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Color', 'State'])\n    \n    # Add a new column with the formatted string\n    df['Color:State'] = df['Color'] + ':' + df['State']\n    \n    return df\ncolors = ['red', 'green', 'blue']\nstates = ['California', 'Texas', 'New York']", "raw_solution": "import pandas as pd\nimport itertools\nimport random\n\ndef task_func(colors, states):\n    # Generate all combinations of colors and states\n    combinations = list(itertools.product(colors, states))\n    \n    # Shuffle the combinations\n    random.shuffle(combinations)\n    \n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Color', 'State'])\n    \n    # Add a new column with the formatted string\n    df['Color:State'] = df['Color'] + ':' + df['State']\n    \n    return df\n\n# Example usage:\ncolors = ['red', 'green', 'blue']\nstates = ['California', 'Texas', 'New York']\ncolor_state_table = task_func(colors, states)"}
{"task_id": "BigCodeBench/1056", "solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\ndef task_func(n_pairs=26):\n    if n_pairs < 1 or n_pairs > 26:\n        raise ValueError(\"n_pairs must be between 1 and 26 inclusive.\")\n\n    # Generate random pairs\n    pairs = random.sample(list(zip(LETTERS, NUMBERS)), n_pairs)\n\n    # Generate random counts for each pair\n    counts = [random.randint(1, 9) for _ in range(n_pairs)]\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    bars = ax.bar([f\"{letter}:{number}\" for letter, number in pairs], counts, color='skyblue')\n\n    # Add labels and title\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n\n    # Add labels to each bar\n    for bar, count in zip(bars, counts):\n        ax.text(bar.get_x() + bar.get_width() / 2, count + 0.5, str(count), ha='center', va='bottom')\n\n    return ax", "raw_solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\n\n\ndef task_func(n_pairs=26):\n    if n_pairs < 1 or n_pairs > 26:\n        raise ValueError(\"n_pairs must be between 1 and 26 inclusive.\")\n\n    # Generate random pairs\n    pairs = random.sample(list(zip(LETTERS, NUMBERS)), n_pairs)\n\n    # Generate random counts for each pair\n    counts = [random.randint(1, 9) for _ in range(n_pairs)]\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    bars = ax.bar([f\"{letter}:{number}\" for letter, number in pairs], counts, color='skyblue')\n\n    # Add labels and title\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n\n    # Add labels to each bar\n    for bar, count in zip(bars, counts):\n        ax.text(bar.get_x() + bar.get_width() / 2, count + 0.5, str(count), ha='center', va='bottom')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Default lists if not provided\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Elephant', 'Tiger', 'Lion', 'Zebra', 'Giraffe', 'Bear', 'Monkey', 'Kangaroo']\n    if foods is None:\n        foods = ['Meat', 'Fish', 'Grass', 'Fruits', 'Insects', 'Seeds', 'Leaves']\n\n    # Handling empty lists\n    if not animals or not foods:\n        return pd.DataFrame()\n\n    # Generate combinations\n    combinations = list(itertools.product(animals, foods))\n\n    # Create DataFrame\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n\n    # Format the cell values\n    df['Animal:Food'] = df.apply(lambda row: f\"{row['Animal']}:{row['Food']}\", axis=1)\n\n    # Drop the original columns\n    df.drop(columns=['Animal', 'Food'], inplace=True)\n\n    return df", "raw_solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Default lists if not provided\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Elephant', 'Tiger', 'Lion', 'Zebra', 'Giraffe', 'Bear', 'Monkey', 'Kangaroo']\n    if foods is None:\n        foods = ['Meat', 'Fish', 'Grass', 'Fruits', 'Insects', 'Seeds', 'Leaves']\n\n    # Handling empty lists\n    if not animals or not foods:\n        return pd.DataFrame()\n\n    # Generate combinations\n    combinations = list(itertools.product(animals, foods))\n\n    # Create DataFrame\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n\n    # Format the cell values\n    df['Animal:Food'] = df.apply(lambda row: f\"{row['Animal']}:{row['Food']}\", axis=1)\n\n    # Drop the original columns\n    df.drop(columns=['Animal', 'Food'], inplace=True)\n\n    return df\n\n# Example usage\nanimal_food_pairs = task_func(['Dog', 'Cat'], ['Meat', 'Fish'])"}
{"task_id": "BigCodeBench/1058", "solution": "import itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nSHAPES = [\n    \"Circle\",\n    \"Square\",\n    \"Triangle\",\n    \"Rectangle\",\n    \"Pentagon\",\n    \"Hexagon\",\n    \"Heptagon\",\n    \"Octagon\",\n    \"Nonagon\",\n    \"Decagon\",\n]\nCOLORS = [\n    \"Red\",\n    \"Blue\",\n    \"Green\",\n    \"Yellow\",\n    \"Black\",\n    \"White\",\n    \"Purple\",\n    \"Orange\",\n    \"Pink\",\n    \"Brown\",\n]\ndef task_func(num_pairs=10):\n    \"\"\"\n    Pre-defined shape-color jorom ka countplot banaem aur dikhaem.\n    Yah function purvanirdharit shape-color combinations ki ek unique number ka ek visual representation banata hai,\n    jinmem se pratyek countplot mem ek bar ke rup mem pradarshit hota hai. Shape-color jorom ko ek purvanirdharit list se chuna jata hai.\n\n    Params:\n    - num_pairs (int): countplot mem pradarshit kie jane vale unique shape-color jorom ki number.\n    - Default 10 hai. Yadi anurodhit number 1 se kam ya total\n    - possible unique combinations (100) se adhik hai, to ise valid range (1 se 100) mem samayojit kiya jata hai.\n\n    Returns:\n        ax (matplotlib.axes._axes.Axes): countplot ka Axes object, jis ka upayog age customizations ya plot ke bare mem jankari prapt karne ke lie kiya ja sakta hai.\n\n    Reqs:\n    - itertools\n    - seaborn\n    - matplotlib\n\n    Examples:\n    - >>> ax = task_func(10)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    - >>> ax = task_func(9)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    - >>> ax = task_func(8)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    - >>> ax = task_func(7)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    - >>> ax = task_func(6)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n\n    \"\"\"\n    # Generate shape-color combinations\n    combinations = list(itertools.product(SHAPES, COLORS))\n    \n    # Limit the number of combinations to num_pairs\n    combinations = combinations[:num_pairs]\n    \n    # Create a countplot\n    sns.set(style=\"whitegrid\")\n    ax = sns.countplot(x=combinations, palette=\"Set2\")\n    \n    # Set the x-axis labels\n    ax.set_xticklabels([f\"{shape}:{color}\" for shape, color in combinations], rotation=45)\n    \n    # Adjust the plot\n    plt.tight_layout()\n    \n    return ax", "raw_solution": "import itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nSHAPES = [\n    \"Circle\",\n    \"Square\",\n    \"Triangle\",\n    \"Rectangle\",\n    \"Pentagon\",\n    \"Hexagon\",\n    \"Heptagon\",\n    \"Octagon\",\n    \"Nonagon\",\n    \"Decagon\",\n]\nCOLORS = [\n    \"Red\",\n    \"Blue\",\n    \"Green\",\n    \"Yellow\",\n    \"Black\",\n    \"White\",\n    \"Purple\",\n    \"Orange\",\n    \"Pink\",\n    \"Brown\",\n]\n\n\ndef task_func(num_pairs=10):\n    \"\"\"\n    Pre-defined shape-color jorom ka countplot banaem aur dikhaem.\n    Yah function purvanirdharit shape-color combinations ki ek unique number ka ek visual representation banata hai,\n    jinmem se pratyek countplot mem ek bar ke rup mem pradarshit hota hai. Shape-color jorom ko ek purvanirdharit list se chuna jata hai.\n\n    Params:\n    - num_pairs (int): countplot mem pradarshit kie jane vale unique shape-color jorom ki number.\n    - Default 10 hai. Yadi anurodhit number 1 se kam ya total\n    - possible unique combinations (100) se adhik hai, to ise valid range (1 se 100) mem samayojit kiya jata hai.\n\n    Returns:\n        ax (matplotlib.axes._axes.Axes): countplot ka Axes object, jis ka upayog age customizations ya plot ke bare mem jankari prapt karne ke lie kiya ja sakta hai.\n\n    Reqs:\n    - itertools\n    - seaborn\n    - matplotlib\n\n    Examples:\n    - >>> ax = task_func(10)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    - >>> ax = task_func(9)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    - >>> ax = task_func(8)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    - >>> ax = task_func(7)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n    - >>> ax = task_func(6)\n    - >>> [tick.get_text() for tick in ax.get_xticklabels()]\n    - ['Circle:Red', 'Circle:Blue', 'Circle:Green', 'Circle:Yellow', 'Circle:Black', 'Circle:White', 'Circle:Purple', 'Circle:Orange', 'Circle:Pink', 'Circle:Brown']\n\n    \"\"\"\n    # Generate shape-color combinations\n    combinations = list(itertools.product(SHAPES, COLORS))\n    \n    # Limit the number of combinations to num_pairs\n    combinations = combinations[:num_pairs]\n    \n    # Create a countplot\n    sns.set(style=\"whitegrid\")\n    ax = sns.countplot(x=combinations, palette=\"Set2\")\n    \n    # Set the x-axis labels\n    ax.set_xticklabels([f\"{shape}:{color}\" for shape, color in combinations], rotation=45)\n    \n    # Adjust the plot\n    plt.tight_layout()\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/1059", "solution": "import numpy as np\nimport random\nimport itertools\nimport pandas as pd\nPLANETS = [\n    \"Mercury\",\n    \"Venus\",\n    \"Earth\",\n    \"Mars\",\n    \"Jupiter\",\n    \"Saturn\",\n    \"Uranus\",\n    \"Neptune\",\n]\nELEMENTS = [\n    \"Hydrogen\",\n    \"Helium\",\n    \"Oxygen\",\n    \"Carbon\",\n    \"Nitrogen\",\n    \"Magnesium\",\n    \"Silicon\",\n    \"Iron\",\n    \"Nickel\",\n]\ndef task_func():\n    \"\"\"\n    Ek DataFrame banaen jahaan pratyek row mein random planet-elements jode hon.\n    Pratyek pair 'Planet:Element' ke roop mein swaroopit hai. rowyon ki number se nirdharit hoti hai\n    planets ki number, aur pratyek row mein utne hi planet-elements jode honge jitne elements hain.\n\n    Params:\n    - none\n\n    Returns:\n        pandas.DataFrame: Ek DataFrame jahaan pratyek cell mein 'Planet:Element' format mein ek string hoti hai.\n        DataFrame mein planets ki number ke baraabar rowyan hoti hain aur\n        elementson ki number ke baraabar column hote hain.\n\n    Reqs:\n    - numpy\n    - random\n    - itertools\n    - pandas\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> planet_elements_table = task_func()\n    - >>> planet_elements_table.head(2)\n    - Hydrogen         Helium  ...          Iron         Nickel\n    - 0   Uranus:Silicon  Earth:Silicon  ...  Earth:Nickel  Uranus:Helium\n    - 1  Venus:Magnesium  Saturn:Helium  ...  Mercury:Iron   Venus:Helium\n    - <BLANKLINE>\n    - [2 rows x 9 columns]\n\n    \"\"\"\n    # Generate all possible combinations of planets and elements\n    planet_elements_pairs = list(itertools.product(PLANETS, ELEMENTS))\n\n    # Shuffle the pairs to ensure randomness\n    random.shuffle(planet_elements_pairs)\n\n    # Create a DataFrame with the shuffled pairs\n    df = pd.DataFrame(planet_elements_pairs, columns=[\"Planet\", \"Element\"])\n\n    # Pivot the DataFrame to have planets as columns and elements as rows\n    df_pivot = df.pivot(index=\"Element\", columns=\"Planet\", values=\"Element\")\n\n    return df_pivot", "raw_solution": "import numpy as np\nimport random\nimport itertools\nimport pandas as pd\n\n# Constants\nPLANETS = [\n    \"Mercury\",\n    \"Venus\",\n    \"Earth\",\n    \"Mars\",\n    \"Jupiter\",\n    \"Saturn\",\n    \"Uranus\",\n    \"Neptune\",\n]\nELEMENTS = [\n    \"Hydrogen\",\n    \"Helium\",\n    \"Oxygen\",\n    \"Carbon\",\n    \"Nitrogen\",\n    \"Magnesium\",\n    \"Silicon\",\n    \"Iron\",\n    \"Nickel\",\n]\n\n\ndef task_func():\n    \"\"\"\n    Ek DataFrame banaen jahaan pratyek row mein random planet-elements jode hon.\n    Pratyek pair 'Planet:Element' ke roop mein swaroopit hai. rowyon ki number se nirdharit hoti hai\n    planets ki number, aur pratyek row mein utne hi planet-elements jode honge jitne elements hain.\n\n    Params:\n    - none\n\n    Returns:\n        pandas.DataFrame: Ek DataFrame jahaan pratyek cell mein 'Planet:Element' format mein ek string hoti hai.\n        DataFrame mein planets ki number ke baraabar rowyan hoti hain aur\n        elementson ki number ke baraabar column hote hain.\n\n    Reqs:\n    - numpy\n    - random\n    - itertools\n    - pandas\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> planet_elements_table = task_func()\n    - >>> planet_elements_table.head(2)\n    - Hydrogen         Helium  ...          Iron         Nickel\n    - 0   Uranus:Silicon  Earth:Silicon  ...  Earth:Nickel  Uranus:Helium\n    - 1  Venus:Magnesium  Saturn:Helium  ...  Mercury:Iron   Venus:Helium\n    - <BLANKLINE>\n    - [2 rows x 9 columns]\n\n    \"\"\"\n    # Generate all possible combinations of planets and elements\n    planet_elements_pairs = list(itertools.product(PLANETS, ELEMENTS))\n\n    # Shuffle the pairs to ensure randomness\n    random.shuffle(planet_elements_pairs)\n\n    # Create a DataFrame with the shuffled pairs\n    df = pd.DataFrame(planet_elements_pairs, columns=[\"Planet\", \"Element\"])\n\n    # Pivot the DataFrame to have planets as columns and elements as rows\n    df_pivot = df.pivot(index=\"Element\", columns=\"Planet\", values=\"Element\")\n\n    return df_pivot\n```\n"}
{"task_id": "BigCodeBench/1060", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame, column_name: str) -> (str, plt.Axes):\n    if df.empty or column_name not in df.columns or df[column_name].nunique() <= 1:\n        message = f\"DataFrame khaali hai ya {column_name} mein koi data nahi hai.\"\n        fig, ax = plt.subplots()\n        ax.set_title(f\"{column_name} mein maanon ka distribution (koi data nahi)\")\n        return message, ax\n\n    unique_values = df[column_name].value_counts()\n    if unique_values.shape[0] == 1:\n        message = \"Maanon ka distribution samaan hai.\"\n    else:\n        message = \"Maanon ka distribution samaan nahi hai.\"\n\n    fig, ax = plt.subplots()\n    ax.hist(df[column_name], bins=unique_values.shape[0], edgecolor='black', alpha=0.7)\n    ax.set_xlabel(\"maan\")\n    ax.set_ylabel(\"frequency\")\n    ax.set_title(f\"{column_name} mein maanon ka distribution\")\n\n    return message, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame, column_name: str) -> (str, plt.Axes):\n    if df.empty or column_name not in df.columns or df[column_name].nunique() <= 1:\n        message = f\"DataFrame khaali hai ya {column_name} mein koi data nahi hai.\"\n        fig, ax = plt.subplots()\n        ax.set_title(f\"{column_name} mein maanon ka distribution (koi data nahi)\")\n        return message, ax\n\n    unique_values = df[column_name].value_counts()\n    if unique_values.shape[0] == 1:\n        message = \"Maanon ka distribution samaan hai.\"\n    else:\n        message = \"Maanon ka distribution samaan nahi hai.\"\n\n    fig, ax = plt.subplots()\n    ax.hist(df[column_name], bins=unique_values.shape[0], edgecolor='black', alpha=0.7)\n    ax.set_xlabel(\"maan\")\n    ax.set_ylabel(\"frequency\")\n    ax.set_title(f\"{column_name} mein maanon ka distribution\")\n\n    return message, ax\n\n# Example usage:\n# df = pd.DataFrame({'Category': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'D', 'E', 'E']})\n# message, ax = task_func(df, 'Category')\n# print(message)\n```\n"}
{"task_id": "BigCodeBench/1061", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Calculate row sums\n    row_sums = np.sum(arr, axis=1)\n    \n    # Normalize row sums\n    mean, std = np.mean(row_sums), np.std(row_sums)\n    if std == 0:\n        normalized_data = (row_sums - mean) / std\n    else:\n        normalized_data = (row_sums - mean) / std\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot pdf\n    pdf_x = np.linspace(min(bins), max(bins), 100)\n    pdf_y = norm.pdf(pdf_x)\n    ax.plot(pdf_x, pdf_y, linewidth=2, color='r')\n\n    # Set title and labels\n    ax.set_title('Standard Normal PDF with Normalized Data Histogram')\n    ax.set_xlabel('Normalized Value')\n    ax.set_ylabel('Density')\n\n    return ax, normalized_data", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Calculate row sums\n    row_sums = np.sum(arr, axis=1)\n    \n    # Normalize row sums\n    mean, std = np.mean(row_sums), np.std(row_sums)\n    if std == 0:\n        normalized_data = (row_sums - mean) / std\n    else:\n        normalized_data = (row_sums - mean) / std\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot pdf\n    pdf_x = np.linspace(min(bins), max(bins), 100)\n    pdf_y = norm.pdf(pdf_x)\n    ax.plot(pdf_x, pdf_y, linewidth=2, color='r')\n\n    # Set title and labels\n    ax.set_title('Standard Normal PDF with Normalized Data Histogram')\n    ax.set_xlabel('Normalized Value')\n    ax.set_ylabel('Density')\n\n    return ax, normalized_data\n```\n"}
{"task_id": "BigCodeBench/1062", "solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\ndef task_func(arr):\n    # Calculate the sum of each row in the array\n    row_sums = np.sum(arr, axis=1)\n\n    # Create a date range starting from 1st January 2020\n    dates = pd.date_range(start='1/1/2020', periods=len(row_sums))\n\n    # Create a DataFrame with the sums and dates\n    df = pd.DataFrame({'Date': dates, 'Sum': row_sums})\n\n    # Plot the sums as a time series\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sum'])\n    ax.set_title('Time Series of Row Sums')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sum')\n\n    return ax", "raw_solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef task_func(arr):\n    # Calculate the sum of each row in the array\n    row_sums = np.sum(arr, axis=1)\n\n    # Create a date range starting from 1st January 2020\n    dates = pd.date_range(start='1/1/2020', periods=len(row_sums))\n\n    # Create a DataFrame with the sums and dates\n    df = pd.DataFrame({'Date': dates, 'Sum': row_sums})\n\n    # Plot the sums as a time series\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Sum'])\n    ax.set_title('Time Series of Row Sums')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sum')\n\n    return ax\n\n# Example usage:\n# arr = np.array([[i + j for i in range(3)] for j in range(5)])\n# ax = task_func(arr)\n# print(ax.get_title())  # Output: Time Series of Row Sums\n```\n"}
{"task_id": "BigCodeBench/1063", "solution": "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(arr):\n    # Ensure input is a 2D numpy array\n    if not isinstance(arr, np.ndarray) or arr.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n\n    # Perform PCA\n    pca = PCA(n_components=1)\n    pca.fit(arr)\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Plot the explained variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, align='center')\n    ax.set_xlabel('Principal Components')\n    ax.set_ylabel('Variance Ratio')\n    ax.set_title('Explained Variance ratio of Principal Components')\n\n    return ax", "raw_solution": "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(arr):\n    # Ensure input is a 2D numpy array\n    if not isinstance(arr, np.ndarray) or arr.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n\n    # Perform PCA\n    pca = PCA(n_components=1)\n    pca.fit(arr)\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Plot the explained variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, align='center')\n    ax.set_xlabel('Principal Components')\n    ax.set_ylabel('Variance Ratio')\n    ax.set_title('Explained Variance ratio of Principal Components')\n\n    return ax\n\n# Example usage:\n# arr = np.array([[i+j for i in range(3)] for j in range(5)])\n# axes = task_func(arr)\n# print(axes.get_title())  # Output: Explained Variance ratio of Principal Components\n```\n"}
{"task_id": "BigCodeBench/1064", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    \"\"\"\n    Ek dee gayee 2D numerical array ka heatmap plot karta hai aur pratyek row ke yog ko prints hai.\n    heatmap ki rang seema array mein minimum aur maximum maanon ke aadhaar par set kee jaati hai.\n\n    Notes:\n        function pratyek row ke yog ki ganana karta hai aur in maanon ko prints hai.\n        heatmap original array ke aadhaar par plot kiya jaata hai, jiski rang seema array mein minimum se maximum maan tak set ki jaati hai.\n\n    Params:\n    - arr (numpy.array): numerical maanon ki ek 2D numpy array.\n\n    Returns:\n        ax (matplotlib.axes.Axes): plot kiye gaye heatmap ke saath Axes object.\n\n    Reqs:\n    - numpy\n    - seaborn\n\n    Examples:\n    - >>> arr = np.array([[i + j for i in range(3)] for j in range(5)])\n    - >>> ax = task_func(arr)\n    - >>> ax.get_title()\n    - 'Heatmap of the 2D Array'\n\n    \"\"\"\n    # Check if input is a 2D array\n    if len(arr.shape) != 2:\n        raise ValueError(\"Input array must be 2D\")\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(arr, annot=True, fmt=\"d\", cmap=\"viridis\", cbar_kws={\"shrink\": 0.8})\n    ax.set_title('Heatmap of the 2D Array')\n    plt.show()\n\n    # Print row sums\n    row_sums = np.sum(arr, axis=1)\n    for i, sum in enumerate(row_sums):\n        print(f\"Row {i+1} sum: {sum}\")\n\n    return ax", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(arr):\n    \"\"\"\n    Ek dee gayee 2D numerical array ka heatmap plot karta hai aur pratyek row ke yog ko prints hai.\n    heatmap ki rang seema array mein minimum aur maximum maanon ke aadhaar par set kee jaati hai.\n\n    Notes:\n        function pratyek row ke yog ki ganana karta hai aur in maanon ko prints hai.\n        heatmap original array ke aadhaar par plot kiya jaata hai, jiski rang seema array mein minimum se maximum maan tak set ki jaati hai.\n\n    Params:\n    - arr (numpy.array): numerical maanon ki ek 2D numpy array.\n\n    Returns:\n        ax (matplotlib.axes.Axes): plot kiye gaye heatmap ke saath Axes object.\n\n    Reqs:\n    - numpy\n    - seaborn\n\n    Examples:\n    - >>> arr = np.array([[i + j for i in range(3)] for j in range(5)])\n    - >>> ax = task_func(arr)\n    - >>> ax.get_title()\n    - 'Heatmap of the 2D Array'\n\n    \"\"\"\n    # Check if input is a 2D array\n    if len(arr.shape) != 2:\n        raise ValueError(\"Input array must be 2D\")\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(arr, annot=True, fmt=\"d\", cmap=\"viridis\", cbar_kws={\"shrink\": 0.8})\n    ax.set_title('Heatmap of the 2D Array')\n    plt.show()\n\n    # Print row sums\n    row_sums = np.sum(arr, axis=1)\n    for i, sum in enumerate(row_sums):\n        print(f\"Row {i+1} sum: {sum}\")\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1065", "solution": "from scipy import fftpack\nfrom matplotlib import pyplot as plt\nimport numpy as np\ndef task_func(arr):\n    \"\"\"\n    2D array ki pratyek row ke yog par ek fast Fourier transform (FFT) karta hai aur\n    FFT coefficients ke absolute maanon ko plot karta hai.\n\n    Params:\n    - arr (numpy.ndarray): ek 2D numpy array.\n\n    Returns:\n        matplotlib.axes.Axes: ek axes object jo FFT coefficients ke absolute maanon ke plot ko pradarshit karta hai.\n\n    Reqs:\n    - scipy.scipy.fftpack\n    - matplotlib.matplotlib.pyplot\n\n    Examples:\n    - >>> import numpy as np\n    - >>> arr = np.array([[i + j for i in range(3)] for j in range(5)])\n    - >>> ax = task_func(arr)\n    - >>> ax.get_title()\n    - 'Absolute values of FFT coefficients'\n\n    \"\"\"\n    # Perform FFT on each row of the array\n    fft_coeffs = fftpack.fft(arr, axis=1)\n    \n    # Calculate absolute values of the FFT coefficients\n    abs_fft_coeffs = np.abs(fft_coeffs)\n    \n    # Plot the absolute values\n    fig, ax = plt.subplots()\n    ax.imshow(abs_fft_coeffs, aspect='auto')\n    ax.set_title('Absolute values of FFT coefficients')\n    ax.set_xlabel('FFT Coefficients')\n    ax.set_ylabel('Array Rows')\n    \n    return ax", "raw_solution": "from scipy import fftpack\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef task_func(arr):\n    \"\"\"\n    2D array ki pratyek row ke yog par ek fast Fourier transform (FFT) karta hai aur\n    FFT coefficients ke absolute maanon ko plot karta hai.\n\n    Params:\n    - arr (numpy.ndarray): ek 2D numpy array.\n\n    Returns:\n        matplotlib.axes.Axes: ek axes object jo FFT coefficients ke absolute maanon ke plot ko pradarshit karta hai.\n\n    Reqs:\n    - scipy.scipy.fftpack\n    - matplotlib.matplotlib.pyplot\n\n    Examples:\n    - >>> import numpy as np\n    - >>> arr = np.array([[i + j for i in range(3)] for j in range(5)])\n    - >>> ax = task_func(arr)\n    - >>> ax.get_title()\n    - 'Absolute values of FFT coefficients'\n\n    \"\"\"\n    # Perform FFT on each row of the array\n    fft_coeffs = fftpack.fft(arr, axis=1)\n    \n    # Calculate absolute values of the FFT coefficients\n    abs_fft_coeffs = np.abs(fft_coeffs)\n    \n    # Plot the absolute values\n    fig, ax = plt.subplots()\n    ax.imshow(abs_fft_coeffs, aspect='auto')\n    ax.set_title('Absolute values of FFT coefficients')\n    ax.set_xlabel('FFT Coefficients')\n    ax.set_ylabel('Array Rows')\n    \n    return ax\n\n# Example usage:\n# arr = np.array([[i + j for i in range(3)] for j in range(5)])\n# ax = task_func(arr)\n# print(ax.get_title())  # Output: Absolute values of FFT coefficients\n```\n"}
{"task_id": "BigCodeBench/1066", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\ndef task_func(num_samples=NUM_SAMPLES, num_outliers=NUM_OUTLIERS):\n    # Generate normal data\n    data = np.random.normal(0, 1, num_samples)\n    \n    # Generate artificial outliers\n    outliers = np.random.uniform(-10, 10, num_outliers)\n    \n    # Combine normal data and outliers\n    data_with_outliers = np.concatenate([data, outliers])\n    \n    # Sort data to find outliers\n    sorted_data = np.sort(data_with_outliers)\n    \n    # Calculate Q1 and Q3\n    Q1 = np.percentile(sorted_data, 25)\n    Q3 = np.percentile(sorted_data, 75)\n    \n    # Calculate IQR\n    IQR = Q3 - Q1\n    \n    # Define outlier range\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # Detect outliers\n    outliers_detected = sorted_data[(sorted_data < lower_bound) | (sorted_data > upper_bound)]\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(data_with_outliers, bins=30, alpha=0.5, color='blue', label='Data with Outliers')\n    ax.axvline(lower_bound, color='red', linestyle='--', label='Lower Bound')\n    ax.axvline(upper_bound, color='red', linestyle='--', label='Upper Bound')\n    ax.set_title('Histogram of Data with Outliers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    \n    return data_with_outliers, outliers_detected, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\n\ndef task_func(num_samples=NUM_SAMPLES, num_outliers=NUM_OUTLIERS):\n    # Generate normal data\n    data = np.random.normal(0, 1, num_samples)\n    \n    # Generate artificial outliers\n    outliers = np.random.uniform(-10, 10, num_outliers)\n    \n    # Combine normal data and outliers\n    data_with_outliers = np.concatenate([data, outliers])\n    \n    # Sort data to find outliers\n    sorted_data = np.sort(data_with_outliers)\n    \n    # Calculate Q1 and Q3\n    Q1 = np.percentile(sorted_data, 25)\n    Q3 = np.percentile(sorted_data, 75)\n    \n    # Calculate IQR\n    IQR = Q3 - Q1\n    \n    # Define outlier range\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # Detect outliers\n    outliers_detected = sorted_data[(sorted_data < lower_bound) | (sorted_data > upper_bound)]\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(data_with_outliers, bins=30, alpha=0.5, color='blue', label='Data with Outliers')\n    ax.axvline(lower_bound, color='red', linestyle='--', label='Lower Bound')\n    ax.axvline(upper_bound, color='red', linestyle='--', label='Upper Bound')\n    ax.set_title('Histogram of Data with Outliers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    \n    return data_with_outliers, outliers_detected, ax\n```\n"}
{"task_id": "BigCodeBench/1067", "solution": "import requests\nimport logging\ndef task_func(repo_url: str) -> dict:\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n    \n    try:\n        response = requests.get(repo_url)\n        response.raise_for_status()\n        repo_info = response.json()\n        \n        # Check for high number of issues\n        if repo_info.get('open_issues_count', 0) > 100:\n            logging.warning(f\"High number of open issues in the repository: {repo_info.get('open_issues_count', 0)}\")\n        \n        return repo_info\n\n    except requests.exceptions.HTTPError as http_err:\n        logging.error(f\"HTTP error occurred: {http_err}\")\n        raise\n    except requests.exceptions.RequestException as err:\n        logging.error(f\"An error occurred: {err}\")\n        raise", "raw_solution": "import requests\nimport logging\n\ndef task_func(repo_url: str) -> dict:\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n    \n    try:\n        response = requests.get(repo_url)\n        response.raise_for_status()\n        repo_info = response.json()\n        \n        # Check for high number of issues\n        if repo_info.get('open_issues_count', 0) > 100:\n            logging.warning(f\"High number of open issues in the repository: {repo_info.get('open_issues_count', 0)}\")\n        \n        return repo_info\n\n    except requests.exceptions.HTTPError as http_err:\n        logging.error(f\"HTTP error occurred: {http_err}\")\n        raise\n    except requests.exceptions.RequestException as err:\n        logging.error(f\"An error occurred: {err}\")\n        raise\n```\nThis script uses the requests library to make an HTTP GET request to the provided GitHub repository URL. It then parses the JSON response to extract the repository information. If the number of open issues exceeds 100, a warning is logged. The script also handles HTTP and general request errors using try-except blocks and logs the errors for debugging purposes.\n"}
{"task_id": "BigCodeBench/1068", "solution": "import warnings\nimport sqlite3\nimport pandas as pd\ndef task_func(db_path, query, warn_large_dataset=True):\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n        # Execute the SQL query\n        df = pd.read_sql_query(query, conn)\n        # Close the database connection\n        conn.close()\n\n        # Check if the dataset is large and warn if required\n        if warn_large_dataset and len(df) > 10000:\n            warnings.warn(\"The dataset contains 10,000 or more rows.\", UserWarning)\n\n        return df\n\n    except Exception as e:\n        raise Exception(f\"Database error: {str(e)}\") from e", "raw_solution": "import warnings\nimport sqlite3\nimport pandas as pd\n\ndef task_func(db_path, query, warn_large_dataset=True):\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n        # Execute the SQL query\n        df = pd.read_sql_query(query, conn)\n        # Close the database connection\n        conn.close()\n\n        # Check if the dataset is large and warn if required\n        if warn_large_dataset and len(df) > 10000:\n            warnings.warn(\"The dataset contains 10,000 or more rows.\", UserWarning)\n\n        return df\n\n    except Exception as e:\n        raise Exception(f\"Database error: {str(e)}\") from e\n\n# Example usage:\n# data = task_func('/path/to/sqlite.db', 'SELECT * FROM table_name')\n# print(data)\n```\n"}
{"task_id": "BigCodeBench/1069", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    df = pd.DataFrame(data_dict)\n    axes = []\n\n    for column in df.columns:\n        ax = df[column].value_counts().plot(kind='bar')\n        ax.set_title(f'Distribution of values in column {column}')\n        axes.append(ax)\n        \n        # Check for uniform distribution\n        unique_values = df[column].nunique()\n        if unique_values > 1:\n            print(f'The distribution of values in column {column} is not uniform.')\n        else:\n            print(f'The distribution of values in column {column} is uniform.')\n\n    return axes", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    df = pd.DataFrame(data_dict)\n    axes = []\n\n    for column in df.columns:\n        ax = df[column].value_counts().plot(kind='bar')\n        ax.set_title(f'Distribution of values in column {column}')\n        axes.append(ax)\n        \n        # Check for uniform distribution\n        unique_values = df[column].nunique()\n        if unique_values > 1:\n            print(f'The distribution of values in column {column} is not uniform.')\n        else:\n            print(f'The distribution of values in column {column} is uniform.')\n\n    return axes\n```\n"}
{"task_id": "BigCodeBench/1070", "solution": "import pandas as pd\nfrom random import shuffle\nPOSSIBLE_VALUES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\ndef task_func(list_of_lists):\n    dataframes = []\n    for sublist in list_of_lists:\n        df = pd.DataFrame({col: sublist for col in sublist})\n        df = df.applymap(lambda x: shuffle(POSSIBLE_VALUES) or POSSIBLE_VALUES[0])\n        dataframes.append(df)\n    return dataframes", "raw_solution": "import pandas as pd\nfrom random import shuffle\n\n# Constants\nPOSSIBLE_VALUES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\n\ndef task_func(list_of_lists):\n    dataframes = []\n    for sublist in list_of_lists:\n        df = pd.DataFrame({col: sublist for col in sublist})\n        df = df.applymap(lambda x: shuffle(POSSIBLE_VALUES) or POSSIBLE_VALUES[0])\n        dataframes.append(df)\n    return dataframes\n```\nThis script defines a function `task_func` that takes a list of lists as input. For each sublist, it creates a DataFrame with columns named after the elements in the sublist. Then, it applies a lambda function to each element of the DataFrame that shuffles the `POSSIBLE_VALUES` list and assigns the first element as the value. The function returns a list of DataFrames.\n\nPlease note that the `shuffle` function modifies the list in-place, so we need to make sure to create a new list for each DataFrame to avoid modifying the original list. Also, the `or` operator is used to handle the case where `shuffle` returns `None` (which is a possible outcome of the `shuffle` function).\n"}
{"task_id": "BigCodeBench/1071", "solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\nCOLORS = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\ndef task_func(list_of_lists):\n    color_cycle = cycle(COLORS)\n    x_values = np.arange(1, max(len(lst) for lst in list_of_lists) + 1)\n\n    fig, ax = plt.subplots()\n\n    for lst in list_of_lists:\n        if not lst:\n            continue\n        shuffle(lst)\n        y_values = np.array(lst)\n        ax.plot(x_values[:len(y_values)], y_values, color=next(color_cycle))\n\n    return fig, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\n\nCOLORS = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\n\n\ndef task_func(list_of_lists):\n    color_cycle = cycle(COLORS)\n    x_values = np.arange(1, max(len(lst) for lst in list_of_lists) + 1)\n\n    fig, ax = plt.subplots()\n\n    for lst in list_of_lists:\n        if not lst:\n            continue\n        shuffle(lst)\n        y_values = np.array(lst)\n        ax.plot(x_values[:len(y_values)], y_values, color=next(color_cycle))\n\n    return fig, ax\n```\n"}
{"task_id": "BigCodeBench/1072", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(list_of_lists):\n    series_list = []\n    for sub_list in list_of_lists:\n        # Generate a range of integers from 1 to the length of the sub_list\n        values = np.arange(1, len(sub_list) + 1)\n        # Shuffle the values\n        np.random.shuffle(values)\n        # Create a pandas Series with the shuffled values and the sub_list as index\n        series = pd.Series(values, index=sub_list)\n        series_list.append(series)\n    return series_list", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(list_of_lists):\n    series_list = []\n    for sub_list in list_of_lists:\n        # Generate a range of integers from 1 to the length of the sub_list\n        values = np.arange(1, len(sub_list) + 1)\n        # Shuffle the values\n        np.random.shuffle(values)\n        # Create a pandas Series with the shuffled values and the sub_list as index\n        series = pd.Series(values, index=sub_list)\n        series_list.append(series)\n    return series_list\n\n# Example usage:\n# np.random.seed(0)  # Setting a seed for reproducibility of the example\n# series = task_func([['x', 'y', 'z'], ['a', 'b', 'c']])\n# for s in series:\n#     print(s)\n```\n"}
{"task_id": "BigCodeBench/1073", "solution": "import time\nimport matplotlib.pyplot as plt\ndef task_func(time_strings, time_format=\"%d/%m/%Y %H:%M:%S.%f\"):\n    try:\n        # Convert time strings to seconds since epoch\n        times = [time.mktime(time.strptime(ts, time_format)) for ts in time_strings]\n        \n        # Plotting the histogram\n        plt.hist(times, bins=10, edgecolor='black')\n        plt.xlabel('Time (seconds since epoch)')\n        plt.ylabel('Frequency')\n        plt.title('Histogram of Time Strings')\n        \n        return plt.gca()\n    except ValueError as e:\n        print(f\"Error parsing time strings: {e}\")\n        return None", "raw_solution": "import time\nimport matplotlib.pyplot as plt\n\ndef task_func(time_strings, time_format=\"%d/%m/%Y %H:%M:%S.%f\"):\n    try:\n        # Convert time strings to seconds since epoch\n        times = [time.mktime(time.strptime(ts, time_format)) for ts in time_strings]\n        \n        # Plotting the histogram\n        plt.hist(times, bins=10, edgecolor='black')\n        plt.xlabel('Time (seconds since epoch)')\n        plt.ylabel('Frequency')\n        plt.title('Histogram of Time Strings')\n        \n        return plt.gca()\n    except ValueError as e:\n        print(f\"Error parsing time strings: {e}\")\n        return None\n```\n"}
{"task_id": "BigCodeBench/1074", "solution": "import pytz\nfrom dateutil.parser import parse\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_string, from_tz, to_tz):\n    \"\"\"\n    Convert a time string from one timezone to another, taking into account daylight saving rules.\n\n    Parameters:\n    - time_string (str): A time string in 'dd/mm/yy HH:MM:SS.fff' format. This string should represent a date and time.\n    - from_tz (str): The timezone of the input time string. This should be a valid IANA timezone name (e.g., 'UTC', 'America/New_York').\n    - to_tz (str): The target timezone for conversion. This should also be a valid IANA timezone name (e.g., 'Asia/Tokyo').\n\n    Returns:\n    - str: The converted time string in 'dd/mm/yy HH:MM:SS.fff' format. The conversion takes into account daylight saving rules.\n\n    Requirements:\n    - pytz\n    - dateutil\n\n    Examples:\n    - >>> task_func('30/03/09 16:31:32.123', 'UTC', 'America/New_York')\n    - '30/03/09 12:31:32.123000'\n    \"\"\"\n    # Parse the input time string\n    naive_datetime = parse(time_string)\n    \n    # Localize the naive datetime to the source timezone\n    from_timezone = pytz.timezone(from_tz)\n    aware_datetime_from = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the target timezone\n    to_timezone = pytz.timezone(to_tz)\n    aware_datetime_to = aware_datetime_from.astimezone(to_timezone)\n    \n    # Return the converted time string in the specified format\n    return aware_datetime_to.strftime(TIME_FORMAT)", "raw_solution": "import pytz\nfrom dateutil.parser import parse\n\n# Constants\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\n\ndef task_func(time_string, from_tz, to_tz):\n    \"\"\"\n    Convert a time string from one timezone to another, taking into account daylight saving rules.\n\n    Parameters:\n    - time_string (str): A time string in 'dd/mm/yy HH:MM:SS.fff' format. This string should represent a date and time.\n    - from_tz (str): The timezone of the input time string. This should be a valid IANA timezone name (e.g., 'UTC', 'America/New_York').\n    - to_tz (str): The target timezone for conversion. This should also be a valid IANA timezone name (e.g., 'Asia/Tokyo').\n\n    Returns:\n    - str: The converted time string in 'dd/mm/yy HH:MM:SS.fff' format. The conversion takes into account daylight saving rules.\n\n    Requirements:\n    - pytz\n    - dateutil\n\n    Examples:\n    - >>> task_func('30/03/09 16:31:32.123', 'UTC', 'America/New_York')\n    - '30/03/09 12:31:32.123000'\n    \"\"\"\n    # Parse the input time string\n    naive_datetime = parse(time_string)\n    \n    # Localize the naive datetime to the source timezone\n    from_timezone = pytz.timezone(from_tz)\n    aware_datetime_from = from_timezone.localize(naive_datetime)\n    \n    # Convert the datetime to the target timezone\n    to_timezone = pytz.timezone(to_tz)\n    aware_datetime_to = aware_datetime_from.astimezone(to_timezone)\n    \n    # Return the converted time string in the specified format\n    return aware_datetime_to.strftime(TIME_FORMAT)\n```\n"}
{"task_id": "BigCodeBench/1075", "solution": "import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings):\n    \"\"\"\n    consecutive datetime strings ke beech integer values mein seconds mein differences ki ganana karein aur in differences ko ek bar chart ke roop mein plot karein.\n\n    Notes:\n        Yeh function datetime, numpy, aur matplotlib.pyplot modules ki maang karta hai. \n        input list mein datetime strings ko TIME_FORMAT mein nirdisht specific format ka paalan karna chahiye.\n        Yeh function list mein pratyek consecutive datetime strings ke pair ke beech time differences ki ganana karta hai.\n\n    Params:\n    - time_strings (list of str): A list of datetime strings in the format 'dd/mm/yy HH:MM:SS.fff'.\n\n    Returns:\n        matplotlib.axes.Axes: Plotted bar chart ka axes object. Yeh object is function ke bahar plot ko aur adhik customize karne ki anumati deta hai.\n\n    Reqs:\n    - datetime\n    - numpy\n    - matplotlib\n\n    Examples:\n    - >>> time_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']\n    - >>> ax = task_func(time_strings)\n    - >>> plt.show()  # This will display the bar chart\n\n    \"\"\"\n    # Convert strings to datetime objects\n    datetime_objs = [datetime.datetime.strptime(ts, TIME_FORMAT) for ts in time_strings]\n    \n    # Calculate differences in seconds\n    differences = np.diff([dt.timestamp() for dt in datetime_objs])\n    \n    # Plot differences\n    fig, ax = plt.subplots()\n    ax.bar(range(len(differences)), differences, color='blue')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Time Difference (seconds)')\n    ax.set_title('Time Differences Between Consecutive Datetime Strings')\n    \n    return ax", "raw_solution": "import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\n\ndef task_func(time_strings):\n    \"\"\"\n    consecutive datetime strings ke beech integer values mein seconds mein differences ki ganana karein aur in differences ko ek bar chart ke roop mein plot karein.\n\n    Notes:\n        Yeh function datetime, numpy, aur matplotlib.pyplot modules ki maang karta hai. \n        input list mein datetime strings ko TIME_FORMAT mein nirdisht specific format ka paalan karna chahiye.\n        Yeh function list mein pratyek consecutive datetime strings ke pair ke beech time differences ki ganana karta hai.\n\n    Params:\n    - time_strings (list of str): A list of datetime strings in the format 'dd/mm/yy HH:MM:SS.fff'.\n\n    Returns:\n        matplotlib.axes.Axes: Plotted bar chart ka axes object. Yeh object is function ke bahar plot ko aur adhik customize karne ki anumati deta hai.\n\n    Reqs:\n    - datetime\n    - numpy\n    - matplotlib\n\n    Examples:\n    - >>> time_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']\n    - >>> ax = task_func(time_strings)\n    - >>> plt.show()  # This will display the bar chart\n\n    \"\"\"\n    # Convert strings to datetime objects\n    datetime_objs = [datetime.datetime.strptime(ts, TIME_FORMAT) for ts in time_strings]\n    \n    # Calculate differences in seconds\n    differences = np.diff([dt.timestamp() for dt in datetime_objs])\n    \n    # Plot differences\n    fig, ax = plt.subplots()\n    ax.bar(range(len(differences)), differences, color='blue')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Time Difference (seconds)')\n    ax.set_title('Time Differences Between Consecutive Datetime Strings')\n    \n    return ax\n```\n"}
{"task_id": "BigCodeBench/1076", "solution": "from datetime import datetime\nimport pandas as pd\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings, target_tz):\n    original_times = []\n    converted_times = []\n\n    target_timezone = ZoneInfo(target_tz)\n\n    for time_str in time_strings:\n        utc_time = datetime.strptime(time_str, TIME_FORMAT)\n        utc_time = utc_time.replace(tzinfo=ZoneInfo('UTC'))\n        converted_time = utc_time.astimezone(target_timezone)\n        original_times.append(utc_time)\n        converted_times.append(converted_time)\n\n    df = pd.DataFrame({\n        'Original Time': original_times,\n        'Converted Time': converted_times\n    })\n\n    return df\ntime_strings = ['30/03/09 16:31:32.123', '15/04/10 14:25:46.789', '20/12/11 12:34:56.000']", "raw_solution": "from datetime import datetime\nimport pandas as pd\n\n# For Python versions lower than 3.9, use 'pytz' instead of 'zoneinfo'\ntry:\n    from zoneinfo import ZoneInfo\nexcept ImportError:\n    from pytz import timezone as ZoneInfo\n\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\ndef task_func(time_strings, target_tz):\n    original_times = []\n    converted_times = []\n\n    target_timezone = ZoneInfo(target_tz)\n\n    for time_str in time_strings:\n        utc_time = datetime.strptime(time_str, TIME_FORMAT)\n        utc_time = utc_time.replace(tzinfo=ZoneInfo('UTC'))\n        converted_time = utc_time.astimezone(target_timezone)\n        original_times.append(utc_time)\n        converted_times.append(converted_time)\n\n    df = pd.DataFrame({\n        'Original Time': original_times,\n        'Converted Time': converted_times\n    })\n\n    return df\n\n# Example usage:\ntime_strings = ['30/03/09 16:31:32.123', '15/04/10 14:25:46.789', '20/12/11 12:34:56.000']\ndf = task_func(time_strings, 'America/New_York')"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    # Convert strings to datetime objects in the specified timezone\n    tz = pytz.timezone(timezone)\n    datetime_objs = [datetime.strptime(ts, '%d/%m/%y %H:%M:%S.%f').replace(tzinfo=pytz.utc).astimezone(tz) for ts in time_strings]\n\n    # Calculate differences in seconds between consecutive timestamps\n    diffs = []\n    for i in range(1, len(datetime_objs)):\n        diff = (datetime_objs[i] - datetime_objs[i-1]).total_seconds()\n        diffs.append(diff)\n\n    # Return the mean of the differences if there are at least two timestamps, else return 0.0\n    return np.mean(diffs) if len(diffs) >= 2 else 0.0\ntime_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']", "raw_solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    # Convert strings to datetime objects in the specified timezone\n    tz = pytz.timezone(timezone)\n    datetime_objs = [datetime.strptime(ts, '%d/%m/%y %H:%M:%S.%f').replace(tzinfo=pytz.utc).astimezone(tz) for ts in time_strings]\n\n    # Calculate differences in seconds between consecutive timestamps\n    diffs = []\n    for i in range(1, len(datetime_objs)):\n        diff = (datetime_objs[i] - datetime_objs[i-1]).total_seconds()\n        diffs.append(diff)\n\n    # Return the mean of the differences if there are at least two timestamps, else return 0.0\n    return np.mean(diffs) if len(diffs) >= 2 else 0.0\n\n# Test cases\ntime_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']\nmean_diff = task_func(time_strings, 'America/New_York')"}
{"task_id": "BigCodeBench/1078", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    bin_edges = np.arange(len(unique) + 1) - 0.5\n    ax = plt.gca()\n    ax.hist(arr, bins=bin_edges, edgecolor='black')\n    ax.set_xticks(bin_edges)\n    ax.set_xticklabels(unique)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Value Distribution Histogram')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    uniform_distribution = len(set(counts)) == 1\n\n    return uniform_distribution, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    bin_edges = np.arange(len(unique) + 1) - 0.5\n    ax = plt.gca()\n    ax.hist(arr, bins=bin_edges, edgecolor='black')\n    ax.set_xticks(bin_edges)\n    ax.set_xticklabels(unique)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Value Distribution Histogram')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    uniform_distribution = len(set(counts)) == 1\n\n    return uniform_distribution, ax\n```\n"}
{"task_id": "BigCodeBench/1079", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert price strings to floats\n    price_strings = data['Price_String']\n    prices = [float(price.replace(',', '')) for price in price_strings]\n\n    # Calculate statistics\n    mean_price = np.mean(prices)\n    median_price = np.median(prices)\n    std_dev_price = np.std(prices, ddof=1)\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(prices, bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n    ax.set_title('Histogram of Product Prices')\n    ax.set_xlabel('Price')\n    ax.set_ylabel('Frequency')\n\n    # Return the statistics and the plot\n    return {\n        'mean': mean_price,\n        'median': median_price,\n        'std_dev': std_dev_price\n    }, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert price strings to floats\n    price_strings = data['Price_String']\n    prices = [float(price.replace(',', '')) for price in price_strings]\n\n    # Calculate statistics\n    mean_price = np.mean(prices)\n    median_price = np.median(prices)\n    std_dev_price = np.std(prices, ddof=1)\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(prices, bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n    ax.set_title('Histogram of Product Prices')\n    ax.set_xlabel('Price')\n    ax.set_ylabel('Frequency')\n\n    # Return the statistics and the plot\n    return {\n        'mean': mean_price,\n        'median': median_price,\n        'std_dev': std_dev_price\n    }, ax\n```\n"}
{"task_id": "BigCodeBench/1080", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nDATA = {\n    \"Area_String\": [\"1,000\", \"2,000\", \"3,000\", \"4,000\", \"5,000\"],\n    \"Price\": [100, 200, 300, 400, 500],\n}\ndef task_func(area_string, data=DATA):\n    # Convert the area string to an integer\n    area = int(area_string.replace(',', ''))\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n    \n    # Convert the 'Area_String' column to integers\n    df['Area'] = df['Area_String'].str.replace(',', '').astype(int)\n    \n    # Create a Linear Regression model\n    model = LinearRegression()\n    \n    # Train the model\n    model.fit(df[['Area']], df['Price'])\n    \n    # Predict the price for the given area\n    predicted_price = model.predict([[area]])[0]\n    \n    return predicted_price", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nDATA = {\n    \"Area_String\": [\"1,000\", \"2,000\", \"3,000\", \"4,000\", \"5,000\"],\n    \"Price\": [100, 200, 300, 400, 500],\n}\n\ndef task_func(area_string, data=DATA):\n    # Convert the area string to an integer\n    area = int(area_string.replace(',', ''))\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n    \n    # Convert the 'Area_String' column to integers\n    df['Area'] = df['Area_String'].str.replace(',', '').astype(int)\n    \n    # Create a Linear Regression model\n    model = LinearRegression()\n    \n    # Train the model\n    model.fit(df[['Area']], df['Price'])\n    \n    # Predict the price for the given area\n    predicted_price = model.predict([[area]])[0]\n    \n    return predicted_price\n\n# Test cases"}
{"task_id": "BigCodeBench/1081", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data=None):\n    if data is None:\n        data = {\n            'Weight_String': ['60.5', '65.7', '70.2', '75.9', '80.1'],\n            'Height': [160, 165, 170, 175, 180]\n        }\n\n    # Convert string weights to floats\n    try:\n        data['Weight'] = [float(weight) for weight in data['Weight_String']]\n    except ValueError:\n        raise ValueError(\"Weight_String values must be convertible to float.\")\n\n    # Create a DataFrame\n    df = pd.DataFrame(data)\n\n    # Create a scatter plot\n    ax = sns.scatterplot(x='Weight', y='Height', data=df)\n    ax.set_title('Weight vs Height')\n\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data=None):\n    if data is None:\n        data = {\n            'Weight_String': ['60.5', '65.7', '70.2', '75.9', '80.1'],\n            'Height': [160, 165, 170, 175, 180]\n        }\n\n    # Convert string weights to floats\n    try:\n        data['Weight'] = [float(weight) for weight in data['Weight_String']]\n    except ValueError:\n        raise ValueError(\"Weight_String values must be convertible to float.\")\n\n    # Create a DataFrame\n    df = pd.DataFrame(data)\n\n    # Create a scatter plot\n    ax = sns.scatterplot(x='Weight', y='Height', data=df)\n    ax.set_title('Weight vs Height')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1082", "solution": "import pandas as pd\nfrom scipy.stats import pearsonr\ndef task_func(data):\n    # Convert string scores to float\n    data['Score'] = pd.to_numeric(data['Score_String'], errors='coerce')\n\n    # Encode grades to numerical values\n    grade_mapping = {'A+': 5, 'A': 4, 'A-': 3, 'B+': 2, 'B': 1, 'B-': 0, 'C+': -1, 'C': -2, 'C-': -3, 'D': -4, 'F': -5}\n    data['Encoded_Grade'] = data['Grade'].map(grade_mapping)\n\n    # Calculate Pearson correlation coefficient\n    correlation, _ = pearsonr(data['Score'], data['Encoded_Grade'])\n\n    return correlation if len(data) > 1 else float('nan')", "raw_solution": "import pandas as pd\nfrom scipy.stats import pearsonr\n\ndef task_func(data):\n    # Convert string scores to float\n    data['Score'] = pd.to_numeric(data['Score_String'], errors='coerce')\n\n    # Encode grades to numerical values\n    grade_mapping = {'A+': 5, 'A': 4, 'A-': 3, 'B+': 2, 'B': 1, 'B-': 0, 'C+': -1, 'C': -2, 'C-': -3, 'D': -4, 'F': -5}\n    data['Encoded_Grade'] = data['Grade'].map(grade_mapping)\n\n    # Calculate Pearson correlation coefficient\n    correlation, _ = pearsonr(data['Score'], data['Encoded_Grade'])\n\n    return correlation if len(data) > 1 else float('nan')\n\n# Example usage:\n# data = {'Score_String': ['80.5', '85.7', '90.2'], 'Grade': ['B', 'B+', 'A-']}\n# print(round(task_func(data), 2))  # Output: -0.46\n```\n"}
{"task_id": "BigCodeBench/1083", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Validate input data\n    required_keys = ['salary_string', 'Experience']\n    if not all(key in data for key in required_keys):\n        raise ValueError(\"Input data must contain 'salary_string' and 'Experience' keys.\")\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Handle empty DataFrame\n    if df.empty:\n        _, ax = plt.subplots()\n        ax.set_title(\"Normalized Salary vs Experience\")\n        return ax\n\n    # Convert 'salary_string' to float\n    try:\n        df['salary_float'] = df['salary_string'].str.replace(',', '').astype(float)\n    except Exception as e:\n        raise ValueError(f\"Error converting 'salary_string' to float: {e}\")\n\n    # Normalize 'salary_float' using Min-Max scaling\n    scaler = MinMaxScaler()\n    df['normalized_salary'] = scaler.fit_transform(df[['salary_float']])\n\n    # Plotting\n    _, ax = plt.subplots()\n    ax.scatter(df['Experience'], df['normalized_salary'], color='blue')\n    ax.set_xlabel('Experience')\n    ax.set_ylabel('Normalized Salary')\n    ax.set_title('Normalized Salary vs Experience')\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Validate input data\n    required_keys = ['salary_string', 'Experience']\n    if not all(key in data for key in required_keys):\n        raise ValueError(\"Input data must contain 'salary_string' and 'Experience' keys.\")\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Handle empty DataFrame\n    if df.empty:\n        _, ax = plt.subplots()\n        ax.set_title(\"Normalized Salary vs Experience\")\n        return ax\n\n    # Convert 'salary_string' to float\n    try:\n        df['salary_float'] = df['salary_string'].str.replace(',', '').astype(float)\n    except Exception as e:\n        raise ValueError(f\"Error converting 'salary_string' to float: {e}\")\n\n    # Normalize 'salary_float' using Min-Max scaling\n    scaler = MinMaxScaler()\n    df['normalized_salary'] = scaler.fit_transform(df[['salary_float']])\n\n    # Plotting\n    _, ax = plt.subplots()\n    ax.scatter(df['Experience'], df['normalized_salary'], color='blue')\n    ax.set_xlabel('Experience')\n    ax.set_ylabel('Normalized Salary')\n    ax.set_title('Normalized Salary vs Experience')\n\n    return ax\n```\n"}
{"task_id": "BigCodeBench/1084", "solution": "import pandas as pd\nfrom sklearn.feature_selection import f_oneway\nimport matplotlib.pyplot as plt\ndef task_func(data_file_path: str):\n    # Load the data\n    df = pd.read_csv(data_file_path)\n\n    # Convert string representations of numbers to floats\n    df = df.apply(pd.to_numeric, errors='coerce')\n\n    # Calculate means and standard deviations\n    means = df.mean()\n    std_devs = df.std()\n\n    # Plot histograms for each numerical column\n    axes = []\n    for column in df.select_dtypes(include='number').columns:\n        ax = df[column].plot(kind='hist', title=f'Histogram of {column}', bins=10)\n        axes.append(ax)\n\n    # Perform ANOVA test if more than one numerical column is present\n    if len(df.select_dtypes(include='number').columns) > 1:\n        anova_results = pd.DataFrame(columns=['f_value', 'p_value'])\n        for i in range(len(df.select_dtypes(include='number').columns)):\n            for j in range(i+1, len(df.select_dtypes(include='number').columns)):\n                col1 = df.select_dtypes(include='number').columns[i]\n                col2 = df.select_dtypes(include='number').columns[j]\n                f_value, p_value = f_oneway(df[col1].dropna(), df[col2].dropna())\n                anova_results = anova_results.append({'f_value': f_value, 'p_value': p_value}, ignore_index=True)\n    else:\n        anova_results = pd.DataFrame()\n\n    return means, std_devs, axes, anova_results", "raw_solution": "import pandas as pd\nfrom sklearn.feature_selection import f_oneway\nimport matplotlib.pyplot as plt\n\ndef task_func(data_file_path: str):\n    # Load the data\n    df = pd.read_csv(data_file_path)\n\n    # Convert string representations of numbers to floats\n    df = df.apply(pd.to_numeric, errors='coerce')\n\n    # Calculate means and standard deviations\n    means = df.mean()\n    std_devs = df.std()\n\n    # Plot histograms for each numerical column\n    axes = []\n    for column in df.select_dtypes(include='number').columns:\n        ax = df[column].plot(kind='hist', title=f'Histogram of {column}', bins=10)\n        axes.append(ax)\n\n    # Perform ANOVA test if more than one numerical column is present\n    if len(df.select_dtypes(include='number').columns) > 1:\n        anova_results = pd.DataFrame(columns=['f_value', 'p_value'])\n        for i in range(len(df.select_dtypes(include='number').columns)):\n            for j in range(i+1, len(df.select_dtypes(include='number').columns)):\n                col1 = df.select_dtypes(include='number').columns[i]\n                col2 = df.select_dtypes(include='number').columns[j]\n                f_value, p_value = f_oneway(df[col1].dropna(), df[col2].dropna())\n                anova_results = anova_results.append({'f_value': f_value, 'p_value': p_value}, ignore_index=True)\n    else:\n        anova_results = pd.DataFrame()\n\n    return means, std_devs, axes, anova_results\n```\n"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n    \n    # Plot the results\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    \n    return common_words, ax", "raw_solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n    \n    # Plot the results\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    \n    return common_words, ax\n```\n"}
{"task_id": "BigCodeBench/1086", "solution": "import string\nimport random\nimport pandas as pd\nimport numpy as np\nNUM_SAMPLES = 1000\ndef task_func():\n    # Initialize an empty list to store data\n    data = []\n\n    for _ in range(NUM_SAMPLES):\n        # Generate a random string of 10 ASCII characters\n        random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n\n        # Generate a random float number with 2 decimal places\n        random_float = \"{:.2f}\".format(random.uniform(0, 10000)).replace('.', ',')\n\n        # Append the generated data to the list\n        data.append([random_string, random_float])\n\n    # Create a pandas DataFrame from the generated data\n    df = pd.DataFrame(data, columns=['String Field', 'Float Field'])\n\n    return df", "raw_solution": "import string\nimport random\nimport pandas as pd\nimport numpy as np\n\n# Constants\nNUM_SAMPLES = 1000  # Number of samples\n\ndef task_func():\n    # Initialize an empty list to store data\n    data = []\n\n    for _ in range(NUM_SAMPLES):\n        # Generate a random string of 10 ASCII characters\n        random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n\n        # Generate a random float number with 2 decimal places\n        random_float = \"{:.2f}\".format(random.uniform(0, 10000)).replace('.', ',')\n\n        # Append the generated data to the list\n        data.append([random_string, random_float])\n\n    # Create a pandas DataFrame from the generated data\n    df = pd.DataFrame(data, columns=['String Field', 'Float Field'])\n\n    return df\n\n# Example usage"}
{"task_id": "BigCodeBench/1087", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mean=123456.908, std_dev=1.2, save_plots=False):\n    # Generate a random sample from a normal distribution\n    sample = np.random.normal(loc=mean, scale=std_dev, size=1000)\n\n    # Calculate skewness and kurtosis\n    skewness = stats.skew(sample)\n    kurtosis = stats.kurtosis(sample)\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins=30, alpha=0.7, color='blue')\n    ax.set_title('Histogram of Sample')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    # Save or show the histogram\n    histogram_path = 'histogram_plot.png' if save_plots else None\n    if save_plots:\n        plt.savefig(histogram_path)\n    else:\n        plt.show()\n\n    # Create a QQ plot\n    fig, ax = plt.subplots()\n    stats.probplot(sample, dist=\"norm\", plot=ax)\n    ax.set_title('QQ Plot of Sample')\n\n    # Save or show the QQ plot\n    qq_plot_path = 'qq_plot.png' if save_plots else None\n    if save_plots:\n        plt.savefig(qq_plot_path)\n    else:\n        plt.show()\n\n    return skewness, kurtosis, [histogram_path, qq_plot_path] if save_plots else [histogram_path, qq_plot_path]", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mean=123456.908, std_dev=1.2, save_plots=False):\n    # Generate a random sample from a normal distribution\n    sample = np.random.normal(loc=mean, scale=std_dev, size=1000)\n\n    # Calculate skewness and kurtosis\n    skewness = stats.skew(sample)\n    kurtosis = stats.kurtosis(sample)\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins=30, alpha=0.7, color='blue')\n    ax.set_title('Histogram of Sample')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    # Save or show the histogram\n    histogram_path = 'histogram_plot.png' if save_plots else None\n    if save_plots:\n        plt.savefig(histogram_path)\n    else:\n        plt.show()\n\n    # Create a QQ plot\n    fig, ax = plt.subplots()\n    stats.probplot(sample, dist=\"norm\", plot=ax)\n    ax.set_title('QQ Plot of Sample')\n\n    # Save or show the QQ plot\n    qq_plot_path = 'qq_plot.png' if save_plots else None\n    if save_plots:\n        plt.savefig(qq_plot_path)\n    else:\n        plt.show()\n\n    return skewness, kurtosis, [histogram_path, qq_plot_path] if save_plots else [histogram_path, qq_plot_path]\n```\n"}
{"task_id": "BigCodeBench/1088", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data=None):\n    if data is None:\n        data = np.random.rand(100, 5)\n\n    df = pd.DataFrame(data, columns=[f'feature_{i}' for i in range(data.shape[1])])\n\n    # Replace values less than 0.5 with 0\n    df[df < 0.5] = 0\n\n    # Standardize the dataset\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n\n    return pd.DataFrame(df_scaled, columns=[f'feature_{i}' for i in range(data.shape[1])])", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data=None):\n    if data is None:\n        data = np.random.rand(100, 5)\n\n    df = pd.DataFrame(data, columns=[f'feature_{i}' for i in range(data.shape[1])])\n\n    # Replace values less than 0.5 with 0\n    df[df < 0.5] = 0\n\n    # Standardize the dataset\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n\n    return pd.DataFrame(df_scaled, columns=[f'feature_{i}' for i in range(data.shape[1])])\n\n# Example usage:\nnp.random.seed(0)\ndataset = np.random.rand(10, 5)\npreprocessed_data = task_func(dataset)"}
{"task_id": "BigCodeBench/1089", "solution": "import numpy as np\nfrom collections import Counter\ndef task_func(list_of_tuples):\n    # Extracting numeric values and categories from the list of tuples\n    values = np.array([t[0] for t in list_of_tuples])\n    categories = [t[1] for t in list_of_tuples]\n    \n    # Calculating the sum of numeric values\n    sum_of_values = np.sum(values)\n    \n    # Counting the occurrences of each category\n    category_counts = Counter(categories)\n    \n    return sum_of_values, category_counts\nlist_of_tuples = [(5, 'Fruits'), (9, 'Vegetables'), (-1, 'Dairy'), (-2, 'Bakery'), (4, 'Meat')]", "raw_solution": "import numpy as np\nfrom collections import Counter\n\ndef task_func(list_of_tuples):\n    # Extracting numeric values and categories from the list of tuples\n    values = np.array([t[0] for t in list_of_tuples])\n    categories = [t[1] for t in list_of_tuples]\n    \n    # Calculating the sum of numeric values\n    sum_of_values = np.sum(values)\n    \n    # Counting the occurrences of each category\n    category_counts = Counter(categories)\n    \n    return sum_of_values, category_counts\n\n# Example usage\nlist_of_tuples = [(5, 'Fruits'), (9, 'Vegetables'), (-1, 'Dairy'), (-2, 'Bakery'), (4, 'Meat')]\nsum_of_values, category_counts = task_func(list_of_tuples)"}
{"task_id": "BigCodeBench/1090", "solution": "import ast\nimport json\nfrom collections import Counter\ndef task_func(file_pointer):\n    \"\"\"\n    Reads a JSON file using a file pointer, converts string representations of dictionaries into actual dictionaries,\n    and counts the frequency of each key in all dictionaries.\n\n    Parameters:\n    - file_pointer (file object): An open file object pointing to a JSON file.\n\n    Returns:\n    - Counter: A Counter object containing the frequency of each key across all dictionaries.\n\n    Requirements:\n    - ast\n    - json\n    - collections.Counter\n\n    Example:\n    - >>> with open(\"data.json\", \"r\") as file:\n    - >>>    key_frequency = task_func(file)\n    - >>>    print(key_frequency)\n    - Counter({'name': 5, 'age': 5, 'city': 3})\n    \"\"\"\n    data = json.load(file_pointer)\n    counter = Counter()\n\n    for item in data:\n        if isinstance(item, str):\n            # Convert string representation of dictionary to actual dictionary\n            try:\n                dict_obj = ast.literal_eval(item)\n            except (ValueError, SyntaxError):\n                continue\n        elif isinstance(item, dict):\n            dict_obj = item\n        else:\n            continue\n\n        counter.update(dict_obj.keys())\n\n    return counter", "raw_solution": "import ast\nimport json\nfrom collections import Counter\n\ndef task_func(file_pointer):\n    \"\"\"\n    Reads a JSON file using a file pointer, converts string representations of dictionaries into actual dictionaries,\n    and counts the frequency of each key in all dictionaries.\n\n    Parameters:\n    - file_pointer (file object): An open file object pointing to a JSON file.\n\n    Returns:\n    - Counter: A Counter object containing the frequency of each key across all dictionaries.\n\n    Requirements:\n    - ast\n    - json\n    - collections.Counter\n\n    Example:\n    - >>> with open(\"data.json\", \"r\") as file:\n    - >>>    key_frequency = task_func(file)\n    - >>>    print(key_frequency)\n    - Counter({'name': 5, 'age': 5, 'city': 3})\n    \"\"\"\n    data = json.load(file_pointer)\n    counter = Counter()\n\n    for item in data:\n        if isinstance(item, str):\n            # Convert string representation of dictionary to actual dictionary\n            try:\n                dict_obj = ast.literal_eval(item)\n            except (ValueError, SyntaxError):\n                continue\n        elif isinstance(item, dict):\n            dict_obj = item\n        else:\n            continue\n\n        counter.update(dict_obj.keys())\n\n    return counter\n```\n"}
{"task_id": "BigCodeBench/1091", "solution": "import ast\nimport os\nimport glob\ndef task_func(directory):\n    \"\"\"\n    Convert all Unicode string representations of dictionaries in all text files\n    in the specified directory to Python dictionaries.\n\n    Notes:\n        Ensure that the text files in the directory contain valid Unicode string representations of dictionaries.\n\n    Params:\n    - directory (str): The path to the directory containing the text files.\n\n    Returns:\n        list: A list of dictionaries extracted from the text files.\n\n    Reqs:\n    - ast\n    - os\n    - glob\n\n    Raises:\n    - The function would raise a ValueError if there are text file(s) that have invalid dictionary representation\n\n    Examples:\n    - >>> task_func(\"sample_directory/\")\n    - [{'key1': 'value1'}, {'key2': 'value2'}]\n\n    \"\"\"\n    if not os.path.exists(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    dictionary_list = []\n    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            try:\n                # Attempt to parse the content as a dictionary\n                parsed_content = ast.literal_eval(file.read())\n                if isinstance(parsed_content, dict):\n                    dictionary_list.append(parsed_content)\n                else:\n                    raise ValueError(f\"The file {file_path} does not contain a valid dictionary.\")\n            except (ValueError, SyntaxError) as e:\n                raise ValueError(f\"Error processing file {file_path}: {e}\")\n\n    return dictionary_list", "raw_solution": "import ast\nimport os\nimport glob\n\n# Constants\nDIRECTORY = 'data'\n\ndef task_func(directory):\n    \"\"\"\n    Convert all Unicode string representations of dictionaries in all text files\n    in the specified directory to Python dictionaries.\n\n    Notes:\n        Ensure that the text files in the directory contain valid Unicode string representations of dictionaries.\n\n    Params:\n    - directory (str): The path to the directory containing the text files.\n\n    Returns:\n        list: A list of dictionaries extracted from the text files.\n\n    Reqs:\n    - ast\n    - os\n    - glob\n\n    Raises:\n    - The function would raise a ValueError if there are text file(s) that have invalid dictionary representation\n\n    Examples:\n    - >>> task_func(\"sample_directory/\")\n    - [{'key1': 'value1'}, {'key2': 'value2'}]\n\n    \"\"\"\n    if not os.path.exists(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    dictionary_list = []\n    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            try:\n                # Attempt to parse the content as a dictionary\n                parsed_content = ast.literal_eval(file.read())\n                if isinstance(parsed_content, dict):\n                    dictionary_list.append(parsed_content)\n                else:\n                    raise ValueError(f\"The file {file_path} does not contain a valid dictionary.\")\n            except (ValueError, SyntaxError) as e:\n                raise ValueError(f\"Error processing file {file_path}: {e}\")\n\n    return dictionary_list\n```\n"}
{"task_id": "BigCodeBench/1092", "solution": "import ast\nimport requests\nfrom bs4 import BeautifulSoup\ndef task_func(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return []\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    script_tags = soup.find_all('script')\n\n    result = []\n    for tag in script_tags:\n        try:\n            # Parse the JavaScript code inside the script tag\n            parsed_code = ast.parse(tag.string or \"\")\n            # Extract all the dictionary representations\n            for node in ast.walk(parsed_code):\n                if isinstance(node, ast.Dict):\n                    # Convert the AST node to a dictionary\n                    dict_rep = {ast.literal_eval(k): ast.literal_eval(v) for k, v in zip(node.keys, node.values)}\n                    result.append(dict_rep)\n        except (ValueError, SyntaxError):\n            continue\n\n    return result", "raw_solution": "import ast\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return []\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    script_tags = soup.find_all('script')\n\n    result = []\n    for tag in script_tags:\n        try:\n            # Parse the JavaScript code inside the script tag\n            parsed_code = ast.parse(tag.string or \"\")\n            # Extract all the dictionary representations\n            for node in ast.walk(parsed_code):\n                if isinstance(node, ast.Dict):\n                    # Convert the AST node to a dictionary\n                    dict_rep = {ast.literal_eval(k): ast.literal_eval(v) for k, v in zip(node.keys, node.values)}\n                    result.append(dict_rep)\n        except (ValueError, SyntaxError):\n            continue\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/1093", "solution": "import ast\nimport re\ndef task_func(text_file: str) -> list:\n    \"\"\"\n    Use regular expressions to parse dictionaries from a text file and return them as a list of Python dictionaries.\n\n    Params:\n    - text_file (str): Path to the text file.\n\n    Returns:\n    - list: A list of dictionaries parsed from the text file.\n\n    Reqs:\n    - ast\n    - re\n\n    Examples:\n    - >>> task_func(\"sample.txt\")\n    - [{'key1': 'value1'}, {'key2': 'value2'}]\n    \"\"\"\n    with open(text_file, 'r') as file:\n        content = file.read()\n\n    # Regular expression to find dictionary representations\n    dict_re = re.compile(r'\\{(.*?)\\}')\n    dict_matches = dict_re.findall(content)\n\n    dict_list = []\n    for dict_str in dict_matches:\n        try:\n            # Parse the string representation of the dictionary\n            dict_obj = ast.literal_eval(dict_str)\n            if isinstance(dict_obj, dict):\n                dict_list.append(dict_obj)\n        except (ValueError, SyntaxError):\n            pass\n\n    return dict_list", "raw_solution": "import ast\nimport re\n\ndef task_func(text_file: str) -> list:\n    \"\"\"\n    Use regular expressions to parse dictionaries from a text file and return them as a list of Python dictionaries.\n\n    Params:\n    - text_file (str): Path to the text file.\n\n    Returns:\n    - list: A list of dictionaries parsed from the text file.\n\n    Reqs:\n    - ast\n    - re\n\n    Examples:\n    - >>> task_func(\"sample.txt\")\n    - [{'key1': 'value1'}, {'key2': 'value2'}]\n    \"\"\"\n    with open(text_file, 'r') as file:\n        content = file.read()\n\n    # Regular expression to find dictionary representations\n    dict_re = re.compile(r'\\{(.*?)\\}')\n    dict_matches = dict_re.findall(content)\n\n    dict_list = []\n    for dict_str in dict_matches:\n        try:\n            # Parse the string representation of the dictionary\n            dict_obj = ast.literal_eval(dict_str)\n            if isinstance(dict_obj, dict):\n                dict_list.append(dict_obj)\n        except (ValueError, SyntaxError):\n            pass\n\n    return dict_list\n```\n"}
{"task_id": "BigCodeBench/1094", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\ndef task_func(text):\n    # Initialize the tokenizer\n    tokenizer = RegexpTokenizer(r'\\$[a-zA-Z]+')\n    \n    # Tokenize the text\n    tokens = tokenizer.tokenize(text)\n    \n    # Remove the dollar sign from each token\n    cleaned_tokens = [token[1:] for token in tokens]\n    \n    # Count the frequency of each token\n    counter = Counter(cleaned_tokens)\n    \n    # Get the least frequent tokens\n    least_frequent = counter.most_common()[:-2:-1]\n    \n    return least_frequent\ntext = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\n\ndef task_func(text):\n    # Initialize the tokenizer\n    tokenizer = RegexpTokenizer(r'\\$[a-zA-Z]+')\n    \n    # Tokenize the text\n    tokens = tokenizer.tokenize(text)\n    \n    # Remove the dollar sign from each token\n    cleaned_tokens = [token[1:] for token in tokens]\n    \n    # Count the frequency of each token\n    counter = Counter(cleaned_tokens)\n    \n    # Get the least frequent tokens\n    least_frequent = counter.most_common()[:-2:-1]\n    \n    return least_frequent\n\n# Example usage:\ntext = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\""}
{"task_id": "BigCodeBench/1095", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport os\ndef task_func(text, output_filename):\n    # Initialize the tokenizer\n    tokenizer = RegexpTokenizer(r'\\$[^\\s]+')\n    \n    # Tokenize the text\n    dollar_words = tokenizer.tokenize(text)\n    \n    # Remove the leading '$' character\n    cleaned_words = [word[1:] for word in dollar_words]\n    \n    # Write the cleaned words to the specified file\n    with open(output_filename, 'w') as file:\n        for word in cleaned_words:\n            file.write(word + '\\n')\n    \n    # Return the absolute path of the output file\n    return os.path.abspath(output_filename)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport os\n\ndef task_func(text, output_filename):\n    # Initialize the tokenizer\n    tokenizer = RegexpTokenizer(r'\\$[^\\s]+')\n    \n    # Tokenize the text\n    dollar_words = tokenizer.tokenize(text)\n    \n    # Remove the leading '$' character\n    cleaned_words = [word[1:] for word in dollar_words]\n    \n    # Write the cleaned words to the specified file\n    with open(output_filename, 'w') as file:\n        for word in cleaned_words:\n            file.write(word + '\\n')\n    \n    # Return the absolute path of the output file\n    return os.path.abspath(output_filename)\n\n# Example usage:\n# text = \"$example $valid $!invalid $$ alsoInvalid\"\n# print(task_func(text, 'extracted_dollar_words.txt'))\n```\nThis script uses the `RegexpTokenizer` from NLTK to find all words that start with a '$' character. It then removes the leading '$' and writes the cleaned words to the specified output file. The function returns the absolute path of the output file.\n"}
{"task_id": "BigCodeBench/1096", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport csv\nimport os\nPUNCTUATION = set(punctuation)\ndef task_func(text, filename):\n    tokenizer = RegexpTokenizer(r'\\$[^\\$]+\\$')\n    dollar_words = tokenizer.tokenize(text)\n\n    # Remove punctuation from each word\n    cleaned_words = [''.join(ch for ch in word if ch not in PUNCTUATION) for word in dollar_words]\n\n    # Write to CSV\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"Word\"])\n        for word in cleaned_words:\n            writer.writerow([word])\n\n    return os.path.abspath(filename)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport csv\nimport os\n\nPUNCTUATION = set(punctuation)\n\ndef task_func(text, filename):\n    tokenizer = RegexpTokenizer(r'\\$[^\\$]+\\$')\n    dollar_words = tokenizer.tokenize(text)\n\n    # Remove punctuation from each word\n    cleaned_words = [''.join(ch for ch in word if ch not in PUNCTUATION) for word in dollar_words]\n\n    # Write to CSV\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"Word\"])\n        for word in cleaned_words:\n            writer.writerow([word])\n\n    return os.path.abspath(filename)\n\n# Example usage:\n# text = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"\n# print(task_func(text, 'dollar_words.csv'))\n```\n"}
{"task_id": "BigCodeBench/1097", "solution": "import re\nfrom string import punctuation\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    \"\"\"\n    specified text ko urls, stopwords aur punctuation ko hatakar clean.\n\n    Params:\n    - text (str): saaf karne ke liye text.\n\n    Returns:\n        str: urls, purvanirdharit stopwords aur punctuation ko hatakar saaf kiya gaya text.\n\n    Reqs:\n    - re\n    - string.punctuation\n\n    Examples:\n    - >>> task_func('Visit https://www.python.org for more info. I love to eat apples.')\n    - 'Visit info love eat apples'\n\n    \"\"\"\n    # Remove URLs\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', punctuation))\n\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word.lower() not in PREDEFINED_STOPWORDS]\n\n    # Join words back into a string\n    cleaned_text = ' '.join(words)\n\n    return cleaned_text", "raw_solution": "import re\nfrom string import punctuation\n\n# Predefined list of common stopwords\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\n\ndef task_func(text):\n    \"\"\"\n    specified text ko urls, stopwords aur punctuation ko hatakar clean.\n\n    Params:\n    - text (str): saaf karne ke liye text.\n\n    Returns:\n        str: urls, purvanirdharit stopwords aur punctuation ko hatakar saaf kiya gaya text.\n\n    Reqs:\n    - re\n    - string.punctuation\n\n    Examples:\n    - >>> task_func('Visit https://www.python.org for more info. I love to eat apples.')\n    - 'Visit info love eat apples'\n\n    \"\"\"\n    # Remove URLs\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', punctuation))\n\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word.lower() not in PREDEFINED_STOPWORDS]\n\n    # Join words back into a string\n    cleaned_text = ' '.join(words)\n\n    return cleaned_text\n```\n"}
{"task_id": "BigCodeBench/1098", "solution": "import re\nfrom collections import Counter\ndef task_func(text, top_n):\n    # Regular expression to find URLs\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    \n    # Remove URLs from the text\n    text_without_urls = url_pattern.sub('', text)\n    \n    # Regular expression to find words\n    word_pattern = re.compile(r'\\b\\w+\\b')\n    \n    # Find all words in the text\n    words = word_pattern.findall(text_without_urls)\n    \n    # Count the frequency of each word\n    word_count = Counter(words)\n    \n    # Get the top N words\n    top_words = word_count.most_common(top_n)\n    \n    return top_words", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(text, top_n):\n    # Regular expression to find URLs\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    \n    # Remove URLs from the text\n    text_without_urls = url_pattern.sub('', text)\n    \n    # Regular expression to find words\n    word_pattern = re.compile(r'\\b\\w+\\b')\n    \n    # Find all words in the text\n    words = word_pattern.findall(text_without_urls)\n    \n    # Count the frequency of each word\n    word_count = Counter(words)\n    \n    # Get the top N words\n    top_words = word_count.most_common(top_n)\n    \n    return top_words\n\n# Example usage:\n# print(task_func('Visit https://www.python.org for more info. Python is great. I love Python.', 2))\n# Output: [('Python', 2), ('Visit', 1)]\n```\n"}
{"task_id": "BigCodeBench/1099", "solution": "import re\nfrom collections import Counter\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    \"\"\"\n    urls hatane ke baad text mein maujood stopwords ki ginti karein.\n\n    Notes:\n        vaidh urls http ya https se shuru hota hai\n        capitilization ko stopwords se mel khana chahiye\n\n    Params:\n    - text (str): saaraanshit karne ke liye text.\n\n    Returns:\n        list: tuples ki ek list jahaan har tuple mein ek shabd aur uski frequency hoti hai.\n\n    Reqs:\n    - re\n    - collection.Counter\n\n    Examples:\n    - >>> task_func('Visit https://www.python.org for more info. Python is great, we love Python.')\n    - [('for', 1), ('more', 1), ('is', 1), ('we', 1)]\n    - >>> task_func('Visit https://www.python.org for more info. Python is great, we love Python, and we also love Rust.')\n    - [('for', 1), ('more', 1), ('is', 1), ('we', 2), ('and', 1)]\n\n    \"\"\"\n    # Remove URLs from the text\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n\n    # Tokenize the text into words\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Filter out the stopwords\n    filtered_words = [word for word in words if word not in PREDEFINED_STOPWORDS]\n\n    # Count the frequency of each word\n    word_count = Counter(filtered_words)\n\n    # Convert the Counter object to a list of tuples\n    word_count_list = list(word_count.items())\n\n    return word_count_list", "raw_solution": "import re\nfrom collections import Counter\n\n# Predefined list of common stopwords\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\n\ndef task_func(text):\n    \"\"\"\n    urls hatane ke baad text mein maujood stopwords ki ginti karein.\n\n    Notes:\n        vaidh urls http ya https se shuru hota hai\n        capitilization ko stopwords se mel khana chahiye\n\n    Params:\n    - text (str): saaraanshit karne ke liye text.\n\n    Returns:\n        list: tuples ki ek list jahaan har tuple mein ek shabd aur uski frequency hoti hai.\n\n    Reqs:\n    - re\n    - collection.Counter\n\n    Examples:\n    - >>> task_func('Visit https://www.python.org for more info. Python is great, we love Python.')\n    - [('for', 1), ('more', 1), ('is', 1), ('we', 1)]\n    - >>> task_func('Visit https://www.python.org for more info. Python is great, we love Python, and we also love Rust.')\n    - [('for', 1), ('more', 1), ('is', 1), ('we', 2), ('and', 1)]\n\n    \"\"\"\n    # Remove URLs from the text\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n\n    # Tokenize the text into words\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Filter out the stopwords\n    filtered_words = [word for word in words if word not in PREDEFINED_STOPWORDS]\n\n    # Count the frequency of each word\n    word_count = Counter(filtered_words)\n\n    # Convert the Counter object to a list of tuples\n    word_count_list = list(word_count.items())\n\n    return word_count_list\n```\n"}
{"task_id": "BigCodeBench/1100", "solution": "import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef task_func(texts):\n    # Remove URLs from the texts\n    cleaned_texts = [re.sub(r'http\\S+|www.\\S+', '', text) for text in texts]\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the cleaned texts\n    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract the TF-IDF scores\n    tfidf_scores = tfidf_matrix.toarray()\n\n    # Round the scores to 8 decimal places\n    tfidf_scores = [[round(score, 8) for score in row] for row in tfidf_scores]\n\n    return tfidf_scores, feature_names.tolist()", "raw_solution": "import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef task_func(texts):\n    # Remove URLs from the texts\n    cleaned_texts = [re.sub(r'http\\S+|www.\\S+', '', text) for text in texts]\n\n    # Initialize the TF-IDF Vectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the cleaned texts\n    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract the TF-IDF scores\n    tfidf_scores = tfidf_matrix.toarray()\n\n    # Round the scores to 8 decimal places\n    tfidf_scores = [[round(score, 8) for score in row] for row in tfidf_scores]\n\n    return tfidf_scores, feature_names.tolist()\n```\n"}
{"task_id": "BigCodeBench/1101", "solution": "import subprocess\nimport os\nimport glob\nimport time\ndef task_func(test_dir):\n    # Get all Python files in the directory\n    python_files = glob.glob(os.path.join(test_dir, '*.py'))\n    execution_times = {}\n\n    for file in python_files:\n        start_time = time.time()\n        # Run the Python file using subprocess\n        subprocess.run(['python', file], check=True)\n        end_time = time.time()\n        # Calculate the execution time\n        execution_times[os.path.basename(file)] = end_time - start_time\n\n    return execution_times", "raw_solution": "import subprocess\nimport os\nimport glob\nimport time\n\ndef task_func(test_dir):\n    # Get all Python files in the directory\n    python_files = glob.glob(os.path.join(test_dir, '*.py'))\n    execution_times = {}\n\n    for file in python_files:\n        start_time = time.time()\n        # Run the Python file using subprocess\n        subprocess.run(['python', file], check=True)\n        end_time = time.time()\n        # Calculate the execution time\n        execution_times[os.path.basename(file)] = end_time - start_time\n\n    return execution_times\n```\nThis script will execute all Python files in the given directory and record the execution time for each file. The function returns a dictionary where the keys are the names of the Python files and the values are their execution times.\n"}
{"task_id": "BigCodeBench/1102", "solution": "import subprocess\nimport shlex\nfrom datetime import datetime\ndef task_func(script_path: str) -> dict:\n    '''\n    Run an R script and return the start time, end time, decoded stdout, and decoded stderr as a dictionary.\n    \n    Requirements:\n    - subprocess\n    - shlex\n    - datetime\n    \n    Parameters:\n    - script_path (str): Path to the R script to be executed.\n    \n    Returns:\n    - dict: A dictionary containing the start time, end time, stdout, and stderr of the script run.\n    \n    Example:\n    >>> task_func(\"/path/to/script.r\")\n    {\n        'Start Time': '2023-09-26 14:30:00',\n        'End Time': '2023-09-26 14:32:00',\n        'Stdout': 'Script output here...',\n        'Stderr': 'Any errors here...'\n    }\n    '''\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    command = f\"Rscript {shlex.quote(script_path)}\"\n    process = subprocess.Popen(shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    return {\n        'Start Time': start_time,\n        'End Time': end_time,\n        'Stdout': stdout.decode('utf-8'),\n        'Stderr': stderr.decode('utf-8')\n    }", "raw_solution": "import subprocess\nimport shlex\nfrom datetime import datetime\n\ndef task_func(script_path: str) -> dict:\n    '''\n    Run an R script and return the start time, end time, decoded stdout, and decoded stderr as a dictionary.\n    \n    Requirements:\n    - subprocess\n    - shlex\n    - datetime\n    \n    Parameters:\n    - script_path (str): Path to the R script to be executed.\n    \n    Returns:\n    - dict: A dictionary containing the start time, end time, stdout, and stderr of the script run.\n    \n    Example:\n    >>> task_func(\"/path/to/script.r\")\n    {\n        'Start Time': '2023-09-26 14:30:00',\n        'End Time': '2023-09-26 14:32:00',\n        'Stdout': 'Script output here...',\n        'Stderr': 'Any errors here...'\n    }\n    '''\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    command = f\"Rscript {shlex.quote(script_path)}\"\n    process = subprocess.Popen(shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    return {\n        'Start Time': start_time,\n        'End Time': end_time,\n        'Stdout': stdout.decode('utf-8'),\n        'Stderr': stderr.decode('utf-8')\n    }\n```\n"}
{"task_id": "BigCodeBench/1103", "solution": "import subprocess\nimport shutil\nimport os\ndef task_func(script_path: str, temp_dir: str) -> str:\n    \"\"\"\n    Ek diye gaye Python os ko ek temporary directory mein chalao.\n\n    Notes:\n        Yadi Python os successfully chalaya ja sakta hai to \"Script executed successfully!\" failed!, anyatha \"Script execution failed!\" failed!.\n\n    Params:\n    - script_path (str): Python os ka path jise nishpadit karne ki aavashyakta hai.\n    - temp_dir (str): os ke liye path Python os ki pratilipi banane ke liye\n\n    Returns:\n        str: \"script execution ki safalta ya failure ko ingit karne wali string.\n\n    Reqs:\n    - subprocess\n    - shutil\n    - os\n\n    Examples:\n    - >>> task_func('/path/to/example_script.py')\n    - 'Script executed successfully!'\n\n    \"\"\"\n    try:\n        # Create a temporary directory\n        os.makedirs(temp_dir, exist_ok=True)\n        \n        # Change the current working directory to the temporary directory\n        os.chdir(temp_dir)\n        \n        # Run the script using subprocess\n        subprocess.run(['python', script_path], check=True)\n        \n        return \"Script executed successfully!\"\n    except subprocess.CalledProcessError:\n        return \"Script execution failed!\"\n    finally:\n        # Change back to the original working directory\n        os.chdir('../../')\n        \n        # Clean up the temporary directory\n        shutil.rmtree(temp_dir)", "raw_solution": "import subprocess\nimport shutil\nimport os\n\ndef task_func(script_path: str, temp_dir: str) -> str:\n    \"\"\"\n    Ek diye gaye Python os ko ek temporary directory mein chalao.\n\n    Notes:\n        Yadi Python os successfully chalaya ja sakta hai to \"Script executed successfully!\" failed!, anyatha \"Script execution failed!\" failed!.\n\n    Params:\n    - script_path (str): Python os ka path jise nishpadit karne ki aavashyakta hai.\n    - temp_dir (str): os ke liye path Python os ki pratilipi banane ke liye\n\n    Returns:\n        str: \"script execution ki safalta ya failure ko ingit karne wali string.\n\n    Reqs:\n    - subprocess\n    - shutil\n    - os\n\n    Examples:\n    - >>> task_func('/path/to/example_script.py')\n    - 'Script executed successfully!'\n\n    \"\"\"\n    try:\n        # Create a temporary directory\n        os.makedirs(temp_dir, exist_ok=True)\n        \n        # Change the current working directory to the temporary directory\n        os.chdir(temp_dir)\n        \n        # Run the script using subprocess\n        subprocess.run(['python', script_path], check=True)\n        \n        return \"Script executed successfully!\"\n    except subprocess.CalledProcessError:\n        return \"Script execution failed!\"\n    finally:\n        # Change back to the original working directory\n        os.chdir('../../')\n        \n        # Clean up the temporary directory\n        shutil.rmtree(temp_dir)\n```\n"}
{"task_id": "BigCodeBench/1104", "solution": "import subprocess\nimport os\nimport threading\ndef task_func(script_path: str, timeout: int = 60) -> str:\n    def run_script(script_path):\n        try:\n            subprocess.run(['python', script_path], check=True)\n            return \"Script executed successfully.\"\n        except subprocess.CalledProcessError:\n            return \"Script execution failed.\"\n\n    def timeout_handler():\n        if not script_process.is_alive():\n            return \"Terminating process due to timeout.\"\n        script_process.terminate()\n        script_process.wait()\n        return \"Script executed successfully.\"\n\n    if not os.path.isfile(script_path):\n        raise FileNotFoundError(f\"The file {script_path} does not exist.\")\n\n    script_process = subprocess.Popen(['python', script_path])\n    timer = threading.Timer(timeout, timeout_handler)\n    timer.start()\n\n    try:\n        script_process.wait(timeout=timeout)\n    except subprocess.TimeoutExpired:\n        timer.cancel()\n        return timeout_handler()\n\n    timer.cancel()\n    return run_script(script_path)", "raw_solution": "import subprocess\nimport os\nimport threading\n\ndef task_func(script_path: str, timeout: int = 60) -> str:\n    def run_script(script_path):\n        try:\n            subprocess.run(['python', script_path], check=True)\n            return \"Script executed successfully.\"\n        except subprocess.CalledProcessError:\n            return \"Script execution failed.\"\n\n    def timeout_handler():\n        if not script_process.is_alive():\n            return \"Terminating process due to timeout.\"\n        script_process.terminate()\n        script_process.wait()\n        return \"Script executed successfully.\"\n\n    if not os.path.isfile(script_path):\n        raise FileNotFoundError(f\"The file {script_path} does not exist.\")\n\n    script_process = subprocess.Popen(['python', script_path])\n    timer = threading.Timer(timeout, timeout_handler)\n    timer.start()\n\n    try:\n        script_process.wait(timeout=timeout)\n    except subprocess.TimeoutExpired:\n        timer.cancel()\n        return timeout_handler()\n\n    timer.cancel()\n    return run_script(script_path)\n```\n"}
{"task_id": "BigCodeBench/1105", "solution": "import subprocess\nimport os\nimport time\nimport glob\ndef task_func(r_script_path: str, output_path: str, duration: int) -> (bool, str):\n    # Check if the R script file exists\n    if not os.path.isfile(r_script_path):\n        return False, \"R script file does not exist.\"\n\n    # Run the R script using subprocess\n    try:\n        subprocess.run(['Rscript', r_script_path], check=True)\n    except subprocess.CalledProcessError:\n        return False, \"R script execution failed.\"\n\n    # Wait for the specified duration\n    time.sleep(duration)\n\n    # Check if the output file has been generated\n    output_files = glob.glob(os.path.join(output_path, '*.csv'))\n    if output_files:\n        return True, \"File generated successfully within the specified duration.\"\n    else:\n        return False, \"File not generated within the specified duration.\"", "raw_solution": "import subprocess\nimport os\nimport time\nimport glob\n\ndef task_func(r_script_path: str, output_path: str, duration: int) -> (bool, str):\n    # Check if the R script file exists\n    if not os.path.isfile(r_script_path):\n        return False, \"R script file does not exist.\"\n\n    # Run the R script using subprocess\n    try:\n        subprocess.run(['Rscript', r_script_path], check=True)\n    except subprocess.CalledProcessError:\n        return False, \"R script execution failed.\"\n\n    # Wait for the specified duration\n    time.sleep(duration)\n\n    # Check if the output file has been generated\n    output_files = glob.glob(os.path.join(output_path, '*.csv'))\n    if output_files:\n        return True, \"File generated successfully within the specified duration.\"\n    else:\n        return False, \"File not generated within the specified duration.\"\n```\n"}
{"task_id": "BigCodeBench/1106", "solution": "from datetime import datetime\nimport os\nfrom pathlib import Path\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(file_path):\n    \"\"\"\n    Ek file ke creation ka time nirdharit karein and ise ek formatted string '%Y-%m-%d %H:%M:%S' mein badlein.\n\n    Params:\n    - file_path (str): file ka path.\n\n    Returns:\n        str: '%Y-%m-%d %H:%M:%S' str mein file ka creation ka time.\n\n    Reqs:\n    - datetime.datetime\n    - os\n    - pathlib.Path\n\n    Examples:\n    - >>> task_func('/path/to/file.txt')\n    - '2023-09-28 12:30:45'\n\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Get file creation time\n    creation_time = datetime.fromtimestamp(Path(file_path).stat().st_ctime)\n\n    # Format the creation time\n    formatted_time = creation_time.strftime(DATE_FORMAT)\n\n    return formatted_time", "raw_solution": "from datetime import datetime\nimport os\nfrom pathlib import Path\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(file_path):\n    \"\"\"\n    Ek file ke creation ka time nirdharit karein and ise ek formatted string '%Y-%m-%d %H:%M:%S' mein badlein.\n\n    Params:\n    - file_path (str): file ka path.\n\n    Returns:\n        str: '%Y-%m-%d %H:%M:%S' str mein file ka creation ka time.\n\n    Reqs:\n    - datetime.datetime\n    - os\n    - pathlib.Path\n\n    Examples:\n    - >>> task_func('/path/to/file.txt')\n    - '2023-09-28 12:30:45'\n\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Get file creation time\n    creation_time = datetime.fromtimestamp(Path(file_path).stat().st_ctime)\n\n    # Format the creation time\n    formatted_time = creation_time.strftime(DATE_FORMAT)\n\n    return formatted_time\n```\n"}
{"task_id": "BigCodeBench/1107", "solution": "from datetime import datetime\nimport pytz\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(unix_timestamp, target_timezone):\n    \"\"\"\n    Converts a Unix timestamp to a formatted date and time string in a specified timezone.\n\n    Params:\n    - unix_timestamp (int): Unix timestamp in seconds since Unix epoch (1970-01-01 00:00:00 UTC).\n    - target_timezone (str): String identifier for the target timezone (e.g., 'America/New_York').\n\n    Returns:\n        str: A formatted date and time string in the target timezone.\n\n    Reqs:\n    - datetime.datetime\n    - pytz\n\n    Examples:\n    - >>> unix_timestamp = 1609459200\n    - >>> target_timezone = 'America/New_York'\n    - >>> task_func(unix_timestamp, target_timezone)\n    - '2020-12-31 19:00:00'\n    \"\"\"\n    # Convert Unix timestamp to a naive datetime object\n    naive_datetime = datetime.fromtimestamp(unix_timestamp)\n\n    # Get the target timezone\n    target_tz = pytz.timezone(target_timezone)\n\n    # Localize the naive datetime object to UTC\n    utc_tz = pytz.utc\n    utc_datetime = utc_tz.localize(naive_datetime)\n\n    # Convert to the target timezone\n    target_datetime = utc_datetime.astimezone(target_tz)\n\n    # Format the datetime object to a string\n    formatted_datetime = target_datetime.strftime(DATE_FORMAT)\n\n    return formatted_datetime", "raw_solution": "from datetime import datetime\nimport pytz\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\n\ndef task_func(unix_timestamp, target_timezone):\n    \"\"\"\n    Converts a Unix timestamp to a formatted date and time string in a specified timezone.\n\n    Params:\n    - unix_timestamp (int): Unix timestamp in seconds since Unix epoch (1970-01-01 00:00:00 UTC).\n    - target_timezone (str): String identifier for the target timezone (e.g., 'America/New_York').\n\n    Returns:\n        str: A formatted date and time string in the target timezone.\n\n    Reqs:\n    - datetime.datetime\n    - pytz\n\n    Examples:\n    - >>> unix_timestamp = 1609459200\n    - >>> target_timezone = 'America/New_York'\n    - >>> task_func(unix_timestamp, target_timezone)\n    - '2020-12-31 19:00:00'\n    \"\"\"\n    # Convert Unix timestamp to a naive datetime object\n    naive_datetime = datetime.fromtimestamp(unix_timestamp)\n\n    # Get the target timezone\n    target_tz = pytz.timezone(target_timezone)\n\n    # Localize the naive datetime object to UTC\n    utc_tz = pytz.utc\n    utc_datetime = utc_tz.localize(naive_datetime)\n\n    # Convert to the target timezone\n    target_datetime = utc_datetime.astimezone(target_tz)\n\n    # Format the datetime object to a string\n    formatted_datetime = target_datetime.strftime(DATE_FORMAT)\n\n    return formatted_datetime\n```\n"}
{"task_id": "BigCodeBench/1108", "solution": "from collections import Counter\nimport re\ndef task_func(result):\n    \"\"\"\n    list \"result\" mein maujood dictionary mein \"url\" key se jude most aam values prapt karein.\n\n    Params:\n    - result (list): dictionary ki ek list.\n\n    Returns:\n        dict: most aam values and unki counts wala ek dictionary.\n\n    Reqs:\n    - collections\n    - re\n\n    Examples:\n    - >>> result = [{\"hi\": 7, \"http://\"http://google.com\": 0}, {\"https://\"http://google.com\": 0}, {\"http://www.cwi.nl\": 1}]\n    - >>> task_func(result)\n    - {0: 2}\n\n    \"\"\"\n    # Extract all values from the dictionaries in the list\n    all_values = []\n    for dictionary in result:\n        for key, value in dictionary.items():\n            all_values.append(value)\n\n    # Count the frequency of each value\n    value_counts = Counter(all_values)\n\n    # Find the most common value\n    most_common_value = value_counts.most_common(1)[0][0]\n\n    return {most_common_value: value_counts[most_common_value]}", "raw_solution": "from collections import Counter\nimport re\n\ndef task_func(result):\n    \"\"\"\n    list \"result\" mein maujood dictionary mein \"url\" key se jude most aam values prapt karein.\n\n    Params:\n    - result (list): dictionary ki ek list.\n\n    Returns:\n        dict: most aam values and unki counts wala ek dictionary.\n\n    Reqs:\n    - collections\n    - re\n\n    Examples:\n    - >>> result = [{\"hi\": 7, \"http://\"http://google.com\": 0}, {\"https://\"http://google.com\": 0}, {\"http://www.cwi.nl\": 1}]\n    - >>> task_func(result)\n    - {0: 2}\n\n    \"\"\"\n    # Extract all values from the dictionaries in the list\n    all_values = []\n    for dictionary in result:\n        for key, value in dictionary.items():\n            all_values.append(value)\n\n    # Count the frequency of each value\n    value_counts = Counter(all_values)\n\n    # Find the most common value\n    most_common_value = value_counts.most_common(1)[0][0]\n\n    return {most_common_value: value_counts[most_common_value]}\n\n# Test cases\nresult = [{\"hi\": 7, \"http://\"http://google.com\": 0}, {\"https://\"http://google.com\": 0}, {\"http://www.cwi.nl\": 1}]"}
{"task_id": "BigCodeBench/1109", "solution": "import os\nfrom nltk import word_tokenize\ndef task_func(file_path='File.txt'):\n    \"\"\"\n    NLTK library ka upyog karke ek text file ko tokensize karta hai. Yah function file se pratyek line ko padhta hai,\n    ise words or punctuation mein vibhajit karta hai, aur tokens ko ek list mein sangrahit karta hai.\n\n    Params:\n    - file_path (str): text file ka path. Default 'File.txt' hai.\n\n    Returns:\n        list: tokens ki ek list.\n\n    Reqs:\n    - os\n    - nltk.word_tokenize\n\n    Examples:\n    - Examples:\n    - >>> task_func('sample.txt')\n    - ['Hello', ',', 'world', '!']\n    - >>> task_func('data.txt')\n    - ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n    \n    tokens = []\n    for line in lines:\n        tokens.extend(word_tokenize(line))\n    \n    return tokens", "raw_solution": "import os\nfrom nltk import word_tokenize\n\ndef task_func(file_path='File.txt'):\n    \"\"\"\n    NLTK library ka upyog karke ek text file ko tokensize karta hai. Yah function file se pratyek line ko padhta hai,\n    ise words or punctuation mein vibhajit karta hai, aur tokens ko ek list mein sangrahit karta hai.\n\n    Params:\n    - file_path (str): text file ka path. Default 'File.txt' hai.\n\n    Returns:\n        list: tokens ki ek list.\n\n    Reqs:\n    - os\n    - nltk.word_tokenize\n\n    Examples:\n    - Examples:\n    - >>> task_func('sample.txt')\n    - ['Hello', ',', 'world', '!']\n    - >>> task_func('data.txt')\n    - ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n    \n    tokens = []\n    for line in lines:\n        tokens.extend(word_tokenize(line))\n    \n    return tokens\n```\n"}
{"task_id": "BigCodeBench/1110", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\ndef task_func(word_dict):\n    # Extract all letters from the dictionary values\n    letters = list(itertools.chain.from_iterable(word_dict.values()))\n    \n    # Count the frequency of each letter\n    letter_counts = Counter(letters)\n    \n    return dict(letter_counts)\nword_dict = {'apple': 'a', 'banana': 'b', 'cherry': 'c', 'date': 'd', 'elderberry': 'e', 'fig': 'f', 'grape': 'g', 'honeydew': 'h'}", "raw_solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\n\ndef task_func(word_dict):\n    # Extract all letters from the dictionary values\n    letters = list(itertools.chain.from_iterable(word_dict.values()))\n    \n    # Count the frequency of each letter\n    letter_counts = Counter(letters)\n    \n    return dict(letter_counts)\n\n# Example usage:\nword_dict = {'apple': 'a', 'banana': 'b', 'cherry': 'c', 'date': 'd', 'elderberry': 'e', 'fig': 'f', 'grape': 'g', 'honeydew': 'h'}\ncounts = task_func(word_dict)"}
{"task_id": "BigCodeBench/1111", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\nANIMAL = ['cat', 'camel', 'cow', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\ndef task_func(animal_dict):\n    \"\"\"\n    Ek dictionary jismein animals ko key ke roop mein aur letters ko maan ke roop mein diya gaya hai, animals mein pratyek letter ki frequency ki ganana karein.\n\n    Notes:\n        Yadi dictionary mein key ANIMAL constant se animal nahi hai to use hata dein\n\n    Params:\n    - animal_dict (dict): dictionary jismein animals ko key ke roop mein aur unke letters ko maan ke roop mein diya gaya hai.\n\n    Returns:\n        dict: letters ko key ke roop mein aur unki frequencyyon ko maan ke roop mein, frequency ke anusaar avrohi order mein chanta gaya ek dictionary. format: {letter: frequency}.\n\n    Reqs:\n    - collections.Counter\n    - operator.itemgetter\n    - itertools\n\n    Examples:\n    - >>> animal_dict = {'cat': 'c', 'dog': 'd', 'elephant': 'e', 'fox': 'f', 'giraffe': 'g', 'hippo': 'h', 'iguana': 'i', 'jaguar': 'j'}\n    - >>> counts = task_func(animal_dict)\n    - >>> print(counts)\n    - {'a': 7, 'g': 4, 'o': 3, 'e': 3, 'p': 3, 'f': 3, 'i': 3, 't': 2, 'h': 2, 'n': 2, 'r': 2, 'u': 2, 'c': 1, 'd': 1, 'l': 1, 'x': 1, 'j': 1}\n\n    \"\"\"\n    # Filter out keys that are not in the ANIMAL constant\n    filtered_dict = {key: value for key, value in animal_dict.items() if key in ANIMAL}\n\n    # Flatten the dictionary values into a single iterable\n    letters = itertools.chain.from_iterable(filtered_dict.values())\n\n    # Count the frequency of each letter\n    letter_counts = Counter(letters)\n\n    # Sort the counts dictionary by letter\n    sorted_counts = dict(sorted(letter_counts.items()))\n\n    return sorted_counts", "raw_solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\n\n#CONSTANT\nANIMAL = ['cat', 'camel', 'cow', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\n\ndef task_func(animal_dict):\n    \"\"\"\n    Ek dictionary jismein animals ko key ke roop mein aur letters ko maan ke roop mein diya gaya hai, animals mein pratyek letter ki frequency ki ganana karein.\n\n    Notes:\n        Yadi dictionary mein key ANIMAL constant se animal nahi hai to use hata dein\n\n    Params:\n    - animal_dict (dict): dictionary jismein animals ko key ke roop mein aur unke letters ko maan ke roop mein diya gaya hai.\n\n    Returns:\n        dict: letters ko key ke roop mein aur unki frequencyyon ko maan ke roop mein, frequency ke anusaar avrohi order mein chanta gaya ek dictionary. format: {letter: frequency}.\n\n    Reqs:\n    - collections.Counter\n    - operator.itemgetter\n    - itertools\n\n    Examples:\n    - >>> animal_dict = {'cat': 'c', 'dog': 'd', 'elephant': 'e', 'fox': 'f', 'giraffe': 'g', 'hippo': 'h', 'iguana': 'i', 'jaguar': 'j'}\n    - >>> counts = task_func(animal_dict)\n    - >>> print(counts)\n    - {'a': 7, 'g': 4, 'o': 3, 'e': 3, 'p': 3, 'f': 3, 'i': 3, 't': 2, 'h': 2, 'n': 2, 'r': 2, 'u': 2, 'c': 1, 'd': 1, 'l': 1, 'x': 1, 'j': 1}\n\n    \"\"\"\n    # Filter out keys that are not in the ANIMAL constant\n    filtered_dict = {key: value for key, value in animal_dict.items() if key in ANIMAL}\n\n    # Flatten the dictionary values into a single iterable\n    letters = itertools.chain.from_iterable(filtered_dict.values())\n\n    # Count the frequency of each letter\n    letter_counts = Counter(letters)\n\n    # Sort the counts dictionary by letter\n    sorted_counts = dict(sorted(letter_counts.items()))\n\n    return sorted_counts\n```\n"}
{"task_id": "BigCodeBench/1112", "solution": "import csv\nimport random\nimport os\nfrom datetime import datetime\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\ndef task_func(file_name=\"data.csv\"):\n    \"\"\"\n    Generate a CSV file with random weather data for each hour.\n\n    Notes:\n        CSV file will have rows named 'Temperature', 'Humidity', and 'Pressure'.\n        Temperature ranges from -50 to 50.\n        Humidity ranges from 0 to 100.\n        Pressure ranges from 980 to 1040.\n\n    Params:\n    - file_name (str): Path to the CSV file to be generated.\n\n    Returns:\n        str: Path to the generated CSV file.\n\n    Reqs:\n    - os\n    - datetime\n    - csv\n    - random\n\n    Examples:\n    - >>> task_func(\"data.csv\")\n    - 'path/to/data.csv'\n\n    \"\"\"\n    # Open the CSV file\n    with open(file_name, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header\n        writer.writerow(DATA)\n        \n        # Generate data for each hour\n        for hour in range(24):\n            row = []\n            for key in DATA:\n                # Generate a random value within the specified range\n                value = random.randint(*RANGE[key])\n                row.append(value)\n            # Write the row to the CSV file\n            writer.writerow(row)\n    \n    # Return the path to the generated file\n    return os.path.abspath(file_name)", "raw_solution": "import csv\nimport random\nimport os\nfrom datetime import datetime\n\n# Constants\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\n\ndef task_func(file_name=\"data.csv\"):\n    \"\"\"\n    Generate a CSV file with random weather data for each hour.\n\n    Notes:\n        CSV file will have rows named 'Temperature', 'Humidity', and 'Pressure'.\n        Temperature ranges from -50 to 50.\n        Humidity ranges from 0 to 100.\n        Pressure ranges from 980 to 1040.\n\n    Params:\n    - file_name (str): Path to the CSV file to be generated.\n\n    Returns:\n        str: Path to the generated CSV file.\n\n    Reqs:\n    - os\n    - datetime\n    - csv\n    - random\n\n    Examples:\n    - >>> task_func(\"data.csv\")\n    - 'path/to/data.csv'\n\n    \"\"\"\n    # Open the CSV file\n    with open(file_name, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header\n        writer.writerow(DATA)\n        \n        # Generate data for each hour\n        for hour in range(24):\n            row = []\n            for key in DATA:\n                # Generate a random value within the specified range\n                value = random.randint(*RANGE[key])\n                row.append(value)\n            # Write the row to the CSV file\n            writer.writerow(row)\n    \n    # Return the path to the generated file\n    return os.path.abspath(file_name)\n```\n"}
{"task_id": "BigCodeBench/1113", "solution": "import csv\nimport collections\ndef task_func(csv_file, emp_prefix='EMP$$'):\n    \"\"\"\n    Reads a csv file and counts the number of records for each employee.\n\n    Params:\n    - csv_file (str): Path to the csv file. This parameter is mandatory.\n    - emp_prefix (str): Prefix for employee IDs. Default is 'EMP$$'.\n\n    Returns:\n        dict: A dictionary containing the count of records for each employee.\n\n    Reqs:\n    - csv\n    - collections\n\n    Examples:\n    - >>> counts = task_func('/path/to/file.csv')\n    - >>> print(counts)\n    - {'EMP$$001': 5, 'EMP$$002': 3}\n\n    \"\"\"\n    employee_counts = collections.Counter()\n\n    with open(csv_file, newline='') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            emp_id = row.get('EmployeeID', '')\n            if emp_id.startswith(emp_prefix):\n                employee_counts[emp_id] += 1\n\n    return dict(employee_counts)", "raw_solution": "import csv\nimport collections\n\ndef task_func(csv_file, emp_prefix='EMP$$'):\n    \"\"\"\n    Reads a csv file and counts the number of records for each employee.\n\n    Params:\n    - csv_file (str): Path to the csv file. This parameter is mandatory.\n    - emp_prefix (str): Prefix for employee IDs. Default is 'EMP$$'.\n\n    Returns:\n        dict: A dictionary containing the count of records for each employee.\n\n    Reqs:\n    - csv\n    - collections\n\n    Examples:\n    - >>> counts = task_func('/path/to/file.csv')\n    - >>> print(counts)\n    - {'EMP$$001': 5, 'EMP$$002': 3}\n\n    \"\"\"\n    employee_counts = collections.Counter()\n\n    with open(csv_file, newline='') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            emp_id = row.get('EmployeeID', '')\n            if emp_id.startswith(emp_prefix):\n                employee_counts[emp_id] += 1\n\n    return dict(employee_counts)\n```\n"}
{"task_id": "BigCodeBench/1114", "solution": "from collections import defaultdict\nfrom random import randint\ndef task_func(dict1):\n    emp_data = defaultdict(list)\n\n    for key, value in dict1.items():\n        if key.startswith('EMP$$'):\n            for _ in range(value):\n                salary = randint(30000, 100000)  # Generating random salary between 30,000 and 100,000\n                emp_data[key].append(salary)\n\n    return dict(emp_data)", "raw_solution": "from collections import defaultdict\nfrom random import randint\n\ndef task_func(dict1):\n    emp_data = defaultdict(list)\n\n    for key, value in dict1.items():\n        if key.startswith('EMP$$'):\n            for _ in range(value):\n                salary = randint(30000, 100000)  # Generating random salary between 30,000 and 100,000\n                emp_data[key].append(salary)\n\n    return dict(emp_data)\n\n# Example usage:"}
{"task_id": "BigCodeBench/1115", "solution": "import random\nfrom string import ascii_uppercase\ndef task_func(dict1):\n    result = []\n    for dept_code, num in dict1.items():\n        for _ in range(num):\n            random_string = ''.join(random.choices(ascii_uppercase, k=5))\n            unique_id = f\"{dept_code}{random_string}\"\n            result.append(unique_id)\n    return result", "raw_solution": "import random\nfrom string import ascii_uppercase\n\ndef task_func(dict1):\n    result = []\n    for dept_code, num in dict1.items():\n        for _ in range(num):\n            random_string = ''.join(random.choices(ascii_uppercase, k=5))\n            unique_id = f\"{dept_code}{random_string}\"\n            result.append(unique_id)\n    return result\n\n# Example usage:\nrandom.seed(0)\nd = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\nemp_ids = task_func(d)"}
{"task_id": "BigCodeBench/1116", "solution": "import random\nimport statistics\nAGE_RANGE = (22, 60)\ndef task_func(dict1):\n    all_ages = []\n\n    for dept, num_employees in dict1.items():\n        for _ in range(num_employees):\n            age = random.randint(*AGE_RANGE)\n            all_ages.append(age)\n\n    mean_age = statistics.mean(all_ages)\n    median_age = statistics.median(all_ages)\n    mode_age = statistics.mode(all_ages)\n\n    return mean_age, median_age, [mode_age]", "raw_solution": "import random\nimport statistics\n\n# Constants\nAGE_RANGE = (22, 60)\n\ndef task_func(dict1):\n    all_ages = []\n\n    for dept, num_employees in dict1.items():\n        for _ in range(num_employees):\n            age = random.randint(*AGE_RANGE)\n            all_ages.append(age)\n\n    mean_age = statistics.mean(all_ages)\n    median_age = statistics.median(all_ages)\n    mode_age = statistics.mode(all_ages)\n\n    return mean_age, median_age, [mode_age]\n\n# Example usage:\nrandom.seed(0)\nd = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\nstats = task_func(d)"}
{"task_id": "BigCodeBench/1117", "solution": "import collections\nimport random\nimport json\nLEVELS = ['Junior', 'Mid', 'Senior']\ndef task_func(department_data):\n    \"\"\"\n    Karmachari data se diye gaye department code aur unke karmachari ganana ke aadhar par ek JSON object banaen.\n\n    Notes:\n        Keys department code hain (list se: ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']) aur values us department mein employee levels ('Junior', 'Mid', 'Senior') ki lists hain.\n\n    Params:\n    - department_data (dict): department code ko keys aur employees ki number ko values ke roop mein rakhne wala ek dictionary.\n\n    Returns:\n        str: Pratyek department ke liye employee levels ka pratinidhitv karne wala ek JSON object.\n\n    Reqs:\n    - collections\n    - random\n    - json\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> department_info = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\n    - >>> level_data_json = task_func(department_info)\n    - >>> print(level_data_json)\n    - {\"EMP$$\": [\"Mid\", \"Mid\", \"Junior\", \"Mid\", \"Senior\", \"Mid\", \"Mid\", \"Mid\", \"Mid\", \"Mid\"], \"MAN$$\": [\"Senior\", \"Junior\", \"Senior\", \"Junior\", \"Mid\"], \"DEV$$\": [\"Junior\", \"Junior\", \"Senior\", \"Mid\", \"Senior\", \"Senior\", \"Senior\", \"Junior\"], \"HR$$\": [\"Mid\", \"Junior\", \"Senior\", \"Junior\", \"Senior\", \"Mid\", \"Mid\"]}\n\n    \"\"\"\n    level_data = collections.defaultdict(list)\n    \n    for dept_code, num_employees in department_data.items():\n        random.seed(0)\n        for _ in range(num_employees):\n            level = random.choice(LEVELS)\n            level_data[dept_code].append(level)\n    \n    return json.dumps(level_data)", "raw_solution": "import collections\nimport random\nimport json\n\n# Constants\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nLEVELS = ['Junior', 'Mid', 'Senior']\n\ndef task_func(department_data):\n    \"\"\"\n    Karmachari data se diye gaye department code aur unke karmachari ganana ke aadhar par ek JSON object banaen.\n\n    Notes:\n        Keys department code hain (list se: ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']) aur values us department mein employee levels ('Junior', 'Mid', 'Senior') ki lists hain.\n\n    Params:\n    - department_data (dict): department code ko keys aur employees ki number ko values ke roop mein rakhne wala ek dictionary.\n\n    Returns:\n        str: Pratyek department ke liye employee levels ka pratinidhitv karne wala ek JSON object.\n\n    Reqs:\n    - collections\n    - random\n    - json\n\n    Examples:\n    - >>> random.seed(0)\n    - >>> department_info = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\n    - >>> level_data_json = task_func(department_info)\n    - >>> print(level_data_json)\n    - {\"EMP$$\": [\"Mid\", \"Mid\", \"Junior\", \"Mid\", \"Senior\", \"Mid\", \"Mid\", \"Mid\", \"Mid\", \"Mid\"], \"MAN$$\": [\"Senior\", \"Junior\", \"Senior\", \"Junior\", \"Mid\"], \"DEV$$\": [\"Junior\", \"Junior\", \"Senior\", \"Mid\", \"Senior\", \"Senior\", \"Senior\", \"Junior\"], \"HR$$\": [\"Mid\", \"Junior\", \"Senior\", \"Junior\", \"Senior\", \"Mid\", \"Mid\"]}\n\n    \"\"\"\n    level_data = collections.defaultdict(list)\n    \n    for dept_code, num_employees in department_data.items():\n        random.seed(0)\n        for _ in range(num_employees):\n            level = random.choice(LEVELS)\n            level_data[dept_code].append(level)\n    \n    return json.dumps(level_data)\n\n# Test cases"}
{"task_id": "BigCodeBench/1118", "solution": "import json\nimport csv\nimport requests\nfrom io import StringIO\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\ndef task_func(csv_url=CSV_URL, json_file_path=JSON_FILE):\n    response = requests.get(csv_url)\n    response.raise_for_status()  # Raise an error for bad responses\n\n    csv_data = response.text\n    csv_reader = csv.DictReader(StringIO(csv_data))\n\n    data = [row for row in csv_reader]\n\n    with open(json_file_path, 'w') as json_file:\n        json.dump(data, json_file, indent=4)\n\n    return json_file_path", "raw_solution": "import json\nimport csv\nimport requests\nfrom io import StringIO\n\n# Constants\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\n\ndef task_func(csv_url=CSV_URL, json_file_path=JSON_FILE):\n    response = requests.get(csv_url)\n    response.raise_for_status()  # Raise an error for bad responses\n\n    csv_data = response.text\n    csv_reader = csv.DictReader(StringIO(csv_data))\n\n    data = [row for row in csv_reader]\n\n    with open(json_file_path, 'w') as json_file:\n        json.dump(data, json_file, indent=4)\n\n    return json_file_path\n\n# Example usage:\n# print(task_func(\"https://example.com/sample.csv\", \"sample.json\"))\n```\n"}
{"task_id": "BigCodeBench/1119", "solution": "import codecs\nimport random\nimport string\nimport hashlib\ndef task_func(password_length=10, salt=\"salty\"):\n    # Generate a random password of the specified length\n    password_chars = string.ascii_letters + string.digits + string.punctuation\n    password = ''.join(random.choice(password_chars) for _ in range(password_length))\n\n    # Combine the password with the salt\n    combined = salt + password\n\n    # Encode the combined string using UTF-8\n    encoded_combined = combined.encode('utf-8')\n\n    # Create a SHA256 hash of the encoded string\n    hash_object = hashlib.sha256(encoded_combined)\n    hashed_password = hash_object.hexdigest()\n\n    return hashed_password", "raw_solution": "import codecs\nimport random\nimport string\nimport hashlib\n\ndef task_func(password_length=10, salt=\"salty\"):\n    # Generate a random password of the specified length\n    password_chars = string.ascii_letters + string.digits + string.punctuation\n    password = ''.join(random.choice(password_chars) for _ in range(password_length))\n\n    # Combine the password with the salt\n    combined = salt + password\n\n    # Encode the combined string using UTF-8\n    encoded_combined = combined.encode('utf-8')\n\n    # Create a SHA256 hash of the encoded string\n    hash_object = hashlib.sha256(encoded_combined)\n    hashed_password = hash_object.hexdigest()\n\n    return hashed_password\n\n# Example usage:\n# random.seed(0)\n# hashed_password = task_func(12, \"my_salt\")\n# print(hashed_password)\n```\n"}
{"task_id": "BigCodeBench/1120", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    result = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f'http://ip-api.com/json/{domain}?key={API_KEY}')\n        if response.status_code == 200:\n            data = response.json()\n            if data['status'] == 'success':\n                result[domain] = {\n                    'status': data['status'],\n                    'country': data['country'],\n                    'countryCode': data['countryCode'],\n                    'region': data['region'],\n                    'regionName': data['regionName'],\n                    'city': data['city'],\n                    'zip': data['zip'],\n                    'lat': data['lat'],\n                    'lon': data['lon'],\n                    'timezone': data['timezone'],\n                    'isp': data['isp'],\n                    'org': data['org'],\n                    'as': data['as'],\n                    'query': data['query']\n                }\n            else:\n                result[domain] = None\n        else:\n            result[domain] = None\n\n    return result", "raw_solution": "import re\nimport urllib.parse\nimport requests\nimport json\n\ndef task_func(myString, API_KEY):\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    result = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f'http://ip-api.com/json/{domain}?key={API_KEY}')\n        if response.status_code == 200:\n            data = response.json()\n            if data['status'] == 'success':\n                result[domain] = {\n                    'status': data['status'],\n                    'country': data['country'],\n                    'countryCode': data['countryCode'],\n                    'region': data['region'],\n                    'regionName': data['regionName'],\n                    'city': data['city'],\n                    'zip': data['zip'],\n                    'lat': data['lat'],\n                    'lon': data['lon'],\n                    'timezone': data['timezone'],\n                    'isp': data['isp'],\n                    'org': data['org'],\n                    'as': data['as'],\n                    'query': data['query']\n                }\n            else:\n                result[domain] = None\n        else:\n            result[domain] = None\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/1121", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    result = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f'http://ip-api.com/json/{domain}?key={API_KEY}')\n        if response.status_code == 200:\n            data = response.json()\n            if data['status'] == 'success':\n                result[domain] = {\n                    'status': data['status'],\n                    'country': data['country'],\n                    'countryCode': data['countryCode'],\n                    'region': data['region'],\n                    'regionName': data['regionName'],\n                    'city': data['city'],\n                    'zip': data['zip'],\n                    'lat': data['lat'],\n                    'lon': data['lon'],\n                    'timezone': data['timezone'],\n                    'isp': data['isp'],\n                    'org': data['org'],\n                    'as': data['as'],\n                    'query': data['query']\n                }\n            else:\n                result[domain] = None\n        else:\n            result[domain] = None\n\n    return result", "raw_solution": "import re\nimport urllib.parse\nimport requests\nimport json\n\ndef task_func(myString, API_KEY):\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    result = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f'http://ip-api.com/json/{domain}?key={API_KEY}')\n        if response.status_code == 200:\n            data = response.json()\n            if data['status'] == 'success':\n                result[domain] = {\n                    'status': data['status'],\n                    'country': data['country'],\n                    'countryCode': data['countryCode'],\n                    'region': data['region'],\n                    'regionName': data['regionName'],\n                    'city': data['city'],\n                    'zip': data['zip'],\n                    'lat': data['lat'],\n                    'lon': data['lon'],\n                    'timezone': data['timezone'],\n                    'isp': data['isp'],\n                    'org': data['org'],\n                    'as': data['as'],\n                    'query': data['query']\n                }\n            else:\n                result[domain] = None\n        else:\n            result[domain] = None\n\n    return result\n```\n"}
{"task_id": "BigCodeBench/1122", "solution": "import re\nimport socket\nimport urllib.parse\ndef task_func(myString):\n    # Regular expression to find all URLs in the string\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    urls = url_pattern.findall(myString)\n\n    # Dictionary to store domain and corresponding IP\n    domain_ip_dict = {}\n\n    for url in urls:\n        # Extract domain from URL\n        domain = urllib.parse.urlparse(url).netloc\n        try:\n            # Get IP address of the domain\n            ip_address = socket.gethostbyname(domain)\n            domain_ip_dict[domain] = ip_address\n        except socket.gaierror:\n            # If domain cannot be resolved, set IP to None\n            domain_ip_dict[domain] = None\n\n    return domain_ip_dict", "raw_solution": "import re\nimport socket\nimport urllib.parse\n\ndef task_func(myString):\n    # Regular expression to find all URLs in the string\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    urls = url_pattern.findall(myString)\n\n    # Dictionary to store domain and corresponding IP\n    domain_ip_dict = {}\n\n    for url in urls:\n        # Extract domain from URL\n        domain = urllib.parse.urlparse(url).netloc\n        try:\n            # Get IP address of the domain\n            ip_address = socket.gethostbyname(domain)\n            domain_ip_dict[domain] = ip_address\n        except socket.gaierror:\n            # If domain cannot be resolved, set IP to None\n            domain_ip_dict[domain] = None\n\n    return domain_ip_dict\n\n# Example usage"}
{"task_id": "BigCodeBench/1123", "solution": "import re\nimport urllib.parse\nimport ssl\nimport socket\ndef task_func(myString):\n    # Extract all URLs from the input string\n    urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n\n    # Dictionary to store domain and SSL certificate expiry dates\n    domain_ssl_info = {}\n\n    for url in urls:\n        try:\n            # Parse the domain from the URL\n            domain = urllib.parse.urlparse(url).netloc\n\n            # Create a context to ignore SSL errors\n            context = ssl.create_default_context()\n\n            # Connect to the domain to get the SSL certificate\n            with socket.create_connection((domain, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=domain) as ssock:\n                    # Get the SSL certificate\n                    cert = ssock.getpeercert()\n\n                    # Extract the expiry date from the certificate\n                    expiry_date = cert['notAfter']\n\n                    # Store the domain and expiry date in the dictionary\n                    domain_ssl_info[domain] = expiry_date\n\n        except Exception as e:\n            # If SSL errors occur, skip the domain\n            print(f\"SSL error for {domain}: {e}\")\n\n    return domain_ssl_info", "raw_solution": "import re\nimport urllib.parse\nimport ssl\nimport socket\n\ndef task_func(myString):\n    # Extract all URLs from the input string\n    urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n\n    # Dictionary to store domain and SSL certificate expiry dates\n    domain_ssl_info = {}\n\n    for url in urls:\n        try:\n            # Parse the domain from the URL\n            domain = urllib.parse.urlparse(url).netloc\n\n            # Create a context to ignore SSL errors\n            context = ssl.create_default_context()\n\n            # Connect to the domain to get the SSL certificate\n            with socket.create_connection((domain, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=domain) as ssock:\n                    # Get the SSL certificate\n                    cert = ssock.getpeercert()\n\n                    # Extract the expiry date from the certificate\n                    expiry_date = cert['notAfter']\n\n                    # Store the domain and expiry date in the dictionary\n                    domain_ssl_info[domain] = expiry_date\n\n        except Exception as e:\n            # If SSL errors occur, skip the domain\n            print(f\"SSL error for {domain}: {e}\")\n\n    return domain_ssl_info\n```\n"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_match = re.search(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    \n    if not url_match:\n        return \"No valid URL found in the provided string.\"\n    \n    url = url_match.group(0)\n    \n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        return f\"URL ka content laane mein unable: {url}\"\n    \n    soup = BeautifulSoup(response.text, 'html.parser')\n    title_tag = soup.find('title')\n    \n    if not title_tag:\n        return \"webpage mein koi title tag nahi mila.\"\n    \n    return title_tag.text", "raw_solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_match = re.search(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    \n    if not url_match:\n        return \"No valid URL found in the provided string.\"\n    \n    url = url_match.group(0)\n    \n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        return f\"URL ka content laane mein unable: {url}\"\n    \n    soup = BeautifulSoup(response.text, 'html.parser')\n    title_tag = soup.find('title')\n    \n    if not title_tag:\n        return \"webpage mein koi title tag nahi mila.\"\n    \n    return title_tag.text\n```\n"}
{"task_id": "BigCodeBench/1125", "solution": "import re\nimport json\nimport requests\ndef task_func(myString, token):\n    # Extracting URL from the string\n    url_match = re.search(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    url = url_match.group(0) if url_match else None\n\n    if not url:\n        return {'message': 'No URL found in the string'}\n\n    # Creating JSON payload\n    payload = json.dumps({'url': url})\n\n    # Headers with Authorization token\n    headers = {\n        'Authorization': f'Bearer {token}',\n        'Content-Type': 'application/json'\n    }\n\n    # Sending POST request to the API\n    response = requests.post('https://api.example.com/endpoint', headers=headers, data=payload)\n\n    # Returning the response from the API\n    return response.json()", "raw_solution": "import re\nimport json\nimport requests\n\ndef task_func(myString, token):\n    # Extracting URL from the string\n    url_match = re.search(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', myString)\n    url = url_match.group(0) if url_match else None\n\n    if not url:\n        return {'message': 'No URL found in the string'}\n\n    # Creating JSON payload\n    payload = json.dumps({'url': url})\n\n    # Headers with Authorization token\n    headers = {\n        'Authorization': f'Bearer {token}',\n        'Content-Type': 'application/json'\n    }\n\n    # Sending POST request to the API\n    response = requests.post('https://api.example.com/endpoint', headers=headers, data=payload)\n\n    # Returning the response from the API\n    return response.json()\n```\nThis script uses regular expressions to extract the URL from the input string, constructs a JSON payload, and sends a POST request to the specified API endpoint with the necessary headers. The response from the API is then returned as a JSON object.\n"}
{"task_id": "BigCodeBench/1126", "solution": "import re\nimport hashlib\ndef task_func(input_str):\n    # Remove special characters, viram marks, and spaces using regular expressions\n    cleaned_str = re.sub(r'[^a-zA-Z0-9]', '', input_str)\n\n    # Generate SHA256 hash of the cleaned string\n    hash_object = hashlib.sha256(cleaned_str.encode())\n    hash_str = hash_object.hexdigest()\n\n    return hash_str", "raw_solution": "import re\nimport hashlib\n\ndef task_func(input_str):\n    # Remove special characters, viram marks, and spaces using regular expressions\n    cleaned_str = re.sub(r'[^a-zA-Z0-9]', '', input_str)\n\n    # Generate SHA256 hash of the cleaned string\n    hash_object = hashlib.sha256(cleaned_str.encode())\n    hash_str = hash_object.hexdigest()\n\n    return hash_str\n\n# Test cases"}
{"task_id": "BigCodeBench/1127", "solution": "import os\nimport re\nimport hashlib\ndef task_func(path, delimiter):\n    # Split the path using the provided delimiter\n    parts = re.split(f'[{re.escape(delimiter)}]', path)\n    \n    # Initialize the result list\n    result = []\n    \n    for part in parts:\n        if part:  # Ignore empty parts\n            # Check if the part is a file\n            if os.path.isfile(os.path.join(*parts)):\n                # Calculate the hash of the file\n                with open(os.path.join(*parts), 'rb') as f:\n                    file_hash = hashlib.sha256(f.read()).hexdigest()\n                result.append((os.path.join(*parts), file_hash))\n            else:\n                result.append((os.path.join(*parts), None))\n    \n    return result", "raw_solution": "import os\nimport re\nimport hashlib\n\ndef task_func(path, delimiter):\n    # Split the path using the provided delimiter\n    parts = re.split(f'[{re.escape(delimiter)}]', path)\n    \n    # Initialize the result list\n    result = []\n    \n    for part in parts:\n        if part:  # Ignore empty parts\n            # Check if the part is a file\n            if os.path.isfile(os.path.join(*parts)):\n                # Calculate the hash of the file\n                with open(os.path.join(*parts), 'rb') as f:\n                    file_hash = hashlib.sha256(f.read()).hexdigest()\n                result.append((os.path.join(*parts), file_hash))\n            else:\n                result.append((os.path.join(*parts), None))\n    \n    return result\n\n# Example usage:\n# print(task_func(\"Docs/src/file.txt\", \"/\"))\n```\nThis script uses the `os` module to check if a part of the path is a file, the `re` module to split the path using the provided delimiter, and the `hashlib` module to calculate the SHA-256 hash of a file. The function returns a list of tuples, where each tuple contains a path part and its hash (if it's a file). If the path part is not a file, the hash is `None`.\n"}
{"task_id": "BigCodeBench/1128", "solution": "import json\nimport os\nimport hashlib\nimport base64\nimport time\ndef task_func(file_path, unknown_key):\n    # Read the JSON file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Extract the nested data using the unknown_key\n    nested_data = data.get('A', {}).get(unknown_key, {}).get('maindata', [{}])[0].get('Info', '')\n\n    # Generate SHA256 hash of the nested data\n    hash_object = hashlib.sha256(nested_data.encode())\n    hash_digest = hash_object.digest()\n\n    # Encode the hash using base64\n    base64_encoded_hash = base64.b64encode(hash_digest).decode()\n\n    # Create a new file name with a timestamp\n    timestamp = str(int(time.time()))\n    new_file_path = os.path.join(os.getcwd(), f\"hashed_data_{timestamp}.txt\")\n\n    # Write the base64 encoded hash to the new file\n    with open(new_file_path, 'w') as new_file:\n        new_file.write(base64_encoded_hash)\n\n    return new_file_path", "raw_solution": "import json\nimport os\nimport hashlib\nimport base64\nimport time\n\ndef task_func(file_path, unknown_key):\n    # Read the JSON file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Extract the nested data using the unknown_key\n    nested_data = data.get('A', {}).get(unknown_key, {}).get('maindata', [{}])[0].get('Info', '')\n\n    # Generate SHA256 hash of the nested data\n    hash_object = hashlib.sha256(nested_data.encode())\n    hash_digest = hash_object.digest()\n\n    # Encode the hash using base64\n    base64_encoded_hash = base64.b64encode(hash_digest).decode()\n\n    # Create a new file name with a timestamp\n    timestamp = str(int(time.time()))\n    new_file_path = os.path.join(os.getcwd(), f\"hashed_data_{timestamp}.txt\")\n\n    # Write the base64 encoded hash to the new file\n    with open(new_file_path, 'w') as new_file:\n        new_file.write(base64_encoded_hash)\n\n    return new_file_path\n```\n"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the URL from the specified key\n    url = data.get(unknown_key)\n    \n    # Check if the key exists in the JSON data\n    if not url:\n        raise ValueError(f\"The key '{unknown_key}' does not exist in the JSON data.\")\n    \n    # Make a request to download the file\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        raise Exception(f\"Failed to download file. Status code: {response.status_code}\")\n    \n    # Create a timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    \n    # Create a filename based on the unknown key and timestamp\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    save_dir = save_dir or os.getcwd()\n    \n    # Ensure the directory exists\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define the full path to save the file\n    file_path = os.path.join(save_dir, filename)\n    \n    # Save the file\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)", "raw_solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the URL from the specified key\n    url = data.get(unknown_key)\n    \n    # Check if the key exists in the JSON data\n    if not url:\n        raise ValueError(f\"The key '{unknown_key}' does not exist in the JSON data.\")\n    \n    # Make a request to download the file\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        raise Exception(f\"Failed to download file. Status code: {response.status_code}\")\n    \n    # Create a timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    \n    # Create a filename based on the unknown key and timestamp\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    save_dir = save_dir or os.getcwd()\n    \n    # Ensure the directory exists\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define the full path to save the file\n    file_path = os.path.join(save_dir, filename)\n    \n    # Save the file\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)\n```\n"}
{"task_id": "BigCodeBench/1130", "solution": "import os\nimport hashlib\nimport json\nfrom pathlib import Path\ndef task_func(directory: str) -> str:\n    \"\"\"\n    specified directory mein, up-directoryon sahit, sabhi files ke liye SHA256 hash banaen,\n    aur in hashes ko specified directory mein 'hashes.json' naamak ek JSON file mein save karen.\n\n    Params:\n    - directory (str): files wali directory ka path jise hash kiya jana hai.\n\n    Returns:\n        str: hashes wali JSON file ('hashes.json') ka poorn path.\n\n    Reqs:\n    - os\n    - hashlib\n    - json\n    - pathlib.Path\n\n    Examples:\n    - >>> json_file = task_func(\"/path/to/directory\")\n    - >>> print(f\"Hashes saved at: {json_file}\")\n\n    \"\"\"\n    # Create a Path object for the directory\n    dir_path = Path(directory)\n    \n    # Dictionary to store file hashes\n    file_hashes = {}\n    \n    # Iterate over all files in the directory and its subdirectories\n    for file_path in dir_path.rglob('*'):\n        if file_path.is_file():\n            # Calculate the SHA256 hash of the file\n            with open(file_path, 'rb') as file:\n                file_hash = hashlib.sha256(file.read()).hexdigest()\n                # Store the hash in the dictionary with the file name as the key\n                file_hashes[file_path.name] = file_hash\n    \n    # Save the hashes to a JSON file in the same directory\n    json_file_path = dir_path / 'hashes.json'\n    with open(json_file_path, 'w') as json_file:\n        json.dump(file_hashes, json_file, indent=4)\n    \n    return str(json_file_path)", "raw_solution": "import os\nimport hashlib\nimport json\nfrom pathlib import Path\n\ndef task_func(directory: str) -> str:\n    \"\"\"\n    specified directory mein, up-directoryon sahit, sabhi files ke liye SHA256 hash banaen,\n    aur in hashes ko specified directory mein 'hashes.json' naamak ek JSON file mein save karen.\n\n    Params:\n    - directory (str): files wali directory ka path jise hash kiya jana hai.\n\n    Returns:\n        str: hashes wali JSON file ('hashes.json') ka poorn path.\n\n    Reqs:\n    - os\n    - hashlib\n    - json\n    - pathlib.Path\n\n    Examples:\n    - >>> json_file = task_func(\"/path/to/directory\")\n    - >>> print(f\"Hashes saved at: {json_file}\")\n\n    \"\"\"\n    # Create a Path object for the directory\n    dir_path = Path(directory)\n    \n    # Dictionary to store file hashes\n    file_hashes = {}\n    \n    # Iterate over all files in the directory and its subdirectories\n    for file_path in dir_path.rglob('*'):\n        if file_path.is_file():\n            # Calculate the SHA256 hash of the file\n            with open(file_path, 'rb') as file:\n                file_hash = hashlib.sha256(file.read()).hexdigest()\n                # Store the hash in the dictionary with the file name as the key\n                file_hashes[file_path.name] = file_hash\n    \n    # Save the hashes to a JSON file in the same directory\n    json_file_path = dir_path / 'hashes.json'\n    with open(json_file_path, 'w') as json_file:\n        json.dump(file_hashes, json_file, indent=4)\n    \n    return str(json_file_path)\n```\n"}
{"task_id": "BigCodeBench/1131", "solution": "import hashlib\nimport binascii\nimport sqlite3\ndef task_func(salt, cursor):\n    if not isinstance(salt, str):\n        raise TypeError(\"Salt must be a string\")\n\n    # Check if the salt is valid\n    if not salt:\n        raise ValueError(\"Salt cannot be empty\")\n\n    # Prepare the SQL command\n    sql_command = \"\"\"\n    UPDATE user\n    SET password = ?\n    WHERE password = ?\n    \"\"\"\n\n    # Count of updated passwords\n    updated_count = 0\n\n    # Fetch all rows from the user table\n    cursor.execute(\"SELECT * FROM user\")\n    rows = cursor.fetchall()\n\n    # Iterate over each row and update the password\n    for row in rows:\n        user_id, old_password = row\n\n        # Create a new salted password\n        salted_password = salt + old_password\n        hashed_password = hashlib.sha256(salted_password.encode()).hexdigest()\n\n        # Update the password in the database\n        cursor.execute(sql_command, (hashed_password, old_password))\n        updated_count += cursor.rowcount\n\n    # Commit the changes\n    cursor.connection.commit()\n\n    return updated_count", "raw_solution": "import hashlib\nimport binascii\nimport sqlite3\n\ndef task_func(salt, cursor):\n    if not isinstance(salt, str):\n        raise TypeError(\"Salt must be a string\")\n\n    # Check if the salt is valid\n    if not salt:\n        raise ValueError(\"Salt cannot be empty\")\n\n    # Prepare the SQL command\n    sql_command = \"\"\"\n    UPDATE user\n    SET password = ?\n    WHERE password = ?\n    \"\"\"\n\n    # Count of updated passwords\n    updated_count = 0\n\n    # Fetch all rows from the user table\n    cursor.execute(\"SELECT * FROM user\")\n    rows = cursor.fetchall()\n\n    # Iterate over each row and update the password\n    for row in rows:\n        user_id, old_password = row\n\n        # Create a new salted password\n        salted_password = salt + old_password\n        hashed_password = hashlib.sha256(salted_password.encode()).hexdigest()\n\n        # Update the password in the database\n        cursor.execute(sql_command, (hashed_password, old_password))\n        updated_count += cursor.rowcount\n\n    # Commit the changes\n    cursor.connection.commit()\n\n    return updated_count\n\n# Example usage:\n# conn = sqlite3.connect('sample.db')\n# cursor = conn.cursor()\n# num_updated = task_func('mysalt', cursor)\n# print(num_updated)\n```\n"}
{"task_id": "BigCodeBench/1132", "solution": "import os\nimport hashlib\nimport base64\ndef task_func(password, PREFIX=\"ME\", SALT_LENGTH=16):\n    if SALT_LENGTH < 0:\n        raise ValueError(\"Salt length cannot be negative\")\n\n    # Generate random salt\n    salt = os.urandom(SALT_LENGTH)\n\n    # Combine prefix, password, and salt\n    combined = PREFIX.encode() + password.encode() + salt\n\n    # Hash the combined string using SHA256\n    hashed = hashlib.sha256(combined).digest()\n\n    # Encode the hashed result in base64\n    base64_encoded = base64.b64encode(hashed).decode()\n\n    return base64_encoded", "raw_solution": "import os\nimport hashlib\nimport base64\n\ndef task_func(password, PREFIX=\"ME\", SALT_LENGTH=16):\n    if SALT_LENGTH < 0:\n        raise ValueError(\"Salt length cannot be negative\")\n\n    # Generate random salt\n    salt = os.urandom(SALT_LENGTH)\n\n    # Combine prefix, password, and salt\n    combined = PREFIX.encode() + password.encode() + salt\n\n    # Hash the combined string using SHA256\n    hashed = hashlib.sha256(combined).digest()\n\n    # Encode the hashed result in base64\n    base64_encoded = base64.b64encode(hashed).decode()\n\n    return base64_encoded\n\n# Test cases"}
{"task_id": "BigCodeBench/1133", "solution": "import json\nimport requests\ndef task_func(API_URL, endpoint, PREFIX):\n    try:\n        response = requests.get(f\"{API_URL}/{endpoint}\")\n        response.raise_for_status()\n        data = response.json()\n\n        filename = f\"{PREFIX}{endpoint}.json\"\n        with open(filename, 'w') as file:\n            json.dump(data, file, indent=4)\n\n        return filename\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {e}\")", "raw_solution": "import json\nimport requests\n\ndef task_func(API_URL, endpoint, PREFIX):\n    try:\n        response = requests.get(f\"{API_URL}/{endpoint}\")\n        response.raise_for_status()\n        data = response.json()\n\n        filename = f\"{PREFIX}{endpoint}.json\"\n        with open(filename, 'w') as file:\n            json.dump(data, file, indent=4)\n\n        return filename\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {e}\")\n\n# Example usage:\n# filename = task_func('https://api.github.com/', 'users', 'ME')\n# print(filename)\n```\n"}
{"task_id": "BigCodeBench/1134", "solution": "import os\nimport glob\nimport hashlib\ndef task_func(source_dir, target_dir, prefix='#Hash: '):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    files = glob.glob(os.path.join(source_dir, '*'))\n    hashed_files = []\n\n    for file_path in files:\n        with open(file_path, 'rb') as file:\n            content = file.read()\n            file_hash = hashlib.md5(content).hexdigest()\n            new_file_name = f\"{prefix}{file_hash}.txt\"\n            new_file_path = os.path.join(target_dir, new_file_name)\n            with open(new_file_path, 'wb') as new_file:\n                new_file.write(content)\n            hashed_files.append(new_file_path)\n\n    return hashed_files", "raw_solution": "import os\nimport glob\nimport hashlib\n\ndef task_func(source_dir, target_dir, prefix='#Hash: '):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    files = glob.glob(os.path.join(source_dir, '*'))\n    hashed_files = []\n\n    for file_path in files:\n        with open(file_path, 'rb') as file:\n            content = file.read()\n            file_hash = hashlib.md5(content).hexdigest()\n            new_file_name = f\"{prefix}{file_hash}.txt\"\n            new_file_path = os.path.join(target_dir, new_file_name)\n            with open(new_file_path, 'wb') as new_file:\n                new_file.write(content)\n            hashed_files.append(new_file_path)\n\n    return hashed_files\n```\n"}
{"task_id": "BigCodeBench/1135", "solution": "import collections\nimport json\nimport requests\ndef task_func(user, API_URL = 'https://api.github.com/users/'):\n    response = requests.get(API_URL + user + '/repos')\n    response.raise_for_status()  # Raise an error for bad responses\n\n    repos = response.json()\n    repo_names = [(repo['name'], repo['created_at']) for repo in repos]\n\n    # Sorting the repositories by creation date\n    sorted_repos = sorted(repo_names, key=lambda x: x[1])\n\n    # Extracting only the names of the repositories\n    sorted_repo_names = [repo[0] for repo in sorted_repos]\n\n    return sorted_repo_names", "raw_solution": "import collections\nimport json\nimport requests\n\ndef task_func(user, API_URL = 'https://api.github.com/users/'):\n    response = requests.get(API_URL + user + '/repos')\n    response.raise_for_status()  # Raise an error for bad responses\n\n    repos = response.json()\n    repo_names = [(repo['name'], repo['created_at']) for repo in repos]\n\n    # Sorting the repositories by creation date\n    sorted_repos = sorted(repo_names, key=lambda x: x[1])\n\n    # Extracting only the names of the repositories\n    sorted_repo_names = [repo[0] for repo in sorted_repos]\n\n    return sorted_repo_names\n\n# Example usage:\n# print(task_func('octocat'))\n```\nThis script uses the GitHub API to fetch a user's repositories and sorts them by creation date in ascending order. The function returns a list of repository names in this order. Note that the actual output will depend on the repositories of the specified user.\n"}
{"task_id": "BigCodeBench/1136", "solution": "import bs4\nimport requests\nimport re\nimport csv\ndef task_func(url=\"http://example.com\", csv_path=\"emails.csv\", \n          regex=r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \n          headers={'User-Agent': 'Mozilla/5.0'}):\n    response = requests.get(url, headers=headers)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    emails = re.findall(regex, soup.get_text())\n\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['email'])\n        for email in set(emails):  # remove duplicates\n            writer.writerow([email])\n\n    return csv_path", "raw_solution": "import bs4\nimport requests\nimport re\nimport csv\n\ndef task_func(url=\"http://example.com\", csv_path=\"emails.csv\", \n          regex=r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \n          headers={'User-Agent': 'Mozilla/5.0'}):\n    response = requests.get(url, headers=headers)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    emails = re.findall(regex, soup.get_text())\n\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['email'])\n        for email in set(emails):  # remove duplicates\n            writer.writerow([email])\n\n    return csv_path\n```\n"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    phone_numbers = []\n    if url.startswith('file://'):\n        with open(url[7:], 'r') as file:\n            content = file.read()\n    else:\n        response = requests.get(url)\n        content = response.text\n\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    text = soup.get_text()\n\n    pattern = re.compile(r'\\+\\d{1,3}[-\\s]?\\(\\d{1,3}\\)\\s?\\d{1,4}[-\\s]?\\d{1,4}[-\\s]?\\d{1,9}')\n    phone_numbers = pattern.findall(text)\n\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    return phone_numbers", "raw_solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    phone_numbers = []\n    if url.startswith('file://'):\n        with open(url[7:], 'r') as file:\n            content = file.read()\n    else:\n        response = requests.get(url)\n        content = response.text\n\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    text = soup.get_text()\n\n    pattern = re.compile(r'\\+\\d{1,3}[-\\s]?\\(\\d{1,3}\\)\\s?\\d{1,4}[-\\s]?\\d{1,4}[-\\s]?\\d{1,9}')\n    phone_numbers = pattern.findall(text)\n\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    return phone_numbers\n```\n"}
{"task_id": "BigCodeBench/1138", "solution": "import numpy as np\nimport itertools\ndef task_func(matrix):\n    # Flatten the matrix into a 1D array\n    flat_array = matrix.flatten()\n\n    # Generate all unique combinations of the elements in the array\n    combinations = list(itertools.combinations(flat_array, 2))\n\n    return flat_array, combinations", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(matrix):\n    # Flatten the matrix into a 1D array\n    flat_array = matrix.flatten()\n\n    # Generate all unique combinations of the elements in the array\n    combinations = list(itertools.combinations(flat_array, 2))\n\n    return flat_array, combinations\n\n# Example usage:\n# matrix = np.array([[1, 3], [2, 4]])\n# print(task_func(matrix))\n# Output: (array([1, 2, 3, 4]), [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)])\n```\n"}
{"task_id": "BigCodeBench/1139", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data):\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df['Hours'].values.reshape(-1, 1), df['Scores'], test_size=0.2, random_state=42)\n\n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the scores for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean squared error\n    mse = np.mean((y_test - y_pred) ** 2)\n\n    return mse", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data):\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df['Hours'].values.reshape(-1, 1), df['Scores'], test_size=0.2, random_state=42)\n\n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the scores for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean squared error\n    mse = np.mean((y_test - y_pred) ** 2)\n\n    return mse\n\n# Example usage"}
