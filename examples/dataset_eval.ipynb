{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset # Assuming datasets are loadable this way\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from google import genai\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer model for semantic similarity\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize Gemini Translator\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "def get_prompt(processed_cand_cm_romanized):\n",
    "    pmt = f\"\"\"I will give you a text in roman-hindi and english below:-\n",
    "------------------------\n",
    "{processed_cand_cm_romanized}\n",
    "------------------------\n",
    "you have to follow these steps below to translate the text into english.\n",
    "1. First translate the sentence exactly into non roman hindi representation of characters.\n",
    "2. Now translate the sentence into its english translation and remember it as SE.\n",
    "3. Now for the sentences that are english words and are common in both the SE in step 2 and the original text, POS tag them.\n",
    "4. Now remembering the POS tags of words, translate the original sentence into hindi remembering the meaning of POS words and cross-language homonyms.\n",
    "5. Now translate the final hindi sentence in step 4 to its english translation.\n",
    "\n",
    "Only give me the translated sentence in step 5 as your response in this template below so that i can extract the sentence using regular expression\n",
    "*****Final Translated Sentence*****\n",
    "[put the final sentence from step 5.]\n",
    "*****End*****\n",
    "\"\"\"\n",
    "    return pmt\n",
    "\n",
    "model_name = \"gemini-2.0-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Lowercase, remove punctuation, and remove code blocks.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # 1. Remove code blocks first\n",
    "    code_block_pattern = r'```.*?```'\n",
    "    text_without_code = re.sub(code_block_pattern, '', text, flags=re.DOTALL)\n",
    "    # 2. Lowercase the remaining text\n",
    "    text_lower = text_without_code.lower()\n",
    "    # 3. Remove punctuation\n",
    "    translator_punct = str.maketrans('', '', string.punctuation)\n",
    "    text_no_punct = text_lower.translate(translator_punct)\n",
    "    # 4. Remove extra whitespace\n",
    "    text_clean = re.sub(r'\\s+', ' ', text_no_punct).strip()\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def extract_translated_sentence(text):\n",
    "    pattern = r\"\\*{5}Final Translated Sentence\\*{5}\\s*(.*?)\\s*\\*{5}End\\*{5}\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_semantic_similarity(text1, text2):\n",
    "    \"\"\"Calculate semantic similarity using Sentence Transformers.\"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        # Normalize embeddings for cosine similarity calculation stability\n",
    "        embedding1 = similarity_model.encode(text1, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        embedding2 = similarity_model.encode(text2, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        # Compute cosine-similarity\n",
    "        cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "        # Clamp score between -1 and 1 just in case of float precision issues\n",
    "        similarity = max(-1.0, min(1.0, cosine_scores.item()))\n",
    "        return similarity\n",
    "    except Exception as e:\n",
    "        print(f\"Similarity Calculation Error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# --- GAME Implementation (Modified) ---\n",
    "\n",
    "class GAME:\n",
    "    def __init__(self, matrix_lang_code='hi'):\n",
    "        self.matrix_lang_code = matrix_lang_code\n",
    "        print(f\"GAME initialized with matrix language: {self.matrix_lang_code}\")\n",
    "\n",
    "    def evaluate(self, reference_en, candidate_cm_romanized):\n",
    "        if not isinstance(reference_en, str) or not isinstance(candidate_cm_romanized, str) or not reference_en or not candidate_cm_romanized:\n",
    "             return 0.0\n",
    "\n",
    "        # 1. Preprocess both sentences\n",
    "        processed_ref_en = preprocess(reference_en)\n",
    "        processed_cand_cm_romanized = preprocess(candidate_cm_romanized)\n",
    "        \n",
    "        if not processed_ref_en or not processed_cand_cm_romanized:\n",
    "             #print(\"Warning: Empty string after preprocessing.\")\n",
    "             return 0.0\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=model_name, contents=get_prompt(processed_cand_cm_romanized)\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            reconstructed_en_sentence = extract_translated_sentence(response.text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting translated sentence: {e}\")\n",
    "        \n",
    "        if not reconstructed_en_sentence:\n",
    "            # print(\"Warning: Final English reconstruction failed.\")\n",
    "             return 0.0 # Cannot compare if reconstruction failed\n",
    "        # 4. Calculate semantic similarity between original reference and final reconstruction\n",
    "        similarity_score = get_semantic_similarity(processed_ref_en, reconstructed_en_sentence)\n",
    "\n",
    "        # 5. Scale score to 0-100\n",
    "        # Max(0, ...) handles potential negative similarity scores (though usually between 0 and 1 after normalization)\n",
    "        game_score = max(0, similarity_score) * 100\n",
    "        return game_score\n",
    "\n",
    "# --- Main Execution Logic (Mostly unchanged, uses the modified GAME class) ---\n",
    "\n",
    "def compare_datasets(dataset_en_path, dataset_cm_path, matrix_lang='hi', split='train'):\n",
    "\n",
    "    print(f\"Loading English dataset from: {dataset_en_path}\")\n",
    "    # Allow specifying a configuration name for the English dataset if needed\n",
    "    ds_en = load_dataset(dataset_en_path, split=\"v0.1.4\")\n",
    "\n",
    "    print(f\"Loading Code-Mixed dataset from: {dataset_cm_path}\")\n",
    "    ds_cm = load_dataset(dataset_cm_path, split=split) # Assuming CM dataset doesn't need a specific config name\n",
    "\n",
    "    if len(ds_en) != len(ds_cm):\n",
    "        print(f\"Warning: Datasets have different lengths ({len(ds_en)} vs {len(ds_cm)}). Comparing row-by-row might be incorrect.\")\n",
    "        min_len = min(len(ds_en), len(ds_cm))\n",
    "        print(f\"Comparing the first {min_len} entries.\")\n",
    "        ds_en = ds_en.select(range(min_len))\n",
    "        ds_cm = ds_cm.select(range(min_len))\n",
    "\n",
    "    game_evaluator = GAME(matrix_lang_code=matrix_lang)\n",
    "    game_scores = []\n",
    "    failed_evaluations = 0\n",
    "\n",
    "    print(f\"Calculating GAME scores for {len(ds_cm)} entries...\")\n",
    "    # Use zip to iterate through corresponding rows\n",
    "    for entry_en, entry_cm in tqdm(zip(ds_en, ds_cm), total=len(ds_cm)):\n",
    "        # Assuming the key containing the prompt text is 'prompt'\n",
    "        # **** ADJUST THIS KEY IF YOUR DATASETS USE A DIFFERENT COLUMN NAME ****\n",
    "        reference_prompt = entry_en.get('instruct_prompt')\n",
    "        candidate_prompt = entry_cm.get('instruct_prompt')\n",
    "        \n",
    "        if reference_prompt and candidate_prompt:\n",
    "            try:\n",
    "                score = game_evaluator.evaluate(reference_prompt, candidate_prompt)\n",
    "                game_scores.append(score)\n",
    "            except Exception as eval_e:\n",
    "                 print(f\"\\nError during GAME evaluation for an entry: {eval_e}\")\n",
    "                 print(f\"Reference: {reference_prompt[:100]}...\")\n",
    "                 print(f\"Candidate: {candidate_prompt[:100]}...\")\n",
    "                 game_scores.append(0.0) # Assign 0 score on evaluation error\n",
    "                 failed_evaluations += 1\n",
    "        else:\n",
    "            # print(f\"Warning: Missing prompt key in an entry. Skipping.\")\n",
    "            game_scores.append(0.0) # Assign 0 score if data is missing\n",
    "            failed_evaluations += 1\n",
    "        break\n",
    "\n",
    "\n",
    "    valid_scores = [s for s in game_scores if s is not None] # Should already be floats\n",
    "    average_game_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n",
    "\n",
    "    print(\"\\n--- Evaluation Summary ---\")\n",
    "    print(f\"Matrix Language: {matrix_lang}\")\n",
    "    print(f\"Number of entries processed: {len(game_scores)}\")\n",
    "    print(f\"Number of failed/skipped evaluations: {failed_evaluations}\")\n",
    "    print(f\"Average GAME Score (over all processed): {average_game_score:.2f}\")\n",
    "\n",
    "    # Filter out potential None scores if any error handling changes\n",
    "    # Plot distribution of actual calculated scores\n",
    "    if valid_scores:\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(np.array(valid_scores), bins=20, range=(0, 100)) # Ensure scores are within 0-100 range for plot\n",
    "            plt.title(f\"Distribution of GAME Scores (Matrix Lang: {matrix_lang})\")\n",
    "            plt.xlabel(\"GAME Score (0-100)\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.grid(axis='y', alpha=0.75)\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"\\nInstall matplotlib to see the score distribution plot: pip install matplotlib numpy\")\n",
    "    else:\n",
    "        print(\"\\nNo valid scores generated to plot.\")\n",
    "\n",
    "\n",
    "    return game_scores, average_game_score, failed_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datasets import load_dataset, Dataset\n",
    "from sentence_transformers import util\n",
    "\n",
    "# --- Configuration for Checkpoint and Rate Limiting ---\n",
    "ONE_MINUTE = 60\n",
    "BATCH_SIZE = 15\n",
    "CHECKPOINT_FILE = \"game_scores_checkpoint.parquet\"\n",
    "ERROR_LOG_FILE = \"game_error_log.txt\"\n",
    "\n",
    "# Define similarity model, model_name, and client before using this code\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    code_block_pattern = r'```.*?```'\n",
    "    text = re.sub(code_block_pattern, '', text, flags=re.DOTALL)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def extract_translated_sentence(text):\n",
    "    pattern = r\"\\*{5}Final Translated Sentence\\*{5}\\s*(.*?)\\s*\\*{5}End\\*{5}\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "def get_semantic_similarity(text1, text2):\n",
    "    if not text1 or not text2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        emb1 = similarity_model.encode(text1, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        emb2 = similarity_model.encode(text2, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        sim = util.pytorch_cos_sim(emb1, emb2)\n",
    "        return max(-1.0, min(1.0, sim.item()))\n",
    "    except Exception as e:\n",
    "        print(f\"Similarity Calculation Error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "class GAME:\n",
    "    def __init__(self, matrix_lang_code='hi'):\n",
    "        self.matrix_lang_code = matrix_lang_code\n",
    "\n",
    "    def evaluate(self, reference_en, candidate_cm_romanized):\n",
    "        try:\n",
    "            processed_ref = preprocess(reference_en)\n",
    "            processed_cand = preprocess(candidate_cm_romanized)\n",
    "            if not processed_ref or not processed_cand:\n",
    "                return 0.0\n",
    "\n",
    "            response = client.models.generate_content(\n",
    "                model=model_name, contents=get_prompt(processed_cand)\n",
    "            )\n",
    "            reconstructed = extract_translated_sentence(response.text)\n",
    "            if not reconstructed:\n",
    "                return 0.0\n",
    "\n",
    "            sim_score = get_semantic_similarity(processed_ref, reconstructed)\n",
    "            return max(0.0, sim_score) * 100\n",
    "\n",
    "        except Exception as e:\n",
    "            return 0.0\n",
    "\n",
    "def log_error(entry, msg):\n",
    "    # with open(ERROR_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "    #     f.write(json.dumps({\"error\": msg, \"entry\": entry}, ensure_ascii=False) + \"\\n\")\n",
    "    print({\"error\": msg, \"entry\": entry})\n",
    "\n",
    "# --- Main Execution with Checkpoint and Rate Limiting ---\n",
    "def compare_datasets_with_checkpointing(dataset_en_path, dataset_cm_path, matrix_lang='hi', split='train'):\n",
    "    ds_en = load_dataset(dataset_en_path, split=\"v0.1.4\")\n",
    "    # ds_cm = load_dataset(dataset_cm_path, split=split)\n",
    "    ds_cm = pd.read_parquet(\"/kaggle/input/0-9cmd/MBigCodeBench-hini-end-cmd0.9.parquet\")\n",
    "    # if len(ds_en) != len(ds_cm):\n",
    "    #     min_len = min(len(ds_en), len(ds_cm))\n",
    "    #     ds_en = ds_en.select(range(min_len))\n",
    "    #     ds_cm = ds_cm.select(range(min_len))\n",
    "\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        checkpoint_df = pd.read_parquet(CHECKPOINT_FILE)\n",
    "        processed_indices = set(checkpoint_df['index'])\n",
    "    else:\n",
    "        checkpoint_df = pd.DataFrame(columns=['index', 'score'])\n",
    "        processed_indices = set()\n",
    "\n",
    "    all_scores = []\n",
    "    failed = 0\n",
    "    game = GAME(matrix_lang)\n",
    "\n",
    "    indices = [i for i in range(len(ds_cm)) if i not in processed_indices]\n",
    "    progress_bar = tqdm(total=len(indices), desc=\"Processing Entries\")\n",
    "    elapsed_times = []\n",
    "\n",
    "    for i in range(0, len(indices), BATCH_SIZE):\n",
    "        batch_indices = indices[i:i+BATCH_SIZE]\n",
    "        start_time = time.time()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:\n",
    "            futures = {\n",
    "                executor.submit(\n",
    "                    game.evaluate,\n",
    "                    ds_en[j]['instruct_prompt'],\n",
    "                    # ds_cm[j]['instruct_prompt']\n",
    "                    ds_cm.iloc[j]['instruct_prompt']\n",
    "                ): j for j in batch_indices\n",
    "            }\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                j = futures[future]\n",
    "                try:\n",
    "                    score = future.result()\n",
    "                    all_scores.append(score)\n",
    "                    checkpoint_df.loc[len(checkpoint_df)] = {'index': j, 'score': score}\n",
    "                    checkpoint_df.to_parquet(CHECKPOINT_FILE, index=False)\n",
    "                    progress_bar.update(1)\n",
    "                except Exception as e:\n",
    "                    log_error({'index': j}, f\"Evaluation Error: {e}\")\n",
    "                    failed += 1\n",
    "\n",
    "        # Enforce rate limiting\n",
    "        elapsed = time.time() - start_time\n",
    "        sleep_time = max(0, ONE_MINUTE - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(f\"Waiting {sleep_time:.2f} seconds before next batch...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    progress_bar.close()\n",
    "    avg_score = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "\n",
    "    print(\"\\n--- Evaluation Complete ---\")\n",
    "    print(f\"Matrix Language: {matrix_lang}\")\n",
    "    print(f\"Entries processed: {len(all_scores)}\")\n",
    "    print(f\"Failed evaluations: {failed}\")\n",
    "    print(f\"Average GAME Score: {avg_score:.2f}\")\n",
    "\n",
    "    return all_scores, avg_score, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration ---\n",
    "# Using the specified HF dataset names\n",
    "ENGLISH_DATASET_PATH = \"bigcode/bigcodebench\"\n",
    "CODE_MIXED_DATASET_PATH = \"cmd0.9\"\n",
    "MATRIX_LANGUAGE = \"hi\" # Hindi - change as needed ('bn', 'es', 'fr')\n",
    "DATASET_SPLIT = 'train' # Usually evaluate on the test split\n",
    "\n",
    "\n",
    "try:\n",
    "     game_scores, avg_score, failed_score = compare_datasets_with_checkpointing(\n",
    "         ENGLISH_DATASET_PATH,\n",
    "         CODE_MIXED_DATASET_PATH,\n",
    "         matrix_lang=MATRIX_LANGUAGE,\n",
    "         split=DATASET_SPLIT,\n",
    "     )\n",
    "     # print(\"\\nIndividual Scores:\", scores) # Uncomment to see all scores\n",
    "except FileNotFoundError:\n",
    "     print(\"\\nError: One or both datasets could not be loaded from Hugging Face Hub.\")\n",
    "     print(\"Please ensure the dataset identifiers and your internet connection are correct.\")\n",
    "except ValueError as ve:\n",
    "     print(f\"\\nValueError during dataset loading or processing: {ve}\")\n",
    "     print(\"This might indicate an issue with the dataset structure, split name, or configuration name.\")\n",
    "except Exception as e:\n",
    "     print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "     # Optionally add more detailed error logging here\n",
    "     import traceback\n",
    "     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "print(f\"Matrix Language: hi\")\n",
    "print(f\"Number of entries processed: {len(game_scores)}\")\n",
    "print(f\"Number of failed/skipped evaluations: {failed_score}\")\n",
    "print(f\"Average GAME Score (over all processed): {avg_score:.2f}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(np.array(game_scores), bins=20, range=(0, 100)) # Ensure scores are within 0-100 range for plot\n",
    "    plt.title(f\"Distribution of GAME Scores (Matrix Lang: hi)\")\n",
    "    plt.xlabel(\"GAME Score (0-100)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"\\nInstall matplotlib to see the score distribution plot: pip install matplotlib numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/kaggle/working/game_scores_checkpoint.parquet\")\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate values\n",
    "total_entries = len(df)\n",
    "failed_score = (df[\"score\"] == 0).sum()\n",
    "processed_scores = df[df[\"score\"] > 0][\"score\"]\n",
    "avg_score = processed_scores.mean()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "print(f\"Matrix Language: hi\")\n",
    "print(f\"Number of entries processed: {total_entries}\")\n",
    "print(f\"Number of failed/skipped evaluations: {failed_score}\")\n",
    "print(f\"Average GAME Score (over all processed): {avg_score:.2f}\")\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(processed_scores, bins=20, range=(0, 100))\n",
    "plt.title(f\"Distribution of GAME Scores (Matrix Lang: hi)\")\n",
    "plt.xlabel(\"GAME Score (0-100)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
